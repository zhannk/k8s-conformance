I0113 09:03:33.040267      20 e2e.go:126] Starting e2e run "5655d73b-a226-4012-b01d-7e72ee24020b" on Ginkgo node 1
Jan 13 09:03:33.102: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1673600612 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 13 09:03:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 09:03:33.479: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 13 09:03:33.552: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 13 09:03:33.762: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 13 09:03:33.762: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 13 09:03:33.762: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 13 09:03:33.783: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 13 09:03:33.783: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 13 09:03:33.783: INFO: e2e test version: v1.26.0
Jan 13 09:03:33.811: INFO: kube-apiserver version: v1.26.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 13 09:03:33.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 09:03:33.841: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.368 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 13 09:03:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 09:03:33.479: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 13 09:03:33.552: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 13 09:03:33.762: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 13 09:03:33.762: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jan 13 09:03:33.762: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 13 09:03:33.783: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 13 09:03:33.783: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 13 09:03:33.783: INFO: e2e test version: v1.26.0
    Jan 13 09:03:33.811: INFO: kube-apiserver version: v1.26.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 13 09:03:33.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 09:03:33.841: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:03:33.891
Jan 13 09:03:33.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:03:33.893
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:34.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:34.072
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 13 09:03:34.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:03:35.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7113" for this suite. 01/13/23 09:03:35.214
------------------------------
• [1.354 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:03:33.891
    Jan 13 09:03:33.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:03:33.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:34.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:34.072
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 13 09:03:34.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:03:35.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7113" for this suite. 01/13/23 09:03:35.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:03:35.249
Jan 13 09:03:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 09:03:35.252
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:35.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:35.315
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/13/23 09:03:35.323
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 09:03:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8502" for this suite. 01/13/23 09:03:35.383
------------------------------
• [0.165 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:03:35.249
    Jan 13 09:03:35.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 09:03:35.252
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:35.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:35.315
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/13/23 09:03:35.323
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:03:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8502" for this suite. 01/13/23 09:03:35.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:03:35.416
Jan 13 09:03:35.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 09:03:35.419
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:35.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:35.475
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-765 01/13/23 09:03:35.491
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[] 01/13/23 09:03:35.52
Jan 13 09:03:35.569: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-765 01/13/23 09:03:35.569
Jan 13 09:03:35.595: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-765" to be "running and ready"
Jan 13 09:03:35.602: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.946557ms
Jan 13 09:03:35.603: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:03:37.609: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013486678s
Jan 13 09:03:37.609: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:03:39.623: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027180243s
Jan 13 09:03:39.623: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 13 09:03:39.623: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[pod1:[100]] 01/13/23 09:03:39.647
Jan 13 09:03:39.733: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-765 01/13/23 09:03:39.733
Jan 13 09:03:39.773: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-765" to be "running and ready"
Jan 13 09:03:39.794: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.420431ms
Jan 13 09:03:39.794: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:03:41.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055094565s
Jan 13 09:03:41.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:03:43.819: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.046481118s
Jan 13 09:03:43.820: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 13 09:03:43.820: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[pod1:[100] pod2:[101]] 01/13/23 09:03:43.871
Jan 13 09:03:43.963: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/13/23 09:03:43.963
Jan 13 09:03:43.964: INFO: Creating new exec pod
Jan 13 09:03:43.997: INFO: Waiting up to 5m0s for pod "execpodjg6vs" in namespace "services-765" to be "running"
Jan 13 09:03:44.021: INFO: Pod "execpodjg6vs": Phase="Pending", Reason="", readiness=false. Elapsed: 19.559346ms
Jan 13 09:03:46.030: INFO: Pod "execpodjg6vs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028116389s
Jan 13 09:03:48.036: INFO: Pod "execpodjg6vs": Phase="Running", Reason="", readiness=true. Elapsed: 4.034445079s
Jan 13 09:03:48.036: INFO: Pod "execpodjg6vs" satisfied condition "running"
Jan 13 09:03:49.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 13 09:03:49.715: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 13 09:03:49.715: INFO: stdout: ""
Jan 13 09:03:49.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 10.98.132.56 80'
Jan 13 09:03:50.220: INFO: stderr: "+ nc -v -z -w 2 10.98.132.56 80\nConnection to 10.98.132.56 80 port [tcp/http] succeeded!\n"
Jan 13 09:03:50.220: INFO: stdout: ""
Jan 13 09:03:50.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 13 09:03:50.653: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 13 09:03:50.653: INFO: stdout: ""
Jan 13 09:03:50.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 10.98.132.56 81'
Jan 13 09:03:51.165: INFO: stderr: "+ nc -v -z -w 2 10.98.132.56 81\nConnection to 10.98.132.56 81 port [tcp/*] succeeded!\n"
Jan 13 09:03:51.165: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-765 01/13/23 09:03:51.165
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[pod2:[101]] 01/13/23 09:03:51.188
Jan 13 09:03:52.286: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-765 01/13/23 09:03:52.286
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[] 01/13/23 09:03:52.467
Jan 13 09:03:52.516: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 09:03:52.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-765" for this suite. 01/13/23 09:03:52.684
------------------------------
• [SLOW TEST] [17.328 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:03:35.416
    Jan 13 09:03:35.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 09:03:35.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:35.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:35.475
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-765 01/13/23 09:03:35.491
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[] 01/13/23 09:03:35.52
    Jan 13 09:03:35.569: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-765 01/13/23 09:03:35.569
    Jan 13 09:03:35.595: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-765" to be "running and ready"
    Jan 13 09:03:35.602: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.946557ms
    Jan 13 09:03:35.603: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:03:37.609: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013486678s
    Jan 13 09:03:37.609: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:03:39.623: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027180243s
    Jan 13 09:03:39.623: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 13 09:03:39.623: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[pod1:[100]] 01/13/23 09:03:39.647
    Jan 13 09:03:39.733: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-765 01/13/23 09:03:39.733
    Jan 13 09:03:39.773: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-765" to be "running and ready"
    Jan 13 09:03:39.794: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.420431ms
    Jan 13 09:03:39.794: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:03:41.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055094565s
    Jan 13 09:03:41.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:03:43.819: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.046481118s
    Jan 13 09:03:43.820: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 13 09:03:43.820: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[pod1:[100] pod2:[101]] 01/13/23 09:03:43.871
    Jan 13 09:03:43.963: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/13/23 09:03:43.963
    Jan 13 09:03:43.964: INFO: Creating new exec pod
    Jan 13 09:03:43.997: INFO: Waiting up to 5m0s for pod "execpodjg6vs" in namespace "services-765" to be "running"
    Jan 13 09:03:44.021: INFO: Pod "execpodjg6vs": Phase="Pending", Reason="", readiness=false. Elapsed: 19.559346ms
    Jan 13 09:03:46.030: INFO: Pod "execpodjg6vs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028116389s
    Jan 13 09:03:48.036: INFO: Pod "execpodjg6vs": Phase="Running", Reason="", readiness=true. Elapsed: 4.034445079s
    Jan 13 09:03:48.036: INFO: Pod "execpodjg6vs" satisfied condition "running"
    Jan 13 09:03:49.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 13 09:03:49.715: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 13 09:03:49.715: INFO: stdout: ""
    Jan 13 09:03:49.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 10.98.132.56 80'
    Jan 13 09:03:50.220: INFO: stderr: "+ nc -v -z -w 2 10.98.132.56 80\nConnection to 10.98.132.56 80 port [tcp/http] succeeded!\n"
    Jan 13 09:03:50.220: INFO: stdout: ""
    Jan 13 09:03:50.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 13 09:03:50.653: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 13 09:03:50.653: INFO: stdout: ""
    Jan 13 09:03:50.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-765 exec execpodjg6vs -- /bin/sh -x -c nc -v -z -w 2 10.98.132.56 81'
    Jan 13 09:03:51.165: INFO: stderr: "+ nc -v -z -w 2 10.98.132.56 81\nConnection to 10.98.132.56 81 port [tcp/*] succeeded!\n"
    Jan 13 09:03:51.165: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-765 01/13/23 09:03:51.165
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[pod2:[101]] 01/13/23 09:03:51.188
    Jan 13 09:03:52.286: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-765 01/13/23 09:03:52.286
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-765 to expose endpoints map[] 01/13/23 09:03:52.467
    Jan 13 09:03:52.516: INFO: successfully validated that service multi-endpoint-test in namespace services-765 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:03:52.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-765" for this suite. 01/13/23 09:03:52.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:03:52.759
Jan 13 09:03:52.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename tables 01/13/23 09:03:52.772
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:52.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:52.818
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 13 09:03:52.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2937" for this suite. 01/13/23 09:03:52.867
------------------------------
• [0.127 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:03:52.759
    Jan 13 09:03:52.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename tables 01/13/23 09:03:52.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:52.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:52.818
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:03:52.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2937" for this suite. 01/13/23 09:03:52.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:03:52.896
Jan 13 09:03:52.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 09:03:52.899
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:52.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:52.945
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/13/23 09:03:52.97
Jan 13 09:03:52.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082" in namespace "downward-api-16" to be "Succeeded or Failed"
Jan 13 09:03:53.012: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 16.338157ms
Jan 13 09:03:55.030: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033875188s
Jan 13 09:03:57.020: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024118955s
Jan 13 09:03:59.020: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02429285s
Jan 13 09:04:01.020: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024623746s
STEP: Saw pod success 01/13/23 09:04:01.021
Jan 13 09:04:01.021: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082" satisfied condition "Succeeded or Failed"
Jan 13 09:04:01.036: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082 container client-container: <nil>
STEP: delete the pod 01/13/23 09:04:01.127
Jan 13 09:04:01.201: INFO: Waiting for pod downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082 to disappear
Jan 13 09:04:01.209: INFO: Pod downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 09:04:01.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-16" for this suite. 01/13/23 09:04:01.223
------------------------------
• [SLOW TEST] [8.343 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:03:52.896
    Jan 13 09:03:52.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 09:03:52.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:03:52.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:03:52.945
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/13/23 09:03:52.97
    Jan 13 09:03:52.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082" in namespace "downward-api-16" to be "Succeeded or Failed"
    Jan 13 09:03:53.012: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 16.338157ms
    Jan 13 09:03:55.030: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033875188s
    Jan 13 09:03:57.020: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024118955s
    Jan 13 09:03:59.020: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02429285s
    Jan 13 09:04:01.020: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024623746s
    STEP: Saw pod success 01/13/23 09:04:01.021
    Jan 13 09:04:01.021: INFO: Pod "downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082" satisfied condition "Succeeded or Failed"
    Jan 13 09:04:01.036: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082 container client-container: <nil>
    STEP: delete the pod 01/13/23 09:04:01.127
    Jan 13 09:04:01.201: INFO: Waiting for pod downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082 to disappear
    Jan 13 09:04:01.209: INFO: Pod downwardapi-volume-d9e18a8c-b393-4548-b9d6-1bbdedcd6082 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:04:01.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-16" for this suite. 01/13/23 09:04:01.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:04:01.247
Jan 13 09:04:01.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:04:01.251
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:01.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:01.306
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 13 09:04:01.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:04:01.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9760" for this suite. 01/13/23 09:04:01.938
------------------------------
• [0.732 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:04:01.247
    Jan 13 09:04:01.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:04:01.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:01.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:01.306
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 13 09:04:01.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:04:01.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9760" for this suite. 01/13/23 09:04:01.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:04:01.994
Jan 13 09:04:01.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:04:01.997
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:02.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:02.029
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-00d424f5-8202-4e30-96fc-02a97ef07de5 01/13/23 09:04:02.042
STEP: Creating a pod to test consume configMaps 01/13/23 09:04:02.056
Jan 13 09:04:02.076: INFO: Waiting up to 5m0s for pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54" in namespace "configmap-3413" to be "Succeeded or Failed"
Jan 13 09:04:02.083: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 7.829362ms
Jan 13 09:04:04.115: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039434033s
Jan 13 09:04:06.095: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019074634s
Jan 13 09:04:08.096: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020282453s
Jan 13 09:04:10.097: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021157573s
STEP: Saw pod success 01/13/23 09:04:10.097
Jan 13 09:04:10.097: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54" satisfied condition "Succeeded or Failed"
Jan 13 09:04:10.105: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 09:04:10.124
Jan 13 09:04:10.146: INFO: Waiting for pod pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54 to disappear
Jan 13 09:04:10.152: INFO: Pod pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:04:10.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3413" for this suite. 01/13/23 09:04:10.16
------------------------------
• [SLOW TEST] [8.184 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:04:01.994
    Jan 13 09:04:01.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:04:01.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:02.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:02.029
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-00d424f5-8202-4e30-96fc-02a97ef07de5 01/13/23 09:04:02.042
    STEP: Creating a pod to test consume configMaps 01/13/23 09:04:02.056
    Jan 13 09:04:02.076: INFO: Waiting up to 5m0s for pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54" in namespace "configmap-3413" to be "Succeeded or Failed"
    Jan 13 09:04:02.083: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 7.829362ms
    Jan 13 09:04:04.115: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039434033s
    Jan 13 09:04:06.095: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019074634s
    Jan 13 09:04:08.096: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020282453s
    Jan 13 09:04:10.097: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021157573s
    STEP: Saw pod success 01/13/23 09:04:10.097
    Jan 13 09:04:10.097: INFO: Pod "pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54" satisfied condition "Succeeded or Failed"
    Jan 13 09:04:10.105: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 09:04:10.124
    Jan 13 09:04:10.146: INFO: Waiting for pod pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54 to disappear
    Jan 13 09:04:10.152: INFO: Pod pod-configmaps-38b8ac08-8b59-472d-a7c4-3152724bbc54 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:04:10.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3413" for this suite. 01/13/23 09:04:10.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:04:10.184
Jan 13 09:04:10.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 09:04:10.186
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:10.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:10.215
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/13/23 09:04:10.227
Jan 13 09:04:10.244: INFO: Waiting up to 5m0s for pod "pod-948f3211-e190-4766-92b6-8b580d757bb6" in namespace "emptydir-3747" to be "Succeeded or Failed"
Jan 13 09:04:10.252: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.953263ms
Jan 13 09:04:12.291: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047403133s
Jan 13 09:04:14.299: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054982142s
Jan 13 09:04:16.260: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016304085s
Jan 13 09:04:18.261: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017399935s
STEP: Saw pod success 01/13/23 09:04:18.261
Jan 13 09:04:18.261: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6" satisfied condition "Succeeded or Failed"
Jan 13 09:04:18.272: INFO: Trying to get logs from node 10.10.102.31-node pod pod-948f3211-e190-4766-92b6-8b580d757bb6 container test-container: <nil>
STEP: delete the pod 01/13/23 09:04:18.298
Jan 13 09:04:18.325: INFO: Waiting for pod pod-948f3211-e190-4766-92b6-8b580d757bb6 to disappear
Jan 13 09:04:18.336: INFO: Pod pod-948f3211-e190-4766-92b6-8b580d757bb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:04:18.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3747" for this suite. 01/13/23 09:04:18.353
------------------------------
• [SLOW TEST] [8.237 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:04:10.184
    Jan 13 09:04:10.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 09:04:10.186
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:10.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:10.215
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/13/23 09:04:10.227
    Jan 13 09:04:10.244: INFO: Waiting up to 5m0s for pod "pod-948f3211-e190-4766-92b6-8b580d757bb6" in namespace "emptydir-3747" to be "Succeeded or Failed"
    Jan 13 09:04:10.252: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.953263ms
    Jan 13 09:04:12.291: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047403133s
    Jan 13 09:04:14.299: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054982142s
    Jan 13 09:04:16.260: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016304085s
    Jan 13 09:04:18.261: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017399935s
    STEP: Saw pod success 01/13/23 09:04:18.261
    Jan 13 09:04:18.261: INFO: Pod "pod-948f3211-e190-4766-92b6-8b580d757bb6" satisfied condition "Succeeded or Failed"
    Jan 13 09:04:18.272: INFO: Trying to get logs from node 10.10.102.31-node pod pod-948f3211-e190-4766-92b6-8b580d757bb6 container test-container: <nil>
    STEP: delete the pod 01/13/23 09:04:18.298
    Jan 13 09:04:18.325: INFO: Waiting for pod pod-948f3211-e190-4766-92b6-8b580d757bb6 to disappear
    Jan 13 09:04:18.336: INFO: Pod pod-948f3211-e190-4766-92b6-8b580d757bb6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:04:18.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3747" for this suite. 01/13/23 09:04:18.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:04:18.427
Jan 13 09:04:18.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename containers 01/13/23 09:04:18.432
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:18.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:18.546
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/13/23 09:04:18.558
Jan 13 09:04:18.581: INFO: Waiting up to 5m0s for pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771" in namespace "containers-2357" to be "Succeeded or Failed"
Jan 13 09:04:18.596: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Pending", Reason="", readiness=false. Elapsed: 14.778746ms
Jan 13 09:04:20.608: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02671852s
Jan 13 09:04:22.632: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05037222s
Jan 13 09:04:24.605: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023471959s
STEP: Saw pod success 01/13/23 09:04:24.605
Jan 13 09:04:24.606: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771" satisfied condition "Succeeded or Failed"
Jan 13 09:04:24.614: INFO: Trying to get logs from node 10.10.102.31-node pod client-containers-befba2fe-3423-4190-8c63-5c7528cc0771 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 09:04:24.646
Jan 13 09:04:24.680: INFO: Waiting for pod client-containers-befba2fe-3423-4190-8c63-5c7528cc0771 to disappear
Jan 13 09:04:24.686: INFO: Pod client-containers-befba2fe-3423-4190-8c63-5c7528cc0771 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 13 09:04:24.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2357" for this suite. 01/13/23 09:04:24.699
------------------------------
• [SLOW TEST] [6.284 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:04:18.427
    Jan 13 09:04:18.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename containers 01/13/23 09:04:18.432
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:18.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:18.546
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/13/23 09:04:18.558
    Jan 13 09:04:18.581: INFO: Waiting up to 5m0s for pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771" in namespace "containers-2357" to be "Succeeded or Failed"
    Jan 13 09:04:18.596: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Pending", Reason="", readiness=false. Elapsed: 14.778746ms
    Jan 13 09:04:20.608: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02671852s
    Jan 13 09:04:22.632: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05037222s
    Jan 13 09:04:24.605: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023471959s
    STEP: Saw pod success 01/13/23 09:04:24.605
    Jan 13 09:04:24.606: INFO: Pod "client-containers-befba2fe-3423-4190-8c63-5c7528cc0771" satisfied condition "Succeeded or Failed"
    Jan 13 09:04:24.614: INFO: Trying to get logs from node 10.10.102.31-node pod client-containers-befba2fe-3423-4190-8c63-5c7528cc0771 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 09:04:24.646
    Jan 13 09:04:24.680: INFO: Waiting for pod client-containers-befba2fe-3423-4190-8c63-5c7528cc0771 to disappear
    Jan 13 09:04:24.686: INFO: Pod client-containers-befba2fe-3423-4190-8c63-5c7528cc0771 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:04:24.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2357" for this suite. 01/13/23 09:04:24.699
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:04:24.712
Jan 13 09:04:24.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 09:04:24.714
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:24.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:24.75
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 09:04:24.79
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:04:25.772
STEP: Deploying the webhook pod 01/13/23 09:04:25.788
STEP: Wait for the deployment to be ready 01/13/23 09:04:25.824
Jan 13 09:04:25.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 09:04:27.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:04:29.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 09:04:31.95
STEP: Verifying the service has paired with the endpoint 01/13/23 09:04:32.014
Jan 13 09:04:33.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/13/23 09:04:33.022
STEP: create a pod that should be denied by the webhook 01/13/23 09:04:33.081
STEP: create a pod that causes the webhook to hang 01/13/23 09:04:33.226
STEP: create a configmap that should be denied by the webhook 01/13/23 09:04:43.241
STEP: create a configmap that should be admitted by the webhook 01/13/23 09:04:43.279
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/13/23 09:04:43.308
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/13/23 09:04:43.326
STEP: create a namespace that bypass the webhook 01/13/23 09:04:43.336
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/13/23 09:04:43.362
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:04:43.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4366" for this suite. 01/13/23 09:04:43.53
STEP: Destroying namespace "webhook-4366-markers" for this suite. 01/13/23 09:04:43.547
------------------------------
• [SLOW TEST] [18.849 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:04:24.712
    Jan 13 09:04:24.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 09:04:24.714
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:24.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:24.75
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 09:04:24.79
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:04:25.772
    STEP: Deploying the webhook pod 01/13/23 09:04:25.788
    STEP: Wait for the deployment to be ready 01/13/23 09:04:25.824
    Jan 13 09:04:25.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 09:04:27.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:04:29.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 09:04:31.95
    STEP: Verifying the service has paired with the endpoint 01/13/23 09:04:32.014
    Jan 13 09:04:33.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/13/23 09:04:33.022
    STEP: create a pod that should be denied by the webhook 01/13/23 09:04:33.081
    STEP: create a pod that causes the webhook to hang 01/13/23 09:04:33.226
    STEP: create a configmap that should be denied by the webhook 01/13/23 09:04:43.241
    STEP: create a configmap that should be admitted by the webhook 01/13/23 09:04:43.279
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/13/23 09:04:43.308
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/13/23 09:04:43.326
    STEP: create a namespace that bypass the webhook 01/13/23 09:04:43.336
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/13/23 09:04:43.362
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:04:43.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4366" for this suite. 01/13/23 09:04:43.53
    STEP: Destroying namespace "webhook-4366-markers" for this suite. 01/13/23 09:04:43.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:04:43.568
Jan 13 09:04:43.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 09:04:43.571
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:43.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:43.79
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/13/23 09:04:43.868
Jan 13 09:04:43.957: INFO: Waiting up to 5m0s for pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4" in namespace "downward-api-8392" to be "Succeeded or Failed"
Jan 13 09:04:44.155: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 198.098747ms
Jan 13 09:04:46.161: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204166123s
Jan 13 09:04:48.211: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253530787s
Jan 13 09:04:50.163: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.205950763s
Jan 13 09:04:52.176: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.218560823s
STEP: Saw pod success 01/13/23 09:04:52.176
Jan 13 09:04:52.176: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4" satisfied condition "Succeeded or Failed"
Jan 13 09:04:52.186: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4 container dapi-container: <nil>
STEP: delete the pod 01/13/23 09:04:52.209
Jan 13 09:04:52.237: INFO: Waiting for pod downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4 to disappear
Jan 13 09:04:52.246: INFO: Pod downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 13 09:04:52.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8392" for this suite. 01/13/23 09:04:52.259
------------------------------
• [SLOW TEST] [8.706 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:04:43.568
    Jan 13 09:04:43.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 09:04:43.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:43.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:43.79
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/13/23 09:04:43.868
    Jan 13 09:04:43.957: INFO: Waiting up to 5m0s for pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4" in namespace "downward-api-8392" to be "Succeeded or Failed"
    Jan 13 09:04:44.155: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 198.098747ms
    Jan 13 09:04:46.161: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204166123s
    Jan 13 09:04:48.211: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253530787s
    Jan 13 09:04:50.163: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.205950763s
    Jan 13 09:04:52.176: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.218560823s
    STEP: Saw pod success 01/13/23 09:04:52.176
    Jan 13 09:04:52.176: INFO: Pod "downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4" satisfied condition "Succeeded or Failed"
    Jan 13 09:04:52.186: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4 container dapi-container: <nil>
    STEP: delete the pod 01/13/23 09:04:52.209
    Jan 13 09:04:52.237: INFO: Waiting for pod downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4 to disappear
    Jan 13 09:04:52.246: INFO: Pod downward-api-4a5b857a-5a89-4d50-be9f-100a6f5322c4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:04:52.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8392" for this suite. 01/13/23 09:04:52.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:04:52.282
Jan 13 09:04:52.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename taint-single-pod 01/13/23 09:04:52.285
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:52.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:52.366
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 13 09:04:52.378: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 09:05:52.449: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 13 09:05:52.470: INFO: Starting informer...
STEP: Starting pod... 01/13/23 09:05:52.47
Jan 13 09:05:52.752: INFO: Pod is running on 10.10.102.31-node. Tainting Node
STEP: Trying to apply a taint on the Node 01/13/23 09:05:52.752
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:05:52.827
STEP: Waiting short time to make sure Pod is queued for deletion 01/13/23 09:05:52.856
Jan 13 09:05:52.856: INFO: Pod wasn't evicted. Proceeding
Jan 13 09:05:52.856: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:05:52.923
STEP: Waiting some time to make sure that toleration time passed. 01/13/23 09:05:52.961
Jan 13 09:07:07.962: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:07:07.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-4967" for this suite. 01/13/23 09:07:07.974
------------------------------
• [SLOW TEST] [135.707 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:04:52.282
    Jan 13 09:04:52.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename taint-single-pod 01/13/23 09:04:52.285
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:04:52.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:04:52.366
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 13 09:04:52.378: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 13 09:05:52.449: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 13 09:05:52.470: INFO: Starting informer...
    STEP: Starting pod... 01/13/23 09:05:52.47
    Jan 13 09:05:52.752: INFO: Pod is running on 10.10.102.31-node. Tainting Node
    STEP: Trying to apply a taint on the Node 01/13/23 09:05:52.752
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:05:52.827
    STEP: Waiting short time to make sure Pod is queued for deletion 01/13/23 09:05:52.856
    Jan 13 09:05:52.856: INFO: Pod wasn't evicted. Proceeding
    Jan 13 09:05:52.856: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:05:52.923
    STEP: Waiting some time to make sure that toleration time passed. 01/13/23 09:05:52.961
    Jan 13 09:07:07.962: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:07:07.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-4967" for this suite. 01/13/23 09:07:07.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:07:07.999
Jan 13 09:07:08.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename podtemplate 01/13/23 09:07:08.003
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:08.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:08.065
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/13/23 09:07:08.075
Jan 13 09:07:08.083: INFO: created test-podtemplate-1
Jan 13 09:07:08.089: INFO: created test-podtemplate-2
Jan 13 09:07:08.099: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/13/23 09:07:08.099
STEP: delete collection of pod templates 01/13/23 09:07:08.105
Jan 13 09:07:08.105: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/13/23 09:07:08.121
Jan 13 09:07:08.121: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 13 09:07:08.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6627" for this suite. 01/13/23 09:07:08.133
------------------------------
• [0.144 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:07:07.999
    Jan 13 09:07:08.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename podtemplate 01/13/23 09:07:08.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:08.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:08.065
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/13/23 09:07:08.075
    Jan 13 09:07:08.083: INFO: created test-podtemplate-1
    Jan 13 09:07:08.089: INFO: created test-podtemplate-2
    Jan 13 09:07:08.099: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/13/23 09:07:08.099
    STEP: delete collection of pod templates 01/13/23 09:07:08.105
    Jan 13 09:07:08.105: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/13/23 09:07:08.121
    Jan 13 09:07:08.121: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:07:08.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6627" for this suite. 01/13/23 09:07:08.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:07:08.146
Jan 13 09:07:08.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 09:07:08.149
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:08.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:08.174
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5481 01/13/23 09:07:08.181
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 13 09:07:08.206: INFO: Found 0 stateful pods, waiting for 1
Jan 13 09:07:18.214: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/13/23 09:07:18.226
W0113 09:07:18.240110      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 13 09:07:18.254: INFO: Found 1 stateful pods, waiting for 2
Jan 13 09:07:28.287: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 13 09:07:38.275: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 09:07:38.275: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/13/23 09:07:38.292
STEP: Delete all of the StatefulSets 01/13/23 09:07:38.301
STEP: Verify that StatefulSets have been deleted 01/13/23 09:07:38.315
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 09:07:38.322: INFO: Deleting all statefulset in ns statefulset-5481
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:07:38.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5481" for this suite. 01/13/23 09:07:38.375
------------------------------
• [SLOW TEST] [30.245 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:07:08.146
    Jan 13 09:07:08.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 09:07:08.149
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:08.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:08.174
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5481 01/13/23 09:07:08.181
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 13 09:07:08.206: INFO: Found 0 stateful pods, waiting for 1
    Jan 13 09:07:18.214: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/13/23 09:07:18.226
    W0113 09:07:18.240110      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 13 09:07:18.254: INFO: Found 1 stateful pods, waiting for 2
    Jan 13 09:07:28.287: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 13 09:07:38.275: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 09:07:38.275: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/13/23 09:07:38.292
    STEP: Delete all of the StatefulSets 01/13/23 09:07:38.301
    STEP: Verify that StatefulSets have been deleted 01/13/23 09:07:38.315
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 09:07:38.322: INFO: Deleting all statefulset in ns statefulset-5481
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:07:38.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5481" for this suite. 01/13/23 09:07:38.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:07:38.396
Jan 13 09:07:38.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 09:07:38.406
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:38.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:38.464
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/13/23 09:07:38.489
Jan 13 09:07:38.528: INFO: Waiting up to 5m0s for pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2" in namespace "downward-api-9385" to be "running and ready"
Jan 13 09:07:38.554: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.462571ms
Jan 13 09:07:38.554: INFO: The phase of Pod annotationupdate48056a76-44a7-46d2-b992-59cda81192b2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:07:40.616: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088108025s
Jan 13 09:07:40.616: INFO: The phase of Pod annotationupdate48056a76-44a7-46d2-b992-59cda81192b2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:07:42.568: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.040576274s
Jan 13 09:07:42.568: INFO: The phase of Pod annotationupdate48056a76-44a7-46d2-b992-59cda81192b2 is Running (Ready = true)
Jan 13 09:07:42.568: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2" satisfied condition "running and ready"
Jan 13 09:07:43.210: INFO: Successfully updated pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 09:07:47.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9385" for this suite. 01/13/23 09:07:47.304
------------------------------
• [SLOW TEST] [8.922 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:07:38.396
    Jan 13 09:07:38.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 09:07:38.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:38.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:38.464
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/13/23 09:07:38.489
    Jan 13 09:07:38.528: INFO: Waiting up to 5m0s for pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2" in namespace "downward-api-9385" to be "running and ready"
    Jan 13 09:07:38.554: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.462571ms
    Jan 13 09:07:38.554: INFO: The phase of Pod annotationupdate48056a76-44a7-46d2-b992-59cda81192b2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:07:40.616: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088108025s
    Jan 13 09:07:40.616: INFO: The phase of Pod annotationupdate48056a76-44a7-46d2-b992-59cda81192b2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:07:42.568: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.040576274s
    Jan 13 09:07:42.568: INFO: The phase of Pod annotationupdate48056a76-44a7-46d2-b992-59cda81192b2 is Running (Ready = true)
    Jan 13 09:07:42.568: INFO: Pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2" satisfied condition "running and ready"
    Jan 13 09:07:43.210: INFO: Successfully updated pod "annotationupdate48056a76-44a7-46d2-b992-59cda81192b2"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:07:47.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9385" for this suite. 01/13/23 09:07:47.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:07:47.332
Jan 13 09:07:47.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename prestop 01/13/23 09:07:47.336
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:47.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:47.375
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2426 01/13/23 09:07:47.381
STEP: Waiting for pods to come up. 01/13/23 09:07:47.394
Jan 13 09:07:47.394: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2426" to be "running"
Jan 13 09:07:47.405: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 10.763994ms
Jan 13 09:07:49.430: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036511413s
Jan 13 09:07:51.414: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.019661659s
Jan 13 09:07:51.414: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2426 01/13/23 09:07:51.424
Jan 13 09:07:51.435: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2426" to be "running"
Jan 13 09:07:51.440: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.749354ms
Jan 13 09:07:53.450: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015691735s
Jan 13 09:07:55.464: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029005903s
Jan 13 09:07:57.470: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 6.035068443s
Jan 13 09:07:57.470: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/13/23 09:07:57.47
Jan 13 09:08:02.528: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/13/23 09:08:02.529
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 13 09:08:02.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2426" for this suite. 01/13/23 09:08:02.594
------------------------------
• [SLOW TEST] [15.280 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:07:47.332
    Jan 13 09:07:47.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename prestop 01/13/23 09:07:47.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:07:47.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:07:47.375
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2426 01/13/23 09:07:47.381
    STEP: Waiting for pods to come up. 01/13/23 09:07:47.394
    Jan 13 09:07:47.394: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2426" to be "running"
    Jan 13 09:07:47.405: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 10.763994ms
    Jan 13 09:07:49.430: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036511413s
    Jan 13 09:07:51.414: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.019661659s
    Jan 13 09:07:51.414: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2426 01/13/23 09:07:51.424
    Jan 13 09:07:51.435: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2426" to be "running"
    Jan 13 09:07:51.440: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.749354ms
    Jan 13 09:07:53.450: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015691735s
    Jan 13 09:07:55.464: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029005903s
    Jan 13 09:07:57.470: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 6.035068443s
    Jan 13 09:07:57.470: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/13/23 09:07:57.47
    Jan 13 09:08:02.528: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/13/23 09:08:02.529
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:08:02.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2426" for this suite. 01/13/23 09:08:02.594
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:08:02.614
Jan 13 09:08:02.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 09:08:02.617
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:02.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:02.749
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 13 09:08:02.827: INFO: Waiting up to 5m0s for pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80" in namespace "pods-2834" to be "running and ready"
Jan 13 09:08:02.852: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80": Phase="Pending", Reason="", readiness=false. Elapsed: 24.422986ms
Jan 13 09:08:02.852: INFO: The phase of Pod server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:08:04.863: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035408619s
Jan 13 09:08:04.863: INFO: The phase of Pod server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:08:06.865: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80": Phase="Running", Reason="", readiness=true. Elapsed: 4.038125665s
Jan 13 09:08:06.866: INFO: The phase of Pod server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80 is Running (Ready = true)
Jan 13 09:08:06.866: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80" satisfied condition "running and ready"
Jan 13 09:08:06.935: INFO: Waiting up to 5m0s for pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8" in namespace "pods-2834" to be "Succeeded or Failed"
Jan 13 09:08:06.946: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.486297ms
Jan 13 09:08:08.951: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016415443s
Jan 13 09:08:10.954: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018567536s
Jan 13 09:08:12.956: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021413152s
Jan 13 09:08:14.957: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021909576s
STEP: Saw pod success 01/13/23 09:08:14.957
Jan 13 09:08:14.958: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8" satisfied condition "Succeeded or Failed"
Jan 13 09:08:14.964: INFO: Trying to get logs from node 10.10.102.31-node pod client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8 container env3cont: <nil>
STEP: delete the pod 01/13/23 09:08:14.994
Jan 13 09:08:15.015: INFO: Waiting for pod client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8 to disappear
Jan 13 09:08:15.021: INFO: Pod client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 09:08:15.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2834" for this suite. 01/13/23 09:08:15.029
------------------------------
• [SLOW TEST] [12.431 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:08:02.614
    Jan 13 09:08:02.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 09:08:02.617
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:02.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:02.749
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 13 09:08:02.827: INFO: Waiting up to 5m0s for pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80" in namespace "pods-2834" to be "running and ready"
    Jan 13 09:08:02.852: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80": Phase="Pending", Reason="", readiness=false. Elapsed: 24.422986ms
    Jan 13 09:08:02.852: INFO: The phase of Pod server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:08:04.863: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035408619s
    Jan 13 09:08:04.863: INFO: The phase of Pod server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:08:06.865: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80": Phase="Running", Reason="", readiness=true. Elapsed: 4.038125665s
    Jan 13 09:08:06.866: INFO: The phase of Pod server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80 is Running (Ready = true)
    Jan 13 09:08:06.866: INFO: Pod "server-envvars-5298f68b-e6cd-47c1-8851-4ee325785e80" satisfied condition "running and ready"
    Jan 13 09:08:06.935: INFO: Waiting up to 5m0s for pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8" in namespace "pods-2834" to be "Succeeded or Failed"
    Jan 13 09:08:06.946: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.486297ms
    Jan 13 09:08:08.951: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016415443s
    Jan 13 09:08:10.954: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018567536s
    Jan 13 09:08:12.956: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021413152s
    Jan 13 09:08:14.957: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021909576s
    STEP: Saw pod success 01/13/23 09:08:14.957
    Jan 13 09:08:14.958: INFO: Pod "client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8" satisfied condition "Succeeded or Failed"
    Jan 13 09:08:14.964: INFO: Trying to get logs from node 10.10.102.31-node pod client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8 container env3cont: <nil>
    STEP: delete the pod 01/13/23 09:08:14.994
    Jan 13 09:08:15.015: INFO: Waiting for pod client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8 to disappear
    Jan 13 09:08:15.021: INFO: Pod client-envvars-ccec5a2a-0d0b-4f0a-a57e-8d02f11a27a8 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:08:15.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2834" for this suite. 01/13/23 09:08:15.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:08:15.051
Jan 13 09:08:15.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replication-controller 01/13/23 09:08:15.055
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:15.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:15.121
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/13/23 09:08:15.138
Jan 13 09:08:15.162: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9571" to be "running and ready"
Jan 13 09:08:15.171: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.129598ms
Jan 13 09:08:15.171: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:08:17.210: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048061183s
Jan 13 09:08:17.210: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:08:19.178: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.01622469s
Jan 13 09:08:19.178: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 13 09:08:19.178: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/13/23 09:08:19.183
STEP: Then the orphan pod is adopted 01/13/23 09:08:19.192
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 13 09:08:20.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9571" for this suite. 01/13/23 09:08:20.238
------------------------------
• [SLOW TEST] [5.225 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:08:15.051
    Jan 13 09:08:15.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replication-controller 01/13/23 09:08:15.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:15.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:15.121
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/13/23 09:08:15.138
    Jan 13 09:08:15.162: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9571" to be "running and ready"
    Jan 13 09:08:15.171: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.129598ms
    Jan 13 09:08:15.171: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:08:17.210: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048061183s
    Jan 13 09:08:17.210: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:08:19.178: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.01622469s
    Jan 13 09:08:19.178: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 13 09:08:19.178: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/13/23 09:08:19.183
    STEP: Then the orphan pod is adopted 01/13/23 09:08:19.192
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:08:20.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9571" for this suite. 01/13/23 09:08:20.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:08:20.281
Jan 13 09:08:20.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename init-container 01/13/23 09:08:20.284
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:20.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:20.335
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/13/23 09:08:20.354
Jan 13 09:08:20.354: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:08:27.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2678" for this suite. 01/13/23 09:08:27.944
------------------------------
• [SLOW TEST] [7.673 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:08:20.281
    Jan 13 09:08:20.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename init-container 01/13/23 09:08:20.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:20.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:20.335
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/13/23 09:08:20.354
    Jan 13 09:08:20.354: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:08:27.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2678" for this suite. 01/13/23 09:08:27.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:08:27.956
Jan 13 09:08:27.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 09:08:27.969
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:27.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:27.999
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/13/23 09:08:28.005
Jan 13 09:08:28.018: INFO: Waiting up to 5m0s for pod "pod-e21db504-097b-4ded-a753-8b5293f3d018" in namespace "emptydir-5835" to be "Succeeded or Failed"
Jan 13 09:08:28.024: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Pending", Reason="", readiness=false. Elapsed: 5.998194ms
Jan 13 09:08:30.060: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041697207s
Jan 13 09:08:32.031: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013043931s
Jan 13 09:08:34.035: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016128166s
STEP: Saw pod success 01/13/23 09:08:34.035
Jan 13 09:08:34.035: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018" satisfied condition "Succeeded or Failed"
Jan 13 09:08:34.044: INFO: Trying to get logs from node 10.10.102.31-node pod pod-e21db504-097b-4ded-a753-8b5293f3d018 container test-container: <nil>
STEP: delete the pod 01/13/23 09:08:34.065
Jan 13 09:08:34.124: INFO: Waiting for pod pod-e21db504-097b-4ded-a753-8b5293f3d018 to disappear
Jan 13 09:08:34.130: INFO: Pod pod-e21db504-097b-4ded-a753-8b5293f3d018 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:08:34.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5835" for this suite. 01/13/23 09:08:34.144
------------------------------
• [SLOW TEST] [6.207 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:08:27.956
    Jan 13 09:08:27.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 09:08:27.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:27.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:27.999
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/13/23 09:08:28.005
    Jan 13 09:08:28.018: INFO: Waiting up to 5m0s for pod "pod-e21db504-097b-4ded-a753-8b5293f3d018" in namespace "emptydir-5835" to be "Succeeded or Failed"
    Jan 13 09:08:28.024: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Pending", Reason="", readiness=false. Elapsed: 5.998194ms
    Jan 13 09:08:30.060: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041697207s
    Jan 13 09:08:32.031: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013043931s
    Jan 13 09:08:34.035: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016128166s
    STEP: Saw pod success 01/13/23 09:08:34.035
    Jan 13 09:08:34.035: INFO: Pod "pod-e21db504-097b-4ded-a753-8b5293f3d018" satisfied condition "Succeeded or Failed"
    Jan 13 09:08:34.044: INFO: Trying to get logs from node 10.10.102.31-node pod pod-e21db504-097b-4ded-a753-8b5293f3d018 container test-container: <nil>
    STEP: delete the pod 01/13/23 09:08:34.065
    Jan 13 09:08:34.124: INFO: Waiting for pod pod-e21db504-097b-4ded-a753-8b5293f3d018 to disappear
    Jan 13 09:08:34.130: INFO: Pod pod-e21db504-097b-4ded-a753-8b5293f3d018 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:08:34.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5835" for this suite. 01/13/23 09:08:34.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:08:34.165
Jan 13 09:08:34.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 09:08:34.167
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:34.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:34.209
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1115 01/13/23 09:08:34.214
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-1115 01/13/23 09:08:34.229
Jan 13 09:08:34.251: INFO: Found 0 stateful pods, waiting for 1
Jan 13 09:08:44.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/13/23 09:08:44.269
STEP: Getting /status 01/13/23 09:08:44.293
Jan 13 09:08:44.309: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/13/23 09:08:44.309
Jan 13 09:08:44.341: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/13/23 09:08:44.341
Jan 13 09:08:44.346: INFO: Observed &StatefulSet event: ADDED
Jan 13 09:08:44.346: INFO: Found Statefulset ss in namespace statefulset-1115 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 13 09:08:44.346: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/13/23 09:08:44.346
Jan 13 09:08:44.346: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 13 09:08:44.379: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/13/23 09:08:44.379
Jan 13 09:08:44.390: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 09:08:44.390: INFO: Deleting all statefulset in ns statefulset-1115
Jan 13 09:08:44.426: INFO: Scaling statefulset ss to 0
Jan 13 09:08:54.505: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:08:54.527: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:08:54.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1115" for this suite. 01/13/23 09:08:54.592
------------------------------
• [SLOW TEST] [20.460 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:08:34.165
    Jan 13 09:08:34.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 09:08:34.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:34.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:34.209
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1115 01/13/23 09:08:34.214
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-1115 01/13/23 09:08:34.229
    Jan 13 09:08:34.251: INFO: Found 0 stateful pods, waiting for 1
    Jan 13 09:08:44.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/13/23 09:08:44.269
    STEP: Getting /status 01/13/23 09:08:44.293
    Jan 13 09:08:44.309: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/13/23 09:08:44.309
    Jan 13 09:08:44.341: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/13/23 09:08:44.341
    Jan 13 09:08:44.346: INFO: Observed &StatefulSet event: ADDED
    Jan 13 09:08:44.346: INFO: Found Statefulset ss in namespace statefulset-1115 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 13 09:08:44.346: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/13/23 09:08:44.346
    Jan 13 09:08:44.346: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 13 09:08:44.379: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/13/23 09:08:44.379
    Jan 13 09:08:44.390: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 09:08:44.390: INFO: Deleting all statefulset in ns statefulset-1115
    Jan 13 09:08:44.426: INFO: Scaling statefulset ss to 0
    Jan 13 09:08:54.505: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:08:54.527: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:08:54.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1115" for this suite. 01/13/23 09:08:54.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:08:54.627
Jan 13 09:08:54.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:08:54.631
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:54.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:54.672
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2135-delete-me 01/13/23 09:08:54.689
STEP: Waiting for the RuntimeClass to disappear 01/13/23 09:08:54.705
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 13 09:08:54.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2135" for this suite. 01/13/23 09:08:54.744
------------------------------
• [0.131 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:08:54.627
    Jan 13 09:08:54.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:08:54.631
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:54.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:54.672
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2135-delete-me 01/13/23 09:08:54.689
    STEP: Waiting for the RuntimeClass to disappear 01/13/23 09:08:54.705
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:08:54.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2135" for this suite. 01/13/23 09:08:54.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:08:54.761
Jan 13 09:08:54.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 09:08:54.765
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:54.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:54.812
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-97f99b78-fb60-4edc-a42f-53a3d7b9eb10 01/13/23 09:08:54.827
STEP: Creating a pod to test consume secrets 01/13/23 09:08:54.845
Jan 13 09:08:54.871: INFO: Waiting up to 5m0s for pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11" in namespace "secrets-138" to be "Succeeded or Failed"
Jan 13 09:08:54.886: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 14.773319ms
Jan 13 09:08:56.896: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024192117s
Jan 13 09:08:58.896: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024410474s
Jan 13 09:09:00.957: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086103139s
Jan 13 09:09:02.895: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023637128s
STEP: Saw pod success 01/13/23 09:09:02.895
Jan 13 09:09:02.896: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11" satisfied condition "Succeeded or Failed"
Jan 13 09:09:02.901: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-31aa5707-755e-41af-9c88-e170575aea11 container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 09:09:02.924
Jan 13 09:09:02.963: INFO: Waiting for pod pod-secrets-31aa5707-755e-41af-9c88-e170575aea11 to disappear
Jan 13 09:09:02.975: INFO: Pod pod-secrets-31aa5707-755e-41af-9c88-e170575aea11 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 09:09:02.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-138" for this suite. 01/13/23 09:09:02.989
------------------------------
• [SLOW TEST] [8.243 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:08:54.761
    Jan 13 09:08:54.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 09:08:54.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:08:54.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:08:54.812
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-97f99b78-fb60-4edc-a42f-53a3d7b9eb10 01/13/23 09:08:54.827
    STEP: Creating a pod to test consume secrets 01/13/23 09:08:54.845
    Jan 13 09:08:54.871: INFO: Waiting up to 5m0s for pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11" in namespace "secrets-138" to be "Succeeded or Failed"
    Jan 13 09:08:54.886: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 14.773319ms
    Jan 13 09:08:56.896: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024192117s
    Jan 13 09:08:58.896: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024410474s
    Jan 13 09:09:00.957: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086103139s
    Jan 13 09:09:02.895: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023637128s
    STEP: Saw pod success 01/13/23 09:09:02.895
    Jan 13 09:09:02.896: INFO: Pod "pod-secrets-31aa5707-755e-41af-9c88-e170575aea11" satisfied condition "Succeeded or Failed"
    Jan 13 09:09:02.901: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-31aa5707-755e-41af-9c88-e170575aea11 container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 09:09:02.924
    Jan 13 09:09:02.963: INFO: Waiting for pod pod-secrets-31aa5707-755e-41af-9c88-e170575aea11 to disappear
    Jan 13 09:09:02.975: INFO: Pod pod-secrets-31aa5707-755e-41af-9c88-e170575aea11 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:09:02.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-138" for this suite. 01/13/23 09:09:02.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:09:03.006
Jan 13 09:09:03.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename gc 01/13/23 09:09:03.009
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:03.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:03.052
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/13/23 09:09:03.061
STEP: delete the rc 01/13/23 09:09:08.09
STEP: wait for all pods to be garbage collected 01/13/23 09:09:08.126
STEP: Gathering metrics 01/13/23 09:09:13.164
Jan 13 09:09:13.267: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
Jan 13 09:09:13.303: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 36.017985ms
Jan 13 09:09:13.304: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
Jan 13 09:09:13.304: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
Jan 13 09:09:13.707: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 13 09:09:13.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3901" for this suite. 01/13/23 09:09:13.728
------------------------------
• [SLOW TEST] [10.760 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:09:03.006
    Jan 13 09:09:03.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename gc 01/13/23 09:09:03.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:03.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:03.052
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/13/23 09:09:03.061
    STEP: delete the rc 01/13/23 09:09:08.09
    STEP: wait for all pods to be garbage collected 01/13/23 09:09:08.126
    STEP: Gathering metrics 01/13/23 09:09:13.164
    Jan 13 09:09:13.267: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
    Jan 13 09:09:13.303: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 36.017985ms
    Jan 13 09:09:13.304: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
    Jan 13 09:09:13.304: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
    Jan 13 09:09:13.707: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:09:13.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3901" for this suite. 01/13/23 09:09:13.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:09:13.77
Jan 13 09:09:13.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 09:09:13.773
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:13.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:13.919
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/13/23 09:09:13.931
Jan 13 09:09:14.005: INFO: Waiting up to 5m0s for pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0" in namespace "emptydir-4248" to be "Succeeded or Failed"
Jan 13 09:09:14.101: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 95.304565ms
Jan 13 09:09:16.115: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110034339s
Jan 13 09:09:18.109: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103605896s
Jan 13 09:09:20.111: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105469195s
Jan 13 09:09:22.113: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.107352229s
STEP: Saw pod success 01/13/23 09:09:22.113
Jan 13 09:09:22.113: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0" satisfied condition "Succeeded or Failed"
Jan 13 09:09:22.122: INFO: Trying to get logs from node 10.10.102.31-node pod pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0 container test-container: <nil>
STEP: delete the pod 01/13/23 09:09:22.149
Jan 13 09:09:22.179: INFO: Waiting for pod pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0 to disappear
Jan 13 09:09:22.186: INFO: Pod pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:09:22.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4248" for this suite. 01/13/23 09:09:22.207
------------------------------
• [SLOW TEST] [8.471 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:09:13.77
    Jan 13 09:09:13.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 09:09:13.773
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:13.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:13.919
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/13/23 09:09:13.931
    Jan 13 09:09:14.005: INFO: Waiting up to 5m0s for pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0" in namespace "emptydir-4248" to be "Succeeded or Failed"
    Jan 13 09:09:14.101: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 95.304565ms
    Jan 13 09:09:16.115: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110034339s
    Jan 13 09:09:18.109: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103605896s
    Jan 13 09:09:20.111: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105469195s
    Jan 13 09:09:22.113: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.107352229s
    STEP: Saw pod success 01/13/23 09:09:22.113
    Jan 13 09:09:22.113: INFO: Pod "pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0" satisfied condition "Succeeded or Failed"
    Jan 13 09:09:22.122: INFO: Trying to get logs from node 10.10.102.31-node pod pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0 container test-container: <nil>
    STEP: delete the pod 01/13/23 09:09:22.149
    Jan 13 09:09:22.179: INFO: Waiting for pod pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0 to disappear
    Jan 13 09:09:22.186: INFO: Pod pod-d0a60f5c-d409-4e33-81a1-0e3c37f22af0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:09:22.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4248" for this suite. 01/13/23 09:09:22.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:09:22.254
Jan 13 09:09:22.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 09:09:22.257
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:22.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:22.326
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 09:09:22.394
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:09:25.624
STEP: Deploying the webhook pod 01/13/23 09:09:25.641
STEP: Wait for the deployment to be ready 01/13/23 09:09:25.671
Jan 13 09:09:25.685: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 09:09:27.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 09:09:29.779
STEP: Verifying the service has paired with the endpoint 01/13/23 09:09:29.821
Jan 13 09:09:30.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 13 09:09:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2520-crds.webhook.example.com via the AdmissionRegistration API 01/13/23 09:09:31.429
STEP: Creating a custom resource that should be mutated by the webhook 01/13/23 09:09:31.487
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:09:34.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8795" for this suite. 01/13/23 09:09:34.428
STEP: Destroying namespace "webhook-8795-markers" for this suite. 01/13/23 09:09:34.481
------------------------------
• [SLOW TEST] [12.263 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:09:22.254
    Jan 13 09:09:22.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 09:09:22.257
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:22.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:22.326
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 09:09:22.394
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:09:25.624
    STEP: Deploying the webhook pod 01/13/23 09:09:25.641
    STEP: Wait for the deployment to be ready 01/13/23 09:09:25.671
    Jan 13 09:09:25.685: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 09:09:27.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 9, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 09:09:29.779
    STEP: Verifying the service has paired with the endpoint 01/13/23 09:09:29.821
    Jan 13 09:09:30.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 13 09:09:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2520-crds.webhook.example.com via the AdmissionRegistration API 01/13/23 09:09:31.429
    STEP: Creating a custom resource that should be mutated by the webhook 01/13/23 09:09:31.487
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:09:34.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8795" for this suite. 01/13/23 09:09:34.428
    STEP: Destroying namespace "webhook-8795-markers" for this suite. 01/13/23 09:09:34.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:09:34.526
Jan 13 09:09:34.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-runtime 01/13/23 09:09:34.53
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:34.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:34.59
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/13/23 09:09:34.62
STEP: wait for the container to reach Succeeded 01/13/23 09:09:34.665
STEP: get the container status 01/13/23 09:09:41.82
STEP: the container should be terminated 01/13/23 09:09:41.829
STEP: the termination message should be set 01/13/23 09:09:41.83
Jan 13 09:09:41.830: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/13/23 09:09:41.83
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 13 09:09:41.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1985" for this suite. 01/13/23 09:09:41.875
------------------------------
• [SLOW TEST] [7.362 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:09:34.526
    Jan 13 09:09:34.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-runtime 01/13/23 09:09:34.53
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:34.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:34.59
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/13/23 09:09:34.62
    STEP: wait for the container to reach Succeeded 01/13/23 09:09:34.665
    STEP: get the container status 01/13/23 09:09:41.82
    STEP: the container should be terminated 01/13/23 09:09:41.829
    STEP: the termination message should be set 01/13/23 09:09:41.83
    Jan 13 09:09:41.830: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/13/23 09:09:41.83
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:09:41.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1985" for this suite. 01/13/23 09:09:41.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:09:41.89
Jan 13 09:09:41.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename subpath 01/13/23 09:09:41.894
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:41.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:41.939
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/13/23 09:09:41.949
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-2bjk 01/13/23 09:09:41.982
STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:09:41.982
Jan 13 09:09:42.004: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2bjk" in namespace "subpath-4889" to be "Succeeded or Failed"
Jan 13 09:09:42.013: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.91267ms
Jan 13 09:09:44.022: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018516373s
Jan 13 09:09:46.034: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 4.029836306s
Jan 13 09:09:48.020: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 6.015685765s
Jan 13 09:09:50.035: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 8.030738588s
Jan 13 09:09:52.021: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 10.017500601s
Jan 13 09:09:54.036: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 12.032600565s
Jan 13 09:09:56.043: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 14.039553078s
Jan 13 09:09:58.033: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 16.029199805s
Jan 13 09:10:00.021: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 18.017246844s
Jan 13 09:10:02.029: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 20.025617783s
Jan 13 09:10:04.040: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 22.035721464s
Jan 13 09:10:06.020: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 24.016093012s
Jan 13 09:10:08.019: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=false. Elapsed: 26.015603277s
Jan 13 09:10:10.025: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.020858733s
STEP: Saw pod success 01/13/23 09:10:10.025
Jan 13 09:10:10.025: INFO: Pod "pod-subpath-test-downwardapi-2bjk" satisfied condition "Succeeded or Failed"
Jan 13 09:10:10.031: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-downwardapi-2bjk container test-container-subpath-downwardapi-2bjk: <nil>
STEP: delete the pod 01/13/23 09:10:10.055
Jan 13 09:10:10.090: INFO: Waiting for pod pod-subpath-test-downwardapi-2bjk to disappear
Jan 13 09:10:10.097: INFO: Pod pod-subpath-test-downwardapi-2bjk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2bjk 01/13/23 09:10:10.097
Jan 13 09:10:10.097: INFO: Deleting pod "pod-subpath-test-downwardapi-2bjk" in namespace "subpath-4889"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 13 09:10:10.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4889" for this suite. 01/13/23 09:10:10.111
------------------------------
• [SLOW TEST] [28.229 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:09:41.89
    Jan 13 09:09:41.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename subpath 01/13/23 09:09:41.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:09:41.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:09:41.939
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/13/23 09:09:41.949
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-2bjk 01/13/23 09:09:41.982
    STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:09:41.982
    Jan 13 09:09:42.004: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2bjk" in namespace "subpath-4889" to be "Succeeded or Failed"
    Jan 13 09:09:42.013: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.91267ms
    Jan 13 09:09:44.022: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018516373s
    Jan 13 09:09:46.034: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 4.029836306s
    Jan 13 09:09:48.020: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 6.015685765s
    Jan 13 09:09:50.035: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 8.030738588s
    Jan 13 09:09:52.021: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 10.017500601s
    Jan 13 09:09:54.036: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 12.032600565s
    Jan 13 09:09:56.043: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 14.039553078s
    Jan 13 09:09:58.033: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 16.029199805s
    Jan 13 09:10:00.021: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 18.017246844s
    Jan 13 09:10:02.029: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 20.025617783s
    Jan 13 09:10:04.040: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 22.035721464s
    Jan 13 09:10:06.020: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=true. Elapsed: 24.016093012s
    Jan 13 09:10:08.019: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Running", Reason="", readiness=false. Elapsed: 26.015603277s
    Jan 13 09:10:10.025: INFO: Pod "pod-subpath-test-downwardapi-2bjk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.020858733s
    STEP: Saw pod success 01/13/23 09:10:10.025
    Jan 13 09:10:10.025: INFO: Pod "pod-subpath-test-downwardapi-2bjk" satisfied condition "Succeeded or Failed"
    Jan 13 09:10:10.031: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-downwardapi-2bjk container test-container-subpath-downwardapi-2bjk: <nil>
    STEP: delete the pod 01/13/23 09:10:10.055
    Jan 13 09:10:10.090: INFO: Waiting for pod pod-subpath-test-downwardapi-2bjk to disappear
    Jan 13 09:10:10.097: INFO: Pod pod-subpath-test-downwardapi-2bjk no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-2bjk 01/13/23 09:10:10.097
    Jan 13 09:10:10.097: INFO: Deleting pod "pod-subpath-test-downwardapi-2bjk" in namespace "subpath-4889"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:10:10.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4889" for this suite. 01/13/23 09:10:10.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:10:10.121
Jan 13 09:10:10.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 09:10:10.124
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:10.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:10.158
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/13/23 09:10:10.167
Jan 13 09:10:10.177: INFO: Waiting up to 5m0s for pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83" in namespace "downward-api-7654" to be "Succeeded or Failed"
Jan 13 09:10:10.184: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.243885ms
Jan 13 09:10:12.192: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014297459s
Jan 13 09:10:14.191: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01408334s
Jan 13 09:10:16.192: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014510235s
STEP: Saw pod success 01/13/23 09:10:16.192
Jan 13 09:10:16.192: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83" satisfied condition "Succeeded or Failed"
Jan 13 09:10:16.198: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83 container dapi-container: <nil>
STEP: delete the pod 01/13/23 09:10:16.214
Jan 13 09:10:16.234: INFO: Waiting for pod downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83 to disappear
Jan 13 09:10:16.240: INFO: Pod downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 13 09:10:16.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7654" for this suite. 01/13/23 09:10:16.262
------------------------------
• [SLOW TEST] [6.157 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:10:10.121
    Jan 13 09:10:10.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 09:10:10.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:10.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:10.158
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/13/23 09:10:10.167
    Jan 13 09:10:10.177: INFO: Waiting up to 5m0s for pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83" in namespace "downward-api-7654" to be "Succeeded or Failed"
    Jan 13 09:10:10.184: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.243885ms
    Jan 13 09:10:12.192: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014297459s
    Jan 13 09:10:14.191: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01408334s
    Jan 13 09:10:16.192: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014510235s
    STEP: Saw pod success 01/13/23 09:10:16.192
    Jan 13 09:10:16.192: INFO: Pod "downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83" satisfied condition "Succeeded or Failed"
    Jan 13 09:10:16.198: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83 container dapi-container: <nil>
    STEP: delete the pod 01/13/23 09:10:16.214
    Jan 13 09:10:16.234: INFO: Waiting for pod downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83 to disappear
    Jan 13 09:10:16.240: INFO: Pod downward-api-c91c0ad1-a9df-4243-b3f8-4d74b91eba83 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:10:16.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7654" for this suite. 01/13/23 09:10:16.262
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:10:16.279
Jan 13 09:10:16.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename daemonsets 01/13/23 09:10:16.281
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:16.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:16.336
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan 13 09:10:16.395: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/13/23 09:10:16.418
Jan 13 09:10:16.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:16.446: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/13/23 09:10:16.446
Jan 13 09:10:16.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:16.513: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:17.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:17.519: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:18.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:18.525: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:19.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:10:19.522: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/13/23 09:10:19.529
Jan 13 09:10:19.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:10:19.591: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 13 09:10:20.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:20.600: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/13/23 09:10:20.6
Jan 13 09:10:20.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:20.627: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:21.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:21.634: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:22.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:22.674: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:23.662: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:23.662: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:24.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:24.640: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:25.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:25.635: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:10:26.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:10:26.636: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:10:26.676
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1837, will wait for the garbage collector to delete the pods 01/13/23 09:10:26.676
Jan 13 09:10:26.799: INFO: Deleting DaemonSet.extensions daemon-set took: 56.641541ms
Jan 13 09:10:27.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.347239ms
Jan 13 09:10:31.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:10:31.212: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 13 09:10:31.219: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"573088"},"items":null}

Jan 13 09:10:31.228: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"573088"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:10:31.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1837" for this suite. 01/13/23 09:10:31.297
------------------------------
• [SLOW TEST] [15.034 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:10:16.279
    Jan 13 09:10:16.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename daemonsets 01/13/23 09:10:16.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:16.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:16.336
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan 13 09:10:16.395: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/13/23 09:10:16.418
    Jan 13 09:10:16.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:16.446: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/13/23 09:10:16.446
    Jan 13 09:10:16.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:16.513: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:17.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:17.519: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:18.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:18.525: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:19.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:10:19.522: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/13/23 09:10:19.529
    Jan 13 09:10:19.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:10:19.591: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 13 09:10:20.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:20.600: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/13/23 09:10:20.6
    Jan 13 09:10:20.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:20.627: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:21.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:21.634: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:22.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:22.674: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:23.662: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:23.662: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:24.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:24.640: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:25.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:25.635: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:10:26.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:10:26.636: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:10:26.676
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1837, will wait for the garbage collector to delete the pods 01/13/23 09:10:26.676
    Jan 13 09:10:26.799: INFO: Deleting DaemonSet.extensions daemon-set took: 56.641541ms
    Jan 13 09:10:27.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.347239ms
    Jan 13 09:10:31.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:10:31.212: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 13 09:10:31.219: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"573088"},"items":null}

    Jan 13 09:10:31.228: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"573088"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:10:31.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1837" for this suite. 01/13/23 09:10:31.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:10:31.314
Jan 13 09:10:31.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pod-network-test 01/13/23 09:10:31.321
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:31.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:31.384
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-559 01/13/23 09:10:31.392
STEP: creating a selector 01/13/23 09:10:31.393
STEP: Creating the service pods in kubernetes 01/13/23 09:10:31.393
Jan 13 09:10:31.393: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 09:10:31.489: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-559" to be "running and ready"
Jan 13 09:10:31.505: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.487566ms
Jan 13 09:10:31.505: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:10:33.540: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050916435s
Jan 13 09:10:33.540: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:10:35.526: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037156437s
Jan 13 09:10:35.526: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:37.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025040304s
Jan 13 09:10:37.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:39.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.028780459s
Jan 13 09:10:39.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:41.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024176848s
Jan 13 09:10:41.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:43.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025441038s
Jan 13 09:10:43.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:45.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.025759768s
Jan 13 09:10:45.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:47.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.028741756s
Jan 13 09:10:47.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:49.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025436984s
Jan 13 09:10:49.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:51.534: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04515733s
Jan 13 09:10:51.534: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 09:10:53.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.028388872s
Jan 13 09:10:53.517: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 13 09:10:53.517: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 13 09:10:53.528: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-559" to be "running and ready"
Jan 13 09:10:53.535: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.369736ms
Jan 13 09:10:53.535: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 13 09:10:53.535: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/13/23 09:10:53.542
Jan 13 09:10:53.554: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-559" to be "running"
Jan 13 09:10:53.569: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.716172ms
Jan 13 09:10:55.578: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023906766s
Jan 13 09:10:57.580: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025567368s
Jan 13 09:10:57.580: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 13 09:10:57.593: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 09:10:57.593: INFO: Breadth first check of 10.244.27.209 on host 10.10.102.31...
Jan 13 09:10:57.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.219:9080/dial?request=hostname&protocol=udp&host=10.244.27.209&port=8081&tries=1'] Namespace:pod-network-test-559 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 09:10:57.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 09:10:57.601: INFO: ExecWithOptions: Clientset creation
Jan 13 09:10:57.602: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-559/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.27.209%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 13 09:10:57.925: INFO: Waiting for responses: map[]
Jan 13 09:10:57.925: INFO: reached 10.244.27.209 after 0/1 tries
Jan 13 09:10:57.925: INFO: Breadth first check of 10.244.169.42 on host 10.10.102.32...
Jan 13 09:10:57.932: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.219:9080/dial?request=hostname&protocol=udp&host=10.244.169.42&port=8081&tries=1'] Namespace:pod-network-test-559 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 09:10:57.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 09:10:57.934: INFO: ExecWithOptions: Clientset creation
Jan 13 09:10:57.934: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-559/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.169.42%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 13 09:10:58.208: INFO: Waiting for responses: map[]
Jan 13 09:10:58.208: INFO: reached 10.244.169.42 after 0/1 tries
Jan 13 09:10:58.208: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 13 09:10:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-559" for this suite. 01/13/23 09:10:58.225
------------------------------
• [SLOW TEST] [26.925 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:10:31.314
    Jan 13 09:10:31.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pod-network-test 01/13/23 09:10:31.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:31.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:31.384
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-559 01/13/23 09:10:31.392
    STEP: creating a selector 01/13/23 09:10:31.393
    STEP: Creating the service pods in kubernetes 01/13/23 09:10:31.393
    Jan 13 09:10:31.393: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 13 09:10:31.489: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-559" to be "running and ready"
    Jan 13 09:10:31.505: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.487566ms
    Jan 13 09:10:31.505: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:10:33.540: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050916435s
    Jan 13 09:10:33.540: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:10:35.526: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037156437s
    Jan 13 09:10:35.526: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:37.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025040304s
    Jan 13 09:10:37.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:39.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.028780459s
    Jan 13 09:10:39.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:41.513: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024176848s
    Jan 13 09:10:41.513: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:43.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025441038s
    Jan 13 09:10:43.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:45.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.025759768s
    Jan 13 09:10:45.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:47.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.028741756s
    Jan 13 09:10:47.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:49.514: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025436984s
    Jan 13 09:10:49.514: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:51.534: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04515733s
    Jan 13 09:10:51.534: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 09:10:53.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.028388872s
    Jan 13 09:10:53.517: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 13 09:10:53.517: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 13 09:10:53.528: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-559" to be "running and ready"
    Jan 13 09:10:53.535: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.369736ms
    Jan 13 09:10:53.535: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 13 09:10:53.535: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/13/23 09:10:53.542
    Jan 13 09:10:53.554: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-559" to be "running"
    Jan 13 09:10:53.569: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.716172ms
    Jan 13 09:10:55.578: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023906766s
    Jan 13 09:10:57.580: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025567368s
    Jan 13 09:10:57.580: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 13 09:10:57.593: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 13 09:10:57.593: INFO: Breadth first check of 10.244.27.209 on host 10.10.102.31...
    Jan 13 09:10:57.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.219:9080/dial?request=hostname&protocol=udp&host=10.244.27.209&port=8081&tries=1'] Namespace:pod-network-test-559 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 09:10:57.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 09:10:57.601: INFO: ExecWithOptions: Clientset creation
    Jan 13 09:10:57.602: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-559/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.27.209%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 13 09:10:57.925: INFO: Waiting for responses: map[]
    Jan 13 09:10:57.925: INFO: reached 10.244.27.209 after 0/1 tries
    Jan 13 09:10:57.925: INFO: Breadth first check of 10.244.169.42 on host 10.10.102.32...
    Jan 13 09:10:57.932: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.219:9080/dial?request=hostname&protocol=udp&host=10.244.169.42&port=8081&tries=1'] Namespace:pod-network-test-559 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 09:10:57.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 09:10:57.934: INFO: ExecWithOptions: Clientset creation
    Jan 13 09:10:57.934: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-559/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.219%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.169.42%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 13 09:10:58.208: INFO: Waiting for responses: map[]
    Jan 13 09:10:58.208: INFO: reached 10.244.169.42 after 0/1 tries
    Jan 13 09:10:58.208: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:10:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-559" for this suite. 01/13/23 09:10:58.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:10:58.241
Jan 13 09:10:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 09:10:58.244
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:58.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:58.294
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 13 09:10:58.303: INFO: Creating deployment "webserver-deployment"
Jan 13 09:10:58.313: INFO: Waiting for observed generation 1
Jan 13 09:11:00.331: INFO: Waiting for all required pods to come up
Jan 13 09:11:00.355: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/13/23 09:11:00.355
Jan 13 09:11:00.355: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z4fhc" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.355: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5ck5r" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dqttn" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dwpxm" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fqxq5" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g2gqp" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jmjkq" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pkpl6" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-s4nbw" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.358: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wxp9h" in namespace "deployment-7666" to be "running"
Jan 13 09:11:00.379: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.970097ms
Jan 13 09:11:00.380: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r": Phase="Pending", Reason="", readiness=false. Elapsed: 24.312561ms
Jan 13 09:11:00.380: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.86563ms
Jan 13 09:11:00.380: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm": Phase="Pending", Reason="", readiness=false. Elapsed: 24.470984ms
Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp": Phase="Pending", Reason="", readiness=false. Elapsed: 24.13071ms
Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq": Phase="Pending", Reason="", readiness=false. Elapsed: 24.123128ms
Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.665995ms
Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn": Phase="Pending", Reason="", readiness=false. Elapsed: 25.332849ms
Jan 13 09:11:00.385: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h": Phase="Pending", Reason="", readiness=false. Elapsed: 27.944975ms
Jan 13 09:11:00.386: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw": Phase="Pending", Reason="", readiness=false. Elapsed: 28.450329ms
Jan 13 09:11:02.387: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032469162s
Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq": Phase="Running", Reason="", readiness=true. Elapsed: 2.04514861s
Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq" satisfied condition "running"
Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044463756s
Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045946352s
Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04660011s
Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047141075s
Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046986852s
Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046304506s
Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045948464s
Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047096856s
Jan 13 09:11:04.392: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn": Phase="Running", Reason="", readiness=true. Elapsed: 4.036703829s
Jan 13 09:11:04.392: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn" satisfied condition "running"
Jan 13 09:11:04.409: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 4.053946182s
Jan 13 09:11:04.409: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r" satisfied condition "running"
Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.053598271s
Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5" satisfied condition "running"
Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw": Phase="Running", Reason="", readiness=true. Elapsed: 4.052971089s
Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw" satisfied condition "running"
Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc": Phase="Running", Reason="", readiness=true. Elapsed: 4.055445892s
Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc" satisfied condition "running"
Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm": Phase="Running", Reason="", readiness=true. Elapsed: 4.054976877s
Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm" satisfied condition "running"
Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h": Phase="Running", Reason="", readiness=true. Elapsed: 4.053650272s
Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h" satisfied condition "running"
Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp": Phase="Running", Reason="", readiness=true. Elapsed: 4.054993313s
Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp" satisfied condition "running"
Jan 13 09:11:04.412: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6": Phase="Running", Reason="", readiness=true. Elapsed: 4.05506415s
Jan 13 09:11:04.412: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6" satisfied condition "running"
Jan 13 09:11:04.412: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 13 09:11:04.442: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 13 09:11:04.465: INFO: Updating deployment webserver-deployment
Jan 13 09:11:04.465: INFO: Waiting for observed generation 2
Jan 13 09:11:06.487: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 13 09:11:06.504: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 13 09:11:06.525: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 13 09:11:06.578: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 13 09:11:06.578: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 13 09:11:06.598: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 13 09:11:06.645: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 13 09:11:06.646: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 13 09:11:06.689: INFO: Updating deployment webserver-deployment
Jan 13 09:11:06.689: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 13 09:11:06.711: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 13 09:11:06.720: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 09:11:06.755: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7666  1d3b4177-dd13-458c-b58f-72b093c876a6 573429 3 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00387e858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 09:11:04 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-13 09:11:04 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 13 09:11:06.777: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-7666  9623132c-3935-4f76-a8ac-fcba750bd267 573433 3 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1d3b4177-dd13-458c-b58f-72b093c876a6 0xc004679b67 0xc004679b68}] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d3b4177-dd13-458c-b58f-72b093c876a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004679c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 09:11:06.777: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 13 09:11:06.778: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-7666  eaa5b168-8152-43a0-92fd-fd5441d8c9d6 573430 3 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1d3b4177-dd13-458c-b58f-72b093c876a6 0xc004679a77 0xc004679a78}] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d3b4177-dd13-458c-b58f-72b093c876a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004679b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 13 09:11:06.869: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5ck5r webserver-deployment-7f5969cbc7- deployment-7666  6758d638-be1f-4ec8-8424-620db79f7f77 573351 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9852a04f144736615878244c67b686f71bc983983a305b6447a55edf197f1326 cni.projectcalico.org/podIP:10.244.169.26/32 cni.projectcalico.org/podIPs:10.244.169.26/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a147 0xc003b0a148}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5tl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5tl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.26,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://d9de09855cdb4aee955802594caf734fb1650cdc7c7b37657625d55be83481da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.869: INFO: Pod "webserver-deployment-7f5969cbc7-bz9ff" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bz9ff webserver-deployment-7f5969cbc7- deployment-7666  474d6e71-e89b-4c10-853c-75c00796144f 573438 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a3b7 0xc003b0a3b8}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gszdq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gszdq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.870: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dqttn webserver-deployment-7f5969cbc7- deployment-7666  70a1baa5-7446-4c9c-b565-ea3d9651d719 573324 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3e67516a3a927554daadab4b17ecd434faf2304cbe69c7faf09e022f91557a8b cni.projectcalico.org/podIP:10.244.27.245/32 cni.projectcalico.org/podIPs:10.244.27.245/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a530 0xc003b0a531}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w76nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w76nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.245,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://97e1a6c772e9754deeef7e2972cca86c8f67f14d98c7e3df3fe77f80b101623e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.870: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dwpxm webserver-deployment-7f5969cbc7- deployment-7666  2385d97e-566f-4c24-9f62-b9909345468f 573315 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2ea137b59bb7cea9a3c705c1399a31ab68b097e6d7662746765ea9bbf710cb14 cni.projectcalico.org/podIP:10.244.169.47/32 cni.projectcalico.org/podIPs:10.244.169.47/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a777 0xc003b0a778}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzwjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzwjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.47,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://ea367f090788a4cf124b7441e59ff0b5d0fd49e24b9c42ced2a9b829c337c0b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.870: INFO: Pod "webserver-deployment-7f5969cbc7-fmdr8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fmdr8 webserver-deployment-7f5969cbc7- deployment-7666  3f4fe5e1-4ae2-4834-ae24-652f39332d40 573446 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a9c7 0xc003b0a9c8}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrs2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrs2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.871: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fqxq5 webserver-deployment-7f5969cbc7- deployment-7666  942ba5f0-8f09-4363-8368-e4a781039a0d 573342 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:76fce19352e505d23dcd34d97109ff921c554ec84fb1bfbc41b7f8ec44cabc19 cni.projectcalico.org/podIP:10.244.169.62/32 cni.projectcalico.org/podIPs:10.244.169.62/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0ab30 0xc003b0ab31}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dnn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dnn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.62,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://7b13bdb1436fbc904c5e749776b92f30e964204b82b0ae2c3e4f16371bb28042,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.871: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g2gqp webserver-deployment-7f5969cbc7- deployment-7666  30f45614-2197-4010-b64b-3249d901e0e1 573321 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ed029953ec517f83a94cdf61054f82ddde76c0e8f9dfdd0d9a1cf9dd30c469a8 cni.projectcalico.org/podIP:10.244.27.194/32 cni.projectcalico.org/podIPs:10.244.27.194/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0ad47 0xc003b0ad48}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rn8wf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rn8wf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.194,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://6c042e8ff777cfaeef621bc2e17735b3d00236a6d43747669fc8d9a4fd32ea22,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.871: INFO: Pod "webserver-deployment-7f5969cbc7-gxw9g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gxw9g webserver-deployment-7f5969cbc7- deployment-7666  3ebbdbfc-a498-4dc2-b7b0-a290718cf0c5 573442 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0af47 0xc003b0af48}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xn7hk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xn7hk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.872: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jmjkq webserver-deployment-7f5969cbc7- deployment-7666  165c2e69-b6a5-47c7-b1a9-d45d1b0f7434 573286 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fcc4f448a512370500a49877d9e59ceed855dd138d11de00a1aad549a72643eb cni.projectcalico.org/podIP:10.244.169.1/32 cni.projectcalico.org/podIPs:10.244.169.1/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b0c0 0xc003b0b0c1}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqv6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqv6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.1,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://d02ba3374d8eb61a718bb4e7c0447530aaae6b6882230c4a726317c85d0a19f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.873: INFO: Pod "webserver-deployment-7f5969cbc7-rq77k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rq77k webserver-deployment-7f5969cbc7- deployment-7666  e45866b8-4f85-4812-99c3-c3210a471cb9 573451 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b2b7 0xc003b0b2b8}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lq558,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lq558,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.873: INFO: Pod "webserver-deployment-7f5969cbc7-s2828" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s2828 webserver-deployment-7f5969cbc7- deployment-7666  b4ea9687-066b-4907-bd85-6c5c7c4d2e82 573447 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b400 0xc003b0b401}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lq7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lq7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.873: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s4nbw webserver-deployment-7f5969cbc7- deployment-7666  e6385fae-41f5-4029-9ed0-29f049ec2da5 573347 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c73bd04d0f67680cb69e74e4f86560ba58089a78980099178d58bb628362982e cni.projectcalico.org/podIP:10.244.169.4/32 cni.projectcalico.org/podIPs:10.244.169.4/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b570 0xc003b0b571}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59nhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59nhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.4,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://cbfb15ca1062923f4218935a38e871ea9df26a36d44d51a71d46f28321aeff70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.874: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wxp9h webserver-deployment-7f5969cbc7- deployment-7666  367b28a2-d24e-4383-b731-745dad0e71fa 573328 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d1797c12aaeb95bfab582b8fa034e1ee5999803f235376586520ce0331835c3c cni.projectcalico.org/podIP:10.244.27.255/32 cni.projectcalico.org/podIPs:10.244.27.255/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b787 0xc003b0b788}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-782rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-782rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.255,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://8d620bfaf7bf2eb926b908271138f336b2fa4c64b177dd03a906795fa41e2af8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.874: INFO: Pod "webserver-deployment-7f5969cbc7-x242v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x242v webserver-deployment-7f5969cbc7- deployment-7666  4f0cdbda-bbaf-4590-a100-48014f56b50b 573454 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b987 0xc003b0b988}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlct5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlct5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.874: INFO: Pod "webserver-deployment-7f5969cbc7-zpf4t" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zpf4t webserver-deployment-7f5969cbc7- deployment-7666  0af6a735-d870-4217-8b8a-0fdd45b0c4fb 573452 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0bad0 0xc003b0bad1}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvkrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvkrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.875: INFO: Pod "webserver-deployment-d9f79cb5-7hsft" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7hsft webserver-deployment-d9f79cb5- deployment-7666  7b116e32-e1c5-4088-8315-62d575ef66cb 573453 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc003b0bbff 0xc003b0bc10}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2nks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2nks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.877: INFO: Pod "webserver-deployment-d9f79cb5-dln5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dln5x webserver-deployment-d9f79cb5- deployment-7666  ed51d119-84f0-4ea0-9f5b-11fce6df1b37 573385 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc003b0bd4f 0xc003b0bd60}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fs7bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fs7bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.878: INFO: Pod "webserver-deployment-d9f79cb5-gdl76" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gdl76 webserver-deployment-d9f79cb5- deployment-7666  177b3c9f-a230-4c63-a51e-5c827deca75c 573415 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc003b0bf37 0xc003b0bf38}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkzbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkzbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.878: INFO: Pod "webserver-deployment-d9f79cb5-k28m4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k28m4 webserver-deployment-d9f79cb5- deployment-7666  852a9dc1-ee34-45aa-b369-2a239395d8d4 573393 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8127 0xc0037b8128}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzf2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzf2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.879: INFO: Pod "webserver-deployment-d9f79cb5-n84jb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-n84jb webserver-deployment-d9f79cb5- deployment-7666  fed658b5-6950-4441-b439-5e3d2376a89a 573394 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8317 0xc0037b8318}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cf2qv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cf2qv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.879: INFO: Pod "webserver-deployment-d9f79cb5-qzlmn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qzlmn webserver-deployment-d9f79cb5- deployment-7666  032303ca-12ad-4f28-bf8a-e48852495a51 573414 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8517 0xc0037b8518}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnm6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnm6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.879: INFO: Pod "webserver-deployment-d9f79cb5-snpw4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-snpw4 webserver-deployment-d9f79cb5- deployment-7666  fcf6c6f5-4e8f-473b-b020-254e61892e58 573455 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8707 0xc0037b8708}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hm59k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hm59k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:11:06.880: INFO: Pod "webserver-deployment-d9f79cb5-wn7d8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wn7d8 webserver-deployment-d9f79cb5- deployment-7666  602147b7-63e1-4be0-b644-920b86c43820 573445 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b884f 0xc0037b8860}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wchm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wchm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 09:11:06.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7666" for this suite. 01/13/23 09:11:06.894
------------------------------
• [SLOW TEST] [8.719 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:10:58.241
    Jan 13 09:10:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 09:10:58.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:10:58.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:10:58.294
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 13 09:10:58.303: INFO: Creating deployment "webserver-deployment"
    Jan 13 09:10:58.313: INFO: Waiting for observed generation 1
    Jan 13 09:11:00.331: INFO: Waiting for all required pods to come up
    Jan 13 09:11:00.355: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/13/23 09:11:00.355
    Jan 13 09:11:00.355: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z4fhc" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.355: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5ck5r" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dqttn" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dwpxm" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fqxq5" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-g2gqp" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jmjkq" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pkpl6" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-s4nbw" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.358: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wxp9h" in namespace "deployment-7666" to be "running"
    Jan 13 09:11:00.379: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.970097ms
    Jan 13 09:11:00.380: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r": Phase="Pending", Reason="", readiness=false. Elapsed: 24.312561ms
    Jan 13 09:11:00.380: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.86563ms
    Jan 13 09:11:00.380: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm": Phase="Pending", Reason="", readiness=false. Elapsed: 24.470984ms
    Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp": Phase="Pending", Reason="", readiness=false. Elapsed: 24.13071ms
    Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq": Phase="Pending", Reason="", readiness=false. Elapsed: 24.123128ms
    Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.665995ms
    Jan 13 09:11:00.381: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn": Phase="Pending", Reason="", readiness=false. Elapsed: 25.332849ms
    Jan 13 09:11:00.385: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h": Phase="Pending", Reason="", readiness=false. Elapsed: 27.944975ms
    Jan 13 09:11:00.386: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw": Phase="Pending", Reason="", readiness=false. Elapsed: 28.450329ms
    Jan 13 09:11:02.387: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032469162s
    Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq": Phase="Running", Reason="", readiness=true. Elapsed: 2.04514861s
    Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq" satisfied condition "running"
    Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044463756s
    Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045946352s
    Jan 13 09:11:02.402: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04660011s
    Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047141075s
    Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046986852s
    Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046304506s
    Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045948464s
    Jan 13 09:11:02.403: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047096856s
    Jan 13 09:11:04.392: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn": Phase="Running", Reason="", readiness=true. Elapsed: 4.036703829s
    Jan 13 09:11:04.392: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn" satisfied condition "running"
    Jan 13 09:11:04.409: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 4.053946182s
    Jan 13 09:11:04.409: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r" satisfied condition "running"
    Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.053598271s
    Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5" satisfied condition "running"
    Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw": Phase="Running", Reason="", readiness=true. Elapsed: 4.052971089s
    Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw" satisfied condition "running"
    Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc": Phase="Running", Reason="", readiness=true. Elapsed: 4.055445892s
    Jan 13 09:11:04.410: INFO: Pod "webserver-deployment-7f5969cbc7-z4fhc" satisfied condition "running"
    Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm": Phase="Running", Reason="", readiness=true. Elapsed: 4.054976877s
    Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm" satisfied condition "running"
    Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h": Phase="Running", Reason="", readiness=true. Elapsed: 4.053650272s
    Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h" satisfied condition "running"
    Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp": Phase="Running", Reason="", readiness=true. Elapsed: 4.054993313s
    Jan 13 09:11:04.411: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp" satisfied condition "running"
    Jan 13 09:11:04.412: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6": Phase="Running", Reason="", readiness=true. Elapsed: 4.05506415s
    Jan 13 09:11:04.412: INFO: Pod "webserver-deployment-7f5969cbc7-pkpl6" satisfied condition "running"
    Jan 13 09:11:04.412: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 13 09:11:04.442: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 13 09:11:04.465: INFO: Updating deployment webserver-deployment
    Jan 13 09:11:04.465: INFO: Waiting for observed generation 2
    Jan 13 09:11:06.487: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 13 09:11:06.504: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 13 09:11:06.525: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 13 09:11:06.578: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 13 09:11:06.578: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 13 09:11:06.598: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 13 09:11:06.645: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 13 09:11:06.646: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 13 09:11:06.689: INFO: Updating deployment webserver-deployment
    Jan 13 09:11:06.689: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 13 09:11:06.711: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 13 09:11:06.720: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 09:11:06.755: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7666  1d3b4177-dd13-458c-b58f-72b093c876a6 573429 3 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00387e858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 09:11:04 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-13 09:11:04 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 13 09:11:06.777: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-7666  9623132c-3935-4f76-a8ac-fcba750bd267 573433 3 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1d3b4177-dd13-458c-b58f-72b093c876a6 0xc004679b67 0xc004679b68}] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d3b4177-dd13-458c-b58f-72b093c876a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004679c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 09:11:06.777: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 13 09:11:06.778: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-7666  eaa5b168-8152-43a0-92fd-fd5441d8c9d6 573430 3 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1d3b4177-dd13-458c-b58f-72b093c876a6 0xc004679a77 0xc004679a78}] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d3b4177-dd13-458c-b58f-72b093c876a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004679b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 09:11:06.869: INFO: Pod "webserver-deployment-7f5969cbc7-5ck5r" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5ck5r webserver-deployment-7f5969cbc7- deployment-7666  6758d638-be1f-4ec8-8424-620db79f7f77 573351 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9852a04f144736615878244c67b686f71bc983983a305b6447a55edf197f1326 cni.projectcalico.org/podIP:10.244.169.26/32 cni.projectcalico.org/podIPs:10.244.169.26/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a147 0xc003b0a148}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5tl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5tl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.26,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://d9de09855cdb4aee955802594caf734fb1650cdc7c7b37657625d55be83481da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.869: INFO: Pod "webserver-deployment-7f5969cbc7-bz9ff" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bz9ff webserver-deployment-7f5969cbc7- deployment-7666  474d6e71-e89b-4c10-853c-75c00796144f 573438 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a3b7 0xc003b0a3b8}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gszdq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gszdq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.870: INFO: Pod "webserver-deployment-7f5969cbc7-dqttn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dqttn webserver-deployment-7f5969cbc7- deployment-7666  70a1baa5-7446-4c9c-b565-ea3d9651d719 573324 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3e67516a3a927554daadab4b17ecd434faf2304cbe69c7faf09e022f91557a8b cni.projectcalico.org/podIP:10.244.27.245/32 cni.projectcalico.org/podIPs:10.244.27.245/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a530 0xc003b0a531}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w76nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w76nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.245,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://97e1a6c772e9754deeef7e2972cca86c8f67f14d98c7e3df3fe77f80b101623e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.870: INFO: Pod "webserver-deployment-7f5969cbc7-dwpxm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dwpxm webserver-deployment-7f5969cbc7- deployment-7666  2385d97e-566f-4c24-9f62-b9909345468f 573315 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2ea137b59bb7cea9a3c705c1399a31ab68b097e6d7662746765ea9bbf710cb14 cni.projectcalico.org/podIP:10.244.169.47/32 cni.projectcalico.org/podIPs:10.244.169.47/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a777 0xc003b0a778}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzwjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzwjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.47,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://ea367f090788a4cf124b7441e59ff0b5d0fd49e24b9c42ced2a9b829c337c0b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.870: INFO: Pod "webserver-deployment-7f5969cbc7-fmdr8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fmdr8 webserver-deployment-7f5969cbc7- deployment-7666  3f4fe5e1-4ae2-4834-ae24-652f39332d40 573446 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0a9c7 0xc003b0a9c8}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrs2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrs2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.871: INFO: Pod "webserver-deployment-7f5969cbc7-fqxq5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fqxq5 webserver-deployment-7f5969cbc7- deployment-7666  942ba5f0-8f09-4363-8368-e4a781039a0d 573342 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:76fce19352e505d23dcd34d97109ff921c554ec84fb1bfbc41b7f8ec44cabc19 cni.projectcalico.org/podIP:10.244.169.62/32 cni.projectcalico.org/podIPs:10.244.169.62/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0ab30 0xc003b0ab31}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dnn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dnn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.62,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://7b13bdb1436fbc904c5e749776b92f30e964204b82b0ae2c3e4f16371bb28042,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.871: INFO: Pod "webserver-deployment-7f5969cbc7-g2gqp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-g2gqp webserver-deployment-7f5969cbc7- deployment-7666  30f45614-2197-4010-b64b-3249d901e0e1 573321 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ed029953ec517f83a94cdf61054f82ddde76c0e8f9dfdd0d9a1cf9dd30c469a8 cni.projectcalico.org/podIP:10.244.27.194/32 cni.projectcalico.org/podIPs:10.244.27.194/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0ad47 0xc003b0ad48}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rn8wf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rn8wf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.194,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://6c042e8ff777cfaeef621bc2e17735b3d00236a6d43747669fc8d9a4fd32ea22,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.871: INFO: Pod "webserver-deployment-7f5969cbc7-gxw9g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gxw9g webserver-deployment-7f5969cbc7- deployment-7666  3ebbdbfc-a498-4dc2-b7b0-a290718cf0c5 573442 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0af47 0xc003b0af48}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xn7hk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xn7hk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.872: INFO: Pod "webserver-deployment-7f5969cbc7-jmjkq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jmjkq webserver-deployment-7f5969cbc7- deployment-7666  165c2e69-b6a5-47c7-b1a9-d45d1b0f7434 573286 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fcc4f448a512370500a49877d9e59ceed855dd138d11de00a1aad549a72643eb cni.projectcalico.org/podIP:10.244.169.1/32 cni.projectcalico.org/podIPs:10.244.169.1/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b0c0 0xc003b0b0c1}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqv6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqv6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.1,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://d02ba3374d8eb61a718bb4e7c0447530aaae6b6882230c4a726317c85d0a19f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.873: INFO: Pod "webserver-deployment-7f5969cbc7-rq77k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rq77k webserver-deployment-7f5969cbc7- deployment-7666  e45866b8-4f85-4812-99c3-c3210a471cb9 573451 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b2b7 0xc003b0b2b8}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lq558,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lq558,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.873: INFO: Pod "webserver-deployment-7f5969cbc7-s2828" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s2828 webserver-deployment-7f5969cbc7- deployment-7666  b4ea9687-066b-4907-bd85-6c5c7c4d2e82 573447 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b400 0xc003b0b401}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lq7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lq7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.873: INFO: Pod "webserver-deployment-7f5969cbc7-s4nbw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s4nbw webserver-deployment-7f5969cbc7- deployment-7666  e6385fae-41f5-4029-9ed0-29f049ec2da5 573347 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c73bd04d0f67680cb69e74e4f86560ba58089a78980099178d58bb628362982e cni.projectcalico.org/podIP:10.244.169.4/32 cni.projectcalico.org/podIPs:10.244.169.4/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b570 0xc003b0b571}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59nhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59nhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.4,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://cbfb15ca1062923f4218935a38e871ea9df26a36d44d51a71d46f28321aeff70,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.874: INFO: Pod "webserver-deployment-7f5969cbc7-wxp9h" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wxp9h webserver-deployment-7f5969cbc7- deployment-7666  367b28a2-d24e-4383-b731-745dad0e71fa 573328 0 2023-01-13 09:10:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d1797c12aaeb95bfab582b8fa034e1ee5999803f235376586520ce0331835c3c cni.projectcalico.org/podIP:10.244.27.255/32 cni.projectcalico.org/podIPs:10.244.27.255/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b787 0xc003b0b788}] [] [{kube-controller-manager Update v1 2023-01-13 09:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:11:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:11:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-782rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-782rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.255,StartTime:2023-01-13 09:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:11:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://8d620bfaf7bf2eb926b908271138f336b2fa4c64b177dd03a906795fa41e2af8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.874: INFO: Pod "webserver-deployment-7f5969cbc7-x242v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x242v webserver-deployment-7f5969cbc7- deployment-7666  4f0cdbda-bbaf-4590-a100-48014f56b50b 573454 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0b987 0xc003b0b988}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlct5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlct5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.874: INFO: Pod "webserver-deployment-7f5969cbc7-zpf4t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zpf4t webserver-deployment-7f5969cbc7- deployment-7666  0af6a735-d870-4217-8b8a-0fdd45b0c4fb 573452 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 eaa5b168-8152-43a0-92fd-fd5441d8c9d6 0xc003b0bad0 0xc003b0bad1}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaa5b168-8152-43a0-92fd-fd5441d8c9d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvkrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvkrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.875: INFO: Pod "webserver-deployment-d9f79cb5-7hsft" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7hsft webserver-deployment-d9f79cb5- deployment-7666  7b116e32-e1c5-4088-8315-62d575ef66cb 573453 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc003b0bbff 0xc003b0bc10}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2nks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2nks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.877: INFO: Pod "webserver-deployment-d9f79cb5-dln5x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dln5x webserver-deployment-d9f79cb5- deployment-7666  ed51d119-84f0-4ea0-9f5b-11fce6df1b37 573385 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc003b0bd4f 0xc003b0bd60}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fs7bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fs7bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.878: INFO: Pod "webserver-deployment-d9f79cb5-gdl76" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gdl76 webserver-deployment-d9f79cb5- deployment-7666  177b3c9f-a230-4c63-a51e-5c827deca75c 573415 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc003b0bf37 0xc003b0bf38}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkzbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkzbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.878: INFO: Pod "webserver-deployment-d9f79cb5-k28m4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k28m4 webserver-deployment-d9f79cb5- deployment-7666  852a9dc1-ee34-45aa-b369-2a239395d8d4 573393 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8127 0xc0037b8128}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzf2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzf2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.879: INFO: Pod "webserver-deployment-d9f79cb5-n84jb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-n84jb webserver-deployment-d9f79cb5- deployment-7666  fed658b5-6950-4441-b439-5e3d2376a89a 573394 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8317 0xc0037b8318}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cf2qv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cf2qv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.879: INFO: Pod "webserver-deployment-d9f79cb5-qzlmn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qzlmn webserver-deployment-d9f79cb5- deployment-7666  032303ca-12ad-4f28-bf8a-e48852495a51 573414 0 2023-01-13 09:11:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8517 0xc0037b8518}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 09:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnm6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnm6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:,StartTime:2023-01-13 09:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.879: INFO: Pod "webserver-deployment-d9f79cb5-snpw4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-snpw4 webserver-deployment-d9f79cb5- deployment-7666  fcf6c6f5-4e8f-473b-b020-254e61892e58 573455 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b8707 0xc0037b8708}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hm59k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hm59k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:11:06.880: INFO: Pod "webserver-deployment-d9f79cb5-wn7d8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wn7d8 webserver-deployment-d9f79cb5- deployment-7666  602147b7-63e1-4be0-b644-920b86c43820 573445 0 2023-01-13 09:11:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 9623132c-3935-4f76-a8ac-fcba750bd267 0xc0037b884f 0xc0037b8860}] [] [{kube-controller-manager Update v1 2023-01-13 09:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9623132c-3935-4f76-a8ac-fcba750bd267\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wchm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wchm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:11:06.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7666" for this suite. 01/13/23 09:11:06.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:11:06.971
Jan 13 09:11:06.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 09:11:06.973
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:11:07.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:11:07.358
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/13/23 09:11:07.374
Jan 13 09:11:07.400: INFO: Waiting up to 5m0s for pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a" in namespace "emptydir-8715" to be "Succeeded or Failed"
Jan 13 09:11:07.426: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 25.483476ms
Jan 13 09:11:09.442: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040789713s
Jan 13 09:11:11.434: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033350366s
Jan 13 09:11:13.446: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044862301s
Jan 13 09:11:15.458: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056826295s
Jan 13 09:11:17.437: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036577171s
STEP: Saw pod success 01/13/23 09:11:17.437
Jan 13 09:11:17.441: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a" satisfied condition "Succeeded or Failed"
Jan 13 09:11:17.458: INFO: Trying to get logs from node 10.10.102.31-node pod pod-fe94dfec-5b51-402f-b09a-02709e6c540a container test-container: <nil>
STEP: delete the pod 01/13/23 09:11:17.499
Jan 13 09:11:17.553: INFO: Waiting for pod pod-fe94dfec-5b51-402f-b09a-02709e6c540a to disappear
Jan 13 09:11:17.567: INFO: Pod pod-fe94dfec-5b51-402f-b09a-02709e6c540a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:11:17.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8715" for this suite. 01/13/23 09:11:17.581
------------------------------
• [SLOW TEST] [10.620 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:11:06.971
    Jan 13 09:11:06.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 09:11:06.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:11:07.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:11:07.358
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/13/23 09:11:07.374
    Jan 13 09:11:07.400: INFO: Waiting up to 5m0s for pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a" in namespace "emptydir-8715" to be "Succeeded or Failed"
    Jan 13 09:11:07.426: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 25.483476ms
    Jan 13 09:11:09.442: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040789713s
    Jan 13 09:11:11.434: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033350366s
    Jan 13 09:11:13.446: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044862301s
    Jan 13 09:11:15.458: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056826295s
    Jan 13 09:11:17.437: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.036577171s
    STEP: Saw pod success 01/13/23 09:11:17.437
    Jan 13 09:11:17.441: INFO: Pod "pod-fe94dfec-5b51-402f-b09a-02709e6c540a" satisfied condition "Succeeded or Failed"
    Jan 13 09:11:17.458: INFO: Trying to get logs from node 10.10.102.31-node pod pod-fe94dfec-5b51-402f-b09a-02709e6c540a container test-container: <nil>
    STEP: delete the pod 01/13/23 09:11:17.499
    Jan 13 09:11:17.553: INFO: Waiting for pod pod-fe94dfec-5b51-402f-b09a-02709e6c540a to disappear
    Jan 13 09:11:17.567: INFO: Pod pod-fe94dfec-5b51-402f-b09a-02709e6c540a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:11:17.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8715" for this suite. 01/13/23 09:11:17.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:11:17.592
Jan 13 09:11:17.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 09:11:17.606
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:11:17.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:11:17.657
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/13/23 09:11:17.695
Jan 13 09:11:17.715: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165" in namespace "downward-api-2229" to be "Succeeded or Failed"
Jan 13 09:11:17.722: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 7.612863ms
Jan 13 09:11:19.736: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020782136s
Jan 13 09:11:21.734: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019516788s
Jan 13 09:11:23.737: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022179482s
Jan 13 09:11:25.730: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015664087s
Jan 13 09:11:27.738: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023365964s
STEP: Saw pod success 01/13/23 09:11:27.738
Jan 13 09:11:27.738: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165" satisfied condition "Succeeded or Failed"
Jan 13 09:11:27.753: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165 container client-container: <nil>
STEP: delete the pod 01/13/23 09:11:27.784
Jan 13 09:11:27.839: INFO: Waiting for pod downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165 to disappear
Jan 13 09:11:27.849: INFO: Pod downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 09:11:27.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2229" for this suite. 01/13/23 09:11:27.866
------------------------------
• [SLOW TEST] [10.287 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:11:17.592
    Jan 13 09:11:17.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 09:11:17.606
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:11:17.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:11:17.657
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/13/23 09:11:17.695
    Jan 13 09:11:17.715: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165" in namespace "downward-api-2229" to be "Succeeded or Failed"
    Jan 13 09:11:17.722: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 7.612863ms
    Jan 13 09:11:19.736: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020782136s
    Jan 13 09:11:21.734: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019516788s
    Jan 13 09:11:23.737: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022179482s
    Jan 13 09:11:25.730: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015664087s
    Jan 13 09:11:27.738: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023365964s
    STEP: Saw pod success 01/13/23 09:11:27.738
    Jan 13 09:11:27.738: INFO: Pod "downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165" satisfied condition "Succeeded or Failed"
    Jan 13 09:11:27.753: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165 container client-container: <nil>
    STEP: delete the pod 01/13/23 09:11:27.784
    Jan 13 09:11:27.839: INFO: Waiting for pod downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165 to disappear
    Jan 13 09:11:27.849: INFO: Pod downwardapi-volume-8322829f-77a5-4d9c-8ddb-fc883b408165 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:11:27.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2229" for this suite. 01/13/23 09:11:27.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:11:27.888
Jan 13 09:11:27.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename cronjob 01/13/23 09:11:27.891
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:11:27.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:11:27.97
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/13/23 09:11:27.979
STEP: Ensuring no jobs are scheduled 01/13/23 09:11:27.989
STEP: Ensuring no job exists by listing jobs explicitly 01/13/23 09:16:28.003
STEP: Removing cronjob 01/13/23 09:16:28.008
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 13 09:16:28.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3504" for this suite. 01/13/23 09:16:28.052
------------------------------
• [SLOW TEST] [300.179 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:11:27.888
    Jan 13 09:11:27.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename cronjob 01/13/23 09:11:27.891
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:11:27.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:11:27.97
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/13/23 09:11:27.979
    STEP: Ensuring no jobs are scheduled 01/13/23 09:11:27.989
    STEP: Ensuring no job exists by listing jobs explicitly 01/13/23 09:16:28.003
    STEP: Removing cronjob 01/13/23 09:16:28.008
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:16:28.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3504" for this suite. 01/13/23 09:16:28.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:16:28.07
Jan 13 09:16:28.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 09:16:28.075
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:16:28.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:16:28.116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 09:16:28.154
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:16:29.382
STEP: Deploying the webhook pod 01/13/23 09:16:29.4
STEP: Wait for the deployment to be ready 01/13/23 09:16:29.425
Jan 13 09:16:29.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 09:16:31.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 09:16:33.467
STEP: Verifying the service has paired with the endpoint 01/13/23 09:16:33.489
Jan 13 09:16:34.489: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/13/23 09:16:34.498
STEP: create a namespace for the webhook 01/13/23 09:16:34.534
STEP: create a configmap should be unconditionally rejected by the webhook 01/13/23 09:16:34.558
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:16:34.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1896" for this suite. 01/13/23 09:16:34.956
STEP: Destroying namespace "webhook-1896-markers" for this suite. 01/13/23 09:16:35.013
------------------------------
• [SLOW TEST] [6.992 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:16:28.07
    Jan 13 09:16:28.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 09:16:28.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:16:28.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:16:28.116
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 09:16:28.154
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:16:29.382
    STEP: Deploying the webhook pod 01/13/23 09:16:29.4
    STEP: Wait for the deployment to be ready 01/13/23 09:16:29.425
    Jan 13 09:16:29.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 09:16:31.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 16, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 09:16:33.467
    STEP: Verifying the service has paired with the endpoint 01/13/23 09:16:33.489
    Jan 13 09:16:34.489: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/13/23 09:16:34.498
    STEP: create a namespace for the webhook 01/13/23 09:16:34.534
    STEP: create a configmap should be unconditionally rejected by the webhook 01/13/23 09:16:34.558
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:16:34.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1896" for this suite. 01/13/23 09:16:34.956
    STEP: Destroying namespace "webhook-1896-markers" for this suite. 01/13/23 09:16:35.013
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:16:35.065
Jan 13 09:16:35.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 09:16:35.072
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:16:35.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:16:35.319
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-d1e9bd3e-a443-434f-a8a1-1ee4666f833e 01/13/23 09:16:35.333
STEP: Creating a pod to test consume secrets 01/13/23 09:16:35.361
Jan 13 09:16:35.404: INFO: Waiting up to 5m0s for pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c" in namespace "secrets-8054" to be "Succeeded or Failed"
Jan 13 09:16:35.487: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 82.366849ms
Jan 13 09:16:37.516: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111218308s
Jan 13 09:16:39.498: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093758703s
Jan 13 09:16:41.499: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094454803s
Jan 13 09:16:43.498: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.093809296s
STEP: Saw pod success 01/13/23 09:16:43.498
Jan 13 09:16:43.498: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c" satisfied condition "Succeeded or Failed"
Jan 13 09:16:43.509: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 09:16:43.564
Jan 13 09:16:43.595: INFO: Waiting for pod pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c to disappear
Jan 13 09:16:43.605: INFO: Pod pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 09:16:43.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8054" for this suite. 01/13/23 09:16:43.62
------------------------------
• [SLOW TEST] [8.569 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:16:35.065
    Jan 13 09:16:35.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 09:16:35.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:16:35.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:16:35.319
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-d1e9bd3e-a443-434f-a8a1-1ee4666f833e 01/13/23 09:16:35.333
    STEP: Creating a pod to test consume secrets 01/13/23 09:16:35.361
    Jan 13 09:16:35.404: INFO: Waiting up to 5m0s for pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c" in namespace "secrets-8054" to be "Succeeded or Failed"
    Jan 13 09:16:35.487: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 82.366849ms
    Jan 13 09:16:37.516: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111218308s
    Jan 13 09:16:39.498: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093758703s
    Jan 13 09:16:41.499: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094454803s
    Jan 13 09:16:43.498: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.093809296s
    STEP: Saw pod success 01/13/23 09:16:43.498
    Jan 13 09:16:43.498: INFO: Pod "pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c" satisfied condition "Succeeded or Failed"
    Jan 13 09:16:43.509: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 09:16:43.564
    Jan 13 09:16:43.595: INFO: Waiting for pod pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c to disappear
    Jan 13 09:16:43.605: INFO: Pod pod-secrets-b340946f-1a85-4637-ba0a-7ed53fbb8d4c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:16:43.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8054" for this suite. 01/13/23 09:16:43.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:16:43.635
Jan 13 09:16:43.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 09:16:43.638
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:16:43.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:16:43.777
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/13/23 09:16:43.798
Jan 13 09:16:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: mark a version not serverd 01/13/23 09:16:52.896
STEP: check the unserved version gets removed 01/13/23 09:16:52.939
STEP: check the other version is not changed 01/13/23 09:16:56.41
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:17:03.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-322" for this suite. 01/13/23 09:17:03.372
------------------------------
• [SLOW TEST] [19.779 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:16:43.635
    Jan 13 09:16:43.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 09:16:43.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:16:43.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:16:43.777
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/13/23 09:16:43.798
    Jan 13 09:16:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: mark a version not serverd 01/13/23 09:16:52.896
    STEP: check the unserved version gets removed 01/13/23 09:16:52.939
    STEP: check the other version is not changed 01/13/23 09:16:56.41
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:17:03.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-322" for this suite. 01/13/23 09:17:03.372
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:17:03.415
Jan 13 09:17:03.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 09:17:03.418
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:17:03.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:17:03.542
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/13/23 09:17:03.574
Jan 13 09:17:03.720: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8776  170a36e7-09f4-478c-a263-e4e212cdb61e 574508 0 2023-01-13 09:17:03 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-13 09:17:03 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6p7nj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6p7nj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 09:17:03.721: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8776" to be "running and ready"
Jan 13 09:17:03.730: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.977251ms
Jan 13 09:17:03.730: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:17:05.739: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018093544s
Jan 13 09:17:05.739: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:17:07.749: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.027468107s
Jan 13 09:17:07.749: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 13 09:17:07.749: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/13/23 09:17:07.749
Jan 13 09:17:07.749: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 09:17:07.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 09:17:07.751: INFO: ExecWithOptions: Clientset creation
Jan 13 09:17:07.751: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/13/23 09:17:07.997
Jan 13 09:17:07.997: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 09:17:07.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 09:17:07.998: INFO: ExecWithOptions: Clientset creation
Jan 13 09:17:07.998: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 13 09:17:08.227: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 09:17:08.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8776" for this suite. 01/13/23 09:17:08.269
------------------------------
• [4.866 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:17:03.415
    Jan 13 09:17:03.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 09:17:03.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:17:03.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:17:03.542
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/13/23 09:17:03.574
    Jan 13 09:17:03.720: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8776  170a36e7-09f4-478c-a263-e4e212cdb61e 574508 0 2023-01-13 09:17:03 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-13 09:17:03 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6p7nj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6p7nj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 09:17:03.721: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8776" to be "running and ready"
    Jan 13 09:17:03.730: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.977251ms
    Jan 13 09:17:03.730: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:17:05.739: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018093544s
    Jan 13 09:17:05.739: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:17:07.749: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.027468107s
    Jan 13 09:17:07.749: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 13 09:17:07.749: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/13/23 09:17:07.749
    Jan 13 09:17:07.749: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 09:17:07.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 09:17:07.751: INFO: ExecWithOptions: Clientset creation
    Jan 13 09:17:07.751: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/13/23 09:17:07.997
    Jan 13 09:17:07.997: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 09:17:07.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 09:17:07.998: INFO: ExecWithOptions: Clientset creation
    Jan 13 09:17:07.998: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 13 09:17:08.227: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:17:08.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8776" for this suite. 01/13/23 09:17:08.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:17:08.285
Jan 13 09:17:08.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-pred 01/13/23 09:17:08.288
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:17:08.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:17:08.372
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 13 09:17:08.381: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 09:17:08.413: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 09:17:08.443: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.31-node before test
Jan 13 09:17:08.496: INFO: calico-kube-controllers-7bdbfc669-wtbkz from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
Jan 13 09:17:08.496: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 13 09:17:08.496: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 09:17:08.496: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 09:17:08.496: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
Jan 13 09:17:08.496: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 09:17:08.496: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 09:17:08.496: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 09:17:08.496: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 09:17:08.496: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.32-node before test
Jan 13 09:17:08.574: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 09:17:08.574: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 09:17:08.574: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
Jan 13 09:17:08.574: INFO: 	Container coredns ready: true, restart count 0
Jan 13 09:17:08.574: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
Jan 13 09:17:08.574: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 09:17:08.574: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
Jan 13 09:17:08.574: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 09:17:08.574: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 09:17:08.575: INFO: 	Container e2e ready: true, restart count 0
Jan 13 09:17:08.575: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 09:17:08.575: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 09:17:08.575: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 09:17:08.575: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/13/23 09:17:08.576
Jan 13 09:17:08.713: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6011" to be "running"
Jan 13 09:17:08.732: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.855461ms
Jan 13 09:17:10.743: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030065762s
Jan 13 09:17:12.758: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.045392068s
Jan 13 09:17:12.758: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/13/23 09:17:12.768
STEP: Trying to apply a random label on the found node. 01/13/23 09:17:12.816
STEP: verifying the node has the label kubernetes.io/e2e-71ffb595-f52c-4b82-b494-a0222b71d908 95 01/13/23 09:17:12.847
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/13/23 09:17:12.874
Jan 13 09:17:12.924: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6011" to be "not pending"
Jan 13 09:17:12.936: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.727019ms
Jan 13 09:17:14.954: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030053191s
Jan 13 09:17:16.966: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.042496094s
Jan 13 09:17:16.966: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.10.102.31 on the node which pod4 resides and expect not scheduled 01/13/23 09:17:16.966
Jan 13 09:17:16.996: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6011" to be "not pending"
Jan 13 09:17:17.014: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.64026ms
Jan 13 09:17:19.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028083439s
Jan 13 09:17:21.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026745403s
Jan 13 09:17:23.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025234939s
Jan 13 09:17:25.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028871789s
Jan 13 09:17:27.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030415098s
Jan 13 09:17:29.019: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023062147s
Jan 13 09:17:31.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.028848353s
Jan 13 09:17:33.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026132735s
Jan 13 09:17:35.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.027172708s
Jan 13 09:17:37.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.032091577s
Jan 13 09:17:39.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024447746s
Jan 13 09:17:41.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.028935933s
Jan 13 09:17:43.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.030063445s
Jan 13 09:17:45.029: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.032293193s
Jan 13 09:17:47.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.034342267s
Jan 13 09:17:49.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.03673909s
Jan 13 09:17:51.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024412018s
Jan 13 09:17:53.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.024158783s
Jan 13 09:17:55.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.028017108s
Jan 13 09:17:57.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025432371s
Jan 13 09:17:59.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.023932928s
Jan 13 09:18:01.047: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.050464251s
Jan 13 09:18:03.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023248647s
Jan 13 09:18:05.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.029133376s
Jan 13 09:18:07.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02690815s
Jan 13 09:18:09.030: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.03405099s
Jan 13 09:18:11.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026425526s
Jan 13 09:18:13.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.028739356s
Jan 13 09:18:15.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.028507084s
Jan 13 09:18:17.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.026262542s
Jan 13 09:18:19.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.025187452s
Jan 13 09:18:21.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023523293s
Jan 13 09:18:23.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024977345s
Jan 13 09:18:25.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.035482656s
Jan 13 09:18:27.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025327762s
Jan 13 09:18:29.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.031151418s
Jan 13 09:18:31.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.036665714s
Jan 13 09:18:33.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.023280905s
Jan 13 09:18:35.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.030302839s
Jan 13 09:18:37.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.028667757s
Jan 13 09:18:39.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025833502s
Jan 13 09:18:41.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024800794s
Jan 13 09:18:43.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.026218354s
Jan 13 09:18:45.045: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.049129226s
Jan 13 09:18:47.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.026968809s
Jan 13 09:18:49.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.023893714s
Jan 13 09:18:51.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024812076s
Jan 13 09:18:53.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024043872s
Jan 13 09:18:55.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024485333s
Jan 13 09:18:57.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024843204s
Jan 13 09:18:59.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.036843386s
Jan 13 09:19:01.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025682847s
Jan 13 09:19:03.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.024335028s
Jan 13 09:19:05.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025210623s
Jan 13 09:19:07.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029978486s
Jan 13 09:19:09.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027693517s
Jan 13 09:19:11.030: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.033691223s
Jan 13 09:19:13.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025208774s
Jan 13 09:19:15.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029285414s
Jan 13 09:19:17.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.02565911s
Jan 13 09:19:19.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.023886947s
Jan 13 09:19:21.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02352756s
Jan 13 09:19:23.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025745956s
Jan 13 09:19:25.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.02661869s
Jan 13 09:19:27.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.026424592s
Jan 13 09:19:29.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.026425068s
Jan 13 09:19:31.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.027504334s
Jan 13 09:19:33.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.024545485s
Jan 13 09:19:35.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.030602518s
Jan 13 09:19:37.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.025176884s
Jan 13 09:19:39.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.023768073s
Jan 13 09:19:41.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.028554999s
Jan 13 09:19:43.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.031217168s
Jan 13 09:19:45.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.027088533s
Jan 13 09:19:47.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.025904521s
Jan 13 09:19:49.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.026806628s
Jan 13 09:19:51.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.025398596s
Jan 13 09:19:53.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.023874959s
Jan 13 09:19:55.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.02621722s
Jan 13 09:19:57.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.024404154s
Jan 13 09:19:59.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024486106s
Jan 13 09:20:01.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.029815471s
Jan 13 09:20:03.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.027084296s
Jan 13 09:20:05.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.027797451s
Jan 13 09:20:07.029: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.032751896s
Jan 13 09:20:09.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.026020013s
Jan 13 09:20:11.037: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.040290428s
Jan 13 09:20:13.030: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.03348088s
Jan 13 09:20:15.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.034768392s
Jan 13 09:20:17.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.028095718s
Jan 13 09:20:19.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026845377s
Jan 13 09:20:21.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.026186699s
Jan 13 09:20:23.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.029054068s
Jan 13 09:20:25.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.037482155s
Jan 13 09:20:27.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.024845538s
Jan 13 09:20:29.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.027890632s
Jan 13 09:20:31.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.032037432s
Jan 13 09:20:33.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.024406858s
Jan 13 09:20:35.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.023989734s
Jan 13 09:20:37.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.025881365s
Jan 13 09:20:39.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024387799s
Jan 13 09:20:41.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.057062448s
Jan 13 09:20:43.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.023702424s
Jan 13 09:20:45.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.02630655s
Jan 13 09:20:47.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.024955067s
Jan 13 09:20:49.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.034251848s
Jan 13 09:20:51.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.024004765s
Jan 13 09:20:53.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.026657861s
Jan 13 09:20:55.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.028235217s
Jan 13 09:20:57.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.035365204s
Jan 13 09:20:59.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.029407893s
Jan 13 09:21:01.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.104879895s
Jan 13 09:21:03.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.023609973s
Jan 13 09:21:05.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.036556199s
Jan 13 09:21:07.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.026968896s
Jan 13 09:21:09.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024011427s
Jan 13 09:21:11.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.030931906s
Jan 13 09:21:13.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.027649763s
Jan 13 09:21:15.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.024562275s
Jan 13 09:21:17.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.02627718s
Jan 13 09:21:19.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.024474566s
Jan 13 09:21:21.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.026398441s
Jan 13 09:21:23.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.024408906s
Jan 13 09:21:25.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.027451208s
Jan 13 09:21:27.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.026671564s
Jan 13 09:21:29.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.030105343s
Jan 13 09:21:31.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.031746187s
Jan 13 09:21:33.019: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.022794386s
Jan 13 09:21:35.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.02791504s
Jan 13 09:21:37.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.03011883s
Jan 13 09:21:39.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.030058747s
Jan 13 09:21:41.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.038857621s
Jan 13 09:21:43.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.025537434s
Jan 13 09:21:45.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.027082194s
Jan 13 09:21:47.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.024172715s
Jan 13 09:21:49.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.030428859s
Jan 13 09:21:51.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.027413248s
Jan 13 09:21:53.049: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.052783237s
Jan 13 09:21:55.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.027417209s
Jan 13 09:21:57.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.030052077s
Jan 13 09:21:59.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.028012397s
Jan 13 09:22:01.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.029933082s
Jan 13 09:22:03.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.025840772s
Jan 13 09:22:05.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.024195404s
Jan 13 09:22:07.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.02974558s
Jan 13 09:22:09.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.026400287s
Jan 13 09:22:11.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.023818416s
Jan 13 09:22:13.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.037081958s
Jan 13 09:22:15.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.034779797s
Jan 13 09:22:17.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027274486s
Jan 13 09:22:17.029: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.033074988s
STEP: removing the label kubernetes.io/e2e-71ffb595-f52c-4b82-b494-a0222b71d908 off the node 10.10.102.31-node 01/13/23 09:22:17.03
STEP: verifying the node doesn't have the label kubernetes.io/e2e-71ffb595-f52c-4b82-b494-a0222b71d908 01/13/23 09:22:17.062
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:22:17.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6011" for this suite. 01/13/23 09:22:17.095
------------------------------
• [SLOW TEST] [308.822 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:17:08.285
    Jan 13 09:17:08.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-pred 01/13/23 09:17:08.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:17:08.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:17:08.372
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 13 09:17:08.381: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 13 09:17:08.413: INFO: Waiting for terminating namespaces to be deleted...
    Jan 13 09:17:08.443: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.31-node before test
    Jan 13 09:17:08.496: INFO: calico-kube-controllers-7bdbfc669-wtbkz from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
    Jan 13 09:17:08.496: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 13 09:17:08.496: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 09:17:08.496: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 09:17:08.496: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
    Jan 13 09:17:08.496: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 09:17:08.496: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 09:17:08.496: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 09:17:08.496: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 13 09:17:08.496: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.32-node before test
    Jan 13 09:17:08.574: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 09:17:08.574: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 09:17:08.574: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
    Jan 13 09:17:08.574: INFO: 	Container coredns ready: true, restart count 0
    Jan 13 09:17:08.574: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
    Jan 13 09:17:08.574: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 09:17:08.574: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
    Jan 13 09:17:08.574: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 13 09:17:08.574: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 09:17:08.575: INFO: 	Container e2e ready: true, restart count 0
    Jan 13 09:17:08.575: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 09:17:08.575: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 09:17:08.575: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 09:17:08.575: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/13/23 09:17:08.576
    Jan 13 09:17:08.713: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6011" to be "running"
    Jan 13 09:17:08.732: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.855461ms
    Jan 13 09:17:10.743: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030065762s
    Jan 13 09:17:12.758: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.045392068s
    Jan 13 09:17:12.758: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/13/23 09:17:12.768
    STEP: Trying to apply a random label on the found node. 01/13/23 09:17:12.816
    STEP: verifying the node has the label kubernetes.io/e2e-71ffb595-f52c-4b82-b494-a0222b71d908 95 01/13/23 09:17:12.847
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/13/23 09:17:12.874
    Jan 13 09:17:12.924: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6011" to be "not pending"
    Jan 13 09:17:12.936: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.727019ms
    Jan 13 09:17:14.954: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030053191s
    Jan 13 09:17:16.966: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.042496094s
    Jan 13 09:17:16.966: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.10.102.31 on the node which pod4 resides and expect not scheduled 01/13/23 09:17:16.966
    Jan 13 09:17:16.996: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6011" to be "not pending"
    Jan 13 09:17:17.014: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.64026ms
    Jan 13 09:17:19.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028083439s
    Jan 13 09:17:21.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026745403s
    Jan 13 09:17:23.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025234939s
    Jan 13 09:17:25.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028871789s
    Jan 13 09:17:27.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030415098s
    Jan 13 09:17:29.019: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023062147s
    Jan 13 09:17:31.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.028848353s
    Jan 13 09:17:33.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026132735s
    Jan 13 09:17:35.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.027172708s
    Jan 13 09:17:37.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.032091577s
    Jan 13 09:17:39.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024447746s
    Jan 13 09:17:41.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.028935933s
    Jan 13 09:17:43.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.030063445s
    Jan 13 09:17:45.029: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.032293193s
    Jan 13 09:17:47.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.034342267s
    Jan 13 09:17:49.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.03673909s
    Jan 13 09:17:51.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024412018s
    Jan 13 09:17:53.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.024158783s
    Jan 13 09:17:55.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.028017108s
    Jan 13 09:17:57.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025432371s
    Jan 13 09:17:59.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.023932928s
    Jan 13 09:18:01.047: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.050464251s
    Jan 13 09:18:03.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023248647s
    Jan 13 09:18:05.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.029133376s
    Jan 13 09:18:07.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02690815s
    Jan 13 09:18:09.030: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.03405099s
    Jan 13 09:18:11.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026425526s
    Jan 13 09:18:13.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.028739356s
    Jan 13 09:18:15.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.028507084s
    Jan 13 09:18:17.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.026262542s
    Jan 13 09:18:19.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.025187452s
    Jan 13 09:18:21.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023523293s
    Jan 13 09:18:23.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024977345s
    Jan 13 09:18:25.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.035482656s
    Jan 13 09:18:27.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025327762s
    Jan 13 09:18:29.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.031151418s
    Jan 13 09:18:31.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.036665714s
    Jan 13 09:18:33.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.023280905s
    Jan 13 09:18:35.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.030302839s
    Jan 13 09:18:37.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.028667757s
    Jan 13 09:18:39.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025833502s
    Jan 13 09:18:41.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024800794s
    Jan 13 09:18:43.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.026218354s
    Jan 13 09:18:45.045: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.049129226s
    Jan 13 09:18:47.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.026968809s
    Jan 13 09:18:49.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.023893714s
    Jan 13 09:18:51.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024812076s
    Jan 13 09:18:53.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024043872s
    Jan 13 09:18:55.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024485333s
    Jan 13 09:18:57.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024843204s
    Jan 13 09:18:59.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.036843386s
    Jan 13 09:19:01.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025682847s
    Jan 13 09:19:03.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.024335028s
    Jan 13 09:19:05.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025210623s
    Jan 13 09:19:07.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029978486s
    Jan 13 09:19:09.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027693517s
    Jan 13 09:19:11.030: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.033691223s
    Jan 13 09:19:13.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025208774s
    Jan 13 09:19:15.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029285414s
    Jan 13 09:19:17.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.02565911s
    Jan 13 09:19:19.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.023886947s
    Jan 13 09:19:21.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02352756s
    Jan 13 09:19:23.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025745956s
    Jan 13 09:19:25.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.02661869s
    Jan 13 09:19:27.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.026424592s
    Jan 13 09:19:29.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.026425068s
    Jan 13 09:19:31.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.027504334s
    Jan 13 09:19:33.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.024545485s
    Jan 13 09:19:35.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.030602518s
    Jan 13 09:19:37.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.025176884s
    Jan 13 09:19:39.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.023768073s
    Jan 13 09:19:41.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.028554999s
    Jan 13 09:19:43.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.031217168s
    Jan 13 09:19:45.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.027088533s
    Jan 13 09:19:47.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.025904521s
    Jan 13 09:19:49.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.026806628s
    Jan 13 09:19:51.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.025398596s
    Jan 13 09:19:53.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.023874959s
    Jan 13 09:19:55.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.02621722s
    Jan 13 09:19:57.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.024404154s
    Jan 13 09:19:59.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024486106s
    Jan 13 09:20:01.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.029815471s
    Jan 13 09:20:03.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.027084296s
    Jan 13 09:20:05.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.027797451s
    Jan 13 09:20:07.029: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.032751896s
    Jan 13 09:20:09.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.026020013s
    Jan 13 09:20:11.037: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.040290428s
    Jan 13 09:20:13.030: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.03348088s
    Jan 13 09:20:15.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.034768392s
    Jan 13 09:20:17.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.028095718s
    Jan 13 09:20:19.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026845377s
    Jan 13 09:20:21.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.026186699s
    Jan 13 09:20:23.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.029054068s
    Jan 13 09:20:25.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.037482155s
    Jan 13 09:20:27.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.024845538s
    Jan 13 09:20:29.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.027890632s
    Jan 13 09:20:31.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.032037432s
    Jan 13 09:20:33.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.024406858s
    Jan 13 09:20:35.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.023989734s
    Jan 13 09:20:37.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.025881365s
    Jan 13 09:20:39.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024387799s
    Jan 13 09:20:41.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.057062448s
    Jan 13 09:20:43.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.023702424s
    Jan 13 09:20:45.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.02630655s
    Jan 13 09:20:47.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.024955067s
    Jan 13 09:20:49.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.034251848s
    Jan 13 09:20:51.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.024004765s
    Jan 13 09:20:53.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.026657861s
    Jan 13 09:20:55.025: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.028235217s
    Jan 13 09:20:57.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.035365204s
    Jan 13 09:20:59.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.029407893s
    Jan 13 09:21:01.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.104879895s
    Jan 13 09:21:03.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.023609973s
    Jan 13 09:21:05.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.036556199s
    Jan 13 09:21:07.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.026968896s
    Jan 13 09:21:09.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024011427s
    Jan 13 09:21:11.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.030931906s
    Jan 13 09:21:13.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.027649763s
    Jan 13 09:21:15.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.024562275s
    Jan 13 09:21:17.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.02627718s
    Jan 13 09:21:19.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.024474566s
    Jan 13 09:21:21.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.026398441s
    Jan 13 09:21:23.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.024408906s
    Jan 13 09:21:25.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.027451208s
    Jan 13 09:21:27.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.026671564s
    Jan 13 09:21:29.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.030105343s
    Jan 13 09:21:31.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.031746187s
    Jan 13 09:21:33.019: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.022794386s
    Jan 13 09:21:35.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.02791504s
    Jan 13 09:21:37.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.03011883s
    Jan 13 09:21:39.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.030058747s
    Jan 13 09:21:41.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.038857621s
    Jan 13 09:21:43.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.025537434s
    Jan 13 09:21:45.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.027082194s
    Jan 13 09:21:47.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.024172715s
    Jan 13 09:21:49.027: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.030428859s
    Jan 13 09:21:51.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.027413248s
    Jan 13 09:21:53.049: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.052783237s
    Jan 13 09:21:55.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.027417209s
    Jan 13 09:21:57.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.030052077s
    Jan 13 09:21:59.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.028012397s
    Jan 13 09:22:01.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.029933082s
    Jan 13 09:22:03.022: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.025840772s
    Jan 13 09:22:05.021: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.024195404s
    Jan 13 09:22:07.026: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.02974558s
    Jan 13 09:22:09.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.026400287s
    Jan 13 09:22:11.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.023818416s
    Jan 13 09:22:13.033: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.037081958s
    Jan 13 09:22:15.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.034779797s
    Jan 13 09:22:17.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027274486s
    Jan 13 09:22:17.029: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.033074988s
    STEP: removing the label kubernetes.io/e2e-71ffb595-f52c-4b82-b494-a0222b71d908 off the node 10.10.102.31-node 01/13/23 09:22:17.03
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-71ffb595-f52c-4b82-b494-a0222b71d908 01/13/23 09:22:17.062
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:22:17.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6011" for this suite. 01/13/23 09:22:17.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:22:17.11
Jan 13 09:22:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename disruption 01/13/23 09:22:17.113
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:17.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:17.175
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/13/23 09:22:17.19
STEP: Waiting for the pdb to be processed 01/13/23 09:22:17.203
STEP: First trying to evict a pod which shouldn't be evictable 01/13/23 09:22:19.243
STEP: Waiting for all pods to be running 01/13/23 09:22:19.244
Jan 13 09:22:19.253: INFO: pods: 0 < 3
Jan 13 09:22:21.266: INFO: running pods: 0 < 3
STEP: locating a running pod 01/13/23 09:22:23.263
STEP: Updating the pdb to allow a pod to be evicted 01/13/23 09:22:23.281
STEP: Waiting for the pdb to be processed 01/13/23 09:22:23.306
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/13/23 09:22:23.316
STEP: Waiting for all pods to be running 01/13/23 09:22:23.317
STEP: Waiting for the pdb to observed all healthy pods 01/13/23 09:22:23.324
STEP: Patching the pdb to disallow a pod to be evicted 01/13/23 09:22:23.42
STEP: Waiting for the pdb to be processed 01/13/23 09:22:23.454
STEP: Waiting for all pods to be running 01/13/23 09:22:25.477
Jan 13 09:22:25.501: INFO: running pods: 2 < 3
STEP: locating a running pod 01/13/23 09:22:27.515
STEP: Deleting the pdb to allow a pod to be evicted 01/13/23 09:22:27.535
STEP: Waiting for the pdb to be deleted 01/13/23 09:22:27.548
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/13/23 09:22:27.556
STEP: Waiting for all pods to be running 01/13/23 09:22:27.556
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 13 09:22:27.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-748" for this suite. 01/13/23 09:22:27.65
------------------------------
• [SLOW TEST] [10.571 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:22:17.11
    Jan 13 09:22:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename disruption 01/13/23 09:22:17.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:17.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:17.175
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/13/23 09:22:17.19
    STEP: Waiting for the pdb to be processed 01/13/23 09:22:17.203
    STEP: First trying to evict a pod which shouldn't be evictable 01/13/23 09:22:19.243
    STEP: Waiting for all pods to be running 01/13/23 09:22:19.244
    Jan 13 09:22:19.253: INFO: pods: 0 < 3
    Jan 13 09:22:21.266: INFO: running pods: 0 < 3
    STEP: locating a running pod 01/13/23 09:22:23.263
    STEP: Updating the pdb to allow a pod to be evicted 01/13/23 09:22:23.281
    STEP: Waiting for the pdb to be processed 01/13/23 09:22:23.306
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/13/23 09:22:23.316
    STEP: Waiting for all pods to be running 01/13/23 09:22:23.317
    STEP: Waiting for the pdb to observed all healthy pods 01/13/23 09:22:23.324
    STEP: Patching the pdb to disallow a pod to be evicted 01/13/23 09:22:23.42
    STEP: Waiting for the pdb to be processed 01/13/23 09:22:23.454
    STEP: Waiting for all pods to be running 01/13/23 09:22:25.477
    Jan 13 09:22:25.501: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/13/23 09:22:27.515
    STEP: Deleting the pdb to allow a pod to be evicted 01/13/23 09:22:27.535
    STEP: Waiting for the pdb to be deleted 01/13/23 09:22:27.548
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/13/23 09:22:27.556
    STEP: Waiting for all pods to be running 01/13/23 09:22:27.556
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:22:27.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-748" for this suite. 01/13/23 09:22:27.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:22:27.682
Jan 13 09:22:27.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename watch 01/13/23 09:22:27.686
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:27.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:27.784
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/13/23 09:22:27.798
STEP: modifying the configmap once 01/13/23 09:22:27.809
STEP: modifying the configmap a second time 01/13/23 09:22:27.83
STEP: deleting the configmap 01/13/23 09:22:27.881
STEP: creating a watch on configmaps from the resource version returned by the first update 01/13/23 09:22:27.911
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/13/23 09:22:27.914
Jan 13 09:22:27.916: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2846  0ff1732b-fe18-4f1b-b8fb-3297b5406ae4 575185 0 2023-01-13 09:22:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-13 09:22:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:22:27.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2846  0ff1732b-fe18-4f1b-b8fb-3297b5406ae4 575186 0 2023-01-13 09:22:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-13 09:22:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 13 09:22:27.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2846" for this suite. 01/13/23 09:22:27.935
------------------------------
• [0.291 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:22:27.682
    Jan 13 09:22:27.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename watch 01/13/23 09:22:27.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:27.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:27.784
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/13/23 09:22:27.798
    STEP: modifying the configmap once 01/13/23 09:22:27.809
    STEP: modifying the configmap a second time 01/13/23 09:22:27.83
    STEP: deleting the configmap 01/13/23 09:22:27.881
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/13/23 09:22:27.911
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/13/23 09:22:27.914
    Jan 13 09:22:27.916: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2846  0ff1732b-fe18-4f1b-b8fb-3297b5406ae4 575185 0 2023-01-13 09:22:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-13 09:22:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:22:27.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2846  0ff1732b-fe18-4f1b-b8fb-3297b5406ae4 575186 0 2023-01-13 09:22:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-13 09:22:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:22:27.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2846" for this suite. 01/13/23 09:22:27.935
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:22:27.974
Jan 13 09:22:27.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 09:22:27.977
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:28.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:28.068
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/13/23 09:22:28.135
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/13/23 09:22:28.219
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/13/23 09:22:28.219
STEP: creating a pod to probe DNS 01/13/23 09:22:28.219
STEP: submitting the pod to kubernetes 01/13/23 09:22:28.224
Jan 13 09:22:28.276: INFO: Waiting up to 15m0s for pod "dns-test-eee96605-818e-4958-8445-838b47fac711" in namespace "dns-3133" to be "running"
Jan 13 09:22:28.292: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Pending", Reason="", readiness=false. Elapsed: 15.968213ms
Jan 13 09:22:30.321: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044902327s
Jan 13 09:22:32.304: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027842396s
Jan 13 09:22:34.308: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Running", Reason="", readiness=true. Elapsed: 6.032460252s
Jan 13 09:22:34.309: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711" satisfied condition "running"
STEP: retrieving the pod 01/13/23 09:22:34.309
STEP: looking for the results for each expected name from probers 01/13/23 09:22:34.318
Jan 13 09:22:34.385: INFO: DNS probes using dns-3133/dns-test-eee96605-818e-4958-8445-838b47fac711 succeeded

STEP: deleting the pod 01/13/23 09:22:34.385
STEP: deleting the test headless service 01/13/23 09:22:34.407
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 09:22:34.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3133" for this suite. 01/13/23 09:22:34.461
------------------------------
• [SLOW TEST] [6.506 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:22:27.974
    Jan 13 09:22:27.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 09:22:27.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:28.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:28.068
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/13/23 09:22:28.135
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/13/23 09:22:28.219
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3133.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/13/23 09:22:28.219
    STEP: creating a pod to probe DNS 01/13/23 09:22:28.219
    STEP: submitting the pod to kubernetes 01/13/23 09:22:28.224
    Jan 13 09:22:28.276: INFO: Waiting up to 15m0s for pod "dns-test-eee96605-818e-4958-8445-838b47fac711" in namespace "dns-3133" to be "running"
    Jan 13 09:22:28.292: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Pending", Reason="", readiness=false. Elapsed: 15.968213ms
    Jan 13 09:22:30.321: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044902327s
    Jan 13 09:22:32.304: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027842396s
    Jan 13 09:22:34.308: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711": Phase="Running", Reason="", readiness=true. Elapsed: 6.032460252s
    Jan 13 09:22:34.309: INFO: Pod "dns-test-eee96605-818e-4958-8445-838b47fac711" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 09:22:34.309
    STEP: looking for the results for each expected name from probers 01/13/23 09:22:34.318
    Jan 13 09:22:34.385: INFO: DNS probes using dns-3133/dns-test-eee96605-818e-4958-8445-838b47fac711 succeeded

    STEP: deleting the pod 01/13/23 09:22:34.385
    STEP: deleting the test headless service 01/13/23 09:22:34.407
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:22:34.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3133" for this suite. 01/13/23 09:22:34.461
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:22:34.48
Jan 13 09:22:34.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename csiinlinevolumes 01/13/23 09:22:34.482
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:34.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:34.531
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/13/23 09:22:34.548
STEP: getting 01/13/23 09:22:34.595
STEP: listing 01/13/23 09:22:34.604
STEP: deleting 01/13/23 09:22:34.612
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:22:34.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-7267" for this suite. 01/13/23 09:22:34.652
------------------------------
• [0.198 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:22:34.48
    Jan 13 09:22:34.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename csiinlinevolumes 01/13/23 09:22:34.482
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:34.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:34.531
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/13/23 09:22:34.548
    STEP: getting 01/13/23 09:22:34.595
    STEP: listing 01/13/23 09:22:34.604
    STEP: deleting 01/13/23 09:22:34.612
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:22:34.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-7267" for this suite. 01/13/23 09:22:34.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:22:34.682
Jan 13 09:22:34.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 09:22:34.684
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:34.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:34.749
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/13/23 09:22:34.768
Jan 13 09:22:34.770: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1949 proxy --unix-socket=/tmp/kubectl-proxy-unix3605817797/test'
STEP: retrieving proxy /api/ output 01/13/23 09:22:34.97
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 09:22:34.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1949" for this suite. 01/13/23 09:22:34.989
------------------------------
• [0.322 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:22:34.682
    Jan 13 09:22:34.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 09:22:34.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:34.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:34.749
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/13/23 09:22:34.768
    Jan 13 09:22:34.770: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1949 proxy --unix-socket=/tmp/kubectl-proxy-unix3605817797/test'
    STEP: retrieving proxy /api/ output 01/13/23 09:22:34.97
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:22:34.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1949" for this suite. 01/13/23 09:22:34.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:22:35.006
Jan 13 09:22:35.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 09:22:35.01
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:35.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:35.041
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-828db238-2e53-4095-8388-9aa17bdc496e 01/13/23 09:22:35.072
STEP: Creating configMap with name cm-test-opt-upd-6e3dfa11-adde-43dd-95d7-5576c30b3288 01/13/23 09:22:35.082
STEP: Creating the pod 01/13/23 09:22:35.098
Jan 13 09:22:35.114: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf" in namespace "projected-4192" to be "running and ready"
Jan 13 09:22:35.123: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.361152ms
Jan 13 09:22:35.123: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:22:37.160: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045840393s
Jan 13 09:22:37.160: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:22:39.132: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017057162s
Jan 13 09:22:39.132: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:22:41.134: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Running", Reason="", readiness=true. Elapsed: 6.019209381s
Jan 13 09:22:41.134: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Running (Ready = true)
Jan 13 09:22:41.134: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-828db238-2e53-4095-8388-9aa17bdc496e 01/13/23 09:22:41.259
STEP: Updating configmap cm-test-opt-upd-6e3dfa11-adde-43dd-95d7-5576c30b3288 01/13/23 09:22:41.271
STEP: Creating configMap with name cm-test-opt-create-8166010e-229c-46b5-9179-e2ac0c79dca3 01/13/23 09:22:41.282
STEP: waiting to observe update in volume 01/13/23 09:22:41.291
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:10.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4192" for this suite. 01/13/23 09:24:10.999
------------------------------
• [SLOW TEST] [96.016 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:22:35.006
    Jan 13 09:22:35.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 09:22:35.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:22:35.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:22:35.041
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-828db238-2e53-4095-8388-9aa17bdc496e 01/13/23 09:22:35.072
    STEP: Creating configMap with name cm-test-opt-upd-6e3dfa11-adde-43dd-95d7-5576c30b3288 01/13/23 09:22:35.082
    STEP: Creating the pod 01/13/23 09:22:35.098
    Jan 13 09:22:35.114: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf" in namespace "projected-4192" to be "running and ready"
    Jan 13 09:22:35.123: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.361152ms
    Jan 13 09:22:35.123: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:22:37.160: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045840393s
    Jan 13 09:22:37.160: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:22:39.132: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017057162s
    Jan 13 09:22:39.132: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:22:41.134: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf": Phase="Running", Reason="", readiness=true. Elapsed: 6.019209381s
    Jan 13 09:22:41.134: INFO: The phase of Pod pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf is Running (Ready = true)
    Jan 13 09:22:41.134: INFO: Pod "pod-projected-configmaps-cb4f572e-e886-4254-84c0-a63e0e042faf" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-828db238-2e53-4095-8388-9aa17bdc496e 01/13/23 09:22:41.259
    STEP: Updating configmap cm-test-opt-upd-6e3dfa11-adde-43dd-95d7-5576c30b3288 01/13/23 09:22:41.271
    STEP: Creating configMap with name cm-test-opt-create-8166010e-229c-46b5-9179-e2ac0c79dca3 01/13/23 09:22:41.282
    STEP: waiting to observe update in volume 01/13/23 09:22:41.291
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:10.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4192" for this suite. 01/13/23 09:24:10.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:11.024
Jan 13 09:24:11.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 09:24:11.027
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:11.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:11.075
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 09:24:11.086
Jan 13 09:24:11.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8759 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 13 09:24:11.379: INFO: stderr: ""
Jan 13 09:24:11.379: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/13/23 09:24:11.379
Jan 13 09:24:11.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8759 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan 13 09:24:13.464: INFO: stderr: ""
Jan 13 09:24:13.464: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 09:24:13.464
Jan 13 09:24:13.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8759 delete pods e2e-test-httpd-pod'
Jan 13 09:24:17.879: INFO: stderr: ""
Jan 13 09:24:17.879: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:17.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8759" for this suite. 01/13/23 09:24:17.915
------------------------------
• [SLOW TEST] [6.914 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:11.024
    Jan 13 09:24:11.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 09:24:11.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:11.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:11.075
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 09:24:11.086
    Jan 13 09:24:11.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8759 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 13 09:24:11.379: INFO: stderr: ""
    Jan 13 09:24:11.379: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/13/23 09:24:11.379
    Jan 13 09:24:11.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8759 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan 13 09:24:13.464: INFO: stderr: ""
    Jan 13 09:24:13.464: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 09:24:13.464
    Jan 13 09:24:13.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8759 delete pods e2e-test-httpd-pod'
    Jan 13 09:24:17.879: INFO: stderr: ""
    Jan 13 09:24:17.879: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:17.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8759" for this suite. 01/13/23 09:24:17.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:17.939
Jan 13 09:24:17.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubelet-test 01/13/23 09:24:17.941
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:17.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:17.997
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:18.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4827" for this suite. 01/13/23 09:24:18.079
------------------------------
• [0.150 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:17.939
    Jan 13 09:24:17.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubelet-test 01/13/23 09:24:17.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:17.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:17.997
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:18.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4827" for this suite. 01/13/23 09:24:18.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:18.094
Jan 13 09:24:18.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename endpointslice 01/13/23 09:24:18.096
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:18.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:18.159
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 13 09:24:18.182: INFO: Endpoints addresses: [10.10.102.30] , ports: [6443]
Jan 13 09:24:18.182: INFO: EndpointSlices addresses: [10.10.102.30] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:18.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-789" for this suite. 01/13/23 09:24:18.189
------------------------------
• [0.106 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:18.094
    Jan 13 09:24:18.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename endpointslice 01/13/23 09:24:18.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:18.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:18.159
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 13 09:24:18.182: INFO: Endpoints addresses: [10.10.102.30] , ports: [6443]
    Jan 13 09:24:18.182: INFO: EndpointSlices addresses: [10.10.102.30] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:18.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-789" for this suite. 01/13/23 09:24:18.189
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:18.2
Jan 13 09:24:18.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename job 01/13/23 09:24:18.203
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:18.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:18.238
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/13/23 09:24:18.248
STEP: Ensuring active pods == parallelism 01/13/23 09:24:18.261
STEP: Orphaning one of the Job's Pods 01/13/23 09:24:24.28
Jan 13 09:24:24.831: INFO: Successfully updated pod "adopt-release-692c8"
STEP: Checking that the Job readopts the Pod 01/13/23 09:24:24.831
Jan 13 09:24:24.831: INFO: Waiting up to 15m0s for pod "adopt-release-692c8" in namespace "job-8407" to be "adopted"
Jan 13 09:24:24.843: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 12.232767ms
Jan 13 09:24:26.856: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.025344844s
Jan 13 09:24:26.857: INFO: Pod "adopt-release-692c8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/13/23 09:24:26.857
Jan 13 09:24:27.415: INFO: Successfully updated pod "adopt-release-692c8"
STEP: Checking that the Job releases the Pod 01/13/23 09:24:27.415
Jan 13 09:24:27.415: INFO: Waiting up to 15m0s for pod "adopt-release-692c8" in namespace "job-8407" to be "released"
Jan 13 09:24:27.431: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 15.569196ms
Jan 13 09:24:29.443: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.027905712s
Jan 13 09:24:29.444: INFO: Pod "adopt-release-692c8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:29.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8407" for this suite. 01/13/23 09:24:29.452
------------------------------
• [SLOW TEST] [11.262 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:18.2
    Jan 13 09:24:18.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename job 01/13/23 09:24:18.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:18.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:18.238
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/13/23 09:24:18.248
    STEP: Ensuring active pods == parallelism 01/13/23 09:24:18.261
    STEP: Orphaning one of the Job's Pods 01/13/23 09:24:24.28
    Jan 13 09:24:24.831: INFO: Successfully updated pod "adopt-release-692c8"
    STEP: Checking that the Job readopts the Pod 01/13/23 09:24:24.831
    Jan 13 09:24:24.831: INFO: Waiting up to 15m0s for pod "adopt-release-692c8" in namespace "job-8407" to be "adopted"
    Jan 13 09:24:24.843: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 12.232767ms
    Jan 13 09:24:26.856: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.025344844s
    Jan 13 09:24:26.857: INFO: Pod "adopt-release-692c8" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/13/23 09:24:26.857
    Jan 13 09:24:27.415: INFO: Successfully updated pod "adopt-release-692c8"
    STEP: Checking that the Job releases the Pod 01/13/23 09:24:27.415
    Jan 13 09:24:27.415: INFO: Waiting up to 15m0s for pod "adopt-release-692c8" in namespace "job-8407" to be "released"
    Jan 13 09:24:27.431: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 15.569196ms
    Jan 13 09:24:29.443: INFO: Pod "adopt-release-692c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.027905712s
    Jan 13 09:24:29.444: INFO: Pod "adopt-release-692c8" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:29.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8407" for this suite. 01/13/23 09:24:29.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:29.465
Jan 13 09:24:29.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubelet-test 01/13/23 09:24:29.473
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:29.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:29.52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 13 09:24:29.561: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f" in namespace "kubelet-test-5949" to be "running and ready"
Jan 13 09:24:29.572: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.223553ms
Jan 13 09:24:29.572: INFO: The phase of Pod busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:24:31.579: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017431637s
Jan 13 09:24:31.579: INFO: The phase of Pod busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:24:33.582: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.020144256s
Jan 13 09:24:33.582: INFO: The phase of Pod busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f is Running (Ready = true)
Jan 13 09:24:33.582: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:33.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5949" for this suite. 01/13/23 09:24:33.62
------------------------------
• [4.167 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:29.465
    Jan 13 09:24:29.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubelet-test 01/13/23 09:24:29.473
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:29.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:29.52
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 13 09:24:29.561: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f" in namespace "kubelet-test-5949" to be "running and ready"
    Jan 13 09:24:29.572: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.223553ms
    Jan 13 09:24:29.572: INFO: The phase of Pod busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:24:31.579: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017431637s
    Jan 13 09:24:31.579: INFO: The phase of Pod busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:24:33.582: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f": Phase="Running", Reason="", readiness=true. Elapsed: 4.020144256s
    Jan 13 09:24:33.582: INFO: The phase of Pod busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f is Running (Ready = true)
    Jan 13 09:24:33.582: INFO: Pod "busybox-readonly-fsc89313ef-8288-45e6-9a2f-66eea22ddb8f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:33.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5949" for this suite. 01/13/23 09:24:33.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:33.637
Jan 13 09:24:33.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:24:33.64
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:33.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:33.717
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-8dfd909b-1161-4b57-a57f-cafd7b052cbb 01/13/23 09:24:33.75
STEP: Creating a pod to test consume configMaps 01/13/23 09:24:33.792
Jan 13 09:24:33.842: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f" in namespace "configmap-6782" to be "Succeeded or Failed"
Jan 13 09:24:33.849: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698413ms
Jan 13 09:24:35.898: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056340042s
Jan 13 09:24:37.860: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017937876s
Jan 13 09:24:39.860: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018293006s
Jan 13 09:24:41.863: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021630783s
STEP: Saw pod success 01/13/23 09:24:41.864
Jan 13 09:24:41.864: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f" satisfied condition "Succeeded or Failed"
Jan 13 09:24:41.872: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f container agnhost-container: <nil>
STEP: delete the pod 01/13/23 09:24:41.904
Jan 13 09:24:41.956: INFO: Waiting for pod pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f to disappear
Jan 13 09:24:41.970: INFO: Pod pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:41.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6782" for this suite. 01/13/23 09:24:41.983
------------------------------
• [SLOW TEST] [8.365 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:33.637
    Jan 13 09:24:33.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:24:33.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:33.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:33.717
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-8dfd909b-1161-4b57-a57f-cafd7b052cbb 01/13/23 09:24:33.75
    STEP: Creating a pod to test consume configMaps 01/13/23 09:24:33.792
    Jan 13 09:24:33.842: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f" in namespace "configmap-6782" to be "Succeeded or Failed"
    Jan 13 09:24:33.849: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698413ms
    Jan 13 09:24:35.898: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056340042s
    Jan 13 09:24:37.860: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017937876s
    Jan 13 09:24:39.860: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018293006s
    Jan 13 09:24:41.863: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021630783s
    STEP: Saw pod success 01/13/23 09:24:41.864
    Jan 13 09:24:41.864: INFO: Pod "pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f" satisfied condition "Succeeded or Failed"
    Jan 13 09:24:41.872: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 09:24:41.904
    Jan 13 09:24:41.956: INFO: Waiting for pod pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f to disappear
    Jan 13 09:24:41.970: INFO: Pod pod-configmaps-4a954a66-73ce-4b7d-ba71-4b104e7c3b8f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:41.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6782" for this suite. 01/13/23 09:24:41.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:42.004
Jan 13 09:24:42.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 09:24:42.006
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:42.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:42.092
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9088 01/13/23 09:24:42.104
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/13/23 09:24:42.138
STEP: creating service externalsvc in namespace services-9088 01/13/23 09:24:42.138
STEP: creating replication controller externalsvc in namespace services-9088 01/13/23 09:24:42.181
I0113 09:24:42.196965      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9088, replica count: 2
I0113 09:24:45.248067      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 09:24:48.248281      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/13/23 09:24:48.266
Jan 13 09:24:48.320: INFO: Creating new exec pod
Jan 13 09:24:48.336: INFO: Waiting up to 5m0s for pod "execpodsk7rc" in namespace "services-9088" to be "running"
Jan 13 09:24:48.354: INFO: Pod "execpodsk7rc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.236819ms
Jan 13 09:24:50.376: INFO: Pod "execpodsk7rc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039872664s
Jan 13 09:24:52.361: INFO: Pod "execpodsk7rc": Phase="Running", Reason="", readiness=true. Elapsed: 4.024967318s
Jan 13 09:24:52.361: INFO: Pod "execpodsk7rc" satisfied condition "running"
Jan 13 09:24:52.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-9088 exec execpodsk7rc -- /bin/sh -x -c nslookup nodeport-service.services-9088.svc.cluster.local'
Jan 13 09:24:53.131: INFO: stderr: "+ nslookup nodeport-service.services-9088.svc.cluster.local\n"
Jan 13 09:24:53.131: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9088.svc.cluster.local\tcanonical name = externalsvc.services-9088.svc.cluster.local.\nName:\texternalsvc.services-9088.svc.cluster.local\nAddress: 10.102.204.158\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9088, will wait for the garbage collector to delete the pods 01/13/23 09:24:53.132
Jan 13 09:24:53.203: INFO: Deleting ReplicationController externalsvc took: 14.690121ms
Jan 13 09:24:53.304: INFO: Terminating ReplicationController externalsvc pods took: 101.225029ms
Jan 13 09:24:57.010: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 09:24:57.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9088" for this suite. 01/13/23 09:24:57.194
------------------------------
• [SLOW TEST] [15.206 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:42.004
    Jan 13 09:24:42.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 09:24:42.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:42.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:42.092
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9088 01/13/23 09:24:42.104
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/13/23 09:24:42.138
    STEP: creating service externalsvc in namespace services-9088 01/13/23 09:24:42.138
    STEP: creating replication controller externalsvc in namespace services-9088 01/13/23 09:24:42.181
    I0113 09:24:42.196965      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9088, replica count: 2
    I0113 09:24:45.248067      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 09:24:48.248281      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/13/23 09:24:48.266
    Jan 13 09:24:48.320: INFO: Creating new exec pod
    Jan 13 09:24:48.336: INFO: Waiting up to 5m0s for pod "execpodsk7rc" in namespace "services-9088" to be "running"
    Jan 13 09:24:48.354: INFO: Pod "execpodsk7rc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.236819ms
    Jan 13 09:24:50.376: INFO: Pod "execpodsk7rc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039872664s
    Jan 13 09:24:52.361: INFO: Pod "execpodsk7rc": Phase="Running", Reason="", readiness=true. Elapsed: 4.024967318s
    Jan 13 09:24:52.361: INFO: Pod "execpodsk7rc" satisfied condition "running"
    Jan 13 09:24:52.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-9088 exec execpodsk7rc -- /bin/sh -x -c nslookup nodeport-service.services-9088.svc.cluster.local'
    Jan 13 09:24:53.131: INFO: stderr: "+ nslookup nodeport-service.services-9088.svc.cluster.local\n"
    Jan 13 09:24:53.131: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9088.svc.cluster.local\tcanonical name = externalsvc.services-9088.svc.cluster.local.\nName:\texternalsvc.services-9088.svc.cluster.local\nAddress: 10.102.204.158\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9088, will wait for the garbage collector to delete the pods 01/13/23 09:24:53.132
    Jan 13 09:24:53.203: INFO: Deleting ReplicationController externalsvc took: 14.690121ms
    Jan 13 09:24:53.304: INFO: Terminating ReplicationController externalsvc pods took: 101.225029ms
    Jan 13 09:24:57.010: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:24:57.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9088" for this suite. 01/13/23 09:24:57.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:24:57.215
Jan 13 09:24:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename security-context-test 01/13/23 09:24:57.218
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:57.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:57.402
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 13 09:24:57.442: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1" in namespace "security-context-test-8248" to be "Succeeded or Failed"
Jan 13 09:24:57.520: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Pending", Reason="", readiness=false. Elapsed: 77.546002ms
Jan 13 09:24:59.527: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085010168s
Jan 13 09:25:01.531: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088902595s
Jan 13 09:25:03.541: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098882354s
Jan 13 09:25:03.541: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1" satisfied condition "Succeeded or Failed"
Jan 13 09:25:03.606: INFO: Got logs for pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 13 09:25:03.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8248" for this suite. 01/13/23 09:25:03.615
------------------------------
• [SLOW TEST] [6.413 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:24:57.215
    Jan 13 09:24:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename security-context-test 01/13/23 09:24:57.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:24:57.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:24:57.402
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 13 09:24:57.442: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1" in namespace "security-context-test-8248" to be "Succeeded or Failed"
    Jan 13 09:24:57.520: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Pending", Reason="", readiness=false. Elapsed: 77.546002ms
    Jan 13 09:24:59.527: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085010168s
    Jan 13 09:25:01.531: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088902595s
    Jan 13 09:25:03.541: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098882354s
    Jan 13 09:25:03.541: INFO: Pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1" satisfied condition "Succeeded or Failed"
    Jan 13 09:25:03.606: INFO: Got logs for pod "busybox-privileged-false-3b41fc5a-7095-41fc-a1a8-c3e13a6fd1a1": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:25:03.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8248" for this suite. 01/13/23 09:25:03.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:25:03.63
Jan 13 09:25:03.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 09:25:03.633
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:03.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:03.68
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/13/23 09:25:03.688
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6401;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6401;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +notcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_tcp@PTR;sleep 1; done
 01/13/23 09:25:03.714
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6401;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6401;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +notcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_tcp@PTR;sleep 1; done
 01/13/23 09:25:03.715
STEP: creating a pod to probe DNS 01/13/23 09:25:03.715
STEP: submitting the pod to kubernetes 01/13/23 09:25:03.715
Jan 13 09:25:03.736: INFO: Waiting up to 15m0s for pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a" in namespace "dns-6401" to be "running"
Jan 13 09:25:03.753: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.385357ms
Jan 13 09:25:05.777: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040716372s
Jan 13 09:25:07.763: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02706803s
Jan 13 09:25:09.768: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032224686s
Jan 13 09:25:11.764: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Running", Reason="", readiness=true. Elapsed: 8.028469583s
Jan 13 09:25:11.764: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a" satisfied condition "running"
STEP: retrieving the pod 01/13/23 09:25:11.764
STEP: looking for the results for each expected name from probers 01/13/23 09:25:11.773
Jan 13 09:25:11.787: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.799: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.808: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.872: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.896: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.916: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.924: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.934: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.986: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:11.998: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:12.006: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:12.015: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:12.024: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:12.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:12.042: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:12.052: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:12.116: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

Jan 13 09:25:17.130: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.145: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.179: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.189: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.201: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.212: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.221: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.276: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.290: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.327: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.410: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.458: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.500: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.543: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:17.655: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

Jan 13 09:25:22.132: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.143: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.155: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.194: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.233: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.251: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.265: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.315: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.324: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.333: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.355: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.380: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.390: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.411: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:22.518: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

Jan 13 09:25:27.164: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.175: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.200: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.211: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.220: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.229: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.247: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.362: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.371: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.384: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.398: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.413: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.434: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.449: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.458: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:27.505: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

Jan 13 09:25:32.128: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.139: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.150: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.185: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.211: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.219: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.261: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.267: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.275: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.283: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.295: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.338: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.357: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:32.446: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

Jan 13 09:25:37.133: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.152: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.198: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.210: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.245: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.257: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.368: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.383: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.403: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.430: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.441: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.463: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.473: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.480: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
Jan 13 09:25:37.561: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

Jan 13 09:25:42.586: INFO: DNS probes using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a succeeded

STEP: deleting the pod 01/13/23 09:25:42.586
STEP: deleting the test service 01/13/23 09:25:42.694
STEP: deleting the test headless service 01/13/23 09:25:42.764
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 09:25:42.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6401" for this suite. 01/13/23 09:25:42.826
------------------------------
• [SLOW TEST] [39.222 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:25:03.63
    Jan 13 09:25:03.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 09:25:03.633
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:03.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:03.68
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/13/23 09:25:03.688
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6401;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6401;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +notcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_tcp@PTR;sleep 1; done
     01/13/23 09:25:03.714
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6401;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6401;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6401.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6401.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6401.svc;check="$$(dig +notcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.71.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.71.181_tcp@PTR;sleep 1; done
     01/13/23 09:25:03.715
    STEP: creating a pod to probe DNS 01/13/23 09:25:03.715
    STEP: submitting the pod to kubernetes 01/13/23 09:25:03.715
    Jan 13 09:25:03.736: INFO: Waiting up to 15m0s for pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a" in namespace "dns-6401" to be "running"
    Jan 13 09:25:03.753: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.385357ms
    Jan 13 09:25:05.777: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040716372s
    Jan 13 09:25:07.763: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02706803s
    Jan 13 09:25:09.768: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032224686s
    Jan 13 09:25:11.764: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a": Phase="Running", Reason="", readiness=true. Elapsed: 8.028469583s
    Jan 13 09:25:11.764: INFO: Pod "dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 09:25:11.764
    STEP: looking for the results for each expected name from probers 01/13/23 09:25:11.773
    Jan 13 09:25:11.787: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.799: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.808: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.872: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.896: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.916: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.924: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.934: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.986: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:11.998: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:12.006: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:12.015: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:12.024: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:12.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:12.042: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:12.052: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:12.116: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

    Jan 13 09:25:17.130: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.145: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.179: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.189: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.201: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.212: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.221: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.276: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.290: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.327: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.410: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.458: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.500: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.543: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:17.655: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

    Jan 13 09:25:22.132: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.143: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.155: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.194: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.233: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.251: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.265: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.315: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.324: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.333: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.355: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.380: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.390: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.411: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:22.518: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

    Jan 13 09:25:27.164: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.175: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.200: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.211: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.220: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.229: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.247: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.362: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.371: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.384: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.398: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.413: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.434: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.449: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.458: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:27.505: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

    Jan 13 09:25:32.128: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.139: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.150: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.174: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.185: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.211: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.219: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.261: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.267: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.275: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.283: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.295: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.338: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.357: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:32.446: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

    Jan 13 09:25:37.133: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.152: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.198: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.210: INFO: Unable to read wheezy_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.245: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.257: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.368: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.383: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.403: INFO: Unable to read jessie_udp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.430: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401 from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.441: INFO: Unable to read jessie_udp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.463: INFO: Unable to read jessie_tcp@dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.473: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.480: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc from pod dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a: the server could not find the requested resource (get pods dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a)
    Jan 13 09:25:37.561: INFO: Lookups using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6401 wheezy_tcp@dns-test-service.dns-6401 wheezy_udp@dns-test-service.dns-6401.svc wheezy_tcp@dns-test-service.dns-6401.svc wheezy_udp@_http._tcp.dns-test-service.dns-6401.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6401.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6401 jessie_tcp@dns-test-service.dns-6401 jessie_udp@dns-test-service.dns-6401.svc jessie_tcp@dns-test-service.dns-6401.svc jessie_udp@_http._tcp.dns-test-service.dns-6401.svc jessie_tcp@_http._tcp.dns-test-service.dns-6401.svc]

    Jan 13 09:25:42.586: INFO: DNS probes using dns-6401/dns-test-2d3cdcb6-afa7-4aa5-8daf-2365312c1d8a succeeded

    STEP: deleting the pod 01/13/23 09:25:42.586
    STEP: deleting the test service 01/13/23 09:25:42.694
    STEP: deleting the test headless service 01/13/23 09:25:42.764
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:25:42.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6401" for this suite. 01/13/23 09:25:42.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:25:42.859
Jan 13 09:25:42.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replication-controller 01/13/23 09:25:42.862
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:42.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:42.926
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-9clzs" 01/13/23 09:25:42.939
Jan 13 09:25:42.953: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
Jan 13 09:25:43.967: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
Jan 13 09:25:43.981: INFO: Found 1 replicas for "e2e-rc-9clzs" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-9clzs" 01/13/23 09:25:43.981
STEP: Updating a scale subresource 01/13/23 09:25:43.988
STEP: Verifying replicas where modified for replication controller "e2e-rc-9clzs" 01/13/23 09:25:43.996
Jan 13 09:25:43.996: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
Jan 13 09:25:45.008: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
Jan 13 09:25:45.020: INFO: Found 2 replicas for "e2e-rc-9clzs" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 13 09:25:45.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3224" for this suite. 01/13/23 09:25:45.031
------------------------------
• [2.184 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:25:42.859
    Jan 13 09:25:42.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replication-controller 01/13/23 09:25:42.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:42.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:42.926
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-9clzs" 01/13/23 09:25:42.939
    Jan 13 09:25:42.953: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
    Jan 13 09:25:43.967: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
    Jan 13 09:25:43.981: INFO: Found 1 replicas for "e2e-rc-9clzs" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-9clzs" 01/13/23 09:25:43.981
    STEP: Updating a scale subresource 01/13/23 09:25:43.988
    STEP: Verifying replicas where modified for replication controller "e2e-rc-9clzs" 01/13/23 09:25:43.996
    Jan 13 09:25:43.996: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
    Jan 13 09:25:45.008: INFO: Get Replication Controller "e2e-rc-9clzs" to confirm replicas
    Jan 13 09:25:45.020: INFO: Found 2 replicas for "e2e-rc-9clzs" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:25:45.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3224" for this suite. 01/13/23 09:25:45.031
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:25:45.044
Jan 13 09:25:45.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename security-context-test 01/13/23 09:25:45.046
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:45.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:45.126
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 13 09:25:45.165: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe" in namespace "security-context-test-6414" to be "Succeeded or Failed"
Jan 13 09:25:45.174: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.69311ms
Jan 13 09:25:47.190: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024444917s
Jan 13 09:25:49.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016648126s
Jan 13 09:25:51.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016402365s
Jan 13 09:25:53.186: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020484247s
Jan 13 09:25:55.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.017219889s
Jan 13 09:25:55.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 13 09:25:55.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6414" for this suite. 01/13/23 09:25:55.194
------------------------------
• [SLOW TEST] [10.173 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:25:45.044
    Jan 13 09:25:45.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename security-context-test 01/13/23 09:25:45.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:45.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:45.126
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 13 09:25:45.165: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe" in namespace "security-context-test-6414" to be "Succeeded or Failed"
    Jan 13 09:25:45.174: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.69311ms
    Jan 13 09:25:47.190: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024444917s
    Jan 13 09:25:49.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016648126s
    Jan 13 09:25:51.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016402365s
    Jan 13 09:25:53.186: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020484247s
    Jan 13 09:25:55.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.017219889s
    Jan 13 09:25:55.182: INFO: Pod "busybox-readonly-false-035ee2f8-cc88-45e3-8e2b-8caa770b55fe" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:25:55.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6414" for this suite. 01/13/23 09:25:55.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:25:55.226
Jan 13 09:25:55.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 09:25:55.229
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:55.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:55.264
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/13/23 09:25:55.272
Jan 13 09:25:55.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8" in namespace "downward-api-6128" to be "Succeeded or Failed"
Jan 13 09:25:55.305: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10187ms
Jan 13 09:25:57.318: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021500865s
Jan 13 09:25:59.311: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014071912s
Jan 13 09:26:01.318: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021042246s
Jan 13 09:26:03.324: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027951047s
STEP: Saw pod success 01/13/23 09:26:03.324
Jan 13 09:26:03.325: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8" satisfied condition "Succeeded or Failed"
Jan 13 09:26:03.347: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8 container client-container: <nil>
STEP: delete the pod 01/13/23 09:26:03.393
Jan 13 09:26:03.465: INFO: Waiting for pod downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8 to disappear
Jan 13 09:26:03.493: INFO: Pod downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:03.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6128" for this suite. 01/13/23 09:26:03.513
------------------------------
• [SLOW TEST] [8.322 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:25:55.226
    Jan 13 09:25:55.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 09:25:55.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:25:55.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:25:55.264
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/13/23 09:25:55.272
    Jan 13 09:25:55.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8" in namespace "downward-api-6128" to be "Succeeded or Failed"
    Jan 13 09:25:55.305: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10187ms
    Jan 13 09:25:57.318: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021500865s
    Jan 13 09:25:59.311: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014071912s
    Jan 13 09:26:01.318: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021042246s
    Jan 13 09:26:03.324: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027951047s
    STEP: Saw pod success 01/13/23 09:26:03.324
    Jan 13 09:26:03.325: INFO: Pod "downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8" satisfied condition "Succeeded or Failed"
    Jan 13 09:26:03.347: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8 container client-container: <nil>
    STEP: delete the pod 01/13/23 09:26:03.393
    Jan 13 09:26:03.465: INFO: Waiting for pod downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8 to disappear
    Jan 13 09:26:03.493: INFO: Pod downwardapi-volume-8758bd9e-d8f3-4c8a-a4b2-661f52a646a8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:03.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6128" for this suite. 01/13/23 09:26:03.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:03.551
Jan 13 09:26:03.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:26:03.553
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:03.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:03.623
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-7716/configmap-test-77ef93ef-0cb8-4e5e-9f91-eed8211ac03f 01/13/23 09:26:03.661
STEP: Creating a pod to test consume configMaps 01/13/23 09:26:03.711
Jan 13 09:26:03.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48" in namespace "configmap-7716" to be "Succeeded or Failed"
Jan 13 09:26:03.792: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 9.524672ms
Jan 13 09:26:05.810: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027483116s
Jan 13 09:26:07.803: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02057485s
Jan 13 09:26:09.799: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016929996s
Jan 13 09:26:11.801: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018542486s
STEP: Saw pod success 01/13/23 09:26:11.801
Jan 13 09:26:11.801: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48" satisfied condition "Succeeded or Failed"
Jan 13 09:26:11.809: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48 container env-test: <nil>
STEP: delete the pod 01/13/23 09:26:11.827
Jan 13 09:26:11.854: INFO: Waiting for pod pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48 to disappear
Jan 13 09:26:11.861: INFO: Pod pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:11.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7716" for this suite. 01/13/23 09:26:11.869
------------------------------
• [SLOW TEST] [8.328 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:03.551
    Jan 13 09:26:03.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:26:03.553
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:03.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:03.623
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-7716/configmap-test-77ef93ef-0cb8-4e5e-9f91-eed8211ac03f 01/13/23 09:26:03.661
    STEP: Creating a pod to test consume configMaps 01/13/23 09:26:03.711
    Jan 13 09:26:03.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48" in namespace "configmap-7716" to be "Succeeded or Failed"
    Jan 13 09:26:03.792: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 9.524672ms
    Jan 13 09:26:05.810: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027483116s
    Jan 13 09:26:07.803: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02057485s
    Jan 13 09:26:09.799: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016929996s
    Jan 13 09:26:11.801: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018542486s
    STEP: Saw pod success 01/13/23 09:26:11.801
    Jan 13 09:26:11.801: INFO: Pod "pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48" satisfied condition "Succeeded or Failed"
    Jan 13 09:26:11.809: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48 container env-test: <nil>
    STEP: delete the pod 01/13/23 09:26:11.827
    Jan 13 09:26:11.854: INFO: Waiting for pod pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48 to disappear
    Jan 13 09:26:11.861: INFO: Pod pod-configmaps-84190d7f-d661-499a-8378-6766d2be6e48 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:11.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7716" for this suite. 01/13/23 09:26:11.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:11.88
Jan 13 09:26:11.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replicaset 01/13/23 09:26:11.883
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:11.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:11.916
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/13/23 09:26:11.931
STEP: Verify that the required pods have come up. 01/13/23 09:26:11.946
Jan 13 09:26:11.951: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 13 09:26:16.965: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/13/23 09:26:16.965
STEP: Getting /status 01/13/23 09:26:16.965
Jan 13 09:26:16.997: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/13/23 09:26:16.997
Jan 13 09:26:17.045: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/13/23 09:26:17.045
Jan 13 09:26:17.050: INFO: Observed &ReplicaSet event: ADDED
Jan 13 09:26:17.050: INFO: Observed &ReplicaSet event: MODIFIED
Jan 13 09:26:17.051: INFO: Observed &ReplicaSet event: MODIFIED
Jan 13 09:26:17.051: INFO: Observed &ReplicaSet event: MODIFIED
Jan 13 09:26:17.051: INFO: Found replicaset test-rs in namespace replicaset-3505 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 13 09:26:17.051: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/13/23 09:26:17.051
Jan 13 09:26:17.052: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 13 09:26:17.075: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/13/23 09:26:17.075
Jan 13 09:26:17.085: INFO: Observed &ReplicaSet event: ADDED
Jan 13 09:26:17.086: INFO: Observed &ReplicaSet event: MODIFIED
Jan 13 09:26:17.086: INFO: Observed &ReplicaSet event: MODIFIED
Jan 13 09:26:17.087: INFO: Observed &ReplicaSet event: MODIFIED
Jan 13 09:26:17.087: INFO: Observed replicaset test-rs in namespace replicaset-3505 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 13 09:26:17.088: INFO: Observed &ReplicaSet event: MODIFIED
Jan 13 09:26:17.088: INFO: Found replicaset test-rs in namespace replicaset-3505 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 13 09:26:17.088: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3505" for this suite. 01/13/23 09:26:17.098
------------------------------
• [SLOW TEST] [5.227 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:11.88
    Jan 13 09:26:11.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replicaset 01/13/23 09:26:11.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:11.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:11.916
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/13/23 09:26:11.931
    STEP: Verify that the required pods have come up. 01/13/23 09:26:11.946
    Jan 13 09:26:11.951: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 13 09:26:16.965: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/13/23 09:26:16.965
    STEP: Getting /status 01/13/23 09:26:16.965
    Jan 13 09:26:16.997: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/13/23 09:26:16.997
    Jan 13 09:26:17.045: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/13/23 09:26:17.045
    Jan 13 09:26:17.050: INFO: Observed &ReplicaSet event: ADDED
    Jan 13 09:26:17.050: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 13 09:26:17.051: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 13 09:26:17.051: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 13 09:26:17.051: INFO: Found replicaset test-rs in namespace replicaset-3505 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 13 09:26:17.051: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/13/23 09:26:17.051
    Jan 13 09:26:17.052: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 13 09:26:17.075: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/13/23 09:26:17.075
    Jan 13 09:26:17.085: INFO: Observed &ReplicaSet event: ADDED
    Jan 13 09:26:17.086: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 13 09:26:17.086: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 13 09:26:17.087: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 13 09:26:17.087: INFO: Observed replicaset test-rs in namespace replicaset-3505 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 13 09:26:17.088: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 13 09:26:17.088: INFO: Found replicaset test-rs in namespace replicaset-3505 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 13 09:26:17.088: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3505" for this suite. 01/13/23 09:26:17.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:17.114
Jan 13 09:26:17.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename csistoragecapacity 01/13/23 09:26:17.116
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:17.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:17.157
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/13/23 09:26:17.189
STEP: getting /apis/storage.k8s.io 01/13/23 09:26:17.203
STEP: getting /apis/storage.k8s.io/v1 01/13/23 09:26:17.209
STEP: creating 01/13/23 09:26:17.212
STEP: watching 01/13/23 09:26:17.267
Jan 13 09:26:17.267: INFO: starting watch
STEP: getting 01/13/23 09:26:17.29
STEP: listing in namespace 01/13/23 09:26:17.298
STEP: listing across namespaces 01/13/23 09:26:17.309
STEP: patching 01/13/23 09:26:17.316
STEP: updating 01/13/23 09:26:17.326
Jan 13 09:26:17.336: INFO: waiting for watch events with expected annotations in namespace
Jan 13 09:26:17.336: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/13/23 09:26:17.336
STEP: deleting a collection 01/13/23 09:26:17.367
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:17.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-7905" for this suite. 01/13/23 09:26:17.404
------------------------------
• [0.309 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:17.114
    Jan 13 09:26:17.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename csistoragecapacity 01/13/23 09:26:17.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:17.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:17.157
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/13/23 09:26:17.189
    STEP: getting /apis/storage.k8s.io 01/13/23 09:26:17.203
    STEP: getting /apis/storage.k8s.io/v1 01/13/23 09:26:17.209
    STEP: creating 01/13/23 09:26:17.212
    STEP: watching 01/13/23 09:26:17.267
    Jan 13 09:26:17.267: INFO: starting watch
    STEP: getting 01/13/23 09:26:17.29
    STEP: listing in namespace 01/13/23 09:26:17.298
    STEP: listing across namespaces 01/13/23 09:26:17.309
    STEP: patching 01/13/23 09:26:17.316
    STEP: updating 01/13/23 09:26:17.326
    Jan 13 09:26:17.336: INFO: waiting for watch events with expected annotations in namespace
    Jan 13 09:26:17.336: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/13/23 09:26:17.336
    STEP: deleting a collection 01/13/23 09:26:17.367
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:17.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-7905" for this suite. 01/13/23 09:26:17.404
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:17.425
Jan 13 09:26:17.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 09:26:17.427
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:17.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:17.461
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/13/23 09:26:17.469
STEP: submitting the pod to kubernetes 01/13/23 09:26:17.469
Jan 13 09:26:17.482: INFO: Waiting up to 5m0s for pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" in namespace "pods-5132" to be "running and ready"
Jan 13 09:26:17.490: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Pending", Reason="", readiness=false. Elapsed: 8.722517ms
Jan 13 09:26:17.490: INFO: The phase of Pod pod-update-84532121-0251-45b7-aba8-d0de10bbe479 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:26:19.500: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018578976s
Jan 13 09:26:19.500: INFO: The phase of Pod pod-update-84532121-0251-45b7-aba8-d0de10bbe479 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:26:21.504: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Running", Reason="", readiness=true. Elapsed: 4.022012393s
Jan 13 09:26:21.504: INFO: The phase of Pod pod-update-84532121-0251-45b7-aba8-d0de10bbe479 is Running (Ready = true)
Jan 13 09:26:21.504: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/13/23 09:26:21.511
STEP: updating the pod 01/13/23 09:26:21.53
Jan 13 09:26:22.093: INFO: Successfully updated pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479"
Jan 13 09:26:22.093: INFO: Waiting up to 5m0s for pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" in namespace "pods-5132" to be "running"
Jan 13 09:26:22.099: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Running", Reason="", readiness=true. Elapsed: 5.4254ms
Jan 13 09:26:22.099: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/13/23 09:26:22.099
Jan 13 09:26:22.106: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:22.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5132" for this suite. 01/13/23 09:26:22.118
------------------------------
• [4.716 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:17.425
    Jan 13 09:26:17.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 09:26:17.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:17.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:17.461
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/13/23 09:26:17.469
    STEP: submitting the pod to kubernetes 01/13/23 09:26:17.469
    Jan 13 09:26:17.482: INFO: Waiting up to 5m0s for pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" in namespace "pods-5132" to be "running and ready"
    Jan 13 09:26:17.490: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Pending", Reason="", readiness=false. Elapsed: 8.722517ms
    Jan 13 09:26:17.490: INFO: The phase of Pod pod-update-84532121-0251-45b7-aba8-d0de10bbe479 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:26:19.500: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018578976s
    Jan 13 09:26:19.500: INFO: The phase of Pod pod-update-84532121-0251-45b7-aba8-d0de10bbe479 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:26:21.504: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Running", Reason="", readiness=true. Elapsed: 4.022012393s
    Jan 13 09:26:21.504: INFO: The phase of Pod pod-update-84532121-0251-45b7-aba8-d0de10bbe479 is Running (Ready = true)
    Jan 13 09:26:21.504: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/13/23 09:26:21.511
    STEP: updating the pod 01/13/23 09:26:21.53
    Jan 13 09:26:22.093: INFO: Successfully updated pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479"
    Jan 13 09:26:22.093: INFO: Waiting up to 5m0s for pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" in namespace "pods-5132" to be "running"
    Jan 13 09:26:22.099: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479": Phase="Running", Reason="", readiness=true. Elapsed: 5.4254ms
    Jan 13 09:26:22.099: INFO: Pod "pod-update-84532121-0251-45b7-aba8-d0de10bbe479" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/13/23 09:26:22.099
    Jan 13 09:26:22.106: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:22.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5132" for this suite. 01/13/23 09:26:22.118
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:22.141
Jan 13 09:26:22.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 09:26:22.144
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:22.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:22.182
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 13 09:26:22.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 create -f -'
Jan 13 09:26:22.864: INFO: stderr: ""
Jan 13 09:26:22.864: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 13 09:26:22.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 create -f -'
Jan 13 09:26:23.432: INFO: stderr: ""
Jan 13 09:26:23.432: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/13/23 09:26:23.432
Jan 13 09:26:24.440: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 09:26:24.440: INFO: Found 0 / 1
Jan 13 09:26:25.457: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 09:26:25.457: INFO: Found 0 / 1
Jan 13 09:26:26.472: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 09:26:26.472: INFO: Found 1 / 1
Jan 13 09:26:26.472: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 13 09:26:26.498: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 09:26:26.498: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 13 09:26:26.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe pod agnhost-primary-6wt8p'
Jan 13 09:26:26.799: INFO: stderr: ""
Jan 13 09:26:26.799: INFO: stdout: "Name:             agnhost-primary-6wt8p\nNamespace:        kubectl-8118\nPriority:         0\nService Account:  default\nNode:             10.10.102.31-node/10.10.102.31\nStart Time:       Fri, 13 Jan 2023 09:26:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 028e3da1068c587f4d0c679bde7499440a6db55fb06798241f5c8ea72138fd29\n                  cni.projectcalico.org/podIP: 10.244.27.246/32\n                  cni.projectcalico.org/podIPs: 10.244.27.246/32\nStatus:           Running\nIP:               10.244.27.246\nIPs:\n  IP:           10.244.27.246\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://c07746a27d7d545a7685f25ab07c884b87e4b7af895f482f59d120b8535d1181\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       docker://sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 13 Jan 2023 09:26:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b9kxm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-b9kxm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-8118/agnhost-primary-6wt8p to 10.10.102.31-node\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 13 09:26:26.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe rc agnhost-primary'
Jan 13 09:26:27.165: INFO: stderr: ""
Jan 13 09:26:27.165: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8118\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-6wt8p\n"
Jan 13 09:26:27.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe service agnhost-primary'
Jan 13 09:26:27.543: INFO: stderr: ""
Jan 13 09:26:27.543: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8118\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.103.242.91\nIPs:               10.103.242.91\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.27.246:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 13 09:26:27.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe node 10.10.102.30-master'
Jan 13 09:26:28.051: INFO: stderr: ""
Jan 13 09:26:28.051: INFO: stdout: "Name:               10.10.102.30-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.10.102.30-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.102.30/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.90.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 09 Jan 2023 08:50:56 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.10.102.30-master\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 13 Jan 2023 09:26:25 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 13 Jan 2023 05:10:13 +0000   Fri, 13 Jan 2023 05:10:13 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 08:50:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 08:50:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 08:50:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 09:43:17 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.102.30\n  Hostname:    10.10.102.30-master\nCapacity:\n  cpu:                4\n  ephemeral-storage:  46110724Ki\n  hugepages-2Mi:      0\n  memory:             8009192Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  42495643169\n  hugepages-2Mi:      0\n  memory:             7906792Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f09fec7cec874c1f8a45c4be408cac99\n  System UUID:                1E3B1242-03D4-1B55-8E02-7FF8197D875D\n  Boot ID:                    39a090df-6615-4b8b-841b-6f5c153eeab9\n  Kernel Version:             3.10.0-957.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.22\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-mbrgl                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         3d2h\n  kube-system                 coredns-5bbd96d687-mnhpc                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     3h50m\n  kube-system                 etcd-10.10.102.30-master                                   100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         4d\n  kube-system                 kube-apiserver-10.10.102.30-master                         250m (6%)     0 (0%)      0 (0%)           0 (0%)         4d\n  kube-system                 kube-controller-manager-10.10.102.30-master                200m (5%)     0 (0%)      0 (0%)           0 (0%)         4d\n  kube-system                 kube-proxy-fb9kl                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d\n  kube-system                 kube-scheduler-10.10.102.30-master                         100m (2%)     0 (0%)      0 (0%)           0 (0%)         4d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-k4gbd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (25%)     0 (0%)\n  memory             170Mi (2%)  170Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jan 13 09:26:28.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe namespace kubectl-8118'
Jan 13 09:26:28.364: INFO: stderr: ""
Jan 13 09:26:28.364: INFO: stdout: "Name:         kubectl-8118\nLabels:       e2e-framework=kubectl\n              e2e-run=5655d73b-a226-4012-b01d-7e72ee24020b\n              kubernetes.io/metadata.name=kubectl-8118\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:28.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8118" for this suite. 01/13/23 09:26:28.378
------------------------------
• [SLOW TEST] [6.267 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:22.141
    Jan 13 09:26:22.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 09:26:22.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:22.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:22.182
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 13 09:26:22.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 create -f -'
    Jan 13 09:26:22.864: INFO: stderr: ""
    Jan 13 09:26:22.864: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 13 09:26:22.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 create -f -'
    Jan 13 09:26:23.432: INFO: stderr: ""
    Jan 13 09:26:23.432: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/13/23 09:26:23.432
    Jan 13 09:26:24.440: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 09:26:24.440: INFO: Found 0 / 1
    Jan 13 09:26:25.457: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 09:26:25.457: INFO: Found 0 / 1
    Jan 13 09:26:26.472: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 09:26:26.472: INFO: Found 1 / 1
    Jan 13 09:26:26.472: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 13 09:26:26.498: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 09:26:26.498: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 13 09:26:26.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe pod agnhost-primary-6wt8p'
    Jan 13 09:26:26.799: INFO: stderr: ""
    Jan 13 09:26:26.799: INFO: stdout: "Name:             agnhost-primary-6wt8p\nNamespace:        kubectl-8118\nPriority:         0\nService Account:  default\nNode:             10.10.102.31-node/10.10.102.31\nStart Time:       Fri, 13 Jan 2023 09:26:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 028e3da1068c587f4d0c679bde7499440a6db55fb06798241f5c8ea72138fd29\n                  cni.projectcalico.org/podIP: 10.244.27.246/32\n                  cni.projectcalico.org/podIPs: 10.244.27.246/32\nStatus:           Running\nIP:               10.244.27.246\nIPs:\n  IP:           10.244.27.246\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://c07746a27d7d545a7685f25ab07c884b87e4b7af895f482f59d120b8535d1181\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       docker://sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 13 Jan 2023 09:26:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-b9kxm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-b9kxm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-8118/agnhost-primary-6wt8p to 10.10.102.31-node\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan 13 09:26:26.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe rc agnhost-primary'
    Jan 13 09:26:27.165: INFO: stderr: ""
    Jan 13 09:26:27.165: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8118\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-6wt8p\n"
    Jan 13 09:26:27.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe service agnhost-primary'
    Jan 13 09:26:27.543: INFO: stderr: ""
    Jan 13 09:26:27.543: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8118\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.103.242.91\nIPs:               10.103.242.91\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.27.246:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 13 09:26:27.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe node 10.10.102.30-master'
    Jan 13 09:26:28.051: INFO: stderr: ""
    Jan 13 09:26:28.051: INFO: stdout: "Name:               10.10.102.30-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.10.102.30-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.102.30/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.90.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 09 Jan 2023 08:50:56 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.10.102.30-master\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 13 Jan 2023 09:26:25 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 13 Jan 2023 05:10:13 +0000   Fri, 13 Jan 2023 05:10:13 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 08:50:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 08:50:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 08:50:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 13 Jan 2023 09:23:28 +0000   Mon, 09 Jan 2023 09:43:17 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.102.30\n  Hostname:    10.10.102.30-master\nCapacity:\n  cpu:                4\n  ephemeral-storage:  46110724Ki\n  hugepages-2Mi:      0\n  memory:             8009192Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  42495643169\n  hugepages-2Mi:      0\n  memory:             7906792Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f09fec7cec874c1f8a45c4be408cac99\n  System UUID:                1E3B1242-03D4-1B55-8E02-7FF8197D875D\n  Boot ID:                    39a090df-6615-4b8b-841b-6f5c153eeab9\n  Kernel Version:             3.10.0-957.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.22\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-mbrgl                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         3d2h\n  kube-system                 coredns-5bbd96d687-mnhpc                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     3h50m\n  kube-system                 etcd-10.10.102.30-master                                   100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         4d\n  kube-system                 kube-apiserver-10.10.102.30-master                         250m (6%)     0 (0%)      0 (0%)           0 (0%)         4d\n  kube-system                 kube-controller-manager-10.10.102.30-master                200m (5%)     0 (0%)      0 (0%)           0 (0%)         4d\n  kube-system                 kube-proxy-fb9kl                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d\n  kube-system                 kube-scheduler-10.10.102.30-master                         100m (2%)     0 (0%)      0 (0%)           0 (0%)         4d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-k4gbd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (25%)     0 (0%)\n  memory             170Mi (2%)  170Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Jan 13 09:26:28.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8118 describe namespace kubectl-8118'
    Jan 13 09:26:28.364: INFO: stderr: ""
    Jan 13 09:26:28.364: INFO: stdout: "Name:         kubectl-8118\nLabels:       e2e-framework=kubectl\n              e2e-run=5655d73b-a226-4012-b01d-7e72ee24020b\n              kubernetes.io/metadata.name=kubectl-8118\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:28.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8118" for this suite. 01/13/23 09:26:28.378
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:28.41
Jan 13 09:26:28.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename gc 01/13/23 09:26:28.414
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:28.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:28.475
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/13/23 09:26:28.504
STEP: Wait for the Deployment to create new ReplicaSet 01/13/23 09:26:28.518
STEP: delete the deployment 01/13/23 09:26:29.225
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/13/23 09:26:29.238
STEP: Gathering metrics 01/13/23 09:26:29.803
Jan 13 09:26:29.865: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
Jan 13 09:26:29.887: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 8.832743ms
Jan 13 09:26:29.887: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
Jan 13 09:26:29.887: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
Jan 13 09:26:30.210: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:30.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3198" for this suite. 01/13/23 09:26:30.22
------------------------------
• [1.827 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:28.41
    Jan 13 09:26:28.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename gc 01/13/23 09:26:28.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:28.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:28.475
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/13/23 09:26:28.504
    STEP: Wait for the Deployment to create new ReplicaSet 01/13/23 09:26:28.518
    STEP: delete the deployment 01/13/23 09:26:29.225
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/13/23 09:26:29.238
    STEP: Gathering metrics 01/13/23 09:26:29.803
    Jan 13 09:26:29.865: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
    Jan 13 09:26:29.887: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 8.832743ms
    Jan 13 09:26:29.887: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
    Jan 13 09:26:29.887: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
    Jan 13 09:26:30.210: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:30.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3198" for this suite. 01/13/23 09:26:30.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:30.24
Jan 13 09:26:30.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename daemonsets 01/13/23 09:26:30.244
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:30.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:30.321
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/13/23 09:26:30.401
STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:26:30.42
Jan 13 09:26:30.449: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:30.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:26:30.459: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:26:31.470: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:31.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:26:31.479: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:26:32.483: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:32.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:26:32.491: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:26:33.471: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:33.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 09:26:33.481: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/13/23 09:26:33.488
Jan 13 09:26:33.533: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:33.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:26:33.559: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:26:34.570: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:34.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:26:34.580: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:26:35.575: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:35.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:26:35.593: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:26:36.577: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:36.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:26:36.588: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:26:37.571: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:26:37.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 09:26:37.585: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/13/23 09:26:37.585
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:26:37.607
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1655, will wait for the garbage collector to delete the pods 01/13/23 09:26:37.607
Jan 13 09:26:37.679: INFO: Deleting DaemonSet.extensions daemon-set took: 14.654429ms
Jan 13 09:26:37.780: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.462003ms
Jan 13 09:26:41.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:26:41.495: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 13 09:26:41.507: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"576614"},"items":null}

Jan 13 09:26:41.523: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"576614"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:26:41.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1655" for this suite. 01/13/23 09:26:41.595
------------------------------
• [SLOW TEST] [11.371 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:30.24
    Jan 13 09:26:30.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename daemonsets 01/13/23 09:26:30.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:30.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:30.321
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/13/23 09:26:30.401
    STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:26:30.42
    Jan 13 09:26:30.449: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:30.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:26:30.459: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:26:31.470: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:31.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:26:31.479: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:26:32.483: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:32.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:26:32.491: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:26:33.471: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:33.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 09:26:33.481: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/13/23 09:26:33.488
    Jan 13 09:26:33.533: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:33.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:26:33.559: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:26:34.570: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:34.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:26:34.580: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:26:35.575: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:35.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:26:35.593: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:26:36.577: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:36.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:26:36.588: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:26:37.571: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:26:37.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 09:26:37.585: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/13/23 09:26:37.585
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:26:37.607
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1655, will wait for the garbage collector to delete the pods 01/13/23 09:26:37.607
    Jan 13 09:26:37.679: INFO: Deleting DaemonSet.extensions daemon-set took: 14.654429ms
    Jan 13 09:26:37.780: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.462003ms
    Jan 13 09:26:41.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:26:41.495: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 13 09:26:41.507: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"576614"},"items":null}

    Jan 13 09:26:41.523: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"576614"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:26:41.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1655" for this suite. 01/13/23 09:26:41.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:26:41.613
Jan 13 09:26:41.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 09:26:41.618
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:41.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:41.726
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/13/23 09:26:58.747
STEP: Creating a ResourceQuota 01/13/23 09:27:03.762
STEP: Ensuring resource quota status is calculated 01/13/23 09:27:03.776
STEP: Creating a ConfigMap 01/13/23 09:27:05.789
STEP: Ensuring resource quota status captures configMap creation 01/13/23 09:27:05.813
STEP: Deleting a ConfigMap 01/13/23 09:27:07.823
STEP: Ensuring resource quota status released usage 01/13/23 09:27:07.843
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 09:27:09.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9148" for this suite. 01/13/23 09:27:09.917
------------------------------
• [SLOW TEST] [28.411 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:26:41.613
    Jan 13 09:26:41.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 09:26:41.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:26:41.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:26:41.726
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/13/23 09:26:58.747
    STEP: Creating a ResourceQuota 01/13/23 09:27:03.762
    STEP: Ensuring resource quota status is calculated 01/13/23 09:27:03.776
    STEP: Creating a ConfigMap 01/13/23 09:27:05.789
    STEP: Ensuring resource quota status captures configMap creation 01/13/23 09:27:05.813
    STEP: Deleting a ConfigMap 01/13/23 09:27:07.823
    STEP: Ensuring resource quota status released usage 01/13/23 09:27:07.843
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:27:09.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9148" for this suite. 01/13/23 09:27:09.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:27:10.029
Jan 13 09:27:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename watch 01/13/23 09:27:10.032
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:10.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:10.124
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/13/23 09:27:10.134
STEP: creating a watch on configmaps with label B 01/13/23 09:27:10.139
STEP: creating a watch on configmaps with label A or B 01/13/23 09:27:10.151
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/13/23 09:27:10.155
Jan 13 09:27:10.164: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576696 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:10.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576696 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/13/23 09:27:10.165
Jan 13 09:27:10.189: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576697 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:10.190: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576697 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/13/23 09:27:10.19
Jan 13 09:27:10.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576698 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:10.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576698 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/13/23 09:27:10.216
Jan 13 09:27:10.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576699 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:10.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576699 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/13/23 09:27:10.23
Jan 13 09:27:10.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576700 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:10.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576700 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/13/23 09:27:20.241
Jan 13 09:27:20.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576723 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:20.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576723 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 13 09:27:30.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7566" for this suite. 01/13/23 09:27:30.269
------------------------------
• [SLOW TEST] [20.252 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:27:10.029
    Jan 13 09:27:10.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename watch 01/13/23 09:27:10.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:10.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:10.124
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/13/23 09:27:10.134
    STEP: creating a watch on configmaps with label B 01/13/23 09:27:10.139
    STEP: creating a watch on configmaps with label A or B 01/13/23 09:27:10.151
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/13/23 09:27:10.155
    Jan 13 09:27:10.164: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576696 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:10.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576696 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/13/23 09:27:10.165
    Jan 13 09:27:10.189: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576697 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:10.190: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576697 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/13/23 09:27:10.19
    Jan 13 09:27:10.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576698 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:10.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576698 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/13/23 09:27:10.216
    Jan 13 09:27:10.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576699 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:10.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7566  2c3a69de-e870-4bfd-9bac-7796a9039dd2 576699 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/13/23 09:27:10.23
    Jan 13 09:27:10.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576700 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:10.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576700 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/13/23 09:27:20.241
    Jan 13 09:27:20.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576723 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:20.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7566  540ceeb8-aef5-4685-a04c-d2d0213b65d6 576723 0 2023-01-13 09:27:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:27:30.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7566" for this suite. 01/13/23 09:27:30.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:27:30.283
Jan 13 09:27:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename watch 01/13/23 09:27:30.286
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:30.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:30.342
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/13/23 09:27:30.358
STEP: creating a new configmap 01/13/23 09:27:30.377
STEP: modifying the configmap once 01/13/23 09:27:30.401
STEP: changing the label value of the configmap 01/13/23 09:27:30.458
STEP: Expecting to observe a delete notification for the watched object 01/13/23 09:27:30.48
Jan 13 09:27:30.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576743 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:30.481: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576744 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:30.481: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576745 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/13/23 09:27:30.481
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/13/23 09:27:30.5
STEP: changing the label value of the configmap back 01/13/23 09:27:40.5
STEP: modifying the configmap a third time 01/13/23 09:27:40.529
STEP: deleting the configmap 01/13/23 09:27:40.557
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/13/23 09:27:40.586
Jan 13 09:27:40.587: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576767 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:40.587: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576768 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:27:40.587: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576769 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 13 09:27:40.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5556" for this suite. 01/13/23 09:27:40.601
------------------------------
• [SLOW TEST] [10.350 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:27:30.283
    Jan 13 09:27:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename watch 01/13/23 09:27:30.286
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:30.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:30.342
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/13/23 09:27:30.358
    STEP: creating a new configmap 01/13/23 09:27:30.377
    STEP: modifying the configmap once 01/13/23 09:27:30.401
    STEP: changing the label value of the configmap 01/13/23 09:27:30.458
    STEP: Expecting to observe a delete notification for the watched object 01/13/23 09:27:30.48
    Jan 13 09:27:30.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576743 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:30.481: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576744 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:30.481: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576745 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/13/23 09:27:30.481
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/13/23 09:27:30.5
    STEP: changing the label value of the configmap back 01/13/23 09:27:40.5
    STEP: modifying the configmap a third time 01/13/23 09:27:40.529
    STEP: deleting the configmap 01/13/23 09:27:40.557
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/13/23 09:27:40.586
    Jan 13 09:27:40.587: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576767 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:40.587: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576768 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:27:40.587: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5556  0b3c2de0-ec95-4ba3-b762-6be58f2b24ce 576769 0 2023-01-13 09:27:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-13 09:27:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:27:40.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5556" for this suite. 01/13/23 09:27:40.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:27:40.634
Jan 13 09:27:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 09:27:40.641
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:40.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:40.773
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/13/23 09:27:40.786
STEP: Getting a ResourceQuota 01/13/23 09:27:40.803
STEP: Updating a ResourceQuota 01/13/23 09:27:40.812
STEP: Verifying a ResourceQuota was modified 01/13/23 09:27:40.858
STEP: Deleting a ResourceQuota 01/13/23 09:27:40.884
STEP: Verifying the deleted ResourceQuota 01/13/23 09:27:40.912
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 09:27:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2740" for this suite. 01/13/23 09:27:40.948
------------------------------
• [0.351 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:27:40.634
    Jan 13 09:27:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 09:27:40.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:40.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:40.773
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/13/23 09:27:40.786
    STEP: Getting a ResourceQuota 01/13/23 09:27:40.803
    STEP: Updating a ResourceQuota 01/13/23 09:27:40.812
    STEP: Verifying a ResourceQuota was modified 01/13/23 09:27:40.858
    STEP: Deleting a ResourceQuota 01/13/23 09:27:40.884
    STEP: Verifying the deleted ResourceQuota 01/13/23 09:27:40.912
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:27:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2740" for this suite. 01/13/23 09:27:40.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:27:40.989
Jan 13 09:27:40.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:27:40.993
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:41.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:41.052
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-6476/configmap-test-90563034-6eb2-4be5-b81b-939e352d9679 01/13/23 09:27:41.06
STEP: Creating a pod to test consume configMaps 01/13/23 09:27:41.089
Jan 13 09:27:41.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037" in namespace "configmap-6476" to be "Succeeded or Failed"
Jan 13 09:27:41.125: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918961ms
Jan 13 09:27:43.134: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014823026s
Jan 13 09:27:45.140: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020672008s
Jan 13 09:27:47.132: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012245023s
STEP: Saw pod success 01/13/23 09:27:47.132
Jan 13 09:27:47.132: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037" satisfied condition "Succeeded or Failed"
Jan 13 09:27:47.137: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037 container env-test: <nil>
STEP: delete the pod 01/13/23 09:27:47.202
Jan 13 09:27:47.223: INFO: Waiting for pod pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037 to disappear
Jan 13 09:27:47.229: INFO: Pod pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:27:47.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6476" for this suite. 01/13/23 09:27:47.238
------------------------------
• [SLOW TEST] [6.262 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:27:40.989
    Jan 13 09:27:40.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:27:40.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:41.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:41.052
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-6476/configmap-test-90563034-6eb2-4be5-b81b-939e352d9679 01/13/23 09:27:41.06
    STEP: Creating a pod to test consume configMaps 01/13/23 09:27:41.089
    Jan 13 09:27:41.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037" in namespace "configmap-6476" to be "Succeeded or Failed"
    Jan 13 09:27:41.125: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918961ms
    Jan 13 09:27:43.134: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014823026s
    Jan 13 09:27:45.140: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020672008s
    Jan 13 09:27:47.132: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012245023s
    STEP: Saw pod success 01/13/23 09:27:47.132
    Jan 13 09:27:47.132: INFO: Pod "pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037" satisfied condition "Succeeded or Failed"
    Jan 13 09:27:47.137: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037 container env-test: <nil>
    STEP: delete the pod 01/13/23 09:27:47.202
    Jan 13 09:27:47.223: INFO: Waiting for pod pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037 to disappear
    Jan 13 09:27:47.229: INFO: Pod pod-configmaps-76fb7656-2925-49ce-b5ba-0a94ee64e037 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:27:47.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6476" for this suite. 01/13/23 09:27:47.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:27:47.254
Jan 13 09:27:47.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-preemption 01/13/23 09:27:47.258
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:47.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:47.299
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 13 09:27:47.350: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 09:28:47.441: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 01/13/23 09:28:47.448
Jan 13 09:28:47.527: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 13 09:28:47.570: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 13 09:28:47.630: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 13 09:28:47.648: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/13/23 09:28:47.648
Jan 13 09:28:47.648: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-443" to be "running"
Jan 13 09:28:47.677: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.291876ms
Jan 13 09:28:49.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068789426s
Jan 13 09:28:51.685: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.037212108s
Jan 13 09:28:51.686: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 13 09:28:51.686: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-443" to be "running"
Jan 13 09:28:51.692: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.846044ms
Jan 13 09:28:51.692: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 13 09:28:51.693: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-443" to be "running"
Jan 13 09:28:51.699: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.25898ms
Jan 13 09:28:51.699: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 13 09:28:51.699: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-443" to be "running"
Jan 13 09:28:51.706: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.737395ms
Jan 13 09:28:51.706: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/13/23 09:28:51.706
Jan 13 09:28:51.729: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 13 09:28:51.737: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.504283ms
Jan 13 09:28:53.751: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021204637s
Jan 13 09:28:55.746: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016023458s
Jan 13 09:28:57.750: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020263634s
Jan 13 09:28:59.747: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.017386716s
Jan 13 09:28:59.747: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:28:59.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-443" for this suite. 01/13/23 09:28:59.97
------------------------------
• [SLOW TEST] [72.736 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:27:47.254
    Jan 13 09:27:47.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-preemption 01/13/23 09:27:47.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:27:47.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:27:47.299
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 13 09:27:47.350: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 13 09:28:47.441: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 01/13/23 09:28:47.448
    Jan 13 09:28:47.527: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 13 09:28:47.570: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 13 09:28:47.630: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 13 09:28:47.648: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/13/23 09:28:47.648
    Jan 13 09:28:47.648: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-443" to be "running"
    Jan 13 09:28:47.677: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.291876ms
    Jan 13 09:28:49.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068789426s
    Jan 13 09:28:51.685: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.037212108s
    Jan 13 09:28:51.686: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 13 09:28:51.686: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-443" to be "running"
    Jan 13 09:28:51.692: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.846044ms
    Jan 13 09:28:51.692: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 13 09:28:51.693: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-443" to be "running"
    Jan 13 09:28:51.699: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.25898ms
    Jan 13 09:28:51.699: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 13 09:28:51.699: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-443" to be "running"
    Jan 13 09:28:51.706: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.737395ms
    Jan 13 09:28:51.706: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/13/23 09:28:51.706
    Jan 13 09:28:51.729: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 13 09:28:51.737: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.504283ms
    Jan 13 09:28:53.751: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021204637s
    Jan 13 09:28:55.746: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016023458s
    Jan 13 09:28:57.750: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020263634s
    Jan 13 09:28:59.747: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.017386716s
    Jan 13 09:28:59.747: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:28:59.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-443" for this suite. 01/13/23 09:28:59.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:28:59.995
Jan 13 09:28:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:28:59.998
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:00.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:00.102
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 13 09:29:00.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9561" for this suite. 01/13/23 09:29:00.161
------------------------------
• [0.199 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:28:59.995
    Jan 13 09:28:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:28:59.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:00.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:00.102
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:29:00.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9561" for this suite. 01/13/23 09:29:00.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:29:00.198
Jan 13 09:29:00.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 09:29:00.201
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:00.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:00.248
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/13/23 09:29:00.276
Jan 13 09:29:00.344: INFO: Waiting up to 5m0s for pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03" in namespace "emptydir-4445" to be "Succeeded or Failed"
Jan 13 09:29:00.365: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 20.913466ms
Jan 13 09:29:02.375: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030479439s
Jan 13 09:29:04.374: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03012266s
Jan 13 09:29:06.388: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043696888s
Jan 13 09:29:08.379: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03452445s
STEP: Saw pod success 01/13/23 09:29:08.379
Jan 13 09:29:08.379: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03" satisfied condition "Succeeded or Failed"
Jan 13 09:29:08.389: INFO: Trying to get logs from node 10.10.102.31-node pod pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03 container test-container: <nil>
STEP: delete the pod 01/13/23 09:29:08.447
Jan 13 09:29:08.481: INFO: Waiting for pod pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03 to disappear
Jan 13 09:29:08.492: INFO: Pod pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:29:08.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4445" for this suite. 01/13/23 09:29:08.529
------------------------------
• [SLOW TEST] [8.408 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:29:00.198
    Jan 13 09:29:00.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 09:29:00.201
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:00.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:00.248
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/13/23 09:29:00.276
    Jan 13 09:29:00.344: INFO: Waiting up to 5m0s for pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03" in namespace "emptydir-4445" to be "Succeeded or Failed"
    Jan 13 09:29:00.365: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 20.913466ms
    Jan 13 09:29:02.375: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030479439s
    Jan 13 09:29:04.374: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03012266s
    Jan 13 09:29:06.388: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043696888s
    Jan 13 09:29:08.379: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03452445s
    STEP: Saw pod success 01/13/23 09:29:08.379
    Jan 13 09:29:08.379: INFO: Pod "pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03" satisfied condition "Succeeded or Failed"
    Jan 13 09:29:08.389: INFO: Trying to get logs from node 10.10.102.31-node pod pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03 container test-container: <nil>
    STEP: delete the pod 01/13/23 09:29:08.447
    Jan 13 09:29:08.481: INFO: Waiting for pod pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03 to disappear
    Jan 13 09:29:08.492: INFO: Pod pod-6d3e3e50-26d4-4f79-afd4-42d0ee914c03 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:29:08.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4445" for this suite. 01/13/23 09:29:08.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:29:08.612
Jan 13 09:29:08.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:29:08.616
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:08.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:08.691
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-01a00c90-65e5-471f-98b3-61afc5c39fe8 01/13/23 09:29:08.706
STEP: Creating a pod to test consume configMaps 01/13/23 09:29:08.72
Jan 13 09:29:08.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa" in namespace "configmap-3428" to be "Succeeded or Failed"
Jan 13 09:29:08.740: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665193ms
Jan 13 09:29:10.754: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020745746s
Jan 13 09:29:12.753: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019846624s
Jan 13 09:29:14.747: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013170648s
STEP: Saw pod success 01/13/23 09:29:14.747
Jan 13 09:29:14.747: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa" satisfied condition "Succeeded or Failed"
Jan 13 09:29:14.753: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa container agnhost-container: <nil>
STEP: delete the pod 01/13/23 09:29:14.807
Jan 13 09:29:14.835: INFO: Waiting for pod pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa to disappear
Jan 13 09:29:14.843: INFO: Pod pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:29:14.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3428" for this suite. 01/13/23 09:29:14.857
------------------------------
• [SLOW TEST] [6.271 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:29:08.612
    Jan 13 09:29:08.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:29:08.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:08.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:08.691
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-01a00c90-65e5-471f-98b3-61afc5c39fe8 01/13/23 09:29:08.706
    STEP: Creating a pod to test consume configMaps 01/13/23 09:29:08.72
    Jan 13 09:29:08.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa" in namespace "configmap-3428" to be "Succeeded or Failed"
    Jan 13 09:29:08.740: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665193ms
    Jan 13 09:29:10.754: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020745746s
    Jan 13 09:29:12.753: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019846624s
    Jan 13 09:29:14.747: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013170648s
    STEP: Saw pod success 01/13/23 09:29:14.747
    Jan 13 09:29:14.747: INFO: Pod "pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa" satisfied condition "Succeeded or Failed"
    Jan 13 09:29:14.753: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 09:29:14.807
    Jan 13 09:29:14.835: INFO: Waiting for pod pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa to disappear
    Jan 13 09:29:14.843: INFO: Pod pod-configmaps-71829be6-8083-4590-b6c8-4946c795d7aa no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:29:14.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3428" for this suite. 01/13/23 09:29:14.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:29:14.885
Jan 13 09:29:14.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 09:29:14.89
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:14.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:14.976
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/13/23 09:29:14.988
Jan 13 09:29:14.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 create -f -'
Jan 13 09:29:15.688: INFO: stderr: ""
Jan 13 09:29:15.689: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 09:29:15.689
Jan 13 09:29:15.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 09:29:15.911: INFO: stderr: ""
Jan 13 09:29:15.911: INFO: stdout: "update-demo-nautilus-w4mfj update-demo-nautilus-znxhh "
Jan 13 09:29:15.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-w4mfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 09:29:16.160: INFO: stderr: ""
Jan 13 09:29:16.160: INFO: stdout: ""
Jan 13 09:29:16.161: INFO: update-demo-nautilus-w4mfj is created but not running
Jan 13 09:29:21.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 09:29:21.393: INFO: stderr: ""
Jan 13 09:29:21.393: INFO: stdout: "update-demo-nautilus-w4mfj update-demo-nautilus-znxhh "
Jan 13 09:29:21.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-w4mfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 09:29:21.636: INFO: stderr: ""
Jan 13 09:29:21.636: INFO: stdout: "true"
Jan 13 09:29:21.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-w4mfj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 09:29:21.884: INFO: stderr: ""
Jan 13 09:29:21.884: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 13 09:29:21.884: INFO: validating pod update-demo-nautilus-w4mfj
Jan 13 09:29:21.895: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 09:29:21.896: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 09:29:21.896: INFO: update-demo-nautilus-w4mfj is verified up and running
Jan 13 09:29:21.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-znxhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 09:29:22.130: INFO: stderr: ""
Jan 13 09:29:22.130: INFO: stdout: "true"
Jan 13 09:29:22.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-znxhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 09:29:22.454: INFO: stderr: ""
Jan 13 09:29:22.454: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 13 09:29:22.454: INFO: validating pod update-demo-nautilus-znxhh
Jan 13 09:29:22.495: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 09:29:22.495: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 09:29:22.495: INFO: update-demo-nautilus-znxhh is verified up and running
STEP: using delete to clean up resources 01/13/23 09:29:22.495
Jan 13 09:29:22.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 delete --grace-period=0 --force -f -'
Jan 13 09:29:22.796: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 09:29:22.796: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 13 09:29:22.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get rc,svc -l name=update-demo --no-headers'
Jan 13 09:29:23.056: INFO: stderr: "No resources found in kubectl-6903 namespace.\n"
Jan 13 09:29:23.057: INFO: stdout: ""
Jan 13 09:29:23.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 13 09:29:23.348: INFO: stderr: ""
Jan 13 09:29:23.348: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 09:29:23.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6903" for this suite. 01/13/23 09:29:23.363
------------------------------
• [SLOW TEST] [8.499 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:29:14.885
    Jan 13 09:29:14.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 09:29:14.89
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:14.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:14.976
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/13/23 09:29:14.988
    Jan 13 09:29:14.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 create -f -'
    Jan 13 09:29:15.688: INFO: stderr: ""
    Jan 13 09:29:15.689: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 09:29:15.689
    Jan 13 09:29:15.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 09:29:15.911: INFO: stderr: ""
    Jan 13 09:29:15.911: INFO: stdout: "update-demo-nautilus-w4mfj update-demo-nautilus-znxhh "
    Jan 13 09:29:15.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-w4mfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 09:29:16.160: INFO: stderr: ""
    Jan 13 09:29:16.160: INFO: stdout: ""
    Jan 13 09:29:16.161: INFO: update-demo-nautilus-w4mfj is created but not running
    Jan 13 09:29:21.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 09:29:21.393: INFO: stderr: ""
    Jan 13 09:29:21.393: INFO: stdout: "update-demo-nautilus-w4mfj update-demo-nautilus-znxhh "
    Jan 13 09:29:21.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-w4mfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 09:29:21.636: INFO: stderr: ""
    Jan 13 09:29:21.636: INFO: stdout: "true"
    Jan 13 09:29:21.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-w4mfj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 13 09:29:21.884: INFO: stderr: ""
    Jan 13 09:29:21.884: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 13 09:29:21.884: INFO: validating pod update-demo-nautilus-w4mfj
    Jan 13 09:29:21.895: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 13 09:29:21.896: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 13 09:29:21.896: INFO: update-demo-nautilus-w4mfj is verified up and running
    Jan 13 09:29:21.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-znxhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 09:29:22.130: INFO: stderr: ""
    Jan 13 09:29:22.130: INFO: stdout: "true"
    Jan 13 09:29:22.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods update-demo-nautilus-znxhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 13 09:29:22.454: INFO: stderr: ""
    Jan 13 09:29:22.454: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 13 09:29:22.454: INFO: validating pod update-demo-nautilus-znxhh
    Jan 13 09:29:22.495: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 13 09:29:22.495: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 13 09:29:22.495: INFO: update-demo-nautilus-znxhh is verified up and running
    STEP: using delete to clean up resources 01/13/23 09:29:22.495
    Jan 13 09:29:22.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 delete --grace-period=0 --force -f -'
    Jan 13 09:29:22.796: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 09:29:22.796: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 13 09:29:22.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get rc,svc -l name=update-demo --no-headers'
    Jan 13 09:29:23.056: INFO: stderr: "No resources found in kubectl-6903 namespace.\n"
    Jan 13 09:29:23.057: INFO: stdout: ""
    Jan 13 09:29:23.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6903 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 13 09:29:23.348: INFO: stderr: ""
    Jan 13 09:29:23.348: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:29:23.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6903" for this suite. 01/13/23 09:29:23.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:29:23.389
Jan 13 09:29:23.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 09:29:23.392
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:23.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:23.432
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5344 01/13/23 09:29:23.441
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-5344 01/13/23 09:29:23.454
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5344 01/13/23 09:29:23.477
Jan 13 09:29:23.494: INFO: Found 0 stateful pods, waiting for 1
Jan 13 09:29:33.511: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/13/23 09:29:33.511
Jan 13 09:29:33.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:29:34.091: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:29:34.091: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:29:34.091: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:29:34.106: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 13 09:29:44.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:29:44.132: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:29:44.217: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jan 13 09:29:44.217: INFO: ss-0  10.10.102.31-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
Jan 13 09:29:44.217: INFO: ss-1                     Pending         []
Jan 13 09:29:44.217: INFO: 
Jan 13 09:29:44.217: INFO: StatefulSet ss has not reached scale 3, at 2
Jan 13 09:29:45.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976312999s
Jan 13 09:29:46.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.922807802s
Jan 13 09:29:47.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.908970752s
Jan 13 09:29:48.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.896790674s
Jan 13 09:29:49.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.887782556s
Jan 13 09:29:50.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.879092725s
Jan 13 09:29:51.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.8687179s
Jan 13 09:29:52.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.859691714s
Jan 13 09:29:53.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 847.356262ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5344 01/13/23 09:29:54.359
Jan 13 09:29:54.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:29:54.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 09:29:54.892: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 09:29:54.892: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 09:29:54.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:29:55.370: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 13 09:29:55.370: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 09:29:55.370: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 09:29:55.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:29:55.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 13 09:29:55.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 09:29:55.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 09:29:55.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 09:29:55.807: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 09:29:55.807: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/13/23 09:29:55.807
Jan 13 09:29:55.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:29:56.134: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:29:56.134: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:29:56.134: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:29:56.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:29:56.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:29:56.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:29:56.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:29:56.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:29:57.146: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:29:57.146: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:29:57.146: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:29:57.146: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:29:57.158: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 13 09:30:07.190: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:30:07.190: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:30:07.190: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:30:07.231: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jan 13 09:30:07.231: INFO: ss-0  10.10.102.31-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
Jan 13 09:30:07.231: INFO: ss-1  10.10.102.32-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
Jan 13 09:30:07.231: INFO: ss-2  10.10.102.31-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
Jan 13 09:30:07.231: INFO: 
Jan 13 09:30:07.231: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 09:30:08.254: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jan 13 09:30:08.254: INFO: ss-0  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
Jan 13 09:30:08.254: INFO: ss-1  10.10.102.32-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
Jan 13 09:30:08.254: INFO: ss-2  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
Jan 13 09:30:08.254: INFO: 
Jan 13 09:30:08.254: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 09:30:09.262: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jan 13 09:30:09.262: INFO: ss-0  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
Jan 13 09:30:09.263: INFO: ss-1  10.10.102.32-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
Jan 13 09:30:09.263: INFO: ss-2  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
Jan 13 09:30:09.263: INFO: 
Jan 13 09:30:09.263: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 09:30:10.269: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.953155892s
Jan 13 09:30:11.277: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.94704672s
Jan 13 09:30:12.285: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.939223736s
Jan 13 09:30:13.296: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.931066229s
Jan 13 09:30:14.305: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.920726093s
Jan 13 09:30:15.314: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.910487648s
Jan 13 09:30:16.320: INFO: Verifying statefulset ss doesn't scale past 0 for another 902.513527ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5344 01/13/23 09:30:17.32
Jan 13 09:30:17.335: INFO: Scaling statefulset ss to 0
Jan 13 09:30:17.387: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 09:30:17.399: INFO: Deleting all statefulset in ns statefulset-5344
Jan 13 09:30:17.408: INFO: Scaling statefulset ss to 0
Jan 13 09:30:17.445: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:30:17.457: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:30:17.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5344" for this suite. 01/13/23 09:30:17.542
------------------------------
• [SLOW TEST] [54.187 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:29:23.389
    Jan 13 09:29:23.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 09:29:23.392
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:29:23.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:29:23.432
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5344 01/13/23 09:29:23.441
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-5344 01/13/23 09:29:23.454
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5344 01/13/23 09:29:23.477
    Jan 13 09:29:23.494: INFO: Found 0 stateful pods, waiting for 1
    Jan 13 09:29:33.511: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/13/23 09:29:33.511
    Jan 13 09:29:33.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:29:34.091: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:29:34.091: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:29:34.091: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:29:34.106: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 13 09:29:44.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:29:44.132: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:29:44.217: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
    Jan 13 09:29:44.217: INFO: ss-0  10.10.102.31-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
    Jan 13 09:29:44.217: INFO: ss-1                     Pending         []
    Jan 13 09:29:44.217: INFO: 
    Jan 13 09:29:44.217: INFO: StatefulSet ss has not reached scale 3, at 2
    Jan 13 09:29:45.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976312999s
    Jan 13 09:29:46.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.922807802s
    Jan 13 09:29:47.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.908970752s
    Jan 13 09:29:48.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.896790674s
    Jan 13 09:29:49.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.887782556s
    Jan 13 09:29:50.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.879092725s
    Jan 13 09:29:51.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.8687179s
    Jan 13 09:29:52.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.859691714s
    Jan 13 09:29:53.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 847.356262ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5344 01/13/23 09:29:54.359
    Jan 13 09:29:54.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:29:54.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 13 09:29:54.892: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 09:29:54.892: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 13 09:29:54.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:29:55.370: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 13 09:29:55.370: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 09:29:55.370: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 13 09:29:55.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:29:55.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 13 09:29:55.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 09:29:55.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 13 09:29:55.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 09:29:55.807: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 09:29:55.807: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/13/23 09:29:55.807
    Jan 13 09:29:55.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:29:56.134: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:29:56.134: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:29:56.134: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:29:56.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:29:56.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:29:56.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:29:56.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:29:56.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-5344 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:29:57.146: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:29:57.146: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:29:57.146: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:29:57.146: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:29:57.158: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 13 09:30:07.190: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:30:07.190: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:30:07.190: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:30:07.231: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
    Jan 13 09:30:07.231: INFO: ss-0  10.10.102.31-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
    Jan 13 09:30:07.231: INFO: ss-1  10.10.102.32-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
    Jan 13 09:30:07.231: INFO: ss-2  10.10.102.31-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
    Jan 13 09:30:07.231: INFO: 
    Jan 13 09:30:07.231: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 13 09:30:08.254: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
    Jan 13 09:30:08.254: INFO: ss-0  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
    Jan 13 09:30:08.254: INFO: ss-1  10.10.102.32-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
    Jan 13 09:30:08.254: INFO: ss-2  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
    Jan 13 09:30:08.254: INFO: 
    Jan 13 09:30:08.254: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 13 09:30:09.262: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
    Jan 13 09:30:09.262: INFO: ss-0  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:23 +0000 UTC  }]
    Jan 13 09:30:09.263: INFO: ss-1  10.10.102.32-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
    Jan 13 09:30:09.263: INFO: ss-2  10.10.102.31-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 09:29:44 +0000 UTC  }]
    Jan 13 09:30:09.263: INFO: 
    Jan 13 09:30:09.263: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 13 09:30:10.269: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.953155892s
    Jan 13 09:30:11.277: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.94704672s
    Jan 13 09:30:12.285: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.939223736s
    Jan 13 09:30:13.296: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.931066229s
    Jan 13 09:30:14.305: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.920726093s
    Jan 13 09:30:15.314: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.910487648s
    Jan 13 09:30:16.320: INFO: Verifying statefulset ss doesn't scale past 0 for another 902.513527ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5344 01/13/23 09:30:17.32
    Jan 13 09:30:17.335: INFO: Scaling statefulset ss to 0
    Jan 13 09:30:17.387: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 09:30:17.399: INFO: Deleting all statefulset in ns statefulset-5344
    Jan 13 09:30:17.408: INFO: Scaling statefulset ss to 0
    Jan 13 09:30:17.445: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:30:17.457: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:30:17.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5344" for this suite. 01/13/23 09:30:17.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:30:17.578
Jan 13 09:30:17.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename job 01/13/23 09:30:17.582
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:30:17.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:30:17.652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/13/23 09:30:17.669
STEP: Ensuring job reaches completions 01/13/23 09:30:17.686
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 13 09:30:35.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6699" for this suite. 01/13/23 09:30:35.707
------------------------------
• [SLOW TEST] [18.147 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:30:17.578
    Jan 13 09:30:17.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename job 01/13/23 09:30:17.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:30:17.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:30:17.652
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/13/23 09:30:17.669
    STEP: Ensuring job reaches completions 01/13/23 09:30:17.686
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:30:35.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6699" for this suite. 01/13/23 09:30:35.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:30:35.726
Jan 13 09:30:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 09:30:35.729
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:30:35.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:30:35.768
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-2364 01/13/23 09:30:35.784
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[] 01/13/23 09:30:35.802
Jan 13 09:30:35.814: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 13 09:30:36.888: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2364 01/13/23 09:30:36.888
Jan 13 09:30:36.915: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2364" to be "running and ready"
Jan 13 09:30:36.933: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.958814ms
Jan 13 09:30:36.933: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:30:38.944: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029600367s
Jan 13 09:30:38.944: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:30:40.977: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.062726844s
Jan 13 09:30:40.977: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 13 09:30:40.978: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[pod1:[80]] 01/13/23 09:30:40.999
Jan 13 09:30:41.033: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/13/23 09:30:41.033
Jan 13 09:30:41.033: INFO: Creating new exec pod
Jan 13 09:30:41.053: INFO: Waiting up to 5m0s for pod "execpod95zsx" in namespace "services-2364" to be "running"
Jan 13 09:30:41.067: INFO: Pod "execpod95zsx": Phase="Pending", Reason="", readiness=false. Elapsed: 14.403644ms
Jan 13 09:30:43.118: INFO: Pod "execpod95zsx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06492545s
Jan 13 09:30:45.087: INFO: Pod "execpod95zsx": Phase="Running", Reason="", readiness=true. Elapsed: 4.033870445s
Jan 13 09:30:45.087: INFO: Pod "execpod95zsx" satisfied condition "running"
Jan 13 09:30:46.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 13 09:30:46.653: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 13 09:30:46.653: INFO: stdout: ""
Jan 13 09:30:46.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 10.96.203.232 80'
Jan 13 09:30:47.228: INFO: stderr: "+ nc -v -z -w 2 10.96.203.232 80\nConnection to 10.96.203.232 80 port [tcp/http] succeeded!\n"
Jan 13 09:30:47.228: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-2364 01/13/23 09:30:47.228
Jan 13 09:30:47.247: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2364" to be "running and ready"
Jan 13 09:30:47.264: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.49064ms
Jan 13 09:30:47.264: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:30:49.270: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022176711s
Jan 13 09:30:49.270: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:30:51.273: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.025488677s
Jan 13 09:30:51.273: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 13 09:30:51.273: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[pod1:[80] pod2:[80]] 01/13/23 09:30:51.279
Jan 13 09:30:51.320: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/13/23 09:30:51.32
Jan 13 09:30:52.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 13 09:30:52.993: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 13 09:30:52.994: INFO: stdout: ""
Jan 13 09:30:52.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 10.96.203.232 80'
Jan 13 09:30:53.520: INFO: stderr: "+ nc -v -z -w 2 10.96.203.232 80\nConnection to 10.96.203.232 80 port [tcp/http] succeeded!\n"
Jan 13 09:30:53.520: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2364 01/13/23 09:30:53.52
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[pod2:[80]] 01/13/23 09:30:53.573
Jan 13 09:30:53.690: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/13/23 09:30:53.69
Jan 13 09:30:54.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 13 09:30:55.316: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 13 09:30:55.316: INFO: stdout: ""
Jan 13 09:30:55.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 10.96.203.232 80'
Jan 13 09:30:55.717: INFO: stderr: "+ nc -v -z -w 2 10.96.203.232 80\nConnection to 10.96.203.232 80 port [tcp/http] succeeded!\n"
Jan 13 09:30:55.717: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-2364 01/13/23 09:30:55.717
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[] 01/13/23 09:30:55.739
Jan 13 09:30:55.773: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 09:30:55.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2364" for this suite. 01/13/23 09:30:55.846
------------------------------
• [SLOW TEST] [20.132 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:30:35.726
    Jan 13 09:30:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 09:30:35.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:30:35.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:30:35.768
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-2364 01/13/23 09:30:35.784
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[] 01/13/23 09:30:35.802
    Jan 13 09:30:35.814: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 13 09:30:36.888: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2364 01/13/23 09:30:36.888
    Jan 13 09:30:36.915: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2364" to be "running and ready"
    Jan 13 09:30:36.933: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.958814ms
    Jan 13 09:30:36.933: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:30:38.944: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029600367s
    Jan 13 09:30:38.944: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:30:40.977: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.062726844s
    Jan 13 09:30:40.977: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 13 09:30:40.978: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[pod1:[80]] 01/13/23 09:30:40.999
    Jan 13 09:30:41.033: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/13/23 09:30:41.033
    Jan 13 09:30:41.033: INFO: Creating new exec pod
    Jan 13 09:30:41.053: INFO: Waiting up to 5m0s for pod "execpod95zsx" in namespace "services-2364" to be "running"
    Jan 13 09:30:41.067: INFO: Pod "execpod95zsx": Phase="Pending", Reason="", readiness=false. Elapsed: 14.403644ms
    Jan 13 09:30:43.118: INFO: Pod "execpod95zsx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06492545s
    Jan 13 09:30:45.087: INFO: Pod "execpod95zsx": Phase="Running", Reason="", readiness=true. Elapsed: 4.033870445s
    Jan 13 09:30:45.087: INFO: Pod "execpod95zsx" satisfied condition "running"
    Jan 13 09:30:46.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 13 09:30:46.653: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 13 09:30:46.653: INFO: stdout: ""
    Jan 13 09:30:46.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 10.96.203.232 80'
    Jan 13 09:30:47.228: INFO: stderr: "+ nc -v -z -w 2 10.96.203.232 80\nConnection to 10.96.203.232 80 port [tcp/http] succeeded!\n"
    Jan 13 09:30:47.228: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-2364 01/13/23 09:30:47.228
    Jan 13 09:30:47.247: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2364" to be "running and ready"
    Jan 13 09:30:47.264: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.49064ms
    Jan 13 09:30:47.264: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:30:49.270: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022176711s
    Jan 13 09:30:49.270: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:30:51.273: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.025488677s
    Jan 13 09:30:51.273: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 13 09:30:51.273: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[pod1:[80] pod2:[80]] 01/13/23 09:30:51.279
    Jan 13 09:30:51.320: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/13/23 09:30:51.32
    Jan 13 09:30:52.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 13 09:30:52.993: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 13 09:30:52.994: INFO: stdout: ""
    Jan 13 09:30:52.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 10.96.203.232 80'
    Jan 13 09:30:53.520: INFO: stderr: "+ nc -v -z -w 2 10.96.203.232 80\nConnection to 10.96.203.232 80 port [tcp/http] succeeded!\n"
    Jan 13 09:30:53.520: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2364 01/13/23 09:30:53.52
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[pod2:[80]] 01/13/23 09:30:53.573
    Jan 13 09:30:53.690: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/13/23 09:30:53.69
    Jan 13 09:30:54.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 13 09:30:55.316: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 13 09:30:55.316: INFO: stdout: ""
    Jan 13 09:30:55.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-2364 exec execpod95zsx -- /bin/sh -x -c nc -v -z -w 2 10.96.203.232 80'
    Jan 13 09:30:55.717: INFO: stderr: "+ nc -v -z -w 2 10.96.203.232 80\nConnection to 10.96.203.232 80 port [tcp/http] succeeded!\n"
    Jan 13 09:30:55.717: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-2364 01/13/23 09:30:55.717
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2364 to expose endpoints map[] 01/13/23 09:30:55.739
    Jan 13 09:30:55.773: INFO: successfully validated that service endpoint-test2 in namespace services-2364 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:30:55.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2364" for this suite. 01/13/23 09:30:55.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:30:55.866
Jan 13 09:30:55.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 09:30:55.87
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:30:55.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:30:55.921
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/13/23 09:30:55.936
Jan 13 09:30:55.937: INFO: Creating simple deployment test-deployment-fmjf4
Jan 13 09:30:55.975: INFO: deployment "test-deployment-fmjf4" doesn't have the required revision set
Jan 13 09:30:58.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-fmjf4-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/13/23 09:31:00.109
Jan 13 09:31:00.119: INFO: Deployment test-deployment-fmjf4 has Conditions: [{Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/13/23 09:31:00.12
Jan 13 09:31:00.228: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 55, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-fmjf4-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/13/23 09:31:00.229
Jan 13 09:31:00.236: INFO: Observed &Deployment event: ADDED
Jan 13 09:31:00.236: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
Jan 13 09:31:00.237: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 13 09:31:00.237: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fmjf4-54bc444df" is progressing.}
Jan 13 09:31:00.237: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
Jan 13 09:31:00.246: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
Jan 13 09:31:00.246: INFO: Found Deployment test-deployment-fmjf4 in namespace deployment-3774 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 13 09:31:00.246: INFO: Deployment test-deployment-fmjf4 has an updated status
STEP: patching the Statefulset Status 01/13/23 09:31:00.246
Jan 13 09:31:00.247: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 13 09:31:00.300: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/13/23 09:31:00.301
Jan 13 09:31:00.321: INFO: Observed &Deployment event: ADDED
Jan 13 09:31:00.321: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
Jan 13 09:31:00.321: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.321: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
Jan 13 09:31:00.322: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 13 09:31:00.322: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.322: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 13 09:31:00.322: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fmjf4-54bc444df" is progressing.}
Jan 13 09:31:00.322: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.323: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 13 09:31:00.323: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
Jan 13 09:31:00.324: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.325: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 13 09:31:00.325: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
Jan 13 09:31:00.326: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 13 09:31:00.327: INFO: Observed &Deployment event: MODIFIED
Jan 13 09:31:00.327: INFO: Found deployment test-deployment-fmjf4 in namespace deployment-3774 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 13 09:31:00.327: INFO: Deployment test-deployment-fmjf4 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 09:31:00.343: INFO: Deployment "test-deployment-fmjf4":
&Deployment{ObjectMeta:{test-deployment-fmjf4  deployment-3774  3511fbec-2bcc-48ea-997e-471cd803f4b3 577844 1 2023-01-13 09:30:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-13 09:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-13 09:31:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-13 09:31:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002faf2e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-fmjf4-54bc444df",LastUpdateTime:2023-01-13 09:31:00 +0000 UTC,LastTransitionTime:2023-01-13 09:31:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 13 09:31:00.350: INFO: New ReplicaSet "test-deployment-fmjf4-54bc444df" of Deployment "test-deployment-fmjf4":
&ReplicaSet{ObjectMeta:{test-deployment-fmjf4-54bc444df  deployment-3774  108d9f11-7603-4f15-8118-359d07f15f9e 577839 1 2023-01-13 09:30:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-fmjf4 3511fbec-2bcc-48ea-997e-471cd803f4b3 0xc004e78110 0xc004e78111}] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3511fbec-2bcc-48ea-997e-471cd803f4b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 09:30:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e781c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 09:31:00.367: INFO: Pod "test-deployment-fmjf4-54bc444df-9wlmt" is available:
&Pod{ObjectMeta:{test-deployment-fmjf4-54bc444df-9wlmt test-deployment-fmjf4-54bc444df- deployment-3774  b5db40c5-c6bb-4281-af3c-20936cd6304c 577838 0 2023-01-13 09:30:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:00fdfcb4da1a0ca2be2412e468c56d371257084741851827ee834004411d2713 cni.projectcalico.org/podIP:10.244.27.202/32 cni.projectcalico.org/podIPs:10.244.27.202/32] [{apps/v1 ReplicaSet test-deployment-fmjf4-54bc444df 108d9f11-7603-4f15-8118-359d07f15f9e 0xc004eda090 0xc004eda091}] [] [{kube-controller-manager Update v1 2023-01-13 09:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"108d9f11-7603-4f15-8118-359d07f15f9e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:30:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:30:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpzxh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpzxh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.202,StartTime:2023-01-13 09:30:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:30:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://5189882742c22e702d55fae72abe6c3c433ea9562e59e8b258c9d47cdd0506b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 09:31:00.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3774" for this suite. 01/13/23 09:31:00.386
------------------------------
• [4.562 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:30:55.866
    Jan 13 09:30:55.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 09:30:55.87
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:30:55.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:30:55.921
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/13/23 09:30:55.936
    Jan 13 09:30:55.937: INFO: Creating simple deployment test-deployment-fmjf4
    Jan 13 09:30:55.975: INFO: deployment "test-deployment-fmjf4" doesn't have the required revision set
    Jan 13 09:30:58.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-fmjf4-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/13/23 09:31:00.109
    Jan 13 09:31:00.119: INFO: Deployment test-deployment-fmjf4 has Conditions: [{Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/13/23 09:31:00.12
    Jan 13 09:31:00.228: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 30, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 30, 55, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-fmjf4-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/13/23 09:31:00.229
    Jan 13 09:31:00.236: INFO: Observed &Deployment event: ADDED
    Jan 13 09:31:00.236: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
    Jan 13 09:31:00.237: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
    Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 13 09:31:00.237: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 13 09:31:00.237: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fmjf4-54bc444df" is progressing.}
    Jan 13 09:31:00.237: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
    Jan 13 09:31:00.246: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 13 09:31:00.246: INFO: Observed Deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
    Jan 13 09:31:00.246: INFO: Found Deployment test-deployment-fmjf4 in namespace deployment-3774 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 13 09:31:00.246: INFO: Deployment test-deployment-fmjf4 has an updated status
    STEP: patching the Statefulset Status 01/13/23 09:31:00.246
    Jan 13 09:31:00.247: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 13 09:31:00.300: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/13/23 09:31:00.301
    Jan 13 09:31:00.321: INFO: Observed &Deployment event: ADDED
    Jan 13 09:31:00.321: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
    Jan 13 09:31:00.321: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.321: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:55 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-fmjf4-54bc444df"}
    Jan 13 09:31:00.322: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 13 09:31:00.322: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.322: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 13 09:31:00.322: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:56 +0000 UTC 2023-01-13 09:30:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-fmjf4-54bc444df" is progressing.}
    Jan 13 09:31:00.322: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.323: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 13 09:31:00.323: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
    Jan 13 09:31:00.324: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.325: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 13 09:31:00.325: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-13 09:30:59 +0000 UTC 2023-01-13 09:30:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-fmjf4-54bc444df" has successfully progressed.}
    Jan 13 09:31:00.326: INFO: Observed deployment test-deployment-fmjf4 in namespace deployment-3774 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 13 09:31:00.327: INFO: Observed &Deployment event: MODIFIED
    Jan 13 09:31:00.327: INFO: Found deployment test-deployment-fmjf4 in namespace deployment-3774 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 13 09:31:00.327: INFO: Deployment test-deployment-fmjf4 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 09:31:00.343: INFO: Deployment "test-deployment-fmjf4":
    &Deployment{ObjectMeta:{test-deployment-fmjf4  deployment-3774  3511fbec-2bcc-48ea-997e-471cd803f4b3 577844 1 2023-01-13 09:30:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-13 09:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-13 09:31:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-13 09:31:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002faf2e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-fmjf4-54bc444df",LastUpdateTime:2023-01-13 09:31:00 +0000 UTC,LastTransitionTime:2023-01-13 09:31:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 13 09:31:00.350: INFO: New ReplicaSet "test-deployment-fmjf4-54bc444df" of Deployment "test-deployment-fmjf4":
    &ReplicaSet{ObjectMeta:{test-deployment-fmjf4-54bc444df  deployment-3774  108d9f11-7603-4f15-8118-359d07f15f9e 577839 1 2023-01-13 09:30:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-fmjf4 3511fbec-2bcc-48ea-997e-471cd803f4b3 0xc004e78110 0xc004e78111}] [] [{kube-controller-manager Update apps/v1 2023-01-13 09:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3511fbec-2bcc-48ea-997e-471cd803f4b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 09:30:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e781c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 09:31:00.367: INFO: Pod "test-deployment-fmjf4-54bc444df-9wlmt" is available:
    &Pod{ObjectMeta:{test-deployment-fmjf4-54bc444df-9wlmt test-deployment-fmjf4-54bc444df- deployment-3774  b5db40c5-c6bb-4281-af3c-20936cd6304c 577838 0 2023-01-13 09:30:55 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:00fdfcb4da1a0ca2be2412e468c56d371257084741851827ee834004411d2713 cni.projectcalico.org/podIP:10.244.27.202/32 cni.projectcalico.org/podIPs:10.244.27.202/32] [{apps/v1 ReplicaSet test-deployment-fmjf4-54bc444df 108d9f11-7603-4f15-8118-359d07f15f9e 0xc004eda090 0xc004eda091}] [] [{kube-controller-manager Update v1 2023-01-13 09:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"108d9f11-7603-4f15-8118-359d07f15f9e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 09:30:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 09:30:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpzxh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpzxh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 09:30:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.202,StartTime:2023-01-13 09:30:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 09:30:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://5189882742c22e702d55fae72abe6c3c433ea9562e59e8b258c9d47cdd0506b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:31:00.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3774" for this suite. 01/13/23 09:31:00.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:31:00.427
Jan 13 09:31:00.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-preemption 01/13/23 09:31:00.43
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:31:00.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:31:00.495
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 13 09:31:00.595: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 09:32:00.726: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 01/13/23 09:32:00.734
Jan 13 09:32:00.803: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 13 09:32:00.840: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 13 09:32:00.943: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 13 09:32:00.958: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/13/23 09:32:00.958
Jan 13 09:32:00.959: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6981" to be "running"
Jan 13 09:32:00.976: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.847701ms
Jan 13 09:32:02.985: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026137864s
Jan 13 09:32:05.004: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0456607s
Jan 13 09:32:06.985: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.025773368s
Jan 13 09:32:06.985: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 13 09:32:06.985: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6981" to be "running"
Jan 13 09:32:06.993: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.375385ms
Jan 13 09:32:06.993: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 13 09:32:06.993: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6981" to be "running"
Jan 13 09:32:07.011: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.64534ms
Jan 13 09:32:09.022: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.029142237s
Jan 13 09:32:09.022: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 13 09:32:09.022: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6981" to be "running"
Jan 13 09:32:09.030: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.877011ms
Jan 13 09:32:09.030: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/13/23 09:32:09.031
Jan 13 09:32:09.042: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6981" to be "running"
Jan 13 09:32:09.048: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162846ms
Jan 13 09:32:11.053: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01081888s
Jan 13 09:32:13.057: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015148634s
Jan 13 09:32:15.065: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02290417s
Jan 13 09:32:17.056: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.013870973s
Jan 13 09:32:17.056: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:32:17.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6981" for this suite. 01/13/23 09:32:17.176
------------------------------
• [SLOW TEST] [76.763 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:31:00.427
    Jan 13 09:31:00.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-preemption 01/13/23 09:31:00.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:31:00.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:31:00.495
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 13 09:31:00.595: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 13 09:32:00.726: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 01/13/23 09:32:00.734
    Jan 13 09:32:00.803: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 13 09:32:00.840: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 13 09:32:00.943: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 13 09:32:00.958: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/13/23 09:32:00.958
    Jan 13 09:32:00.959: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6981" to be "running"
    Jan 13 09:32:00.976: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.847701ms
    Jan 13 09:32:02.985: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026137864s
    Jan 13 09:32:05.004: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0456607s
    Jan 13 09:32:06.985: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.025773368s
    Jan 13 09:32:06.985: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 13 09:32:06.985: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6981" to be "running"
    Jan 13 09:32:06.993: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.375385ms
    Jan 13 09:32:06.993: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 13 09:32:06.993: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6981" to be "running"
    Jan 13 09:32:07.011: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 17.64534ms
    Jan 13 09:32:09.022: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.029142237s
    Jan 13 09:32:09.022: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 13 09:32:09.022: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6981" to be "running"
    Jan 13 09:32:09.030: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.877011ms
    Jan 13 09:32:09.030: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/13/23 09:32:09.031
    Jan 13 09:32:09.042: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6981" to be "running"
    Jan 13 09:32:09.048: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162846ms
    Jan 13 09:32:11.053: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01081888s
    Jan 13 09:32:13.057: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015148634s
    Jan 13 09:32:15.065: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02290417s
    Jan 13 09:32:17.056: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.013870973s
    Jan 13 09:32:17.056: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:32:17.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6981" for this suite. 01/13/23 09:32:17.176
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:32:17.191
Jan 13 09:32:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename disruption 01/13/23 09:32:17.194
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:17.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:17.239
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/13/23 09:32:17.256
STEP: Waiting for all pods to be running 01/13/23 09:32:19.334
Jan 13 09:32:19.350: INFO: running pods: 0 < 3
Jan 13 09:32:21.367: INFO: running pods: 0 < 3
Jan 13 09:32:23.360: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 13 09:32:25.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3393" for this suite. 01/13/23 09:32:25.513
------------------------------
• [SLOW TEST] [8.359 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:32:17.191
    Jan 13 09:32:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename disruption 01/13/23 09:32:17.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:17.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:17.239
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/13/23 09:32:17.256
    STEP: Waiting for all pods to be running 01/13/23 09:32:19.334
    Jan 13 09:32:19.350: INFO: running pods: 0 < 3
    Jan 13 09:32:21.367: INFO: running pods: 0 < 3
    Jan 13 09:32:23.360: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:32:25.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3393" for this suite. 01/13/23 09:32:25.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:32:25.555
Jan 13 09:32:25.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename watch 01/13/23 09:32:25.558
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:25.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:25.639
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/13/23 09:32:25.647
STEP: creating a new configmap 01/13/23 09:32:25.649
STEP: modifying the configmap once 01/13/23 09:32:25.659
STEP: closing the watch once it receives two notifications 01/13/23 09:32:25.677
Jan 13 09:32:25.678: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578229 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:32:25.678: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578230 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/13/23 09:32:25.678
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/13/23 09:32:25.695
STEP: deleting the configmap 01/13/23 09:32:25.698
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/13/23 09:32:25.711
Jan 13 09:32:25.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578231 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:32:25.712: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578232 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 13 09:32:25.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1827" for this suite. 01/13/23 09:32:25.722
------------------------------
• [0.183 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:32:25.555
    Jan 13 09:32:25.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename watch 01/13/23 09:32:25.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:25.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:25.639
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/13/23 09:32:25.647
    STEP: creating a new configmap 01/13/23 09:32:25.649
    STEP: modifying the configmap once 01/13/23 09:32:25.659
    STEP: closing the watch once it receives two notifications 01/13/23 09:32:25.677
    Jan 13 09:32:25.678: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578229 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:32:25.678: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578230 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/13/23 09:32:25.678
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/13/23 09:32:25.695
    STEP: deleting the configmap 01/13/23 09:32:25.698
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/13/23 09:32:25.711
    Jan 13 09:32:25.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578231 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 13 09:32:25.712: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1827  d0e662e7-f779-4eb9-837a-9d832c88faa7 578232 0 2023-01-13 09:32:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-13 09:32:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:32:25.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1827" for this suite. 01/13/23 09:32:25.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:32:25.741
Jan 13 09:32:25.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir-wrapper 01/13/23 09:32:25.743
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:25.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:25.782
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 13 09:32:25.829: INFO: Waiting up to 5m0s for pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25" in namespace "emptydir-wrapper-1836" to be "running and ready"
Jan 13 09:32:25.840: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.537598ms
Jan 13 09:32:25.840: INFO: The phase of Pod pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:32:27.853: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023611539s
Jan 13 09:32:27.853: INFO: The phase of Pod pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:32:29.856: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25": Phase="Running", Reason="", readiness=true. Elapsed: 4.026362049s
Jan 13 09:32:29.856: INFO: The phase of Pod pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25 is Running (Ready = true)
Jan 13 09:32:29.856: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/13/23 09:32:29.865
STEP: Cleaning up the configmap 01/13/23 09:32:29.893
STEP: Cleaning up the pod 01/13/23 09:32:29.908
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:32:29.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1836" for this suite. 01/13/23 09:32:29.959
------------------------------
• [4.239 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:32:25.741
    Jan 13 09:32:25.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir-wrapper 01/13/23 09:32:25.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:25.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:25.782
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 13 09:32:25.829: INFO: Waiting up to 5m0s for pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25" in namespace "emptydir-wrapper-1836" to be "running and ready"
    Jan 13 09:32:25.840: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.537598ms
    Jan 13 09:32:25.840: INFO: The phase of Pod pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:32:27.853: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023611539s
    Jan 13 09:32:27.853: INFO: The phase of Pod pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:32:29.856: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25": Phase="Running", Reason="", readiness=true. Elapsed: 4.026362049s
    Jan 13 09:32:29.856: INFO: The phase of Pod pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25 is Running (Ready = true)
    Jan 13 09:32:29.856: INFO: Pod "pod-secrets-c1090c92-7385-40fa-bea3-a3cdae15ea25" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/13/23 09:32:29.865
    STEP: Cleaning up the configmap 01/13/23 09:32:29.893
    STEP: Cleaning up the pod 01/13/23 09:32:29.908
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:32:29.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1836" for this suite. 01/13/23 09:32:29.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:32:29.981
Jan 13 09:32:29.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 09:32:29.985
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:30.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:30.053
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/13/23 09:32:30.062
Jan 13 09:32:30.082: INFO: Waiting up to 5m0s for pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1" in namespace "emptydir-1360" to be "Succeeded or Failed"
Jan 13 09:32:30.088: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.765144ms
Jan 13 09:32:32.094: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01185503s
Jan 13 09:32:34.100: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018202883s
Jan 13 09:32:36.096: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013760324s
Jan 13 09:32:38.103: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020700017s
STEP: Saw pod success 01/13/23 09:32:38.103
Jan 13 09:32:38.103: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1" satisfied condition "Succeeded or Failed"
Jan 13 09:32:38.113: INFO: Trying to get logs from node 10.10.102.32-node pod pod-369e73b2-c9d4-47e1-8289-21571d8266b1 container test-container: <nil>
STEP: delete the pod 01/13/23 09:32:38.187
Jan 13 09:32:38.206: INFO: Waiting for pod pod-369e73b2-c9d4-47e1-8289-21571d8266b1 to disappear
Jan 13 09:32:38.215: INFO: Pod pod-369e73b2-c9d4-47e1-8289-21571d8266b1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:32:38.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1360" for this suite. 01/13/23 09:32:38.226
------------------------------
• [SLOW TEST] [8.278 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:32:29.981
    Jan 13 09:32:29.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 09:32:29.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:30.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:30.053
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/13/23 09:32:30.062
    Jan 13 09:32:30.082: INFO: Waiting up to 5m0s for pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1" in namespace "emptydir-1360" to be "Succeeded or Failed"
    Jan 13 09:32:30.088: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.765144ms
    Jan 13 09:32:32.094: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01185503s
    Jan 13 09:32:34.100: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018202883s
    Jan 13 09:32:36.096: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013760324s
    Jan 13 09:32:38.103: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020700017s
    STEP: Saw pod success 01/13/23 09:32:38.103
    Jan 13 09:32:38.103: INFO: Pod "pod-369e73b2-c9d4-47e1-8289-21571d8266b1" satisfied condition "Succeeded or Failed"
    Jan 13 09:32:38.113: INFO: Trying to get logs from node 10.10.102.32-node pod pod-369e73b2-c9d4-47e1-8289-21571d8266b1 container test-container: <nil>
    STEP: delete the pod 01/13/23 09:32:38.187
    Jan 13 09:32:38.206: INFO: Waiting for pod pod-369e73b2-c9d4-47e1-8289-21571d8266b1 to disappear
    Jan 13 09:32:38.215: INFO: Pod pod-369e73b2-c9d4-47e1-8289-21571d8266b1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:32:38.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1360" for this suite. 01/13/23 09:32:38.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:32:38.265
Jan 13 09:32:38.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename conformance-tests 01/13/23 09:32:38.27
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:38.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:38.336
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/13/23 09:32:38.343
Jan 13 09:32:38.343: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 13 09:32:38.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-2676" for this suite. 01/13/23 09:32:38.373
------------------------------
• [0.121 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:32:38.265
    Jan 13 09:32:38.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename conformance-tests 01/13/23 09:32:38.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:38.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:38.336
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/13/23 09:32:38.343
    Jan 13 09:32:38.343: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:32:38.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-2676" for this suite. 01/13/23 09:32:38.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:32:38.396
Jan 13 09:32:38.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename taint-multiple-pods 01/13/23 09:32:38.399
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:38.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:38.451
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 13 09:32:38.468: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 09:33:38.518: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 13 09:33:38.523: INFO: Starting informer...
STEP: Starting pods... 01/13/23 09:33:38.523
Jan 13 09:33:38.760: INFO: Pod1 is running on 10.10.102.31-node. Tainting Node
Jan 13 09:33:38.982: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3172" to be "running"
Jan 13 09:33:38.991: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385949ms
Jan 13 09:33:40.999: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017095756s
Jan 13 09:33:42.998: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.015757751s
Jan 13 09:33:42.998: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 13 09:33:42.998: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3172" to be "running"
Jan 13 09:33:43.004: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 6.698678ms
Jan 13 09:33:43.004: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 13 09:33:43.004: INFO: Pod2 is running on 10.10.102.31-node. Tainting Node
STEP: Trying to apply a taint on the Node 01/13/23 09:33:43.004
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:33:43.038
STEP: Waiting for Pod1 and Pod2 to be deleted 01/13/23 09:33:43.046
Jan 13 09:33:49.701: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 13 09:34:10.184: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:34:10.219
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:34:10.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-3172" for this suite. 01/13/23 09:34:10.236
------------------------------
• [SLOW TEST] [91.858 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:32:38.396
    Jan 13 09:32:38.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename taint-multiple-pods 01/13/23 09:32:38.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:32:38.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:32:38.451
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 13 09:32:38.468: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 13 09:33:38.518: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 13 09:33:38.523: INFO: Starting informer...
    STEP: Starting pods... 01/13/23 09:33:38.523
    Jan 13 09:33:38.760: INFO: Pod1 is running on 10.10.102.31-node. Tainting Node
    Jan 13 09:33:38.982: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3172" to be "running"
    Jan 13 09:33:38.991: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385949ms
    Jan 13 09:33:40.999: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017095756s
    Jan 13 09:33:42.998: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.015757751s
    Jan 13 09:33:42.998: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 13 09:33:42.998: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3172" to be "running"
    Jan 13 09:33:43.004: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 6.698678ms
    Jan 13 09:33:43.004: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 13 09:33:43.004: INFO: Pod2 is running on 10.10.102.31-node. Tainting Node
    STEP: Trying to apply a taint on the Node 01/13/23 09:33:43.004
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:33:43.038
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/13/23 09:33:43.046
    Jan 13 09:33:49.701: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 13 09:34:10.184: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/13/23 09:34:10.219
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:34:10.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-3172" for this suite. 01/13/23 09:34:10.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:34:10.259
Jan 13 09:34:10.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:34:10.267
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:10.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:10.356
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 13 09:34:10.433: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5826 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 13 09:34:10.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5826" for this suite. 01/13/23 09:34:10.478
------------------------------
• [0.233 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:34:10.259
    Jan 13 09:34:10.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:34:10.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:10.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:10.356
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 13 09:34:10.433: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5826 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:34:10.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5826" for this suite. 01/13/23 09:34:10.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:34:10.514
Jan 13 09:34:10.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replicaset 01/13/23 09:34:10.516
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:10.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:10.578
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/13/23 09:34:10.594
STEP: Verify that the required pods have come up 01/13/23 09:34:10.62
Jan 13 09:34:10.628: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 13 09:34:15.652: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/13/23 09:34:15.652
Jan 13 09:34:15.680: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/13/23 09:34:15.681
STEP: DeleteCollection of the ReplicaSets 01/13/23 09:34:15.715
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/13/23 09:34:15.752
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:34:15.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9265" for this suite. 01/13/23 09:34:15.789
------------------------------
• [SLOW TEST] [5.307 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:34:10.514
    Jan 13 09:34:10.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replicaset 01/13/23 09:34:10.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:10.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:10.578
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/13/23 09:34:10.594
    STEP: Verify that the required pods have come up 01/13/23 09:34:10.62
    Jan 13 09:34:10.628: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 13 09:34:15.652: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/13/23 09:34:15.652
    Jan 13 09:34:15.680: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/13/23 09:34:15.681
    STEP: DeleteCollection of the ReplicaSets 01/13/23 09:34:15.715
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/13/23 09:34:15.752
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:34:15.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9265" for this suite. 01/13/23 09:34:15.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:34:15.822
Jan 13 09:34:15.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename daemonsets 01/13/23 09:34:15.827
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:15.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:15.904
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/13/23 09:34:15.968
STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:34:15.98
Jan 13 09:34:15.991: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:15.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:34:15.998: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:17.026: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:17.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:34:17.074: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:18.015: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:18.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:34:18.035: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:19.013: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:19.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:34:19.026: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:20.014: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:20.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:34:20.026: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
Jan 13 09:34:21.017: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:21.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 09:34:21.028: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/13/23 09:34:21.033
Jan 13 09:34:21.071: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:21.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:34:21.095: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:22.110: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:22.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:34:22.118: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:23.105: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:23.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:34:23.115: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:24.156: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:24.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:34:24.166: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:25.107: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:25.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:34:25.115: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:26.108: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:26.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:34:26.115: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:34:27.104: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:34:27.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 09:34:27.117: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:34:27.131
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8928, will wait for the garbage collector to delete the pods 01/13/23 09:34:27.131
Jan 13 09:34:27.223: INFO: Deleting DaemonSet.extensions daemon-set took: 17.392117ms
Jan 13 09:34:27.325: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.537345ms
Jan 13 09:34:30.855: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:34:30.855: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 13 09:34:30.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"578823"},"items":null}

Jan 13 09:34:30.942: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"578823"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:34:31.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8928" for this suite. 01/13/23 09:34:31.068
------------------------------
• [SLOW TEST] [15.284 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:34:15.822
    Jan 13 09:34:15.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename daemonsets 01/13/23 09:34:15.827
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:15.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:15.904
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/13/23 09:34:15.968
    STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:34:15.98
    Jan 13 09:34:15.991: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:15.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:34:15.998: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:17.026: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:17.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:34:17.074: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:18.015: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:18.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:34:18.035: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:19.013: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:19.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:34:19.026: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:20.014: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:20.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:34:20.026: INFO: Node 10.10.102.32-node is running 0 daemon pod, expected 1
    Jan 13 09:34:21.017: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:21.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 09:34:21.028: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/13/23 09:34:21.033
    Jan 13 09:34:21.071: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:21.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:34:21.095: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:22.110: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:22.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:34:22.118: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:23.105: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:23.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:34:23.115: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:24.156: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:24.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:34:24.166: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:25.107: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:25.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:34:25.115: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:26.108: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:26.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:34:26.115: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:34:27.104: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:34:27.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 09:34:27.117: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:34:27.131
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8928, will wait for the garbage collector to delete the pods 01/13/23 09:34:27.131
    Jan 13 09:34:27.223: INFO: Deleting DaemonSet.extensions daemon-set took: 17.392117ms
    Jan 13 09:34:27.325: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.537345ms
    Jan 13 09:34:30.855: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:34:30.855: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 13 09:34:30.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"578823"},"items":null}

    Jan 13 09:34:30.942: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"578823"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:34:31.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8928" for this suite. 01/13/23 09:34:31.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:34:31.111
Jan 13 09:34:31.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 09:34:31.125
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:31.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:31.194
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1020 01/13/23 09:34:31.202
STEP: changing the ExternalName service to type=NodePort 01/13/23 09:34:31.216
STEP: creating replication controller externalname-service in namespace services-1020 01/13/23 09:34:31.266
I0113 09:34:31.283068      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1020, replica count: 2
I0113 09:34:34.334119      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 09:34:37.341665      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 09:34:37.341: INFO: Creating new exec pod
Jan 13 09:34:37.351: INFO: Waiting up to 5m0s for pod "execpodbz4hj" in namespace "services-1020" to be "running"
Jan 13 09:34:37.361: INFO: Pod "execpodbz4hj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.907712ms
Jan 13 09:34:39.374: INFO: Pod "execpodbz4hj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022576965s
Jan 13 09:34:41.368: INFO: Pod "execpodbz4hj": Phase="Running", Reason="", readiness=true. Elapsed: 4.016380194s
Jan 13 09:34:41.368: INFO: Pod "execpodbz4hj" satisfied condition "running"
Jan 13 09:34:42.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 13 09:34:43.064: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 13 09:34:43.064: INFO: stdout: ""
Jan 13 09:34:43.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 10.106.150.59 80'
Jan 13 09:34:43.449: INFO: stderr: "+ nc -v -z -w 2 10.106.150.59 80\nConnection to 10.106.150.59 80 port [tcp/http] succeeded!\n"
Jan 13 09:34:43.449: INFO: stdout: ""
Jan 13 09:34:43.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 30652'
Jan 13 09:34:44.032: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 30652\nConnection to 10.10.102.31 30652 port [tcp/*] succeeded!\n"
Jan 13 09:34:44.032: INFO: stdout: ""
Jan 13 09:34:44.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 30652'
Jan 13 09:34:44.530: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 30652\nConnection to 10.10.102.32 30652 port [tcp/*] succeeded!\n"
Jan 13 09:34:44.530: INFO: stdout: ""
Jan 13 09:34:44.530: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 09:34:44.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1020" for this suite. 01/13/23 09:34:44.609
------------------------------
• [SLOW TEST] [13.507 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:34:31.111
    Jan 13 09:34:31.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 09:34:31.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:31.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:31.194
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1020 01/13/23 09:34:31.202
    STEP: changing the ExternalName service to type=NodePort 01/13/23 09:34:31.216
    STEP: creating replication controller externalname-service in namespace services-1020 01/13/23 09:34:31.266
    I0113 09:34:31.283068      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1020, replica count: 2
    I0113 09:34:34.334119      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 09:34:37.341665      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 09:34:37.341: INFO: Creating new exec pod
    Jan 13 09:34:37.351: INFO: Waiting up to 5m0s for pod "execpodbz4hj" in namespace "services-1020" to be "running"
    Jan 13 09:34:37.361: INFO: Pod "execpodbz4hj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.907712ms
    Jan 13 09:34:39.374: INFO: Pod "execpodbz4hj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022576965s
    Jan 13 09:34:41.368: INFO: Pod "execpodbz4hj": Phase="Running", Reason="", readiness=true. Elapsed: 4.016380194s
    Jan 13 09:34:41.368: INFO: Pod "execpodbz4hj" satisfied condition "running"
    Jan 13 09:34:42.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 13 09:34:43.064: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 13 09:34:43.064: INFO: stdout: ""
    Jan 13 09:34:43.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 10.106.150.59 80'
    Jan 13 09:34:43.449: INFO: stderr: "+ nc -v -z -w 2 10.106.150.59 80\nConnection to 10.106.150.59 80 port [tcp/http] succeeded!\n"
    Jan 13 09:34:43.449: INFO: stdout: ""
    Jan 13 09:34:43.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 30652'
    Jan 13 09:34:44.032: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 30652\nConnection to 10.10.102.31 30652 port [tcp/*] succeeded!\n"
    Jan 13 09:34:44.032: INFO: stdout: ""
    Jan 13 09:34:44.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1020 exec execpodbz4hj -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 30652'
    Jan 13 09:34:44.530: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 30652\nConnection to 10.10.102.32 30652 port [tcp/*] succeeded!\n"
    Jan 13 09:34:44.530: INFO: stdout: ""
    Jan 13 09:34:44.530: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:34:44.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1020" for this suite. 01/13/23 09:34:44.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:34:44.622
Jan 13 09:34:44.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 09:34:44.624
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:44.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:44.702
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3281 01/13/23 09:34:44.714
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/13/23 09:34:44.741
STEP: Creating pod with conflicting port in namespace statefulset-3281 01/13/23 09:34:44.754
STEP: Waiting until pod test-pod will start running in namespace statefulset-3281 01/13/23 09:34:44.775
Jan 13 09:34:44.776: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3281" to be "running"
Jan 13 09:34:44.794: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.741012ms
Jan 13 09:34:46.806: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030854137s
Jan 13 09:34:48.803: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.027123168s
Jan 13 09:34:48.803: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3281 01/13/23 09:34:48.803
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3281 01/13/23 09:34:48.815
Jan 13 09:34:48.842: INFO: Observed stateful pod in namespace: statefulset-3281, name: ss-0, uid: e0749664-0fca-49d4-95d0-557f62ce3e32, status phase: Pending. Waiting for statefulset controller to delete.
Jan 13 09:34:48.917: INFO: Observed stateful pod in namespace: statefulset-3281, name: ss-0, uid: e0749664-0fca-49d4-95d0-557f62ce3e32, status phase: Failed. Waiting for statefulset controller to delete.
Jan 13 09:34:48.959: INFO: Observed stateful pod in namespace: statefulset-3281, name: ss-0, uid: e0749664-0fca-49d4-95d0-557f62ce3e32, status phase: Failed. Waiting for statefulset controller to delete.
Jan 13 09:34:48.970: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3281
STEP: Removing pod with conflicting port in namespace statefulset-3281 01/13/23 09:34:48.97
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3281 and will be in running state 01/13/23 09:34:49.025
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 09:34:55.059: INFO: Deleting all statefulset in ns statefulset-3281
Jan 13 09:34:55.068: INFO: Scaling statefulset ss to 0
Jan 13 09:35:05.118: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:35:05.126: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:35:05.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3281" for this suite. 01/13/23 09:35:05.175
------------------------------
• [SLOW TEST] [20.581 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:34:44.622
    Jan 13 09:34:44.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 09:34:44.624
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:34:44.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:34:44.702
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3281 01/13/23 09:34:44.714
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/13/23 09:34:44.741
    STEP: Creating pod with conflicting port in namespace statefulset-3281 01/13/23 09:34:44.754
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3281 01/13/23 09:34:44.775
    Jan 13 09:34:44.776: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3281" to be "running"
    Jan 13 09:34:44.794: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.741012ms
    Jan 13 09:34:46.806: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030854137s
    Jan 13 09:34:48.803: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.027123168s
    Jan 13 09:34:48.803: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3281 01/13/23 09:34:48.803
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3281 01/13/23 09:34:48.815
    Jan 13 09:34:48.842: INFO: Observed stateful pod in namespace: statefulset-3281, name: ss-0, uid: e0749664-0fca-49d4-95d0-557f62ce3e32, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 13 09:34:48.917: INFO: Observed stateful pod in namespace: statefulset-3281, name: ss-0, uid: e0749664-0fca-49d4-95d0-557f62ce3e32, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 13 09:34:48.959: INFO: Observed stateful pod in namespace: statefulset-3281, name: ss-0, uid: e0749664-0fca-49d4-95d0-557f62ce3e32, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 13 09:34:48.970: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3281
    STEP: Removing pod with conflicting port in namespace statefulset-3281 01/13/23 09:34:48.97
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3281 and will be in running state 01/13/23 09:34:49.025
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 09:34:55.059: INFO: Deleting all statefulset in ns statefulset-3281
    Jan 13 09:34:55.068: INFO: Scaling statefulset ss to 0
    Jan 13 09:35:05.118: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:35:05.126: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:35:05.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3281" for this suite. 01/13/23 09:35:05.175
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:35:05.21
Jan 13 09:35:05.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 09:35:05.22
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:05.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:05.302
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/13/23 09:35:05.319
Jan 13 09:35:05.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 09:35:08.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:35:22.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5187" for this suite. 01/13/23 09:35:22.216
------------------------------
• [SLOW TEST] [17.053 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:35:05.21
    Jan 13 09:35:05.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 09:35:05.22
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:05.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:05.302
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/13/23 09:35:05.319
    Jan 13 09:35:05.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 09:35:08.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:35:22.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5187" for this suite. 01/13/23 09:35:22.216
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:35:22.264
Jan 13 09:35:22.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:35:22.267
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:22.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:22.311
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/13/23 09:35:22.319
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/13/23 09:35:22.327
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/13/23 09:35:22.327
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/13/23 09:35:22.328
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/13/23 09:35:22.332
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/13/23 09:35:22.332
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/13/23 09:35:22.334
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:35:22.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3554" for this suite. 01/13/23 09:35:22.346
------------------------------
• [0.100 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:35:22.264
    Jan 13 09:35:22.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:35:22.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:22.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:22.311
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/13/23 09:35:22.319
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/13/23 09:35:22.327
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/13/23 09:35:22.327
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/13/23 09:35:22.328
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/13/23 09:35:22.332
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/13/23 09:35:22.332
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/13/23 09:35:22.334
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:35:22.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3554" for this suite. 01/13/23 09:35:22.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:35:22.368
Jan 13 09:35:22.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename cronjob 01/13/23 09:35:22.374
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:22.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:22.428
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/13/23 09:35:22.44
STEP: creating 01/13/23 09:35:22.441
STEP: getting 01/13/23 09:35:22.449
STEP: listing 01/13/23 09:35:22.456
STEP: watching 01/13/23 09:35:22.469
Jan 13 09:35:22.469: INFO: starting watch
STEP: cluster-wide listing 01/13/23 09:35:22.474
STEP: cluster-wide watching 01/13/23 09:35:22.486
Jan 13 09:35:22.487: INFO: starting watch
STEP: patching 01/13/23 09:35:22.491
STEP: updating 01/13/23 09:35:22.523
Jan 13 09:35:22.584: INFO: waiting for watch events with expected annotations
Jan 13 09:35:22.584: INFO: saw patched and updated annotations
STEP: patching /status 01/13/23 09:35:22.584
STEP: updating /status 01/13/23 09:35:22.613
STEP: get /status 01/13/23 09:35:22.639
STEP: deleting 01/13/23 09:35:22.656
STEP: deleting a collection 01/13/23 09:35:22.72
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 13 09:35:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7103" for this suite. 01/13/23 09:35:22.753
------------------------------
• [0.396 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:35:22.368
    Jan 13 09:35:22.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename cronjob 01/13/23 09:35:22.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:22.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:22.428
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/13/23 09:35:22.44
    STEP: creating 01/13/23 09:35:22.441
    STEP: getting 01/13/23 09:35:22.449
    STEP: listing 01/13/23 09:35:22.456
    STEP: watching 01/13/23 09:35:22.469
    Jan 13 09:35:22.469: INFO: starting watch
    STEP: cluster-wide listing 01/13/23 09:35:22.474
    STEP: cluster-wide watching 01/13/23 09:35:22.486
    Jan 13 09:35:22.487: INFO: starting watch
    STEP: patching 01/13/23 09:35:22.491
    STEP: updating 01/13/23 09:35:22.523
    Jan 13 09:35:22.584: INFO: waiting for watch events with expected annotations
    Jan 13 09:35:22.584: INFO: saw patched and updated annotations
    STEP: patching /status 01/13/23 09:35:22.584
    STEP: updating /status 01/13/23 09:35:22.613
    STEP: get /status 01/13/23 09:35:22.639
    STEP: deleting 01/13/23 09:35:22.656
    STEP: deleting a collection 01/13/23 09:35:22.72
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:35:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7103" for this suite. 01/13/23 09:35:22.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:35:22.765
Jan 13 09:35:22.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 09:35:22.768
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:22.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:22.821
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/13/23 09:35:22.834
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/13/23 09:35:22.834
STEP: creating a pod to probe DNS 01/13/23 09:35:22.834
STEP: submitting the pod to kubernetes 01/13/23 09:35:22.835
Jan 13 09:35:22.856: INFO: Waiting up to 15m0s for pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758" in namespace "dns-8493" to be "running"
Jan 13 09:35:22.864: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758": Phase="Pending", Reason="", readiness=false. Elapsed: 7.696177ms
Jan 13 09:35:24.883: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026908478s
Jan 13 09:35:26.872: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758": Phase="Running", Reason="", readiness=true. Elapsed: 4.016125674s
Jan 13 09:35:26.872: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758" satisfied condition "running"
STEP: retrieving the pod 01/13/23 09:35:26.872
STEP: looking for the results for each expected name from probers 01/13/23 09:35:26.879
Jan 13 09:35:26.926: INFO: DNS probes using dns-8493/dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758 succeeded

STEP: deleting the pod 01/13/23 09:35:26.926
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 09:35:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8493" for this suite. 01/13/23 09:35:26.979
------------------------------
• [4.231 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:35:22.765
    Jan 13 09:35:22.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 09:35:22.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:22.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:22.821
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/13/23 09:35:22.834
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/13/23 09:35:22.834
    STEP: creating a pod to probe DNS 01/13/23 09:35:22.834
    STEP: submitting the pod to kubernetes 01/13/23 09:35:22.835
    Jan 13 09:35:22.856: INFO: Waiting up to 15m0s for pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758" in namespace "dns-8493" to be "running"
    Jan 13 09:35:22.864: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758": Phase="Pending", Reason="", readiness=false. Elapsed: 7.696177ms
    Jan 13 09:35:24.883: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026908478s
    Jan 13 09:35:26.872: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758": Phase="Running", Reason="", readiness=true. Elapsed: 4.016125674s
    Jan 13 09:35:26.872: INFO: Pod "dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 09:35:26.872
    STEP: looking for the results for each expected name from probers 01/13/23 09:35:26.879
    Jan 13 09:35:26.926: INFO: DNS probes using dns-8493/dns-test-fcb00d9f-74ed-4929-8b91-443d67e8f758 succeeded

    STEP: deleting the pod 01/13/23 09:35:26.926
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:35:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8493" for this suite. 01/13/23 09:35:26.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:35:27.004
Jan 13 09:35:27.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename namespaces 01/13/23 09:35:27.008
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:27.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:27.093
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/13/23 09:35:27.1
Jan 13 09:35:27.107: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/13/23 09:35:27.107
Jan 13 09:35:27.128: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/13/23 09:35:27.128
Jan 13 09:35:27.153: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:35:27.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6895" for this suite. 01/13/23 09:35:27.178
------------------------------
• [0.191 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:35:27.004
    Jan 13 09:35:27.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename namespaces 01/13/23 09:35:27.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:27.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:27.093
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/13/23 09:35:27.1
    Jan 13 09:35:27.107: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/13/23 09:35:27.107
    Jan 13 09:35:27.128: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/13/23 09:35:27.128
    Jan 13 09:35:27.153: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:35:27.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6895" for this suite. 01/13/23 09:35:27.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:35:27.197
Jan 13 09:35:27.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 09:35:27.199
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:27.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:27.277
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 13 09:35:27.379: INFO: Waiting up to 2m0s for pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" in namespace "var-expansion-3201" to be "container 0 failed with reason CreateContainerConfigError"
Jan 13 09:35:27.418: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 38.083422ms
Jan 13 09:35:29.440: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060166495s
Jan 13 09:35:31.431: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051823947s
Jan 13 09:35:31.431: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 13 09:35:31.431: INFO: Deleting pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" in namespace "var-expansion-3201"
Jan 13 09:35:31.451: INFO: Wait up to 5m0s for pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 09:35:35.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3201" for this suite. 01/13/23 09:35:35.539
------------------------------
• [SLOW TEST] [8.400 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:35:27.197
    Jan 13 09:35:27.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 09:35:27.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:27.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:27.277
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 13 09:35:27.379: INFO: Waiting up to 2m0s for pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" in namespace "var-expansion-3201" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 13 09:35:27.418: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 38.083422ms
    Jan 13 09:35:29.440: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060166495s
    Jan 13 09:35:31.431: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051823947s
    Jan 13 09:35:31.431: INFO: Pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 13 09:35:31.431: INFO: Deleting pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" in namespace "var-expansion-3201"
    Jan 13 09:35:31.451: INFO: Wait up to 5m0s for pod "var-expansion-49f86454-89f2-4521-beb3-4bbf16aff5dc" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:35:35.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3201" for this suite. 01/13/23 09:35:35.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:35:35.598
Jan 13 09:35:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 09:35:35.601
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:35.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:35.661
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-02b309a4-2f73-4fec-b1ed-086616259f7b in namespace container-probe-6086 01/13/23 09:35:35.692
Jan 13 09:35:35.779: INFO: Waiting up to 5m0s for pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b" in namespace "container-probe-6086" to be "not pending"
Jan 13 09:35:35.787: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.782404ms
Jan 13 09:35:37.801: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021431156s
Jan 13 09:35:39.816: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b": Phase="Running", Reason="", readiness=true. Elapsed: 4.037384393s
Jan 13 09:35:39.817: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b" satisfied condition "not pending"
Jan 13 09:35:39.817: INFO: Started pod liveness-02b309a4-2f73-4fec-b1ed-086616259f7b in namespace container-probe-6086
STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 09:35:39.817
Jan 13 09:35:39.853: INFO: Initial restart count of pod liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is 0
Jan 13 09:35:58.008: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 1 (18.154756234s elapsed)
Jan 13 09:36:18.149: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 2 (38.295759645s elapsed)
Jan 13 09:36:38.268: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 3 (58.414402276s elapsed)
Jan 13 09:36:58.397: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 4 (1m18.543635194s elapsed)
Jan 13 09:38:00.770: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 5 (2m20.916878403s elapsed)
STEP: deleting the pod 01/13/23 09:38:00.77
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 09:38:00.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6086" for this suite. 01/13/23 09:38:00.975
------------------------------
• [SLOW TEST] [145.420 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:35:35.598
    Jan 13 09:35:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 09:35:35.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:35:35.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:35:35.661
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-02b309a4-2f73-4fec-b1ed-086616259f7b in namespace container-probe-6086 01/13/23 09:35:35.692
    Jan 13 09:35:35.779: INFO: Waiting up to 5m0s for pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b" in namespace "container-probe-6086" to be "not pending"
    Jan 13 09:35:35.787: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.782404ms
    Jan 13 09:35:37.801: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021431156s
    Jan 13 09:35:39.816: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b": Phase="Running", Reason="", readiness=true. Elapsed: 4.037384393s
    Jan 13 09:35:39.817: INFO: Pod "liveness-02b309a4-2f73-4fec-b1ed-086616259f7b" satisfied condition "not pending"
    Jan 13 09:35:39.817: INFO: Started pod liveness-02b309a4-2f73-4fec-b1ed-086616259f7b in namespace container-probe-6086
    STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 09:35:39.817
    Jan 13 09:35:39.853: INFO: Initial restart count of pod liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is 0
    Jan 13 09:35:58.008: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 1 (18.154756234s elapsed)
    Jan 13 09:36:18.149: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 2 (38.295759645s elapsed)
    Jan 13 09:36:38.268: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 3 (58.414402276s elapsed)
    Jan 13 09:36:58.397: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 4 (1m18.543635194s elapsed)
    Jan 13 09:38:00.770: INFO: Restart count of pod container-probe-6086/liveness-02b309a4-2f73-4fec-b1ed-086616259f7b is now 5 (2m20.916878403s elapsed)
    STEP: deleting the pod 01/13/23 09:38:00.77
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:38:00.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6086" for this suite. 01/13/23 09:38:00.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:38:01.021
Jan 13 09:38:01.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename aggregator 01/13/23 09:38:01.025
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:01.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:01.147
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 13 09:38:01.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/13/23 09:38:01.173
Jan 13 09:38:02.020: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 13 09:38:04.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:06.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:08.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:10.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:12.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:14.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:16.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:18.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:20.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:22.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:24.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:38:26.520: INFO: Waited 229.594147ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/13/23 09:38:26.683
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/13/23 09:38:26.692
STEP: List APIServices 01/13/23 09:38:26.709
Jan 13 09:38:26.727: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 13 09:38:27.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-679" for this suite. 01/13/23 09:38:27.076
------------------------------
• [SLOW TEST] [26.090 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:38:01.021
    Jan 13 09:38:01.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename aggregator 01/13/23 09:38:01.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:01.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:01.147
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 13 09:38:01.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/13/23 09:38:01.173
    Jan 13 09:38:02.020: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 13 09:38:04.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:06.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:08.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:10.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:12.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:14.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:16.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:18.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:20.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:22.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:24.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 38, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:38:26.520: INFO: Waited 229.594147ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/13/23 09:38:26.683
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/13/23 09:38:26.692
    STEP: List APIServices 01/13/23 09:38:26.709
    Jan 13 09:38:26.727: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:38:27.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-679" for this suite. 01/13/23 09:38:27.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:38:27.113
Jan 13 09:38:27.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replicaset 01/13/23 09:38:27.115
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:27.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:27.171
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 13 09:38:27.182: INFO: Creating ReplicaSet my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3
Jan 13 09:38:27.208: INFO: Pod name my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3: Found 0 pods out of 1
Jan 13 09:38:32.218: INFO: Pod name my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3: Found 1 pods out of 1
Jan 13 09:38:32.218: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3" is running
Jan 13 09:38:32.218: INFO: Waiting up to 5m0s for pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25" in namespace "replicaset-4521" to be "running"
Jan 13 09:38:32.231: INFO: Pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25": Phase="Running", Reason="", readiness=true. Elapsed: 12.750798ms
Jan 13 09:38:32.231: INFO: Pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25" satisfied condition "running"
Jan 13 09:38:32.231: INFO: Pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:27 +0000 UTC Reason: Message:}])
Jan 13 09:38:32.231: INFO: Trying to dial the pod
Jan 13 09:38:37.258: INFO: Controller my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3: Got expected result from replica 1 [my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25]: "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:38:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4521" for this suite. 01/13/23 09:38:37.268
------------------------------
• [SLOW TEST] [10.169 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:38:27.113
    Jan 13 09:38:27.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replicaset 01/13/23 09:38:27.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:27.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:27.171
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 13 09:38:27.182: INFO: Creating ReplicaSet my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3
    Jan 13 09:38:27.208: INFO: Pod name my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3: Found 0 pods out of 1
    Jan 13 09:38:32.218: INFO: Pod name my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3: Found 1 pods out of 1
    Jan 13 09:38:32.218: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3" is running
    Jan 13 09:38:32.218: INFO: Waiting up to 5m0s for pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25" in namespace "replicaset-4521" to be "running"
    Jan 13 09:38:32.231: INFO: Pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25": Phase="Running", Reason="", readiness=true. Elapsed: 12.750798ms
    Jan 13 09:38:32.231: INFO: Pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25" satisfied condition "running"
    Jan 13 09:38:32.231: INFO: Pod "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 09:38:27 +0000 UTC Reason: Message:}])
    Jan 13 09:38:32.231: INFO: Trying to dial the pod
    Jan 13 09:38:37.258: INFO: Controller my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3: Got expected result from replica 1 [my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25]: "my-hostname-basic-bb041a77-b543-4b2f-975d-106029be9bb3-r7v25", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:38:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4521" for this suite. 01/13/23 09:38:37.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:38:37.285
Jan 13 09:38:37.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svcaccounts 01/13/23 09:38:37.289
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:37.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:37.319
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 13 09:38:37.336: INFO: Got root ca configmap in namespace "svcaccounts-4551"
Jan 13 09:38:37.345: INFO: Deleted root ca configmap in namespace "svcaccounts-4551"
STEP: waiting for a new root ca configmap created 01/13/23 09:38:37.846
Jan 13 09:38:37.856: INFO: Recreated root ca configmap in namespace "svcaccounts-4551"
Jan 13 09:38:37.871: INFO: Updated root ca configmap in namespace "svcaccounts-4551"
STEP: waiting for the root ca configmap reconciled 01/13/23 09:38:38.372
Jan 13 09:38:38.381: INFO: Reconciled root ca configmap in namespace "svcaccounts-4551"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 13 09:38:38.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4551" for this suite. 01/13/23 09:38:38.391
------------------------------
• [1.136 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:38:37.285
    Jan 13 09:38:37.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svcaccounts 01/13/23 09:38:37.289
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:37.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:37.319
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 13 09:38:37.336: INFO: Got root ca configmap in namespace "svcaccounts-4551"
    Jan 13 09:38:37.345: INFO: Deleted root ca configmap in namespace "svcaccounts-4551"
    STEP: waiting for a new root ca configmap created 01/13/23 09:38:37.846
    Jan 13 09:38:37.856: INFO: Recreated root ca configmap in namespace "svcaccounts-4551"
    Jan 13 09:38:37.871: INFO: Updated root ca configmap in namespace "svcaccounts-4551"
    STEP: waiting for the root ca configmap reconciled 01/13/23 09:38:38.372
    Jan 13 09:38:38.381: INFO: Reconciled root ca configmap in namespace "svcaccounts-4551"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:38:38.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4551" for this suite. 01/13/23 09:38:38.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:38:38.425
Jan 13 09:38:38.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 09:38:38.428
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:38.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:38.51
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-8f62x" 01/13/23 09:38:38.528
Jan 13 09:38:38.584: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard cpu limit of 500m
Jan 13 09:38:38.584: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-8f62x" /status 01/13/23 09:38:38.584
STEP: Confirm /status for "e2e-rq-status-8f62x" resourceQuota via watch 01/13/23 09:38:38.617
Jan 13 09:38:38.621: INFO: observed resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList(nil)
Jan 13 09:38:38.622: INFO: Found resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 13 09:38:38.622: INFO: ResourceQuota "e2e-rq-status-8f62x" /status was updated
STEP: Patching hard spec values for cpu & memory 01/13/23 09:38:38.637
Jan 13 09:38:38.650: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard cpu limit of 1
Jan 13 09:38:38.650: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-8f62x" /status 01/13/23 09:38:38.65
STEP: Confirm /status for "e2e-rq-status-8f62x" resourceQuota via watch 01/13/23 09:38:38.662
Jan 13 09:38:38.665: INFO: observed resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 13 09:38:38.665: INFO: Found resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 13 09:38:38.665: INFO: ResourceQuota "e2e-rq-status-8f62x" /status was patched
STEP: Get "e2e-rq-status-8f62x" /status 01/13/23 09:38:38.665
Jan 13 09:38:38.674: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard cpu of 1
Jan 13 09:38:38.675: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-8f62x" /status before checking Spec is unchanged 01/13/23 09:38:38.681
Jan 13 09:38:38.696: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard cpu of 2
Jan 13 09:38:38.696: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard memory of 2Gi
Jan 13 09:38:38.699: INFO: Found resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 13 09:40:28.726: INFO: ResourceQuota "e2e-rq-status-8f62x" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 09:40:28.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8312" for this suite. 01/13/23 09:40:28.741
------------------------------
• [SLOW TEST] [110.332 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:38:38.425
    Jan 13 09:38:38.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 09:38:38.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:38:38.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:38:38.51
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-8f62x" 01/13/23 09:38:38.528
    Jan 13 09:38:38.584: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard cpu limit of 500m
    Jan 13 09:38:38.584: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-8f62x" /status 01/13/23 09:38:38.584
    STEP: Confirm /status for "e2e-rq-status-8f62x" resourceQuota via watch 01/13/23 09:38:38.617
    Jan 13 09:38:38.621: INFO: observed resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList(nil)
    Jan 13 09:38:38.622: INFO: Found resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 13 09:38:38.622: INFO: ResourceQuota "e2e-rq-status-8f62x" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/13/23 09:38:38.637
    Jan 13 09:38:38.650: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard cpu limit of 1
    Jan 13 09:38:38.650: INFO: Resource quota "e2e-rq-status-8f62x" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-8f62x" /status 01/13/23 09:38:38.65
    STEP: Confirm /status for "e2e-rq-status-8f62x" resourceQuota via watch 01/13/23 09:38:38.662
    Jan 13 09:38:38.665: INFO: observed resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 13 09:38:38.665: INFO: Found resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 13 09:38:38.665: INFO: ResourceQuota "e2e-rq-status-8f62x" /status was patched
    STEP: Get "e2e-rq-status-8f62x" /status 01/13/23 09:38:38.665
    Jan 13 09:38:38.674: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard cpu of 1
    Jan 13 09:38:38.675: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-8f62x" /status before checking Spec is unchanged 01/13/23 09:38:38.681
    Jan 13 09:38:38.696: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard cpu of 2
    Jan 13 09:38:38.696: INFO: Resourcequota "e2e-rq-status-8f62x" reports status: hard memory of 2Gi
    Jan 13 09:38:38.699: INFO: Found resourceQuota "e2e-rq-status-8f62x" in namespace "resourcequota-8312" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 13 09:40:28.726: INFO: ResourceQuota "e2e-rq-status-8f62x" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:40:28.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8312" for this suite. 01/13/23 09:40:28.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:40:28.762
Jan 13 09:40:28.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 09:40:28.766
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:40:28.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:40:28.816
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-73a3a1f9-2b93-4d05-9c05-15e4ba709249 01/13/23 09:40:28.827
STEP: Creating a pod to test consume secrets 01/13/23 09:40:28.84
Jan 13 09:40:28.877: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33" in namespace "projected-9995" to be "Succeeded or Failed"
Jan 13 09:40:28.890: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Pending", Reason="", readiness=false. Elapsed: 13.038075ms
Jan 13 09:40:30.919: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041980011s
Jan 13 09:40:32.902: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Running", Reason="", readiness=true. Elapsed: 4.024556421s
Jan 13 09:40:34.908: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Running", Reason="", readiness=false. Elapsed: 6.030839626s
Jan 13 09:40:36.910: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.033126931s
STEP: Saw pod success 01/13/23 09:40:36.91
Jan 13 09:40:36.911: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33" satisfied condition "Succeeded or Failed"
Jan 13 09:40:36.920: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/13/23 09:40:36.99
Jan 13 09:40:37.011: INFO: Waiting for pod pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33 to disappear
Jan 13 09:40:37.017: INFO: Pod pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 13 09:40:37.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9995" for this suite. 01/13/23 09:40:37.031
------------------------------
• [SLOW TEST] [8.290 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:40:28.762
    Jan 13 09:40:28.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 09:40:28.766
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:40:28.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:40:28.816
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-73a3a1f9-2b93-4d05-9c05-15e4ba709249 01/13/23 09:40:28.827
    STEP: Creating a pod to test consume secrets 01/13/23 09:40:28.84
    Jan 13 09:40:28.877: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33" in namespace "projected-9995" to be "Succeeded or Failed"
    Jan 13 09:40:28.890: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Pending", Reason="", readiness=false. Elapsed: 13.038075ms
    Jan 13 09:40:30.919: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041980011s
    Jan 13 09:40:32.902: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Running", Reason="", readiness=true. Elapsed: 4.024556421s
    Jan 13 09:40:34.908: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Running", Reason="", readiness=false. Elapsed: 6.030839626s
    Jan 13 09:40:36.910: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.033126931s
    STEP: Saw pod success 01/13/23 09:40:36.91
    Jan 13 09:40:36.911: INFO: Pod "pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33" satisfied condition "Succeeded or Failed"
    Jan 13 09:40:36.920: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 09:40:36.99
    Jan 13 09:40:37.011: INFO: Waiting for pod pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33 to disappear
    Jan 13 09:40:37.017: INFO: Pod pod-projected-secrets-fdc0ce47-4732-43ee-a6ab-6c26dffb6a33 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:40:37.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9995" for this suite. 01/13/23 09:40:37.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:40:37.057
Jan 13 09:40:37.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 09:40:37.06
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:40:37.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:40:37.106
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-55e1370c-7070-4585-9486-10c614f97813 01/13/23 09:40:37.115
STEP: Creating a pod to test consume configMaps 01/13/23 09:40:37.122
Jan 13 09:40:37.144: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4" in namespace "projected-4552" to be "Succeeded or Failed"
Jan 13 09:40:37.151: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.569514ms
Jan 13 09:40:39.166: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021581876s
Jan 13 09:40:41.164: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01999109s
Jan 13 09:40:43.177: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032843919s
STEP: Saw pod success 01/13/23 09:40:43.177
Jan 13 09:40:43.178: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4" satisfied condition "Succeeded or Failed"
Jan 13 09:40:43.192: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 09:40:43.226
Jan 13 09:40:43.257: INFO: Waiting for pod pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4 to disappear
Jan 13 09:40:43.264: INFO: Pod pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:40:43.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4552" for this suite. 01/13/23 09:40:43.276
------------------------------
• [SLOW TEST] [6.229 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:40:37.057
    Jan 13 09:40:37.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 09:40:37.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:40:37.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:40:37.106
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-55e1370c-7070-4585-9486-10c614f97813 01/13/23 09:40:37.115
    STEP: Creating a pod to test consume configMaps 01/13/23 09:40:37.122
    Jan 13 09:40:37.144: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4" in namespace "projected-4552" to be "Succeeded or Failed"
    Jan 13 09:40:37.151: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.569514ms
    Jan 13 09:40:39.166: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021581876s
    Jan 13 09:40:41.164: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01999109s
    Jan 13 09:40:43.177: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032843919s
    STEP: Saw pod success 01/13/23 09:40:43.177
    Jan 13 09:40:43.178: INFO: Pod "pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4" satisfied condition "Succeeded or Failed"
    Jan 13 09:40:43.192: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 09:40:43.226
    Jan 13 09:40:43.257: INFO: Waiting for pod pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4 to disappear
    Jan 13 09:40:43.264: INFO: Pod pod-projected-configmaps-92d919c6-0888-413d-aae5-7b0feb5ef1d4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:40:43.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4552" for this suite. 01/13/23 09:40:43.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:40:43.288
Jan 13 09:40:43.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename job 01/13/23 09:40:43.291
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:40:43.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:40:43.322
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/13/23 09:40:43.33
STEP: Ensuring job reaches completions 01/13/23 09:40:43.34
STEP: Ensuring pods with index for job exist 01/13/23 09:41:01.401
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 13 09:41:01.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-134" for this suite. 01/13/23 09:41:01.478
------------------------------
• [SLOW TEST] [18.214 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:40:43.288
    Jan 13 09:40:43.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename job 01/13/23 09:40:43.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:40:43.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:40:43.322
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/13/23 09:40:43.33
    STEP: Ensuring job reaches completions 01/13/23 09:40:43.34
    STEP: Ensuring pods with index for job exist 01/13/23 09:41:01.401
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:41:01.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-134" for this suite. 01/13/23 09:41:01.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:41:01.508
Jan 13 09:41:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-webhook 01/13/23 09:41:01.512
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:01.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:01.608
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/13/23 09:41:01.62
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/13/23 09:41:04.737
STEP: Deploying the custom resource conversion webhook pod 01/13/23 09:41:04.769
STEP: Wait for the deployment to be ready 01/13/23 09:41:04.838
Jan 13 09:41:04.940: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jan 13 09:41:07.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:41:09.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 09:41:11.043
STEP: Verifying the service has paired with the endpoint 01/13/23 09:41:11.102
Jan 13 09:41:12.102: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 13 09:41:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Creating a v1 custom resource 01/13/23 09:41:15.149
STEP: v2 custom resource should be converted 01/13/23 09:41:15.166
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:41:15.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9174" for this suite. 01/13/23 09:41:15.945
------------------------------
• [SLOW TEST] [14.504 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:41:01.508
    Jan 13 09:41:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-webhook 01/13/23 09:41:01.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:01.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:01.608
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/13/23 09:41:01.62
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/13/23 09:41:04.737
    STEP: Deploying the custom resource conversion webhook pod 01/13/23 09:41:04.769
    STEP: Wait for the deployment to be ready 01/13/23 09:41:04.838
    Jan 13 09:41:04.940: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Jan 13 09:41:07.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:41:09.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 41, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 41, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 09:41:11.043
    STEP: Verifying the service has paired with the endpoint 01/13/23 09:41:11.102
    Jan 13 09:41:12.102: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 13 09:41:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Creating a v1 custom resource 01/13/23 09:41:15.149
    STEP: v2 custom resource should be converted 01/13/23 09:41:15.166
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:41:15.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9174" for this suite. 01/13/23 09:41:15.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:41:16.017
Jan 13 09:41:16.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename podtemplate 01/13/23 09:41:16.02
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:16.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:16.057
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/13/23 09:41:16.064
STEP: Replace a pod template 01/13/23 09:41:16.073
Jan 13 09:41:16.094: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 13 09:41:16.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1988" for this suite. 01/13/23 09:41:16.103
------------------------------
• [0.098 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:41:16.017
    Jan 13 09:41:16.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename podtemplate 01/13/23 09:41:16.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:16.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:16.057
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/13/23 09:41:16.064
    STEP: Replace a pod template 01/13/23 09:41:16.073
    Jan 13 09:41:16.094: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:41:16.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1988" for this suite. 01/13/23 09:41:16.103
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:41:16.113
Jan 13 09:41:16.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 09:41:16.116
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:16.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:16.16
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/13/23 09:41:16.172
Jan 13 09:41:16.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66" in namespace "downward-api-4357" to be "Succeeded or Failed"
Jan 13 09:41:16.193: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.446489ms
Jan 13 09:41:18.203: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018221279s
Jan 13 09:41:20.202: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017299982s
Jan 13 09:41:22.208: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022993772s
Jan 13 09:41:24.215: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030189553s
STEP: Saw pod success 01/13/23 09:41:24.215
Jan 13 09:41:24.216: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66" satisfied condition "Succeeded or Failed"
Jan 13 09:41:24.241: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66 container client-container: <nil>
STEP: delete the pod 01/13/23 09:41:24.265
Jan 13 09:41:24.321: INFO: Waiting for pod downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66 to disappear
Jan 13 09:41:24.343: INFO: Pod downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 09:41:24.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4357" for this suite. 01/13/23 09:41:24.378
------------------------------
• [SLOW TEST] [8.281 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:41:16.113
    Jan 13 09:41:16.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 09:41:16.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:16.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:16.16
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/13/23 09:41:16.172
    Jan 13 09:41:16.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66" in namespace "downward-api-4357" to be "Succeeded or Failed"
    Jan 13 09:41:16.193: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.446489ms
    Jan 13 09:41:18.203: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018221279s
    Jan 13 09:41:20.202: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017299982s
    Jan 13 09:41:22.208: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022993772s
    Jan 13 09:41:24.215: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030189553s
    STEP: Saw pod success 01/13/23 09:41:24.215
    Jan 13 09:41:24.216: INFO: Pod "downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66" satisfied condition "Succeeded or Failed"
    Jan 13 09:41:24.241: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66 container client-container: <nil>
    STEP: delete the pod 01/13/23 09:41:24.265
    Jan 13 09:41:24.321: INFO: Waiting for pod downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66 to disappear
    Jan 13 09:41:24.343: INFO: Pod downwardapi-volume-07e88124-1fbe-4fcb-9bec-a737b1dcbe66 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:41:24.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4357" for this suite. 01/13/23 09:41:24.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:41:24.398
Jan 13 09:41:24.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 09:41:24.402
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:24.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:24.441
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2305 01/13/23 09:41:24.447
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/13/23 09:41:24.462
STEP: Creating stateful set ss in namespace statefulset-2305 01/13/23 09:41:24.499
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2305 01/13/23 09:41:24.514
Jan 13 09:41:24.533: INFO: Found 0 stateful pods, waiting for 1
Jan 13 09:41:34.539: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/13/23 09:41:34.539
Jan 13 09:41:34.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:41:35.068: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:41:35.068: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:41:35.068: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:41:35.077: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 13 09:41:45.086: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:41:45.086: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:41:45.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999134s
Jan 13 09:41:46.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993219547s
Jan 13 09:41:47.140: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98658304s
Jan 13 09:41:48.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977580576s
Jan 13 09:41:49.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955929313s
Jan 13 09:41:50.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.937382265s
Jan 13 09:41:51.198: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.930099418s
Jan 13 09:41:52.206: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.920125052s
Jan 13 09:41:53.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.911072219s
Jan 13 09:41:54.224: INFO: Verifying statefulset ss doesn't scale past 1 for another 902.975703ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2305 01/13/23 09:41:55.224
Jan 13 09:41:55.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:41:55.734: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 09:41:55.734: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 09:41:55.734: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 09:41:55.741: INFO: Found 1 stateful pods, waiting for 3
Jan 13 09:42:05.756: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 09:42:05.756: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 09:42:05.756: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/13/23 09:42:05.756
STEP: Scale down will halt with unhealthy stateful pod 01/13/23 09:42:05.756
Jan 13 09:42:05.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:42:06.205: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:42:06.205: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:42:06.205: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:42:06.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:42:06.717: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:42:06.717: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:42:06.717: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:42:06.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 09:42:07.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 09:42:07.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 09:42:07.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 09:42:07.198: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:42:07.205: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 13 09:42:17.226: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:42:17.226: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:42:17.226: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 09:42:17.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998727s
Jan 13 09:42:18.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987091761s
Jan 13 09:42:19.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968576499s
Jan 13 09:42:20.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960783089s
Jan 13 09:42:21.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951971333s
Jan 13 09:42:22.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.941025803s
Jan 13 09:42:23.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916077318s
Jan 13 09:42:24.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906893644s
Jan 13 09:42:25.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.888003618s
Jan 13 09:42:26.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 875.040639ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2305 01/13/23 09:42:27.388
Jan 13 09:42:27.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:42:27.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 09:42:27.890: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 09:42:27.890: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 09:42:27.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:42:28.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 09:42:28.330: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 09:42:28.330: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 09:42:28.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:42:28.714: INFO: rc: 1
Jan 13 09:42:28.714: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jan 13 09:42:38.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:42:38.950: INFO: rc: 1
Jan 13 09:42:38.950: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:42:48.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:42:49.259: INFO: rc: 1
Jan 13 09:42:49.259: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:42:59.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:42:59.593: INFO: rc: 1
Jan 13 09:42:59.593: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:43:09.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:43:09.898: INFO: rc: 1
Jan 13 09:43:09.898: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:43:19.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:43:20.149: INFO: rc: 1
Jan 13 09:43:20.149: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:43:30.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:43:30.404: INFO: rc: 1
Jan 13 09:43:30.404: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:43:40.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:43:40.721: INFO: rc: 1
Jan 13 09:43:40.721: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:43:50.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:43:51.071: INFO: rc: 1
Jan 13 09:43:51.071: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:44:01.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:44:01.375: INFO: rc: 1
Jan 13 09:44:01.375: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:44:11.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:44:11.653: INFO: rc: 1
Jan 13 09:44:11.653: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:44:21.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:44:21.873: INFO: rc: 1
Jan 13 09:44:21.873: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:44:31.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:44:32.156: INFO: rc: 1
Jan 13 09:44:32.156: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:44:42.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:44:42.450: INFO: rc: 1
Jan 13 09:44:42.450: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:44:52.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:44:52.683: INFO: rc: 1
Jan 13 09:44:52.683: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:45:02.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:45:02.911: INFO: rc: 1
Jan 13 09:45:02.912: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:45:12.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:45:13.197: INFO: rc: 1
Jan 13 09:45:13.198: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:45:23.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:45:23.559: INFO: rc: 1
Jan 13 09:45:23.559: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:45:33.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:45:33.845: INFO: rc: 1
Jan 13 09:45:33.845: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:45:43.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:45:44.405: INFO: rc: 1
Jan 13 09:45:44.405: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:45:54.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:45:54.778: INFO: rc: 1
Jan 13 09:45:54.778: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:46:04.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:46:05.167: INFO: rc: 1
Jan 13 09:46:05.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:46:15.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:46:15.518: INFO: rc: 1
Jan 13 09:46:15.518: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:46:25.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:46:25.805: INFO: rc: 1
Jan 13 09:46:25.805: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:46:35.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:46:36.160: INFO: rc: 1
Jan 13 09:46:36.160: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:46:46.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:46:46.502: INFO: rc: 1
Jan 13 09:46:46.502: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:46:56.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:46:56.847: INFO: rc: 1
Jan 13 09:46:56.847: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:47:06.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:47:07.151: INFO: rc: 1
Jan 13 09:47:07.152: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:47:17.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:47:17.443: INFO: rc: 1
Jan 13 09:47:17.443: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:47:27.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:47:27.727: INFO: rc: 1
Jan 13 09:47:27.727: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 09:47:37.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 09:47:38.068: INFO: rc: 1
Jan 13 09:47:38.069: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jan 13 09:47:38.069: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/13/23 09:47:38.103
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 09:47:38.109: INFO: Deleting all statefulset in ns statefulset-2305
Jan 13 09:47:38.121: INFO: Scaling statefulset ss to 0
Jan 13 09:47:38.151: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:47:38.159: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:47:38.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2305" for this suite. 01/13/23 09:47:38.23
------------------------------
• [SLOW TEST] [373.850 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:41:24.398
    Jan 13 09:41:24.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 09:41:24.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:41:24.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:41:24.441
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2305 01/13/23 09:41:24.447
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/13/23 09:41:24.462
    STEP: Creating stateful set ss in namespace statefulset-2305 01/13/23 09:41:24.499
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2305 01/13/23 09:41:24.514
    Jan 13 09:41:24.533: INFO: Found 0 stateful pods, waiting for 1
    Jan 13 09:41:34.539: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/13/23 09:41:34.539
    Jan 13 09:41:34.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:41:35.068: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:41:35.068: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:41:35.068: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:41:35.077: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 13 09:41:45.086: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:41:45.086: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:41:45.125: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999134s
    Jan 13 09:41:46.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993219547s
    Jan 13 09:41:47.140: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98658304s
    Jan 13 09:41:48.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977580576s
    Jan 13 09:41:49.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955929313s
    Jan 13 09:41:50.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.937382265s
    Jan 13 09:41:51.198: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.930099418s
    Jan 13 09:41:52.206: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.920125052s
    Jan 13 09:41:53.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.911072219s
    Jan 13 09:41:54.224: INFO: Verifying statefulset ss doesn't scale past 1 for another 902.975703ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2305 01/13/23 09:41:55.224
    Jan 13 09:41:55.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:41:55.734: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 13 09:41:55.734: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 09:41:55.734: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 13 09:41:55.741: INFO: Found 1 stateful pods, waiting for 3
    Jan 13 09:42:05.756: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 09:42:05.756: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 09:42:05.756: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/13/23 09:42:05.756
    STEP: Scale down will halt with unhealthy stateful pod 01/13/23 09:42:05.756
    Jan 13 09:42:05.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:42:06.205: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:42:06.205: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:42:06.205: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:42:06.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:42:06.717: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:42:06.717: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:42:06.717: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:42:06.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 09:42:07.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 09:42:07.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 09:42:07.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 09:42:07.198: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:42:07.205: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 13 09:42:17.226: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:42:17.226: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:42:17.226: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 13 09:42:17.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998727s
    Jan 13 09:42:18.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987091761s
    Jan 13 09:42:19.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968576499s
    Jan 13 09:42:20.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960783089s
    Jan 13 09:42:21.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951971333s
    Jan 13 09:42:22.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.941025803s
    Jan 13 09:42:23.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916077318s
    Jan 13 09:42:24.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906893644s
    Jan 13 09:42:25.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.888003618s
    Jan 13 09:42:26.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 875.040639ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2305 01/13/23 09:42:27.388
    Jan 13 09:42:27.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:42:27.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 13 09:42:27.890: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 09:42:27.890: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 13 09:42:27.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:42:28.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 13 09:42:28.330: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 09:42:28.330: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 13 09:42:28.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:42:28.714: INFO: rc: 1
    Jan 13 09:42:28.714: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: unable to upgrade connection: container not found ("webserver")

    error:
    exit status 1
    Jan 13 09:42:38.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:42:38.950: INFO: rc: 1
    Jan 13 09:42:38.950: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:42:48.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:42:49.259: INFO: rc: 1
    Jan 13 09:42:49.259: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:42:59.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:42:59.593: INFO: rc: 1
    Jan 13 09:42:59.593: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:43:09.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:43:09.898: INFO: rc: 1
    Jan 13 09:43:09.898: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:43:19.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:43:20.149: INFO: rc: 1
    Jan 13 09:43:20.149: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:43:30.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:43:30.404: INFO: rc: 1
    Jan 13 09:43:30.404: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:43:40.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:43:40.721: INFO: rc: 1
    Jan 13 09:43:40.721: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:43:50.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:43:51.071: INFO: rc: 1
    Jan 13 09:43:51.071: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:44:01.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:44:01.375: INFO: rc: 1
    Jan 13 09:44:01.375: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:44:11.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:44:11.653: INFO: rc: 1
    Jan 13 09:44:11.653: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:44:21.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:44:21.873: INFO: rc: 1
    Jan 13 09:44:21.873: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:44:31.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:44:32.156: INFO: rc: 1
    Jan 13 09:44:32.156: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:44:42.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:44:42.450: INFO: rc: 1
    Jan 13 09:44:42.450: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:44:52.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:44:52.683: INFO: rc: 1
    Jan 13 09:44:52.683: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:45:02.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:45:02.911: INFO: rc: 1
    Jan 13 09:45:02.912: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:45:12.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:45:13.197: INFO: rc: 1
    Jan 13 09:45:13.198: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:45:23.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:45:23.559: INFO: rc: 1
    Jan 13 09:45:23.559: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:45:33.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:45:33.845: INFO: rc: 1
    Jan 13 09:45:33.845: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:45:43.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:45:44.405: INFO: rc: 1
    Jan 13 09:45:44.405: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:45:54.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:45:54.778: INFO: rc: 1
    Jan 13 09:45:54.778: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:46:04.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:46:05.167: INFO: rc: 1
    Jan 13 09:46:05.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:46:15.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:46:15.518: INFO: rc: 1
    Jan 13 09:46:15.518: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:46:25.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:46:25.805: INFO: rc: 1
    Jan 13 09:46:25.805: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:46:35.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:46:36.160: INFO: rc: 1
    Jan 13 09:46:36.160: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:46:46.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:46:46.502: INFO: rc: 1
    Jan 13 09:46:46.502: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:46:56.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:46:56.847: INFO: rc: 1
    Jan 13 09:46:56.847: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:47:06.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:47:07.151: INFO: rc: 1
    Jan 13 09:47:07.152: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:47:17.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:47:17.443: INFO: rc: 1
    Jan 13 09:47:17.443: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:47:27.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:47:27.727: INFO: rc: 1
    Jan 13 09:47:27.727: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    Jan 13 09:47:37.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-2305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 09:47:38.068: INFO: rc: 1
    Jan 13 09:47:38.069: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    Jan 13 09:47:38.069: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/13/23 09:47:38.103
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 09:47:38.109: INFO: Deleting all statefulset in ns statefulset-2305
    Jan 13 09:47:38.121: INFO: Scaling statefulset ss to 0
    Jan 13 09:47:38.151: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 09:47:38.159: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:47:38.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2305" for this suite. 01/13/23 09:47:38.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:47:38.249
Jan 13 09:47:38.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:47:38.255
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:38.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:38.333
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/13/23 09:47:38.353
STEP: fetching the ConfigMap 01/13/23 09:47:38.42
STEP: patching the ConfigMap 01/13/23 09:47:38.43
STEP: listing all ConfigMaps in all namespaces with a label selector 01/13/23 09:47:38.445
STEP: deleting the ConfigMap by collection with a label selector 01/13/23 09:47:38.456
STEP: listing all ConfigMaps in test namespace 01/13/23 09:47:38.489
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:47:38.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2495" for this suite. 01/13/23 09:47:38.533
------------------------------
• [0.321 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:47:38.249
    Jan 13 09:47:38.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:47:38.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:38.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:38.333
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/13/23 09:47:38.353
    STEP: fetching the ConfigMap 01/13/23 09:47:38.42
    STEP: patching the ConfigMap 01/13/23 09:47:38.43
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/13/23 09:47:38.445
    STEP: deleting the ConfigMap by collection with a label selector 01/13/23 09:47:38.456
    STEP: listing all ConfigMaps in test namespace 01/13/23 09:47:38.489
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:47:38.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2495" for this suite. 01/13/23 09:47:38.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:47:38.573
Jan 13 09:47:38.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 09:47:38.577
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:38.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:38.639
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/13/23 09:47:38.659
Jan 13 09:47:38.702: INFO: Waiting up to 5m0s for pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626" in namespace "projected-6756" to be "running and ready"
Jan 13 09:47:38.713: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626": Phase="Pending", Reason="", readiness=false. Elapsed: 11.441335ms
Jan 13 09:47:38.713: INFO: The phase of Pod labelsupdate72ca7807-c031-4968-8b94-0d2706932626 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:47:40.727: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025234861s
Jan 13 09:47:40.727: INFO: The phase of Pod labelsupdate72ca7807-c031-4968-8b94-0d2706932626 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:47:42.729: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626": Phase="Running", Reason="", readiness=true. Elapsed: 4.027702477s
Jan 13 09:47:42.730: INFO: The phase of Pod labelsupdate72ca7807-c031-4968-8b94-0d2706932626 is Running (Ready = true)
Jan 13 09:47:42.730: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626" satisfied condition "running and ready"
Jan 13 09:47:43.374: INFO: Successfully updated pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 09:47:47.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6756" for this suite. 01/13/23 09:47:47.48
------------------------------
• [SLOW TEST] [8.917 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:47:38.573
    Jan 13 09:47:38.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 09:47:38.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:38.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:38.639
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/13/23 09:47:38.659
    Jan 13 09:47:38.702: INFO: Waiting up to 5m0s for pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626" in namespace "projected-6756" to be "running and ready"
    Jan 13 09:47:38.713: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626": Phase="Pending", Reason="", readiness=false. Elapsed: 11.441335ms
    Jan 13 09:47:38.713: INFO: The phase of Pod labelsupdate72ca7807-c031-4968-8b94-0d2706932626 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:47:40.727: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025234861s
    Jan 13 09:47:40.727: INFO: The phase of Pod labelsupdate72ca7807-c031-4968-8b94-0d2706932626 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:47:42.729: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626": Phase="Running", Reason="", readiness=true. Elapsed: 4.027702477s
    Jan 13 09:47:42.730: INFO: The phase of Pod labelsupdate72ca7807-c031-4968-8b94-0d2706932626 is Running (Ready = true)
    Jan 13 09:47:42.730: INFO: Pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626" satisfied condition "running and ready"
    Jan 13 09:47:43.374: INFO: Successfully updated pod "labelsupdate72ca7807-c031-4968-8b94-0d2706932626"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:47:47.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6756" for this suite. 01/13/23 09:47:47.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:47:47.493
Jan 13 09:47:47.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename ingress 01/13/23 09:47:47.498
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:47.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:47.538
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/13/23 09:47:47.545
STEP: getting /apis/networking.k8s.io 01/13/23 09:47:47.559
STEP: getting /apis/networking.k8s.iov1 01/13/23 09:47:47.562
STEP: creating 01/13/23 09:47:47.564
STEP: getting 01/13/23 09:47:47.594
STEP: listing 01/13/23 09:47:47.6
STEP: watching 01/13/23 09:47:47.605
Jan 13 09:47:47.605: INFO: starting watch
STEP: cluster-wide listing 01/13/23 09:47:47.613
STEP: cluster-wide watching 01/13/23 09:47:47.62
Jan 13 09:47:47.621: INFO: starting watch
STEP: patching 01/13/23 09:47:47.625
STEP: updating 01/13/23 09:47:47.635
Jan 13 09:47:47.666: INFO: waiting for watch events with expected annotations
Jan 13 09:47:47.666: INFO: saw patched and updated annotations
STEP: patching /status 01/13/23 09:47:47.666
STEP: updating /status 01/13/23 09:47:47.681
STEP: get /status 01/13/23 09:47:47.734
STEP: deleting 01/13/23 09:47:47.749
STEP: deleting a collection 01/13/23 09:47:47.804
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 13 09:47:47.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-8282" for this suite. 01/13/23 09:47:47.853
------------------------------
• [0.377 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:47:47.493
    Jan 13 09:47:47.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename ingress 01/13/23 09:47:47.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:47.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:47.538
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/13/23 09:47:47.545
    STEP: getting /apis/networking.k8s.io 01/13/23 09:47:47.559
    STEP: getting /apis/networking.k8s.iov1 01/13/23 09:47:47.562
    STEP: creating 01/13/23 09:47:47.564
    STEP: getting 01/13/23 09:47:47.594
    STEP: listing 01/13/23 09:47:47.6
    STEP: watching 01/13/23 09:47:47.605
    Jan 13 09:47:47.605: INFO: starting watch
    STEP: cluster-wide listing 01/13/23 09:47:47.613
    STEP: cluster-wide watching 01/13/23 09:47:47.62
    Jan 13 09:47:47.621: INFO: starting watch
    STEP: patching 01/13/23 09:47:47.625
    STEP: updating 01/13/23 09:47:47.635
    Jan 13 09:47:47.666: INFO: waiting for watch events with expected annotations
    Jan 13 09:47:47.666: INFO: saw patched and updated annotations
    STEP: patching /status 01/13/23 09:47:47.666
    STEP: updating /status 01/13/23 09:47:47.681
    STEP: get /status 01/13/23 09:47:47.734
    STEP: deleting 01/13/23 09:47:47.749
    STEP: deleting a collection 01/13/23 09:47:47.804
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:47:47.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-8282" for this suite. 01/13/23 09:47:47.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:47:47.874
Jan 13 09:47:47.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename init-container 01/13/23 09:47:47.878
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:47.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:47.92
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/13/23 09:47:47.93
Jan 13 09:47:47.930: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:47:55.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3397" for this suite. 01/13/23 09:47:55.708
------------------------------
• [SLOW TEST] [7.851 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:47:47.874
    Jan 13 09:47:47.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename init-container 01/13/23 09:47:47.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:47.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:47.92
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/13/23 09:47:47.93
    Jan 13 09:47:47.930: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:47:55.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3397" for this suite. 01/13/23 09:47:55.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:47:55.727
Jan 13 09:47:55.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 09:47:55.733
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:55.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:55.771
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/13/23 09:47:55.777
Jan 13 09:47:55.797: INFO: Waiting up to 5m0s for pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419" in namespace "emptydir-9959" to be "Succeeded or Failed"
Jan 13 09:47:55.814: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 17.397954ms
Jan 13 09:47:57.854: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057444569s
Jan 13 09:47:59.826: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029480592s
Jan 13 09:48:01.836: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038870945s
Jan 13 09:48:03.834: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03765234s
STEP: Saw pod success 01/13/23 09:48:03.834
Jan 13 09:48:03.835: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419" satisfied condition "Succeeded or Failed"
Jan 13 09:48:03.897: INFO: Trying to get logs from node 10.10.102.31-node pod pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419 container test-container: <nil>
STEP: delete the pod 01/13/23 09:48:03.937
Jan 13 09:48:04.053: INFO: Waiting for pod pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419 to disappear
Jan 13 09:48:04.075: INFO: Pod pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 09:48:04.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9959" for this suite. 01/13/23 09:48:04.13
------------------------------
• [SLOW TEST] [8.493 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:47:55.727
    Jan 13 09:47:55.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 09:47:55.733
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:47:55.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:47:55.771
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/13/23 09:47:55.777
    Jan 13 09:47:55.797: INFO: Waiting up to 5m0s for pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419" in namespace "emptydir-9959" to be "Succeeded or Failed"
    Jan 13 09:47:55.814: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 17.397954ms
    Jan 13 09:47:57.854: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057444569s
    Jan 13 09:47:59.826: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029480592s
    Jan 13 09:48:01.836: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038870945s
    Jan 13 09:48:03.834: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03765234s
    STEP: Saw pod success 01/13/23 09:48:03.834
    Jan 13 09:48:03.835: INFO: Pod "pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419" satisfied condition "Succeeded or Failed"
    Jan 13 09:48:03.897: INFO: Trying to get logs from node 10.10.102.31-node pod pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419 container test-container: <nil>
    STEP: delete the pod 01/13/23 09:48:03.937
    Jan 13 09:48:04.053: INFO: Waiting for pod pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419 to disappear
    Jan 13 09:48:04.075: INFO: Pod pod-e6b21ebb-f201-4df1-92a7-5937fc0d7419 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:48:04.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9959" for this suite. 01/13/23 09:48:04.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:48:04.227
Jan 13 09:48:04.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replicaset 01/13/23 09:48:04.23
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:04.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:04.318
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/13/23 09:48:04.326
Jan 13 09:48:04.356: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-716" to be "running and ready"
Jan 13 09:48:04.366: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 9.218703ms
Jan 13 09:48:04.366: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:48:06.388: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031844941s
Jan 13 09:48:06.388: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:48:08.383: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.026411543s
Jan 13 09:48:08.383: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 13 09:48:08.383: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/13/23 09:48:08.403
STEP: Then the orphan pod is adopted 01/13/23 09:48:08.455
STEP: When the matched label of one of its pods change 01/13/23 09:48:09.48
Jan 13 09:48:09.499: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/13/23 09:48:09.548
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 13 09:48:10.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-716" for this suite. 01/13/23 09:48:10.594
------------------------------
• [SLOW TEST] [6.409 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:48:04.227
    Jan 13 09:48:04.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replicaset 01/13/23 09:48:04.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:04.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:04.318
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/13/23 09:48:04.326
    Jan 13 09:48:04.356: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-716" to be "running and ready"
    Jan 13 09:48:04.366: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 9.218703ms
    Jan 13 09:48:04.366: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:48:06.388: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031844941s
    Jan 13 09:48:06.388: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:48:08.383: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.026411543s
    Jan 13 09:48:08.383: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 13 09:48:08.383: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/13/23 09:48:08.403
    STEP: Then the orphan pod is adopted 01/13/23 09:48:08.455
    STEP: When the matched label of one of its pods change 01/13/23 09:48:09.48
    Jan 13 09:48:09.499: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/13/23 09:48:09.548
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:48:10.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-716" for this suite. 01/13/23 09:48:10.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:48:10.641
Jan 13 09:48:10.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 09:48:10.644
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:10.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:10.792
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-b85b5b11-2cf3-4b9c-801d-4dd49ee15e62 01/13/23 09:48:10.818
STEP: Creating a pod to test consume secrets 01/13/23 09:48:10.828
Jan 13 09:48:10.868: INFO: Waiting up to 5m0s for pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a" in namespace "secrets-6631" to be "Succeeded or Failed"
Jan 13 09:48:10.874: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383153ms
Jan 13 09:48:12.880: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011732014s
Jan 13 09:48:14.883: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01467205s
Jan 13 09:48:16.887: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018342045s
Jan 13 09:48:18.909: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040797592s
STEP: Saw pod success 01/13/23 09:48:18.909
Jan 13 09:48:18.910: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a" satisfied condition "Succeeded or Failed"
Jan 13 09:48:18.948: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 09:48:18.994
Jan 13 09:48:19.034: INFO: Waiting for pod pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a to disappear
Jan 13 09:48:19.041: INFO: Pod pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 09:48:19.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6631" for this suite. 01/13/23 09:48:19.057
------------------------------
• [SLOW TEST] [8.489 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:48:10.641
    Jan 13 09:48:10.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 09:48:10.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:10.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:10.792
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-b85b5b11-2cf3-4b9c-801d-4dd49ee15e62 01/13/23 09:48:10.818
    STEP: Creating a pod to test consume secrets 01/13/23 09:48:10.828
    Jan 13 09:48:10.868: INFO: Waiting up to 5m0s for pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a" in namespace "secrets-6631" to be "Succeeded or Failed"
    Jan 13 09:48:10.874: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383153ms
    Jan 13 09:48:12.880: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011732014s
    Jan 13 09:48:14.883: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01467205s
    Jan 13 09:48:16.887: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018342045s
    Jan 13 09:48:18.909: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040797592s
    STEP: Saw pod success 01/13/23 09:48:18.909
    Jan 13 09:48:18.910: INFO: Pod "pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a" satisfied condition "Succeeded or Failed"
    Jan 13 09:48:18.948: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 09:48:18.994
    Jan 13 09:48:19.034: INFO: Waiting for pod pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a to disappear
    Jan 13 09:48:19.041: INFO: Pod pod-secrets-3dbb818e-05b2-4633-b29c-cd319a797a1a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:48:19.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6631" for this suite. 01/13/23 09:48:19.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:48:19.137
Jan 13 09:48:19.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename endpointslice 01/13/23 09:48:19.14
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:19.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:19.212
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 13 09:48:21.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-18" for this suite. 01/13/23 09:48:21.451
------------------------------
• [2.347 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:48:19.137
    Jan 13 09:48:19.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename endpointslice 01/13/23 09:48:19.14
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:19.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:19.212
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:48:21.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-18" for this suite. 01/13/23 09:48:21.451
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:48:21.485
Jan 13 09:48:21.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 09:48:21.489
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:21.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:21.545
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-b7fdcf2c-7606-47f8-9d03-d0d703fc5412 01/13/23 09:48:21.555
STEP: Creating a pod to test consume configMaps 01/13/23 09:48:21.574
Jan 13 09:48:21.603: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a" in namespace "projected-7185" to be "Succeeded or Failed"
Jan 13 09:48:21.619: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.256384ms
Jan 13 09:48:23.635: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032376779s
Jan 13 09:48:25.627: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023821775s
Jan 13 09:48:27.644: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041390211s
Jan 13 09:48:29.628: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025328687s
STEP: Saw pod success 01/13/23 09:48:29.628
Jan 13 09:48:29.629: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a" satisfied condition "Succeeded or Failed"
Jan 13 09:48:29.642: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a container agnhost-container: <nil>
STEP: delete the pod 01/13/23 09:48:29.681
Jan 13 09:48:29.749: INFO: Waiting for pod pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a to disappear
Jan 13 09:48:29.755: INFO: Pod pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:48:29.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7185" for this suite. 01/13/23 09:48:29.771
------------------------------
• [SLOW TEST] [8.300 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:48:21.485
    Jan 13 09:48:21.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 09:48:21.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:21.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:21.545
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-b7fdcf2c-7606-47f8-9d03-d0d703fc5412 01/13/23 09:48:21.555
    STEP: Creating a pod to test consume configMaps 01/13/23 09:48:21.574
    Jan 13 09:48:21.603: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a" in namespace "projected-7185" to be "Succeeded or Failed"
    Jan 13 09:48:21.619: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.256384ms
    Jan 13 09:48:23.635: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032376779s
    Jan 13 09:48:25.627: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023821775s
    Jan 13 09:48:27.644: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041390211s
    Jan 13 09:48:29.628: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.025328687s
    STEP: Saw pod success 01/13/23 09:48:29.628
    Jan 13 09:48:29.629: INFO: Pod "pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a" satisfied condition "Succeeded or Failed"
    Jan 13 09:48:29.642: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 09:48:29.681
    Jan 13 09:48:29.749: INFO: Waiting for pod pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a to disappear
    Jan 13 09:48:29.755: INFO: Pod pod-projected-configmaps-44484a31-8994-49aa-8035-317a2cc2510a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:48:29.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7185" for this suite. 01/13/23 09:48:29.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:48:29.79
Jan 13 09:48:29.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 09:48:29.793
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:29.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:29.854
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-28e90f52-5113-47cb-b54d-d5fc2723098d in namespace container-probe-6739 01/13/23 09:48:29.869
Jan 13 09:48:29.918: INFO: Waiting up to 5m0s for pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d" in namespace "container-probe-6739" to be "not pending"
Jan 13 09:48:29.924: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.455365ms
Jan 13 09:48:31.934: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016284099s
Jan 13 09:48:33.933: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d": Phase="Running", Reason="", readiness=true. Elapsed: 4.014811034s
Jan 13 09:48:33.933: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d" satisfied condition "not pending"
Jan 13 09:48:33.933: INFO: Started pod liveness-28e90f52-5113-47cb-b54d-d5fc2723098d in namespace container-probe-6739
STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 09:48:33.933
Jan 13 09:48:33.938: INFO: Initial restart count of pod liveness-28e90f52-5113-47cb-b54d-d5fc2723098d is 0
Jan 13 09:48:54.044: INFO: Restart count of pod container-probe-6739/liveness-28e90f52-5113-47cb-b54d-d5fc2723098d is now 1 (20.105354546s elapsed)
STEP: deleting the pod 01/13/23 09:48:54.044
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 09:48:54.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6739" for this suite. 01/13/23 09:48:54.106
------------------------------
• [SLOW TEST] [24.337 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:48:29.79
    Jan 13 09:48:29.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 09:48:29.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:29.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:29.854
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-28e90f52-5113-47cb-b54d-d5fc2723098d in namespace container-probe-6739 01/13/23 09:48:29.869
    Jan 13 09:48:29.918: INFO: Waiting up to 5m0s for pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d" in namespace "container-probe-6739" to be "not pending"
    Jan 13 09:48:29.924: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.455365ms
    Jan 13 09:48:31.934: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016284099s
    Jan 13 09:48:33.933: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d": Phase="Running", Reason="", readiness=true. Elapsed: 4.014811034s
    Jan 13 09:48:33.933: INFO: Pod "liveness-28e90f52-5113-47cb-b54d-d5fc2723098d" satisfied condition "not pending"
    Jan 13 09:48:33.933: INFO: Started pod liveness-28e90f52-5113-47cb-b54d-d5fc2723098d in namespace container-probe-6739
    STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 09:48:33.933
    Jan 13 09:48:33.938: INFO: Initial restart count of pod liveness-28e90f52-5113-47cb-b54d-d5fc2723098d is 0
    Jan 13 09:48:54.044: INFO: Restart count of pod container-probe-6739/liveness-28e90f52-5113-47cb-b54d-d5fc2723098d is now 1 (20.105354546s elapsed)
    STEP: deleting the pod 01/13/23 09:48:54.044
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:48:54.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6739" for this suite. 01/13/23 09:48:54.106
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:48:54.128
Jan 13 09:48:54.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename discovery 01/13/23 09:48:54.133
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:54.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:54.185
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/13/23 09:48:54.198
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 13 09:48:55.150: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 13 09:48:55.154: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 13 09:48:55.154: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 13 09:48:55.154: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 13 09:48:55.154: INFO: Checking APIGroup: apps
Jan 13 09:48:55.157: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 13 09:48:55.158: INFO: Versions found [{apps/v1 v1}]
Jan 13 09:48:55.158: INFO: apps/v1 matches apps/v1
Jan 13 09:48:55.158: INFO: Checking APIGroup: events.k8s.io
Jan 13 09:48:55.162: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 13 09:48:55.162: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 13 09:48:55.162: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 13 09:48:55.162: INFO: Checking APIGroup: authentication.k8s.io
Jan 13 09:48:55.165: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 13 09:48:55.165: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 13 09:48:55.165: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 13 09:48:55.165: INFO: Checking APIGroup: authorization.k8s.io
Jan 13 09:48:55.172: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 13 09:48:55.172: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 13 09:48:55.172: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 13 09:48:55.172: INFO: Checking APIGroup: autoscaling
Jan 13 09:48:55.177: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 13 09:48:55.177: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 13 09:48:55.177: INFO: autoscaling/v2 matches autoscaling/v2
Jan 13 09:48:55.177: INFO: Checking APIGroup: batch
Jan 13 09:48:55.180: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 13 09:48:55.180: INFO: Versions found [{batch/v1 v1}]
Jan 13 09:48:55.180: INFO: batch/v1 matches batch/v1
Jan 13 09:48:55.180: INFO: Checking APIGroup: certificates.k8s.io
Jan 13 09:48:55.183: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 13 09:48:55.183: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 13 09:48:55.183: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 13 09:48:55.183: INFO: Checking APIGroup: networking.k8s.io
Jan 13 09:48:55.186: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 13 09:48:55.186: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 13 09:48:55.187: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 13 09:48:55.187: INFO: Checking APIGroup: policy
Jan 13 09:48:55.189: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 13 09:48:55.189: INFO: Versions found [{policy/v1 v1}]
Jan 13 09:48:55.189: INFO: policy/v1 matches policy/v1
Jan 13 09:48:55.189: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 13 09:48:55.192: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 13 09:48:55.192: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 13 09:48:55.192: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 13 09:48:55.192: INFO: Checking APIGroup: storage.k8s.io
Jan 13 09:48:55.203: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 13 09:48:55.203: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 13 09:48:55.203: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 13 09:48:55.203: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 13 09:48:55.206: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 13 09:48:55.206: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 13 09:48:55.206: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 13 09:48:55.206: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 13 09:48:55.210: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 13 09:48:55.210: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 13 09:48:55.210: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 13 09:48:55.210: INFO: Checking APIGroup: scheduling.k8s.io
Jan 13 09:48:55.213: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 13 09:48:55.213: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 13 09:48:55.213: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 13 09:48:55.213: INFO: Checking APIGroup: coordination.k8s.io
Jan 13 09:48:55.217: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 13 09:48:55.218: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 13 09:48:55.218: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 13 09:48:55.218: INFO: Checking APIGroup: node.k8s.io
Jan 13 09:48:55.220: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 13 09:48:55.220: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 13 09:48:55.220: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 13 09:48:55.220: INFO: Checking APIGroup: discovery.k8s.io
Jan 13 09:48:55.225: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 13 09:48:55.225: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 13 09:48:55.225: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 13 09:48:55.225: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 13 09:48:55.228: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 13 09:48:55.228: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 13 09:48:55.228: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 13 09:48:55.228: INFO: Checking APIGroup: crd.projectcalico.org
Jan 13 09:48:55.230: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 13 09:48:55.230: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 13 09:48:55.230: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 13 09:48:55.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-9784" for this suite. 01/13/23 09:48:55.238
------------------------------
• [1.130 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:48:54.128
    Jan 13 09:48:54.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename discovery 01/13/23 09:48:54.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:54.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:54.185
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/13/23 09:48:54.198
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 13 09:48:55.150: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 13 09:48:55.154: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 13 09:48:55.154: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 13 09:48:55.154: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 13 09:48:55.154: INFO: Checking APIGroup: apps
    Jan 13 09:48:55.157: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 13 09:48:55.158: INFO: Versions found [{apps/v1 v1}]
    Jan 13 09:48:55.158: INFO: apps/v1 matches apps/v1
    Jan 13 09:48:55.158: INFO: Checking APIGroup: events.k8s.io
    Jan 13 09:48:55.162: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 13 09:48:55.162: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 13 09:48:55.162: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 13 09:48:55.162: INFO: Checking APIGroup: authentication.k8s.io
    Jan 13 09:48:55.165: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 13 09:48:55.165: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 13 09:48:55.165: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 13 09:48:55.165: INFO: Checking APIGroup: authorization.k8s.io
    Jan 13 09:48:55.172: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 13 09:48:55.172: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 13 09:48:55.172: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 13 09:48:55.172: INFO: Checking APIGroup: autoscaling
    Jan 13 09:48:55.177: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 13 09:48:55.177: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 13 09:48:55.177: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 13 09:48:55.177: INFO: Checking APIGroup: batch
    Jan 13 09:48:55.180: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 13 09:48:55.180: INFO: Versions found [{batch/v1 v1}]
    Jan 13 09:48:55.180: INFO: batch/v1 matches batch/v1
    Jan 13 09:48:55.180: INFO: Checking APIGroup: certificates.k8s.io
    Jan 13 09:48:55.183: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 13 09:48:55.183: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 13 09:48:55.183: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 13 09:48:55.183: INFO: Checking APIGroup: networking.k8s.io
    Jan 13 09:48:55.186: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 13 09:48:55.186: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 13 09:48:55.187: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 13 09:48:55.187: INFO: Checking APIGroup: policy
    Jan 13 09:48:55.189: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 13 09:48:55.189: INFO: Versions found [{policy/v1 v1}]
    Jan 13 09:48:55.189: INFO: policy/v1 matches policy/v1
    Jan 13 09:48:55.189: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 13 09:48:55.192: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 13 09:48:55.192: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 13 09:48:55.192: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 13 09:48:55.192: INFO: Checking APIGroup: storage.k8s.io
    Jan 13 09:48:55.203: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 13 09:48:55.203: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 13 09:48:55.203: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 13 09:48:55.203: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 13 09:48:55.206: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 13 09:48:55.206: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 13 09:48:55.206: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 13 09:48:55.206: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 13 09:48:55.210: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 13 09:48:55.210: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 13 09:48:55.210: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 13 09:48:55.210: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 13 09:48:55.213: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 13 09:48:55.213: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 13 09:48:55.213: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 13 09:48:55.213: INFO: Checking APIGroup: coordination.k8s.io
    Jan 13 09:48:55.217: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 13 09:48:55.218: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 13 09:48:55.218: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 13 09:48:55.218: INFO: Checking APIGroup: node.k8s.io
    Jan 13 09:48:55.220: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 13 09:48:55.220: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 13 09:48:55.220: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 13 09:48:55.220: INFO: Checking APIGroup: discovery.k8s.io
    Jan 13 09:48:55.225: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 13 09:48:55.225: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 13 09:48:55.225: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 13 09:48:55.225: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 13 09:48:55.228: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 13 09:48:55.228: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 13 09:48:55.228: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 13 09:48:55.228: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 13 09:48:55.230: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 13 09:48:55.230: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 13 09:48:55.230: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:48:55.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-9784" for this suite. 01/13/23 09:48:55.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:48:55.261
Jan 13 09:48:55.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename gc 01/13/23 09:48:55.264
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:55.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:55.298
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/13/23 09:48:55.316
STEP: create the rc2 01/13/23 09:48:55.347
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/13/23 09:49:00.378
STEP: delete the rc simpletest-rc-to-be-deleted 01/13/23 09:49:04.017
STEP: wait for the rc to be deleted 01/13/23 09:49:04.076
Jan 13 09:49:09.200: INFO: 71 pods remaining
Jan 13 09:49:09.200: INFO: 71 pods has nil DeletionTimestamp
Jan 13 09:49:09.200: INFO: 
STEP: Gathering metrics 01/13/23 09:49:14.353
Jan 13 09:49:14.655: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
Jan 13 09:49:14.678: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 22.68612ms
Jan 13 09:49:14.678: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
Jan 13 09:49:14.678: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
Jan 13 09:49:15.635: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 13 09:49:15.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-22fdj" in namespace "gc-9447"
Jan 13 09:49:15.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-28j72" in namespace "gc-9447"
Jan 13 09:49:15.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b2vz" in namespace "gc-9447"
Jan 13 09:49:15.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bcjm" in namespace "gc-9447"
Jan 13 09:49:15.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jwwt" in namespace "gc-9447"
Jan 13 09:49:15.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rfjh" in namespace "gc-9447"
Jan 13 09:49:15.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wngx" in namespace "gc-9447"
Jan 13 09:49:15.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zgcc" in namespace "gc-9447"
Jan 13 09:49:16.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-47sx4" in namespace "gc-9447"
Jan 13 09:49:16.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b6qq" in namespace "gc-9447"
Jan 13 09:49:16.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cvzb" in namespace "gc-9447"
Jan 13 09:49:16.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h6dc" in namespace "gc-9447"
Jan 13 09:49:16.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p9rd" in namespace "gc-9447"
Jan 13 09:49:16.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s52r" in namespace "gc-9447"
Jan 13 09:49:16.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tpvk" in namespace "gc-9447"
Jan 13 09:49:16.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-52vfh" in namespace "gc-9447"
Jan 13 09:49:16.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-58xlc" in namespace "gc-9447"
Jan 13 09:49:16.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-5g4sv" in namespace "gc-9447"
Jan 13 09:49:16.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ghdq" in namespace "gc-9447"
Jan 13 09:49:16.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-5h859" in namespace "gc-9447"
Jan 13 09:49:16.824: INFO: Deleting pod "simpletest-rc-to-be-deleted-62rfv" in namespace "gc-9447"
Jan 13 09:49:16.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-67pks" in namespace "gc-9447"
Jan 13 09:49:16.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jh4z" in namespace "gc-9447"
Jan 13 09:49:17.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xnnl" in namespace "gc-9447"
Jan 13 09:49:17.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pptp" in namespace "gc-9447"
Jan 13 09:49:17.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rrnd" in namespace "gc-9447"
Jan 13 09:49:17.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w2db" in namespace "gc-9447"
Jan 13 09:49:17.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x2kf" in namespace "gc-9447"
Jan 13 09:49:17.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f4ww" in namespace "gc-9447"
Jan 13 09:49:17.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r9bt" in namespace "gc-9447"
Jan 13 09:49:17.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vtgd" in namespace "gc-9447"
Jan 13 09:49:17.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5jsl" in namespace "gc-9447"
Jan 13 09:49:17.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct4hs" in namespace "gc-9447"
Jan 13 09:49:17.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-d562t" in namespace "gc-9447"
Jan 13 09:49:17.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5wdn" in namespace "gc-9447"
Jan 13 09:49:17.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7m57" in namespace "gc-9447"
Jan 13 09:49:17.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc87d" in namespace "gc-9447"
Jan 13 09:49:17.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqlw7" in namespace "gc-9447"
Jan 13 09:49:17.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvb66" in namespace "gc-9447"
Jan 13 09:49:17.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-f52tj" in namespace "gc-9447"
Jan 13 09:49:17.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-f59vg" in namespace "gc-9447"
Jan 13 09:49:17.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6wdj" in namespace "gc-9447"
Jan 13 09:49:17.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6x5v" in namespace "gc-9447"
Jan 13 09:49:18.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8qqg" in namespace "gc-9447"
Jan 13 09:49:18.090: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd8fl" in namespace "gc-9447"
Jan 13 09:49:18.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsc42" in namespace "gc-9447"
Jan 13 09:49:18.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx4f9" in namespace "gc-9447"
Jan 13 09:49:18.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-g66tw" in namespace "gc-9447"
Jan 13 09:49:18.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsstl" in namespace "gc-9447"
Jan 13 09:49:18.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcp7d" in namespace "gc-9447"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 13 09:49:18.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9447" for this suite. 01/13/23 09:49:18.284
------------------------------
• [SLOW TEST] [23.040 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:48:55.261
    Jan 13 09:48:55.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename gc 01/13/23 09:48:55.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:48:55.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:48:55.298
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/13/23 09:48:55.316
    STEP: create the rc2 01/13/23 09:48:55.347
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/13/23 09:49:00.378
    STEP: delete the rc simpletest-rc-to-be-deleted 01/13/23 09:49:04.017
    STEP: wait for the rc to be deleted 01/13/23 09:49:04.076
    Jan 13 09:49:09.200: INFO: 71 pods remaining
    Jan 13 09:49:09.200: INFO: 71 pods has nil DeletionTimestamp
    Jan 13 09:49:09.200: INFO: 
    STEP: Gathering metrics 01/13/23 09:49:14.353
    Jan 13 09:49:14.655: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
    Jan 13 09:49:14.678: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 22.68612ms
    Jan 13 09:49:14.678: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
    Jan 13 09:49:14.678: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
    Jan 13 09:49:15.635: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 13 09:49:15.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-22fdj" in namespace "gc-9447"
    Jan 13 09:49:15.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-28j72" in namespace "gc-9447"
    Jan 13 09:49:15.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b2vz" in namespace "gc-9447"
    Jan 13 09:49:15.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bcjm" in namespace "gc-9447"
    Jan 13 09:49:15.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jwwt" in namespace "gc-9447"
    Jan 13 09:49:15.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rfjh" in namespace "gc-9447"
    Jan 13 09:49:15.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wngx" in namespace "gc-9447"
    Jan 13 09:49:15.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zgcc" in namespace "gc-9447"
    Jan 13 09:49:16.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-47sx4" in namespace "gc-9447"
    Jan 13 09:49:16.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b6qq" in namespace "gc-9447"
    Jan 13 09:49:16.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cvzb" in namespace "gc-9447"
    Jan 13 09:49:16.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h6dc" in namespace "gc-9447"
    Jan 13 09:49:16.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p9rd" in namespace "gc-9447"
    Jan 13 09:49:16.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s52r" in namespace "gc-9447"
    Jan 13 09:49:16.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tpvk" in namespace "gc-9447"
    Jan 13 09:49:16.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-52vfh" in namespace "gc-9447"
    Jan 13 09:49:16.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-58xlc" in namespace "gc-9447"
    Jan 13 09:49:16.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-5g4sv" in namespace "gc-9447"
    Jan 13 09:49:16.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ghdq" in namespace "gc-9447"
    Jan 13 09:49:16.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-5h859" in namespace "gc-9447"
    Jan 13 09:49:16.824: INFO: Deleting pod "simpletest-rc-to-be-deleted-62rfv" in namespace "gc-9447"
    Jan 13 09:49:16.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-67pks" in namespace "gc-9447"
    Jan 13 09:49:16.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jh4z" in namespace "gc-9447"
    Jan 13 09:49:17.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xnnl" in namespace "gc-9447"
    Jan 13 09:49:17.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pptp" in namespace "gc-9447"
    Jan 13 09:49:17.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rrnd" in namespace "gc-9447"
    Jan 13 09:49:17.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w2db" in namespace "gc-9447"
    Jan 13 09:49:17.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x2kf" in namespace "gc-9447"
    Jan 13 09:49:17.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f4ww" in namespace "gc-9447"
    Jan 13 09:49:17.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r9bt" in namespace "gc-9447"
    Jan 13 09:49:17.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vtgd" in namespace "gc-9447"
    Jan 13 09:49:17.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5jsl" in namespace "gc-9447"
    Jan 13 09:49:17.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct4hs" in namespace "gc-9447"
    Jan 13 09:49:17.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-d562t" in namespace "gc-9447"
    Jan 13 09:49:17.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5wdn" in namespace "gc-9447"
    Jan 13 09:49:17.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7m57" in namespace "gc-9447"
    Jan 13 09:49:17.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc87d" in namespace "gc-9447"
    Jan 13 09:49:17.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqlw7" in namespace "gc-9447"
    Jan 13 09:49:17.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvb66" in namespace "gc-9447"
    Jan 13 09:49:17.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-f52tj" in namespace "gc-9447"
    Jan 13 09:49:17.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-f59vg" in namespace "gc-9447"
    Jan 13 09:49:17.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6wdj" in namespace "gc-9447"
    Jan 13 09:49:17.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6x5v" in namespace "gc-9447"
    Jan 13 09:49:18.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8qqg" in namespace "gc-9447"
    Jan 13 09:49:18.090: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd8fl" in namespace "gc-9447"
    Jan 13 09:49:18.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsc42" in namespace "gc-9447"
    Jan 13 09:49:18.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx4f9" in namespace "gc-9447"
    Jan 13 09:49:18.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-g66tw" in namespace "gc-9447"
    Jan 13 09:49:18.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsstl" in namespace "gc-9447"
    Jan 13 09:49:18.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcp7d" in namespace "gc-9447"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:49:18.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9447" for this suite. 01/13/23 09:49:18.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:49:18.347
Jan 13 09:49:18.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename watch 01/13/23 09:49:18.365
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:49:18.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:49:18.576
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/13/23 09:49:18.624
STEP: starting a background goroutine to produce watch events 01/13/23 09:49:18.656
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/13/23 09:49:18.656
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 13 09:49:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-675" for this suite. 01/13/23 09:49:22.33
------------------------------
• [3.997 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:49:18.347
    Jan 13 09:49:18.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename watch 01/13/23 09:49:18.365
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:49:18.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:49:18.576
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/13/23 09:49:18.624
    STEP: starting a background goroutine to produce watch events 01/13/23 09:49:18.656
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/13/23 09:49:18.656
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:49:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-675" for this suite. 01/13/23 09:49:22.33
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:49:22.344
Jan 13 09:49:22.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename subpath 01/13/23 09:49:22.347
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:49:22.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:49:22.416
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/13/23 09:49:22.443
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-mdch 01/13/23 09:49:22.48
STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:49:22.48
Jan 13 09:49:22.495: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mdch" in namespace "subpath-1477" to be "Succeeded or Failed"
Jan 13 09:49:22.506: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 10.87244ms
Jan 13 09:49:24.554: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058023861s
Jan 13 09:49:26.564: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068222151s
Jan 13 09:49:28.521: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02595577s
Jan 13 09:49:30.566: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070903463s
Jan 13 09:49:32.649: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 10.153499538s
Jan 13 09:49:34.516: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020007608s
Jan 13 09:49:36.547: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 14.051834484s
Jan 13 09:49:38.515: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 16.019091063s
Jan 13 09:49:40.536: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 18.040196643s
Jan 13 09:49:42.666: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 20.1706151s
Jan 13 09:49:44.541: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 22.045465563s
Jan 13 09:49:46.521: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 24.025942954s
Jan 13 09:49:48.526: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 26.030485773s
Jan 13 09:49:50.517: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 28.021458877s
Jan 13 09:49:52.524: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=false. Elapsed: 30.028274203s
Jan 13 09:49:54.520: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=false. Elapsed: 32.024004456s
Jan 13 09:49:56.527: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.031314356s
STEP: Saw pod success 01/13/23 09:49:56.527
Jan 13 09:49:56.528: INFO: Pod "pod-subpath-test-configmap-mdch" satisfied condition "Succeeded or Failed"
Jan 13 09:49:56.535: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-configmap-mdch container test-container-subpath-configmap-mdch: <nil>
STEP: delete the pod 01/13/23 09:49:56.567
Jan 13 09:49:56.599: INFO: Waiting for pod pod-subpath-test-configmap-mdch to disappear
Jan 13 09:49:56.619: INFO: Pod pod-subpath-test-configmap-mdch no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mdch 01/13/23 09:49:56.619
Jan 13 09:49:56.619: INFO: Deleting pod "pod-subpath-test-configmap-mdch" in namespace "subpath-1477"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 13 09:49:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1477" for this suite. 01/13/23 09:49:56.635
------------------------------
• [SLOW TEST] [34.303 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:49:22.344
    Jan 13 09:49:22.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename subpath 01/13/23 09:49:22.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:49:22.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:49:22.416
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/13/23 09:49:22.443
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-mdch 01/13/23 09:49:22.48
    STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:49:22.48
    Jan 13 09:49:22.495: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mdch" in namespace "subpath-1477" to be "Succeeded or Failed"
    Jan 13 09:49:22.506: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 10.87244ms
    Jan 13 09:49:24.554: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058023861s
    Jan 13 09:49:26.564: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068222151s
    Jan 13 09:49:28.521: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02595577s
    Jan 13 09:49:30.566: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070903463s
    Jan 13 09:49:32.649: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 10.153499538s
    Jan 13 09:49:34.516: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020007608s
    Jan 13 09:49:36.547: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Pending", Reason="", readiness=false. Elapsed: 14.051834484s
    Jan 13 09:49:38.515: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 16.019091063s
    Jan 13 09:49:40.536: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 18.040196643s
    Jan 13 09:49:42.666: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 20.1706151s
    Jan 13 09:49:44.541: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 22.045465563s
    Jan 13 09:49:46.521: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 24.025942954s
    Jan 13 09:49:48.526: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 26.030485773s
    Jan 13 09:49:50.517: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=true. Elapsed: 28.021458877s
    Jan 13 09:49:52.524: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=false. Elapsed: 30.028274203s
    Jan 13 09:49:54.520: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Running", Reason="", readiness=false. Elapsed: 32.024004456s
    Jan 13 09:49:56.527: INFO: Pod "pod-subpath-test-configmap-mdch": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.031314356s
    STEP: Saw pod success 01/13/23 09:49:56.527
    Jan 13 09:49:56.528: INFO: Pod "pod-subpath-test-configmap-mdch" satisfied condition "Succeeded or Failed"
    Jan 13 09:49:56.535: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-configmap-mdch container test-container-subpath-configmap-mdch: <nil>
    STEP: delete the pod 01/13/23 09:49:56.567
    Jan 13 09:49:56.599: INFO: Waiting for pod pod-subpath-test-configmap-mdch to disappear
    Jan 13 09:49:56.619: INFO: Pod pod-subpath-test-configmap-mdch no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-mdch 01/13/23 09:49:56.619
    Jan 13 09:49:56.619: INFO: Deleting pod "pod-subpath-test-configmap-mdch" in namespace "subpath-1477"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:49:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1477" for this suite. 01/13/23 09:49:56.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:49:56.65
Jan 13 09:49:56.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:49:56.653
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:49:56.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:49:56.697
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-8f1c606d-bd49-41f2-af8c-b149443eda9f 01/13/23 09:49:56.719
STEP: Creating configMap with name cm-test-opt-upd-51ab863c-1883-45a2-a216-f74d6c39bc2d 01/13/23 09:49:56.729
STEP: Creating the pod 01/13/23 09:49:56.753
Jan 13 09:49:56.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10" in namespace "configmap-8639" to be "running and ready"
Jan 13 09:49:56.782: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Pending", Reason="", readiness=false. Elapsed: 8.642122ms
Jan 13 09:49:56.782: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:49:58.791: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017355191s
Jan 13 09:49:58.791: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:50:00.805: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031388911s
Jan 13 09:50:00.805: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:50:02.802: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Running", Reason="", readiness=true. Elapsed: 6.028789357s
Jan 13 09:50:02.802: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Running (Ready = true)
Jan 13 09:50:02.802: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-8f1c606d-bd49-41f2-af8c-b149443eda9f 01/13/23 09:50:02.951
STEP: Updating configmap cm-test-opt-upd-51ab863c-1883-45a2-a216-f74d6c39bc2d 01/13/23 09:50:02.967
STEP: Creating configMap with name cm-test-opt-create-465f13f1-e636-4f16-bbc7-87e63113b17e 01/13/23 09:50:02.975
STEP: waiting to observe update in volume 01/13/23 09:50:02.984
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:50:07.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8639" for this suite. 01/13/23 09:50:07.257
------------------------------
• [SLOW TEST] [10.618 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:49:56.65
    Jan 13 09:49:56.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:49:56.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:49:56.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:49:56.697
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-8f1c606d-bd49-41f2-af8c-b149443eda9f 01/13/23 09:49:56.719
    STEP: Creating configMap with name cm-test-opt-upd-51ab863c-1883-45a2-a216-f74d6c39bc2d 01/13/23 09:49:56.729
    STEP: Creating the pod 01/13/23 09:49:56.753
    Jan 13 09:49:56.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10" in namespace "configmap-8639" to be "running and ready"
    Jan 13 09:49:56.782: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Pending", Reason="", readiness=false. Elapsed: 8.642122ms
    Jan 13 09:49:56.782: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:49:58.791: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017355191s
    Jan 13 09:49:58.791: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:50:00.805: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031388911s
    Jan 13 09:50:00.805: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:50:02.802: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10": Phase="Running", Reason="", readiness=true. Elapsed: 6.028789357s
    Jan 13 09:50:02.802: INFO: The phase of Pod pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10 is Running (Ready = true)
    Jan 13 09:50:02.802: INFO: Pod "pod-configmaps-cc272a3b-1585-4359-929a-ac0855c65a10" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-8f1c606d-bd49-41f2-af8c-b149443eda9f 01/13/23 09:50:02.951
    STEP: Updating configmap cm-test-opt-upd-51ab863c-1883-45a2-a216-f74d6c39bc2d 01/13/23 09:50:02.967
    STEP: Creating configMap with name cm-test-opt-create-465f13f1-e636-4f16-bbc7-87e63113b17e 01/13/23 09:50:02.975
    STEP: waiting to observe update in volume 01/13/23 09:50:02.984
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:50:07.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8639" for this suite. 01/13/23 09:50:07.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:50:07.27
Jan 13 09:50:07.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 09:50:07.275
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:07.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:07.316
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/13/23 09:50:07.335
Jan 13 09:50:07.363: INFO: Waiting up to 5m0s for pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0" in namespace "var-expansion-6467" to be "Succeeded or Failed"
Jan 13 09:50:07.374: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.366975ms
Jan 13 09:50:09.384: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020273908s
Jan 13 09:50:11.383: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019528388s
Jan 13 09:50:13.384: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02108304s
Jan 13 09:50:15.380: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016566182s
STEP: Saw pod success 01/13/23 09:50:15.38
Jan 13 09:50:15.380: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0" satisfied condition "Succeeded or Failed"
Jan 13 09:50:15.386: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0 container dapi-container: <nil>
STEP: delete the pod 01/13/23 09:50:15.405
Jan 13 09:50:15.456: INFO: Waiting for pod var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0 to disappear
Jan 13 09:50:15.461: INFO: Pod var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 09:50:15.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6467" for this suite. 01/13/23 09:50:15.471
------------------------------
• [SLOW TEST] [8.212 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:50:07.27
    Jan 13 09:50:07.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 09:50:07.275
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:07.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:07.316
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/13/23 09:50:07.335
    Jan 13 09:50:07.363: INFO: Waiting up to 5m0s for pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0" in namespace "var-expansion-6467" to be "Succeeded or Failed"
    Jan 13 09:50:07.374: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.366975ms
    Jan 13 09:50:09.384: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020273908s
    Jan 13 09:50:11.383: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019528388s
    Jan 13 09:50:13.384: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02108304s
    Jan 13 09:50:15.380: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016566182s
    STEP: Saw pod success 01/13/23 09:50:15.38
    Jan 13 09:50:15.380: INFO: Pod "var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0" satisfied condition "Succeeded or Failed"
    Jan 13 09:50:15.386: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0 container dapi-container: <nil>
    STEP: delete the pod 01/13/23 09:50:15.405
    Jan 13 09:50:15.456: INFO: Waiting for pod var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0 to disappear
    Jan 13 09:50:15.461: INFO: Pod var-expansion-c659a939-8058-48ff-a7ff-204907f7a6d0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:50:15.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6467" for this suite. 01/13/23 09:50:15.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:50:15.486
Jan 13 09:50:15.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 09:50:15.488
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:15.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:15.559
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-27b9747a-bbd6-4cd4-b61c-74664a1e3183 01/13/23 09:50:15.57
STEP: Creating a pod to test consume secrets 01/13/23 09:50:15.58
Jan 13 09:50:15.623: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6" in namespace "projected-8392" to be "Succeeded or Failed"
Jan 13 09:50:15.656: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 32.579445ms
Jan 13 09:50:17.668: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044716243s
Jan 13 09:50:19.664: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040757028s
Jan 13 09:50:21.671: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04783239s
Jan 13 09:50:23.697: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.07382398s
STEP: Saw pod success 01/13/23 09:50:23.698
Jan 13 09:50:23.698: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6" satisfied condition "Succeeded or Failed"
Jan 13 09:50:23.726: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6 container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 09:50:23.858
Jan 13 09:50:23.906: INFO: Waiting for pod pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6 to disappear
Jan 13 09:50:23.919: INFO: Pod pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 13 09:50:23.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8392" for this suite. 01/13/23 09:50:23.928
------------------------------
• [SLOW TEST] [8.467 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:50:15.486
    Jan 13 09:50:15.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 09:50:15.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:15.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:15.559
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-27b9747a-bbd6-4cd4-b61c-74664a1e3183 01/13/23 09:50:15.57
    STEP: Creating a pod to test consume secrets 01/13/23 09:50:15.58
    Jan 13 09:50:15.623: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6" in namespace "projected-8392" to be "Succeeded or Failed"
    Jan 13 09:50:15.656: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 32.579445ms
    Jan 13 09:50:17.668: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044716243s
    Jan 13 09:50:19.664: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040757028s
    Jan 13 09:50:21.671: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04783239s
    Jan 13 09:50:23.697: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.07382398s
    STEP: Saw pod success 01/13/23 09:50:23.698
    Jan 13 09:50:23.698: INFO: Pod "pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6" satisfied condition "Succeeded or Failed"
    Jan 13 09:50:23.726: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6 container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 09:50:23.858
    Jan 13 09:50:23.906: INFO: Waiting for pod pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6 to disappear
    Jan 13 09:50:23.919: INFO: Pod pod-projected-secrets-c7028535-f567-4b48-9c3c-a5d7de0714a6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:50:23.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8392" for this suite. 01/13/23 09:50:23.928
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:50:23.954
Jan 13 09:50:23.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename daemonsets 01/13/23 09:50:23.957
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:24.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:24.13
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan 13 09:50:24.248: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:50:24.276
Jan 13 09:50:24.292: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:24.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:24.312: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:25.339: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:25.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:25.354: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:26.329: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:26.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:26.350: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:27.328: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:27.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:27.343: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:28.354: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:28.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 09:50:28.368: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/13/23 09:50:28.469
STEP: Check that daemon pods images are updated. 01/13/23 09:50:28.533
Jan 13 09:50:28.545: INFO: Wrong image for pod: daemon-set-9fv57. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:28.545: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:28.565: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:29.580: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:29.592: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:30.587: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:30.603: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:31.578: INFO: Pod daemon-set-kkd95 is not available
Jan 13 09:50:31.578: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:31.588: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:32.582: INFO: Pod daemon-set-kkd95 is not available
Jan 13 09:50:32.582: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:32.606: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:33.575: INFO: Pod daemon-set-kkd95 is not available
Jan 13 09:50:33.575: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:33.586: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:34.575: INFO: Pod daemon-set-kkd95 is not available
Jan 13 09:50:34.575: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 13 09:50:34.589: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:35.599: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:36.593: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:37.576: INFO: Pod daemon-set-pjtqj is not available
Jan 13 09:50:37.589: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/13/23 09:50:37.589
Jan 13 09:50:37.603: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:37.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:50:37.616: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:38.698: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:38.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:50:38.739: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:39.639: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:39.660: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:50:39.660: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:40.627: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:40.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 13 09:50:40.636: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:41.625: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:41.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 09:50:41.634: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:50:41.673
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6231, will wait for the garbage collector to delete the pods 01/13/23 09:50:41.674
Jan 13 09:50:41.744: INFO: Deleting DaemonSet.extensions daemon-set took: 11.980867ms
Jan 13 09:50:41.844: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.8696ms
Jan 13 09:50:45.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:45.471: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 13 09:50:45.509: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"583415"},"items":null}

Jan 13 09:50:45.555: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"583415"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:50:45.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6231" for this suite. 01/13/23 09:50:45.779
------------------------------
• [SLOW TEST] [21.894 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:50:23.954
    Jan 13 09:50:23.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename daemonsets 01/13/23 09:50:23.957
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:24.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:24.13
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan 13 09:50:24.248: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:50:24.276
    Jan 13 09:50:24.292: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:24.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:24.312: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:25.339: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:25.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:25.354: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:26.329: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:26.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:26.350: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:27.328: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:27.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:27.343: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:28.354: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:28.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 09:50:28.368: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/13/23 09:50:28.469
    STEP: Check that daemon pods images are updated. 01/13/23 09:50:28.533
    Jan 13 09:50:28.545: INFO: Wrong image for pod: daemon-set-9fv57. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:28.545: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:28.565: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:29.580: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:29.592: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:30.587: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:30.603: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:31.578: INFO: Pod daemon-set-kkd95 is not available
    Jan 13 09:50:31.578: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:31.588: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:32.582: INFO: Pod daemon-set-kkd95 is not available
    Jan 13 09:50:32.582: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:32.606: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:33.575: INFO: Pod daemon-set-kkd95 is not available
    Jan 13 09:50:33.575: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:33.586: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:34.575: INFO: Pod daemon-set-kkd95 is not available
    Jan 13 09:50:34.575: INFO: Wrong image for pod: daemon-set-mwrhc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 13 09:50:34.589: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:35.599: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:36.593: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:37.576: INFO: Pod daemon-set-pjtqj is not available
    Jan 13 09:50:37.589: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/13/23 09:50:37.589
    Jan 13 09:50:37.603: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:37.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:50:37.616: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:38.698: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:38.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:50:38.739: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:39.639: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:39.660: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:50:39.660: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:40.627: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:40.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 13 09:50:40.636: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:41.625: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:41.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 09:50:41.634: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/13/23 09:50:41.673
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6231, will wait for the garbage collector to delete the pods 01/13/23 09:50:41.674
    Jan 13 09:50:41.744: INFO: Deleting DaemonSet.extensions daemon-set took: 11.980867ms
    Jan 13 09:50:41.844: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.8696ms
    Jan 13 09:50:45.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:45.471: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 13 09:50:45.509: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"583415"},"items":null}

    Jan 13 09:50:45.555: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"583415"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:50:45.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6231" for this suite. 01/13/23 09:50:45.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:50:45.85
Jan 13 09:50:45.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename daemonsets 01/13/23 09:50:45.857
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:45.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:45.931
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/13/23 09:50:45.979
STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:50:45.999
Jan 13 09:50:46.015: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:46.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:46.028: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:47.040: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:47.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:47.051: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:48.062: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:48.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:48.076: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:49.036: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:49.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 09:50:49.040: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:50:50.168: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:50:50.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 09:50:50.228: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/13/23 09:50:50.277
STEP: DeleteCollection of the DaemonSets 01/13/23 09:50:50.313
STEP: Verify that ReplicaSets have been deleted 01/13/23 09:50:50.379
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan 13 09:50:50.608: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"583458"},"items":null}

Jan 13 09:50:50.702: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"583458"},"items":[{"metadata":{"name":"daemon-set-8njg8","generateName":"daemon-set-","namespace":"daemonsets-7201","uid":"b5b890cf-5259-4c3d-88ec-721cdf8fe205","resourceVersion":"583454","creationTimestamp":"2023-01-13T09:50:46Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6c9fa68f5d44f75c4b28edee128dab4427d767d1c706ce341a2911792bd6b593","cni.projectcalico.org/podIP":"10.244.27.210/32","cni.projectcalico.org/podIPs":"10.244.27.210/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95885ce3-e3a7-49cb-870f-d24535f70e84","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95885ce3-e3a7-49cb-870f-d24535f70e84\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6sq8n","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6sq8n","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.10.102.31-node","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.10.102.31-node"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"}],"hostIP":"10.10.102.31","podIP":"10.244.27.210","podIPs":[{"ip":"10.244.27.210"}],"startTime":"2023-01-13T09:50:46Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-13T09:50:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"docker://cef37f3661fd5414c2f20524489313fa4d629002a7b96cbc8721e7bd39f1dbaa","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fvjsm","generateName":"daemon-set-","namespace":"daemonsets-7201","uid":"8558b431-e964-450b-b789-a9c17627e68b","resourceVersion":"583456","creationTimestamp":"2023-01-13T09:50:46Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5d0a18d0f12fc0d29ff983348c82dd69d7bf332ac415f8e2e2c86db5c1a7dac7","cni.projectcalico.org/podIP":"10.244.169.41/32","cni.projectcalico.org/podIPs":"10.244.169.41/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95885ce3-e3a7-49cb-870f-d24535f70e84","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95885ce3-e3a7-49cb-870f-d24535f70e84\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r9p7n","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r9p7n","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.10.102.32-node","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.10.102.32-node"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"}],"hostIP":"10.10.102.32","podIP":"10.244.169.41","podIPs":[{"ip":"10.244.169.41"}],"startTime":"2023-01-13T09:50:46Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-13T09:50:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"docker://a6914ac94da1000aac10775c27d47ffc4c2c8848df378cb2a560aa7817052ad1","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:50:50.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7201" for this suite. 01/13/23 09:50:50.954
------------------------------
• [SLOW TEST] [5.123 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:50:45.85
    Jan 13 09:50:45.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename daemonsets 01/13/23 09:50:45.857
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:45.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:45.931
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/13/23 09:50:45.979
    STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:50:45.999
    Jan 13 09:50:46.015: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:46.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:46.028: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:47.040: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:47.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:47.051: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:48.062: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:48.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:48.076: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:49.036: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:49.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 09:50:49.040: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:50:50.168: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:50:50.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 09:50:50.228: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/13/23 09:50:50.277
    STEP: DeleteCollection of the DaemonSets 01/13/23 09:50:50.313
    STEP: Verify that ReplicaSets have been deleted 01/13/23 09:50:50.379
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan 13 09:50:50.608: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"583458"},"items":null}

    Jan 13 09:50:50.702: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"583458"},"items":[{"metadata":{"name":"daemon-set-8njg8","generateName":"daemon-set-","namespace":"daemonsets-7201","uid":"b5b890cf-5259-4c3d-88ec-721cdf8fe205","resourceVersion":"583454","creationTimestamp":"2023-01-13T09:50:46Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6c9fa68f5d44f75c4b28edee128dab4427d767d1c706ce341a2911792bd6b593","cni.projectcalico.org/podIP":"10.244.27.210/32","cni.projectcalico.org/podIPs":"10.244.27.210/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95885ce3-e3a7-49cb-870f-d24535f70e84","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95885ce3-e3a7-49cb-870f-d24535f70e84\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6sq8n","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6sq8n","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.10.102.31-node","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.10.102.31-node"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"}],"hostIP":"10.10.102.31","podIP":"10.244.27.210","podIPs":[{"ip":"10.244.27.210"}],"startTime":"2023-01-13T09:50:46Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-13T09:50:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"docker://cef37f3661fd5414c2f20524489313fa4d629002a7b96cbc8721e7bd39f1dbaa","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fvjsm","generateName":"daemon-set-","namespace":"daemonsets-7201","uid":"8558b431-e964-450b-b789-a9c17627e68b","resourceVersion":"583456","creationTimestamp":"2023-01-13T09:50:46Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5d0a18d0f12fc0d29ff983348c82dd69d7bf332ac415f8e2e2c86db5c1a7dac7","cni.projectcalico.org/podIP":"10.244.169.41/32","cni.projectcalico.org/podIPs":"10.244.169.41/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"95885ce3-e3a7-49cb-870f-d24535f70e84","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95885ce3-e3a7-49cb-870f-d24535f70e84\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-13T09:50:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r9p7n","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r9p7n","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.10.102.32-node","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.10.102.32-node"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:50Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:50Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-13T09:50:46Z"}],"hostIP":"10.10.102.32","podIP":"10.244.169.41","podIPs":[{"ip":"10.244.169.41"}],"startTime":"2023-01-13T09:50:46Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-13T09:50:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"docker://a6914ac94da1000aac10775c27d47ffc4c2c8848df378cb2a560aa7817052ad1","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:50:50.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7201" for this suite. 01/13/23 09:50:50.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:50:50.976
Jan 13 09:50:50.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename job 01/13/23 09:50:50.986
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:51.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:51.025
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/13/23 09:50:51.031
STEP: Ensure pods equal to parallelism count is attached to the job 01/13/23 09:50:51.039
STEP: patching /status 01/13/23 09:50:57.054
STEP: updating /status 01/13/23 09:50:57.111
STEP: get /status 01/13/23 09:50:57.149
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 13 09:50:57.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-603" for this suite. 01/13/23 09:50:57.188
------------------------------
• [SLOW TEST] [6.238 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:50:50.976
    Jan 13 09:50:50.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename job 01/13/23 09:50:50.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:51.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:51.025
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/13/23 09:50:51.031
    STEP: Ensure pods equal to parallelism count is attached to the job 01/13/23 09:50:51.039
    STEP: patching /status 01/13/23 09:50:57.054
    STEP: updating /status 01/13/23 09:50:57.111
    STEP: get /status 01/13/23 09:50:57.149
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:50:57.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-603" for this suite. 01/13/23 09:50:57.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:50:57.216
Jan 13 09:50:57.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 09:50:57.219
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:57.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:57.25
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 09:50:57.255
Jan 13 09:50:57.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 13 09:50:57.519: INFO: stderr: ""
Jan 13 09:50:57.519: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/13/23 09:50:57.519
STEP: verifying the pod e2e-test-httpd-pod was created 01/13/23 09:51:02.576
Jan 13 09:51:02.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 get pod e2e-test-httpd-pod -o json'
Jan 13 09:51:02.923: INFO: stderr: ""
Jan 13 09:51:02.923: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"c92669baee0799bacd9b059de780816f17aea5db02fadae1ec28762d784634d2\",\n            \"cni.projectcalico.org/podIP\": \"10.244.27.254/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.27.254/32\"\n        },\n        \"creationTimestamp\": \"2023-01-13T09:50:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4454\",\n        \"resourceVersion\": \"583583\",\n        \"uid\": \"08f4c0d0-5213-4d2a-85e8-1c245ce64ad0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qtzcn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.10.102.31-node\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qtzcn\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:50:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:51:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:51:01Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:50:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://428bccb75c316cac00f71aa95bc9812e1d068172d6c683e6a2b26e9c41274439\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-13T09:51:01Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.102.31\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.27.254\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.27.254\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-13T09:50:57Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/13/23 09:51:02.924
Jan 13 09:51:02.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 replace -f -'
Jan 13 09:51:06.840: INFO: stderr: ""
Jan 13 09:51:06.840: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/13/23 09:51:06.84
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 13 09:51:06.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 delete pods e2e-test-httpd-pod'
Jan 13 09:51:09.971: INFO: stderr: ""
Jan 13 09:51:09.971: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 09:51:09.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4454" for this suite. 01/13/23 09:51:09.983
------------------------------
• [SLOW TEST] [12.790 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:50:57.216
    Jan 13 09:50:57.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 09:50:57.219
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:50:57.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:50:57.25
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 09:50:57.255
    Jan 13 09:50:57.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 13 09:50:57.519: INFO: stderr: ""
    Jan 13 09:50:57.519: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/13/23 09:50:57.519
    STEP: verifying the pod e2e-test-httpd-pod was created 01/13/23 09:51:02.576
    Jan 13 09:51:02.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 get pod e2e-test-httpd-pod -o json'
    Jan 13 09:51:02.923: INFO: stderr: ""
    Jan 13 09:51:02.923: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"c92669baee0799bacd9b059de780816f17aea5db02fadae1ec28762d784634d2\",\n            \"cni.projectcalico.org/podIP\": \"10.244.27.254/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.27.254/32\"\n        },\n        \"creationTimestamp\": \"2023-01-13T09:50:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4454\",\n        \"resourceVersion\": \"583583\",\n        \"uid\": \"08f4c0d0-5213-4d2a-85e8-1c245ce64ad0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qtzcn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.10.102.31-node\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qtzcn\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:50:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:51:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:51:01Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-13T09:50:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://428bccb75c316cac00f71aa95bc9812e1d068172d6c683e6a2b26e9c41274439\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-13T09:51:01Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.102.31\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.27.254\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.27.254\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-13T09:50:57Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/13/23 09:51:02.924
    Jan 13 09:51:02.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 replace -f -'
    Jan 13 09:51:06.840: INFO: stderr: ""
    Jan 13 09:51:06.840: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/13/23 09:51:06.84
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 13 09:51:06.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-4454 delete pods e2e-test-httpd-pod'
    Jan 13 09:51:09.971: INFO: stderr: ""
    Jan 13 09:51:09.971: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:51:09.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4454" for this suite. 01/13/23 09:51:09.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:51:10.009
Jan 13 09:51:10.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 09:51:10.011
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:51:10.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:51:10.074
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/13/23 09:51:10.084
STEP: submitting the pod to kubernetes 01/13/23 09:51:10.085
Jan 13 09:51:10.108: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" in namespace "pods-3144" to be "running and ready"
Jan 13 09:51:10.115: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.721994ms
Jan 13 09:51:10.115: INFO: The phase of Pod pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:51:12.123: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015290707s
Jan 13 09:51:12.123: INFO: The phase of Pod pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:51:14.135: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.027669569s
Jan 13 09:51:14.136: INFO: The phase of Pod pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca is Running (Ready = true)
Jan 13 09:51:14.136: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/13/23 09:51:14.173
STEP: updating the pod 01/13/23 09:51:14.186
Jan 13 09:51:14.760: INFO: Successfully updated pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca"
Jan 13 09:51:14.761: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" in namespace "pods-3144" to be "terminated with reason DeadlineExceeded"
Jan 13 09:51:14.778: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=true. Elapsed: 17.891132ms
Jan 13 09:51:16.804: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=false. Elapsed: 2.043058657s
Jan 13 09:51:18.788: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=false. Elapsed: 4.027899689s
Jan 13 09:51:20.787: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.026441936s
Jan 13 09:51:20.787: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 09:51:20.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3144" for this suite. 01/13/23 09:51:20.806
------------------------------
• [SLOW TEST] [10.808 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:51:10.009
    Jan 13 09:51:10.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 09:51:10.011
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:51:10.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:51:10.074
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/13/23 09:51:10.084
    STEP: submitting the pod to kubernetes 01/13/23 09:51:10.085
    Jan 13 09:51:10.108: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" in namespace "pods-3144" to be "running and ready"
    Jan 13 09:51:10.115: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.721994ms
    Jan 13 09:51:10.115: INFO: The phase of Pod pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:51:12.123: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015290707s
    Jan 13 09:51:12.123: INFO: The phase of Pod pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:51:14.135: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.027669569s
    Jan 13 09:51:14.136: INFO: The phase of Pod pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca is Running (Ready = true)
    Jan 13 09:51:14.136: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/13/23 09:51:14.173
    STEP: updating the pod 01/13/23 09:51:14.186
    Jan 13 09:51:14.760: INFO: Successfully updated pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca"
    Jan 13 09:51:14.761: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" in namespace "pods-3144" to be "terminated with reason DeadlineExceeded"
    Jan 13 09:51:14.778: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=true. Elapsed: 17.891132ms
    Jan 13 09:51:16.804: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=false. Elapsed: 2.043058657s
    Jan 13 09:51:18.788: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Running", Reason="", readiness=false. Elapsed: 4.027899689s
    Jan 13 09:51:20.787: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.026441936s
    Jan 13 09:51:20.787: INFO: Pod "pod-update-activedeadlineseconds-59a4ff21-fb34-4b66-ac4a-616fa3c8e2ca" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:51:20.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3144" for this suite. 01/13/23 09:51:20.806
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:51:20.817
Jan 13 09:51:20.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 09:51:20.819
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:51:20.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:51:20.858
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/13/23 09:51:20.882
Jan 13 09:51:20.917: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-358" to be "running and ready"
Jan 13 09:51:20.934: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.078409ms
Jan 13 09:51:20.934: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:51:22.947: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030385218s
Jan 13 09:51:22.947: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:51:24.947: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.029803207s
Jan 13 09:51:24.947: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 13 09:51:24.947: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/13/23 09:51:24.967
Jan 13 09:51:25.010: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-358" to be "running and ready"
Jan 13 09:51:25.021: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.90554ms
Jan 13 09:51:25.021: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:51:27.028: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018170674s
Jan 13 09:51:27.028: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:51:29.028: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.018687327s
Jan 13 09:51:29.028: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 13 09:51:29.028: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/13/23 09:51:29.034
Jan 13 09:51:29.046: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 09:51:29.059: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 09:51:31.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 09:51:31.100: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 09:51:33.060: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 09:51:33.067: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/13/23 09:51:33.067
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 13 09:51:33.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-358" for this suite. 01/13/23 09:51:33.229
------------------------------
• [SLOW TEST] [12.453 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:51:20.817
    Jan 13 09:51:20.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 09:51:20.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:51:20.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:51:20.858
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/13/23 09:51:20.882
    Jan 13 09:51:20.917: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-358" to be "running and ready"
    Jan 13 09:51:20.934: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.078409ms
    Jan 13 09:51:20.934: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:51:22.947: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030385218s
    Jan 13 09:51:22.947: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:51:24.947: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.029803207s
    Jan 13 09:51:24.947: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 13 09:51:24.947: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/13/23 09:51:24.967
    Jan 13 09:51:25.010: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-358" to be "running and ready"
    Jan 13 09:51:25.021: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.90554ms
    Jan 13 09:51:25.021: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:51:27.028: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018170674s
    Jan 13 09:51:27.028: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:51:29.028: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.018687327s
    Jan 13 09:51:29.028: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 13 09:51:29.028: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/13/23 09:51:29.034
    Jan 13 09:51:29.046: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 13 09:51:29.059: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 13 09:51:31.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 13 09:51:31.100: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 13 09:51:33.060: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 13 09:51:33.067: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/13/23 09:51:33.067
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:51:33.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-358" for this suite. 01/13/23 09:51:33.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:51:33.273
Jan 13 09:51:33.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename cronjob 01/13/23 09:51:33.277
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:51:33.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:51:33.399
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/13/23 09:51:33.434
STEP: Ensuring a job is scheduled 01/13/23 09:51:33.444
STEP: Ensuring exactly one is scheduled 01/13/23 09:52:01.452
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/13/23 09:52:01.458
STEP: Ensuring the job is replaced with a new one 01/13/23 09:52:01.468
STEP: Removing cronjob 01/13/23 09:53:01.484
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 13 09:53:01.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8668" for this suite. 01/13/23 09:53:01.511
------------------------------
• [SLOW TEST] [88.253 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:51:33.273
    Jan 13 09:51:33.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename cronjob 01/13/23 09:51:33.277
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:51:33.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:51:33.399
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/13/23 09:51:33.434
    STEP: Ensuring a job is scheduled 01/13/23 09:51:33.444
    STEP: Ensuring exactly one is scheduled 01/13/23 09:52:01.452
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/13/23 09:52:01.458
    STEP: Ensuring the job is replaced with a new one 01/13/23 09:52:01.468
    STEP: Removing cronjob 01/13/23 09:53:01.484
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:53:01.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8668" for this suite. 01/13/23 09:53:01.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:53:01.528
Jan 13 09:53:01.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replication-controller 01/13/23 09:53:01.53
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:01.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:01.583
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/13/23 09:53:01.591
STEP: When the matched label of one of its pods change 01/13/23 09:53:01.606
Jan 13 09:53:01.612: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 13 09:53:06.626: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/13/23 09:53:06.688
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 13 09:53:07.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7771" for this suite. 01/13/23 09:53:07.725
------------------------------
• [SLOW TEST] [6.215 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:53:01.528
    Jan 13 09:53:01.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replication-controller 01/13/23 09:53:01.53
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:01.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:01.583
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/13/23 09:53:01.591
    STEP: When the matched label of one of its pods change 01/13/23 09:53:01.606
    Jan 13 09:53:01.612: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 13 09:53:06.626: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/13/23 09:53:06.688
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:53:07.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7771" for this suite. 01/13/23 09:53:07.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:53:07.745
Jan 13 09:53:07.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:53:07.747
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:07.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:07.814
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 13 09:53:07.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:53:11.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8856" for this suite. 01/13/23 09:53:11.454
------------------------------
• [3.718 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:53:07.745
    Jan 13 09:53:07.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:53:07.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:07.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:07.814
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 13 09:53:07.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:53:11.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8856" for this suite. 01/13/23 09:53:11.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:53:11.464
Jan 13 09:53:11.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename subpath 01/13/23 09:53:11.466
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:11.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:11.499
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/13/23 09:53:11.504
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-b6zs 01/13/23 09:53:11.52
STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:53:11.52
Jan 13 09:53:11.532: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-b6zs" in namespace "subpath-2640" to be "Succeeded or Failed"
Jan 13 09:53:11.537: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.711059ms
Jan 13 09:53:13.552: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019505851s
Jan 13 09:53:15.549: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 4.017108344s
Jan 13 09:53:17.547: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 6.01468039s
Jan 13 09:53:19.546: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 8.013694026s
Jan 13 09:53:21.548: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 10.016042092s
Jan 13 09:53:23.546: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 12.013882287s
Jan 13 09:53:25.551: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 14.018676549s
Jan 13 09:53:27.546: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 16.013332167s
Jan 13 09:53:29.547: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 18.014198811s
Jan 13 09:53:31.548: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 20.015892257s
Jan 13 09:53:33.545: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 22.012471356s
Jan 13 09:53:35.544: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=false. Elapsed: 24.012007448s
Jan 13 09:53:37.543: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.01097629s
STEP: Saw pod success 01/13/23 09:53:37.543
Jan 13 09:53:37.544: INFO: Pod "pod-subpath-test-configmap-b6zs" satisfied condition "Succeeded or Failed"
Jan 13 09:53:37.550: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-configmap-b6zs container test-container-subpath-configmap-b6zs: <nil>
STEP: delete the pod 01/13/23 09:53:37.6
Jan 13 09:53:37.636: INFO: Waiting for pod pod-subpath-test-configmap-b6zs to disappear
Jan 13 09:53:37.646: INFO: Pod pod-subpath-test-configmap-b6zs no longer exists
STEP: Deleting pod pod-subpath-test-configmap-b6zs 01/13/23 09:53:37.646
Jan 13 09:53:37.646: INFO: Deleting pod "pod-subpath-test-configmap-b6zs" in namespace "subpath-2640"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 13 09:53:37.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2640" for this suite. 01/13/23 09:53:37.666
------------------------------
• [SLOW TEST] [26.213 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:53:11.464
    Jan 13 09:53:11.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename subpath 01/13/23 09:53:11.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:11.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:11.499
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/13/23 09:53:11.504
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-b6zs 01/13/23 09:53:11.52
    STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:53:11.52
    Jan 13 09:53:11.532: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-b6zs" in namespace "subpath-2640" to be "Succeeded or Failed"
    Jan 13 09:53:11.537: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.711059ms
    Jan 13 09:53:13.552: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019505851s
    Jan 13 09:53:15.549: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 4.017108344s
    Jan 13 09:53:17.547: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 6.01468039s
    Jan 13 09:53:19.546: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 8.013694026s
    Jan 13 09:53:21.548: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 10.016042092s
    Jan 13 09:53:23.546: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 12.013882287s
    Jan 13 09:53:25.551: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 14.018676549s
    Jan 13 09:53:27.546: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 16.013332167s
    Jan 13 09:53:29.547: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 18.014198811s
    Jan 13 09:53:31.548: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 20.015892257s
    Jan 13 09:53:33.545: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=true. Elapsed: 22.012471356s
    Jan 13 09:53:35.544: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Running", Reason="", readiness=false. Elapsed: 24.012007448s
    Jan 13 09:53:37.543: INFO: Pod "pod-subpath-test-configmap-b6zs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.01097629s
    STEP: Saw pod success 01/13/23 09:53:37.543
    Jan 13 09:53:37.544: INFO: Pod "pod-subpath-test-configmap-b6zs" satisfied condition "Succeeded or Failed"
    Jan 13 09:53:37.550: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-configmap-b6zs container test-container-subpath-configmap-b6zs: <nil>
    STEP: delete the pod 01/13/23 09:53:37.6
    Jan 13 09:53:37.636: INFO: Waiting for pod pod-subpath-test-configmap-b6zs to disappear
    Jan 13 09:53:37.646: INFO: Pod pod-subpath-test-configmap-b6zs no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-b6zs 01/13/23 09:53:37.646
    Jan 13 09:53:37.646: INFO: Deleting pod "pod-subpath-test-configmap-b6zs" in namespace "subpath-2640"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:53:37.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2640" for this suite. 01/13/23 09:53:37.666
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:53:37.68
Jan 13 09:53:37.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 09:53:37.683
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:37.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:37.716
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/13/23 09:53:37.725
STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:53:37.739
STEP: Creating a ResourceQuota with not terminating scope 01/13/23 09:53:39.766
STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:53:39.778
STEP: Creating a long running pod 01/13/23 09:53:41.787
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/13/23 09:53:41.807
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/13/23 09:53:43.824
STEP: Deleting the pod 01/13/23 09:53:45.832
STEP: Ensuring resource quota status released the pod usage 01/13/23 09:53:45.852
STEP: Creating a terminating pod 01/13/23 09:53:47.86
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/13/23 09:53:47.88
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/13/23 09:53:49.943
STEP: Deleting the pod 01/13/23 09:53:51.95
STEP: Ensuring resource quota status released the pod usage 01/13/23 09:53:51.975
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 09:53:53.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6929" for this suite. 01/13/23 09:53:53.993
------------------------------
• [SLOW TEST] [16.340 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:53:37.68
    Jan 13 09:53:37.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 09:53:37.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:37.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:37.716
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/13/23 09:53:37.725
    STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:53:37.739
    STEP: Creating a ResourceQuota with not terminating scope 01/13/23 09:53:39.766
    STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:53:39.778
    STEP: Creating a long running pod 01/13/23 09:53:41.787
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/13/23 09:53:41.807
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/13/23 09:53:43.824
    STEP: Deleting the pod 01/13/23 09:53:45.832
    STEP: Ensuring resource quota status released the pod usage 01/13/23 09:53:45.852
    STEP: Creating a terminating pod 01/13/23 09:53:47.86
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/13/23 09:53:47.88
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/13/23 09:53:49.943
    STEP: Deleting the pod 01/13/23 09:53:51.95
    STEP: Ensuring resource quota status released the pod usage 01/13/23 09:53:51.975
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:53:53.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6929" for this suite. 01/13/23 09:53:53.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:53:54.026
Jan 13 09:53:54.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 09:53:54.028
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:54.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:54.112
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/13/23 09:53:54.12
Jan 13 09:53:54.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: rename a version 01/13/23 09:54:01.399
STEP: check the new version name is served 01/13/23 09:54:01.442
STEP: check the old version name is removed 01/13/23 09:54:04.636
STEP: check the other version is not changed 01/13/23 09:54:05.897
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:54:11.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6831" for this suite. 01/13/23 09:54:11.649
------------------------------
• [SLOW TEST] [17.631 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:53:54.026
    Jan 13 09:53:54.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 09:53:54.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:53:54.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:53:54.112
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/13/23 09:53:54.12
    Jan 13 09:53:54.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: rename a version 01/13/23 09:54:01.399
    STEP: check the new version name is served 01/13/23 09:54:01.442
    STEP: check the old version name is removed 01/13/23 09:54:04.636
    STEP: check the other version is not changed 01/13/23 09:54:05.897
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:54:11.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6831" for this suite. 01/13/23 09:54:11.649
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:54:11.657
Jan 13 09:54:11.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 09:54:11.663
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:11.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:11.699
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 13 09:54:11.719: INFO: Waiting up to 5m0s for pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a" in namespace "container-probe-5654" to be "running and ready"
Jan 13 09:54:11.724: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028405ms
Jan 13 09:54:11.724: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:54:13.751: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032185085s
Jan 13 09:54:13.751: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:54:15.756: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036733524s
Jan 13 09:54:15.756: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:17.737: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 6.017297406s
Jan 13 09:54:17.737: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:19.736: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 8.016808387s
Jan 13 09:54:19.736: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:21.740: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 10.021048623s
Jan 13 09:54:21.740: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:23.734: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 12.014900554s
Jan 13 09:54:23.734: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:25.731: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 14.011278487s
Jan 13 09:54:25.731: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:27.741: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 16.021576155s
Jan 13 09:54:27.741: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:29.731: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 18.012038297s
Jan 13 09:54:29.731: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:31.732: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 20.013186987s
Jan 13 09:54:31.733: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
Jan 13 09:54:33.732: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=true. Elapsed: 22.012451991s
Jan 13 09:54:33.732: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = true)
Jan 13 09:54:33.732: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a" satisfied condition "running and ready"
Jan 13 09:54:33.741: INFO: Container started at 2023-01-13 09:54:13 +0000 UTC, pod became ready at 2023-01-13 09:54:32 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 09:54:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5654" for this suite. 01/13/23 09:54:33.755
------------------------------
• [SLOW TEST] [22.116 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:54:11.657
    Jan 13 09:54:11.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 09:54:11.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:11.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:11.699
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 13 09:54:11.719: INFO: Waiting up to 5m0s for pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a" in namespace "container-probe-5654" to be "running and ready"
    Jan 13 09:54:11.724: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.028405ms
    Jan 13 09:54:11.724: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:54:13.751: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032185085s
    Jan 13 09:54:13.751: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:54:15.756: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036733524s
    Jan 13 09:54:15.756: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:17.737: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 6.017297406s
    Jan 13 09:54:17.737: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:19.736: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 8.016808387s
    Jan 13 09:54:19.736: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:21.740: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 10.021048623s
    Jan 13 09:54:21.740: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:23.734: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 12.014900554s
    Jan 13 09:54:23.734: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:25.731: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 14.011278487s
    Jan 13 09:54:25.731: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:27.741: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 16.021576155s
    Jan 13 09:54:27.741: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:29.731: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 18.012038297s
    Jan 13 09:54:29.731: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:31.732: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=false. Elapsed: 20.013186987s
    Jan 13 09:54:31.733: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = false)
    Jan 13 09:54:33.732: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a": Phase="Running", Reason="", readiness=true. Elapsed: 22.012451991s
    Jan 13 09:54:33.732: INFO: The phase of Pod test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a is Running (Ready = true)
    Jan 13 09:54:33.732: INFO: Pod "test-webserver-2aa19685-b702-4538-a9ff-9a5ac106976a" satisfied condition "running and ready"
    Jan 13 09:54:33.741: INFO: Container started at 2023-01-13 09:54:13 +0000 UTC, pod became ready at 2023-01-13 09:54:32 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:54:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5654" for this suite. 01/13/23 09:54:33.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:54:33.777
Jan 13 09:54:33.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename controllerrevisions 01/13/23 09:54:33.78
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:33.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:33.832
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-bqsdz-daemon-set" 01/13/23 09:54:33.898
STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:54:33.91
Jan 13 09:54:33.919: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:54:33.930: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
Jan 13 09:54:33.931: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:54:34.962: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:54:34.977: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
Jan 13 09:54:34.977: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:54:35.946: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:54:35.954: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
Jan 13 09:54:35.954: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 09:54:36.951: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:54:36.962: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 2
Jan 13 09:54:36.962: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-bqsdz-daemon-set
STEP: Confirm DaemonSet "e2e-bqsdz-daemon-set" successfully created with "daemonset-name=e2e-bqsdz-daemon-set" label 01/13/23 09:54:36.977
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-bqsdz-daemon-set" 01/13/23 09:54:37.007
Jan 13 09:54:37.022: INFO: Located ControllerRevision: "e2e-bqsdz-daemon-set-5b8d6847d9"
STEP: Patching ControllerRevision "e2e-bqsdz-daemon-set-5b8d6847d9" 01/13/23 09:54:37.029
Jan 13 09:54:37.041: INFO: e2e-bqsdz-daemon-set-5b8d6847d9 has been patched
STEP: Create a new ControllerRevision 01/13/23 09:54:37.041
Jan 13 09:54:37.051: INFO: Created ControllerRevision: e2e-bqsdz-daemon-set-b44997c57
STEP: Confirm that there are two ControllerRevisions 01/13/23 09:54:37.051
Jan 13 09:54:37.051: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 13 09:54:37.057: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-bqsdz-daemon-set-5b8d6847d9" 01/13/23 09:54:37.057
STEP: Confirm that there is only one ControllerRevision 01/13/23 09:54:37.074
Jan 13 09:54:37.074: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 13 09:54:37.079: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-bqsdz-daemon-set-b44997c57" 01/13/23 09:54:37.085
Jan 13 09:54:37.105: INFO: e2e-bqsdz-daemon-set-b44997c57 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/13/23 09:54:37.105
W0113 09:54:37.129007      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/13/23 09:54:37.129
Jan 13 09:54:37.129: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 13 09:54:38.143: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 13 09:54:38.150: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-bqsdz-daemon-set-b44997c57=updated" 01/13/23 09:54:38.15
STEP: Confirm that there is only one ControllerRevision 01/13/23 09:54:38.161
Jan 13 09:54:38.162: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 13 09:54:38.167: INFO: Found 1 ControllerRevisions
Jan 13 09:54:38.171: INFO: ControllerRevision "e2e-bqsdz-daemon-set-6487f74796" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-bqsdz-daemon-set" 01/13/23 09:54:38.177
STEP: deleting DaemonSet.extensions e2e-bqsdz-daemon-set in namespace controllerrevisions-6941, will wait for the garbage collector to delete the pods 01/13/23 09:54:38.177
Jan 13 09:54:38.243: INFO: Deleting DaemonSet.extensions e2e-bqsdz-daemon-set took: 9.525331ms
Jan 13 09:54:38.344: INFO: Terminating DaemonSet.extensions e2e-bqsdz-daemon-set pods took: 100.767295ms
Jan 13 09:54:41.354: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
Jan 13 09:54:41.354: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-bqsdz-daemon-set
Jan 13 09:54:41.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"584417"},"items":null}

Jan 13 09:54:41.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"584417"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:54:41.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-6941" for this suite. 01/13/23 09:54:41.386
------------------------------
• [SLOW TEST] [7.622 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:54:33.777
    Jan 13 09:54:33.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename controllerrevisions 01/13/23 09:54:33.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:33.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:33.832
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-bqsdz-daemon-set" 01/13/23 09:54:33.898
    STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 09:54:33.91
    Jan 13 09:54:33.919: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:54:33.930: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
    Jan 13 09:54:33.931: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:54:34.962: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:54:34.977: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
    Jan 13 09:54:34.977: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:54:35.946: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:54:35.954: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
    Jan 13 09:54:35.954: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 09:54:36.951: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 09:54:36.962: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 2
    Jan 13 09:54:36.962: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-bqsdz-daemon-set
    STEP: Confirm DaemonSet "e2e-bqsdz-daemon-set" successfully created with "daemonset-name=e2e-bqsdz-daemon-set" label 01/13/23 09:54:36.977
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-bqsdz-daemon-set" 01/13/23 09:54:37.007
    Jan 13 09:54:37.022: INFO: Located ControllerRevision: "e2e-bqsdz-daemon-set-5b8d6847d9"
    STEP: Patching ControllerRevision "e2e-bqsdz-daemon-set-5b8d6847d9" 01/13/23 09:54:37.029
    Jan 13 09:54:37.041: INFO: e2e-bqsdz-daemon-set-5b8d6847d9 has been patched
    STEP: Create a new ControllerRevision 01/13/23 09:54:37.041
    Jan 13 09:54:37.051: INFO: Created ControllerRevision: e2e-bqsdz-daemon-set-b44997c57
    STEP: Confirm that there are two ControllerRevisions 01/13/23 09:54:37.051
    Jan 13 09:54:37.051: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 13 09:54:37.057: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-bqsdz-daemon-set-5b8d6847d9" 01/13/23 09:54:37.057
    STEP: Confirm that there is only one ControllerRevision 01/13/23 09:54:37.074
    Jan 13 09:54:37.074: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 13 09:54:37.079: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-bqsdz-daemon-set-b44997c57" 01/13/23 09:54:37.085
    Jan 13 09:54:37.105: INFO: e2e-bqsdz-daemon-set-b44997c57 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/13/23 09:54:37.105
    W0113 09:54:37.129007      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/13/23 09:54:37.129
    Jan 13 09:54:37.129: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 13 09:54:38.143: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 13 09:54:38.150: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-bqsdz-daemon-set-b44997c57=updated" 01/13/23 09:54:38.15
    STEP: Confirm that there is only one ControllerRevision 01/13/23 09:54:38.161
    Jan 13 09:54:38.162: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 13 09:54:38.167: INFO: Found 1 ControllerRevisions
    Jan 13 09:54:38.171: INFO: ControllerRevision "e2e-bqsdz-daemon-set-6487f74796" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-bqsdz-daemon-set" 01/13/23 09:54:38.177
    STEP: deleting DaemonSet.extensions e2e-bqsdz-daemon-set in namespace controllerrevisions-6941, will wait for the garbage collector to delete the pods 01/13/23 09:54:38.177
    Jan 13 09:54:38.243: INFO: Deleting DaemonSet.extensions e2e-bqsdz-daemon-set took: 9.525331ms
    Jan 13 09:54:38.344: INFO: Terminating DaemonSet.extensions e2e-bqsdz-daemon-set pods took: 100.767295ms
    Jan 13 09:54:41.354: INFO: Number of nodes with available pods controlled by daemonset e2e-bqsdz-daemon-set: 0
    Jan 13 09:54:41.354: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-bqsdz-daemon-set
    Jan 13 09:54:41.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"584417"},"items":null}

    Jan 13 09:54:41.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"584417"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:54:41.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-6941" for this suite. 01/13/23 09:54:41.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:54:41.401
Jan 13 09:54:41.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename gc 01/13/23 09:54:41.403
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:41.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:41.434
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 13 09:54:41.512: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"413dc7ce-3073-436e-8971-3aea7cc18f10", Controller:(*bool)(0xc004ea4a2a), BlockOwnerDeletion:(*bool)(0xc004ea4a2b)}}
Jan 13 09:54:41.523: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cc7fb1b0-51f8-47e6-a4c7-920b8453b41e", Controller:(*bool)(0xc0044761b2), BlockOwnerDeletion:(*bool)(0xc0044761b3)}}
Jan 13 09:54:41.546: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e9f21c3a-0162-4cb5-be5a-90a1e5eee3df", Controller:(*bool)(0xc004ea4cc2), BlockOwnerDeletion:(*bool)(0xc004ea4cc3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 13 09:54:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7869" for this suite. 01/13/23 09:54:46.585
------------------------------
• [SLOW TEST] [5.211 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:54:41.401
    Jan 13 09:54:41.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename gc 01/13/23 09:54:41.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:41.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:41.434
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 13 09:54:41.512: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"413dc7ce-3073-436e-8971-3aea7cc18f10", Controller:(*bool)(0xc004ea4a2a), BlockOwnerDeletion:(*bool)(0xc004ea4a2b)}}
    Jan 13 09:54:41.523: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cc7fb1b0-51f8-47e6-a4c7-920b8453b41e", Controller:(*bool)(0xc0044761b2), BlockOwnerDeletion:(*bool)(0xc0044761b3)}}
    Jan 13 09:54:41.546: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e9f21c3a-0162-4cb5-be5a-90a1e5eee3df", Controller:(*bool)(0xc004ea4cc2), BlockOwnerDeletion:(*bool)(0xc004ea4cc3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:54:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7869" for this suite. 01/13/23 09:54:46.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:54:46.619
Jan 13 09:54:46.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-runtime 01/13/23 09:54:46.622
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:46.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:46.75
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/13/23 09:54:46.759
STEP: wait for the container to reach Failed 01/13/23 09:54:46.783
STEP: get the container status 01/13/23 09:54:52.874
STEP: the container should be terminated 01/13/23 09:54:52.885
STEP: the termination message should be set 01/13/23 09:54:52.886
Jan 13 09:54:52.886: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/13/23 09:54:52.886
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 13 09:54:52.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5774" for this suite. 01/13/23 09:54:52.941
------------------------------
• [SLOW TEST] [6.333 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:54:46.619
    Jan 13 09:54:46.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-runtime 01/13/23 09:54:46.622
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:46.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:46.75
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/13/23 09:54:46.759
    STEP: wait for the container to reach Failed 01/13/23 09:54:46.783
    STEP: get the container status 01/13/23 09:54:52.874
    STEP: the container should be terminated 01/13/23 09:54:52.885
    STEP: the termination message should be set 01/13/23 09:54:52.886
    Jan 13 09:54:52.886: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/13/23 09:54:52.886
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:54:52.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5774" for this suite. 01/13/23 09:54:52.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:54:52.953
Jan 13 09:54:52.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:54:52.955
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:52.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:52.985
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 13 09:54:53.036: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8649 to be scheduled
Jan 13 09:54:53.046: INFO: 1 pods are not scheduled: [runtimeclass-8649/test-runtimeclass-runtimeclass-8649-preconfigured-handler-x2pql(f8a4526a-1f94-481b-a38f-768a884ef883)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 13 09:54:55.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8649" for this suite. 01/13/23 09:54:55.135
------------------------------
• [2.242 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:54:52.953
    Jan 13 09:54:52.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename runtimeclass 01/13/23 09:54:52.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:52.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:52.985
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 13 09:54:53.036: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8649 to be scheduled
    Jan 13 09:54:53.046: INFO: 1 pods are not scheduled: [runtimeclass-8649/test-runtimeclass-runtimeclass-8649-preconfigured-handler-x2pql(f8a4526a-1f94-481b-a38f-768a884ef883)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:54:55.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8649" for this suite. 01/13/23 09:54:55.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:54:55.197
Jan 13 09:54:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 09:54:55.199
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:55.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:55.273
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/13/23 09:54:55.314
Jan 13 09:54:55.364: INFO: Waiting up to 5m0s for pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6" in namespace "var-expansion-3145" to be "Succeeded or Failed"
Jan 13 09:54:55.382: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.82233ms
Jan 13 09:54:57.420: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056230556s
Jan 13 09:54:59.394: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030434655s
Jan 13 09:55:01.394: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030366479s
STEP: Saw pod success 01/13/23 09:55:01.394
Jan 13 09:55:01.394: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6" satisfied condition "Succeeded or Failed"
Jan 13 09:55:01.414: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6 container dapi-container: <nil>
STEP: delete the pod 01/13/23 09:55:01.438
Jan 13 09:55:01.461: INFO: Waiting for pod var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6 to disappear
Jan 13 09:55:01.472: INFO: Pod var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 09:55:01.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3145" for this suite. 01/13/23 09:55:01.48
------------------------------
• [SLOW TEST] [6.295 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:54:55.197
    Jan 13 09:54:55.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 09:54:55.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:54:55.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:54:55.273
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/13/23 09:54:55.314
    Jan 13 09:54:55.364: INFO: Waiting up to 5m0s for pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6" in namespace "var-expansion-3145" to be "Succeeded or Failed"
    Jan 13 09:54:55.382: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.82233ms
    Jan 13 09:54:57.420: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056230556s
    Jan 13 09:54:59.394: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030434655s
    Jan 13 09:55:01.394: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030366479s
    STEP: Saw pod success 01/13/23 09:55:01.394
    Jan 13 09:55:01.394: INFO: Pod "var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6" satisfied condition "Succeeded or Failed"
    Jan 13 09:55:01.414: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6 container dapi-container: <nil>
    STEP: delete the pod 01/13/23 09:55:01.438
    Jan 13 09:55:01.461: INFO: Waiting for pod var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6 to disappear
    Jan 13 09:55:01.472: INFO: Pod var-expansion-f2339ee8-14df-4e67-8297-5b047e5fdcd6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:55:01.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3145" for this suite. 01/13/23 09:55:01.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:55:01.495
Jan 13 09:55:01.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 09:55:01.497
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:01.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:01.528
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 09:55:01.549
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:55:02.904
STEP: Deploying the webhook pod 01/13/23 09:55:02.942
STEP: Wait for the deployment to be ready 01/13/23 09:55:02.978
Jan 13 09:55:03.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 09:55:05.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 09:55:07.095
STEP: Verifying the service has paired with the endpoint 01/13/23 09:55:07.135
Jan 13 09:55:08.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/13/23 09:55:08.145
STEP: create a pod 01/13/23 09:55:08.244
Jan 13 09:55:08.262: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-614" to be "running"
Jan 13 09:55:08.277: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.433349ms
Jan 13 09:55:10.286: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023997542s
Jan 13 09:55:12.283: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021026673s
Jan 13 09:55:14.297: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.034794671s
Jan 13 09:55:14.297: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/13/23 09:55:14.297
Jan 13 09:55:14.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=webhook-614 attach --namespace=webhook-614 to-be-attached-pod -i -c=container1'
Jan 13 09:55:14.611: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:55:14.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-614" for this suite. 01/13/23 09:55:14.756
STEP: Destroying namespace "webhook-614-markers" for this suite. 01/13/23 09:55:14.775
------------------------------
• [SLOW TEST] [13.295 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:55:01.495
    Jan 13 09:55:01.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 09:55:01.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:01.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:01.528
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 09:55:01.549
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:55:02.904
    STEP: Deploying the webhook pod 01/13/23 09:55:02.942
    STEP: Wait for the deployment to be ready 01/13/23 09:55:02.978
    Jan 13 09:55:03.008: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 09:55:05.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 09:55:07.095
    STEP: Verifying the service has paired with the endpoint 01/13/23 09:55:07.135
    Jan 13 09:55:08.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/13/23 09:55:08.145
    STEP: create a pod 01/13/23 09:55:08.244
    Jan 13 09:55:08.262: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-614" to be "running"
    Jan 13 09:55:08.277: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.433349ms
    Jan 13 09:55:10.286: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023997542s
    Jan 13 09:55:12.283: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021026673s
    Jan 13 09:55:14.297: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.034794671s
    Jan 13 09:55:14.297: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/13/23 09:55:14.297
    Jan 13 09:55:14.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=webhook-614 attach --namespace=webhook-614 to-be-attached-pod -i -c=container1'
    Jan 13 09:55:14.611: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:55:14.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-614" for this suite. 01/13/23 09:55:14.756
    STEP: Destroying namespace "webhook-614-markers" for this suite. 01/13/23 09:55:14.775
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:55:14.792
Jan 13 09:55:14.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 09:55:14.802
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:14.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:14.852
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/13/23 09:55:14.865
STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:55:14.886
STEP: Creating a ResourceQuota with not best effort scope 01/13/23 09:55:16.894
STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:55:16.92
STEP: Creating a best-effort pod 01/13/23 09:55:18.927
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/13/23 09:55:18.954
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/13/23 09:55:20.97
STEP: Deleting the pod 01/13/23 09:55:22.977
STEP: Ensuring resource quota status released the pod usage 01/13/23 09:55:23
STEP: Creating a not best-effort pod 01/13/23 09:55:25.021
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/13/23 09:55:25.064
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/13/23 09:55:27.081
STEP: Deleting the pod 01/13/23 09:55:29.089
STEP: Ensuring resource quota status released the pod usage 01/13/23 09:55:29.114
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 09:55:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8729" for this suite. 01/13/23 09:55:31.14
------------------------------
• [SLOW TEST] [16.361 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:55:14.792
    Jan 13 09:55:14.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 09:55:14.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:14.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:14.852
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/13/23 09:55:14.865
    STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:55:14.886
    STEP: Creating a ResourceQuota with not best effort scope 01/13/23 09:55:16.894
    STEP: Ensuring ResourceQuota status is calculated 01/13/23 09:55:16.92
    STEP: Creating a best-effort pod 01/13/23 09:55:18.927
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/13/23 09:55:18.954
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/13/23 09:55:20.97
    STEP: Deleting the pod 01/13/23 09:55:22.977
    STEP: Ensuring resource quota status released the pod usage 01/13/23 09:55:23
    STEP: Creating a not best-effort pod 01/13/23 09:55:25.021
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/13/23 09:55:25.064
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/13/23 09:55:27.081
    STEP: Deleting the pod 01/13/23 09:55:29.089
    STEP: Ensuring resource quota status released the pod usage 01/13/23 09:55:29.114
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:55:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8729" for this suite. 01/13/23 09:55:31.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:55:31.154
Jan 13 09:55:31.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename security-context 01/13/23 09:55:31.157
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:31.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:31.19
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/13/23 09:55:31.199
Jan 13 09:55:31.214: INFO: Waiting up to 5m0s for pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91" in namespace "security-context-7434" to be "Succeeded or Failed"
Jan 13 09:55:31.226: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101754ms
Jan 13 09:55:33.235: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021311556s
Jan 13 09:55:35.240: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025850163s
Jan 13 09:55:37.237: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023009666s
STEP: Saw pod success 01/13/23 09:55:37.237
Jan 13 09:55:37.237: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91" satisfied condition "Succeeded or Failed"
Jan 13 09:55:37.251: INFO: Trying to get logs from node 10.10.102.31-node pod security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91 container test-container: <nil>
STEP: delete the pod 01/13/23 09:55:37.3
Jan 13 09:55:37.324: INFO: Waiting for pod security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91 to disappear
Jan 13 09:55:37.343: INFO: Pod security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 13 09:55:37.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7434" for this suite. 01/13/23 09:55:37.36
------------------------------
• [SLOW TEST] [6.219 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:55:31.154
    Jan 13 09:55:31.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename security-context 01/13/23 09:55:31.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:31.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:31.19
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/13/23 09:55:31.199
    Jan 13 09:55:31.214: INFO: Waiting up to 5m0s for pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91" in namespace "security-context-7434" to be "Succeeded or Failed"
    Jan 13 09:55:31.226: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101754ms
    Jan 13 09:55:33.235: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021311556s
    Jan 13 09:55:35.240: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025850163s
    Jan 13 09:55:37.237: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023009666s
    STEP: Saw pod success 01/13/23 09:55:37.237
    Jan 13 09:55:37.237: INFO: Pod "security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91" satisfied condition "Succeeded or Failed"
    Jan 13 09:55:37.251: INFO: Trying to get logs from node 10.10.102.31-node pod security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91 container test-container: <nil>
    STEP: delete the pod 01/13/23 09:55:37.3
    Jan 13 09:55:37.324: INFO: Waiting for pod security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91 to disappear
    Jan 13 09:55:37.343: INFO: Pod security-context-d30fd01a-eccd-4cd6-bd74-3bd4fe727c91 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:55:37.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7434" for this suite. 01/13/23 09:55:37.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:55:37.383
Jan 13 09:55:37.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 09:55:37.386
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:37.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:37.478
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 09:55:37.562
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:55:38.859
STEP: Deploying the webhook pod 01/13/23 09:55:38.924
STEP: Wait for the deployment to be ready 01/13/23 09:55:38.969
Jan 13 09:55:38.983: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 09:55:41.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:55:43.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 09:55:45.034
STEP: Verifying the service has paired with the endpoint 01/13/23 09:55:45.148
Jan 13 09:55:46.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/13/23 09:55:46.154
STEP: Creating a custom resource definition that should be denied by the webhook 01/13/23 09:55:46.198
Jan 13 09:55:46.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:55:46.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4049" for this suite. 01/13/23 09:55:46.548
STEP: Destroying namespace "webhook-4049-markers" for this suite. 01/13/23 09:55:46.564
------------------------------
• [SLOW TEST] [9.223 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:55:37.383
    Jan 13 09:55:37.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 09:55:37.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:37.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:37.478
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 09:55:37.562
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:55:38.859
    STEP: Deploying the webhook pod 01/13/23 09:55:38.924
    STEP: Wait for the deployment to be ready 01/13/23 09:55:38.969
    Jan 13 09:55:38.983: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 09:55:41.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:55:43.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 55, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 55, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 09:55:45.034
    STEP: Verifying the service has paired with the endpoint 01/13/23 09:55:45.148
    Jan 13 09:55:46.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/13/23 09:55:46.154
    STEP: Creating a custom resource definition that should be denied by the webhook 01/13/23 09:55:46.198
    Jan 13 09:55:46.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:55:46.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4049" for this suite. 01/13/23 09:55:46.548
    STEP: Destroying namespace "webhook-4049-markers" for this suite. 01/13/23 09:55:46.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:55:46.608
Jan 13 09:55:46.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:55:46.611
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:46.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:46.73
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 13 09:55:46.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:55:53.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9490" for this suite. 01/13/23 09:55:53.793
------------------------------
• [SLOW TEST] [7.197 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:55:46.608
    Jan 13 09:55:46.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename custom-resource-definition 01/13/23 09:55:46.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:46.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:46.73
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 13 09:55:46.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:55:53.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9490" for this suite. 01/13/23 09:55:53.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:55:53.808
Jan 13 09:55:53.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 09:55:53.813
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:53.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:53.844
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/13/23 09:55:53.852
Jan 13 09:55:53.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8" in namespace "projected-6229" to be "Succeeded or Failed"
Jan 13 09:55:53.873: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.125738ms
Jan 13 09:55:55.880: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01311922s
Jan 13 09:55:57.885: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01872755s
Jan 13 09:55:59.974: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.107777943s
Jan 13 09:56:01.922: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055398102s
STEP: Saw pod success 01/13/23 09:56:01.922
Jan 13 09:56:01.922: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8" satisfied condition "Succeeded or Failed"
Jan 13 09:56:01.978: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8 container client-container: <nil>
STEP: delete the pod 01/13/23 09:56:02.186
Jan 13 09:56:02.667: INFO: Waiting for pod downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8 to disappear
Jan 13 09:56:02.712: INFO: Pod downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 09:56:02.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6229" for this suite. 01/13/23 09:56:02.747
------------------------------
• [SLOW TEST] [9.184 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:55:53.808
    Jan 13 09:55:53.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 09:55:53.813
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:55:53.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:55:53.844
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/13/23 09:55:53.852
    Jan 13 09:55:53.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8" in namespace "projected-6229" to be "Succeeded or Failed"
    Jan 13 09:55:53.873: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.125738ms
    Jan 13 09:55:55.880: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01311922s
    Jan 13 09:55:57.885: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01872755s
    Jan 13 09:55:59.974: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.107777943s
    Jan 13 09:56:01.922: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055398102s
    STEP: Saw pod success 01/13/23 09:56:01.922
    Jan 13 09:56:01.922: INFO: Pod "downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8" satisfied condition "Succeeded or Failed"
    Jan 13 09:56:01.978: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8 container client-container: <nil>
    STEP: delete the pod 01/13/23 09:56:02.186
    Jan 13 09:56:02.667: INFO: Waiting for pod downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8 to disappear
    Jan 13 09:56:02.712: INFO: Pod downwardapi-volume-1e790f08-2795-4fb9-935a-0d35eae2a0b8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:56:02.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6229" for this suite. 01/13/23 09:56:02.747
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:56:02.992
Jan 13 09:56:02.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename ingressclass 01/13/23 09:56:02.996
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:03.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:03.472
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/13/23 09:56:03.487
STEP: getting /apis/networking.k8s.io 01/13/23 09:56:03.502
STEP: getting /apis/networking.k8s.iov1 01/13/23 09:56:03.515
STEP: creating 01/13/23 09:56:03.531
STEP: getting 01/13/23 09:56:03.615
STEP: listing 01/13/23 09:56:03.631
STEP: watching 01/13/23 09:56:03.643
Jan 13 09:56:03.644: INFO: starting watch
STEP: patching 01/13/23 09:56:03.649
STEP: updating 01/13/23 09:56:03.688
Jan 13 09:56:03.718: INFO: waiting for watch events with expected annotations
Jan 13 09:56:03.718: INFO: saw patched and updated annotations
STEP: deleting 01/13/23 09:56:03.718
STEP: deleting a collection 01/13/23 09:56:03.843
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 13 09:56:03.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3478" for this suite. 01/13/23 09:56:03.96
------------------------------
• [1.067 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:56:02.992
    Jan 13 09:56:02.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename ingressclass 01/13/23 09:56:02.996
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:03.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:03.472
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/13/23 09:56:03.487
    STEP: getting /apis/networking.k8s.io 01/13/23 09:56:03.502
    STEP: getting /apis/networking.k8s.iov1 01/13/23 09:56:03.515
    STEP: creating 01/13/23 09:56:03.531
    STEP: getting 01/13/23 09:56:03.615
    STEP: listing 01/13/23 09:56:03.631
    STEP: watching 01/13/23 09:56:03.643
    Jan 13 09:56:03.644: INFO: starting watch
    STEP: patching 01/13/23 09:56:03.649
    STEP: updating 01/13/23 09:56:03.688
    Jan 13 09:56:03.718: INFO: waiting for watch events with expected annotations
    Jan 13 09:56:03.718: INFO: saw patched and updated annotations
    STEP: deleting 01/13/23 09:56:03.718
    STEP: deleting a collection 01/13/23 09:56:03.843
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:56:03.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3478" for this suite. 01/13/23 09:56:03.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:56:04.063
Jan 13 09:56:04.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename events 01/13/23 09:56:04.066
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:04.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:04.185
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/13/23 09:56:04.198
STEP: listing events in all namespaces 01/13/23 09:56:04.217
STEP: listing events in test namespace 01/13/23 09:56:04.254
STEP: listing events with field selection filtering on source 01/13/23 09:56:04.263
STEP: listing events with field selection filtering on reportingController 01/13/23 09:56:04.285
STEP: getting the test event 01/13/23 09:56:04.321
STEP: patching the test event 01/13/23 09:56:04.376
STEP: getting the test event 01/13/23 09:56:04.423
STEP: updating the test event 01/13/23 09:56:04.443
STEP: getting the test event 01/13/23 09:56:04.492
STEP: deleting the test event 01/13/23 09:56:04.518
STEP: listing events in all namespaces 01/13/23 09:56:04.591
STEP: listing events in test namespace 01/13/23 09:56:04.611
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 13 09:56:04.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7643" for this suite. 01/13/23 09:56:04.659
------------------------------
• [0.635 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:56:04.063
    Jan 13 09:56:04.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename events 01/13/23 09:56:04.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:04.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:04.185
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/13/23 09:56:04.198
    STEP: listing events in all namespaces 01/13/23 09:56:04.217
    STEP: listing events in test namespace 01/13/23 09:56:04.254
    STEP: listing events with field selection filtering on source 01/13/23 09:56:04.263
    STEP: listing events with field selection filtering on reportingController 01/13/23 09:56:04.285
    STEP: getting the test event 01/13/23 09:56:04.321
    STEP: patching the test event 01/13/23 09:56:04.376
    STEP: getting the test event 01/13/23 09:56:04.423
    STEP: updating the test event 01/13/23 09:56:04.443
    STEP: getting the test event 01/13/23 09:56:04.492
    STEP: deleting the test event 01/13/23 09:56:04.518
    STEP: listing events in all namespaces 01/13/23 09:56:04.591
    STEP: listing events in test namespace 01/13/23 09:56:04.611
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:56:04.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7643" for this suite. 01/13/23 09:56:04.659
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:56:04.7
Jan 13 09:56:04.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename endpointslice 01/13/23 09:56:04.704
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:04.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:04.781
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/13/23 09:56:10.274
STEP: referencing matching pods with named port 01/13/23 09:56:15.332
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/13/23 09:56:20.517
STEP: recreating EndpointSlices after they've been deleted 01/13/23 09:56:25.561
Jan 13 09:56:25.627: INFO: EndpointSlice for Service endpointslice-819/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 13 09:56:35.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-819" for this suite. 01/13/23 09:56:35.664
------------------------------
• [SLOW TEST] [30.976 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:56:04.7
    Jan 13 09:56:04.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename endpointslice 01/13/23 09:56:04.704
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:04.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:04.781
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/13/23 09:56:10.274
    STEP: referencing matching pods with named port 01/13/23 09:56:15.332
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/13/23 09:56:20.517
    STEP: recreating EndpointSlices after they've been deleted 01/13/23 09:56:25.561
    Jan 13 09:56:25.627: INFO: EndpointSlice for Service endpointslice-819/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:56:35.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-819" for this suite. 01/13/23 09:56:35.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:56:35.678
Jan 13 09:56:35.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 09:56:35.681
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:35.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:35.722
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-346639fc-9668-44a4-8603-bb9786233009 01/13/23 09:56:35.757
STEP: Creating a pod to test consume secrets 01/13/23 09:56:35.765
Jan 13 09:56:35.777: INFO: Waiting up to 5m0s for pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f" in namespace "secrets-6751" to be "Succeeded or Failed"
Jan 13 09:56:35.791: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.489585ms
Jan 13 09:56:37.800: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02298833s
Jan 13 09:56:39.809: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032415709s
Jan 13 09:56:41.799: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022654047s
STEP: Saw pod success 01/13/23 09:56:41.799
Jan 13 09:56:41.800: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f" satisfied condition "Succeeded or Failed"
Jan 13 09:56:41.805: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 09:56:41.83
Jan 13 09:56:41.852: INFO: Waiting for pod pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f to disappear
Jan 13 09:56:41.858: INFO: Pod pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 09:56:41.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6751" for this suite. 01/13/23 09:56:41.866
------------------------------
• [SLOW TEST] [6.213 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:56:35.678
    Jan 13 09:56:35.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 09:56:35.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:35.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:35.722
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-346639fc-9668-44a4-8603-bb9786233009 01/13/23 09:56:35.757
    STEP: Creating a pod to test consume secrets 01/13/23 09:56:35.765
    Jan 13 09:56:35.777: INFO: Waiting up to 5m0s for pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f" in namespace "secrets-6751" to be "Succeeded or Failed"
    Jan 13 09:56:35.791: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.489585ms
    Jan 13 09:56:37.800: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02298833s
    Jan 13 09:56:39.809: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032415709s
    Jan 13 09:56:41.799: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022654047s
    STEP: Saw pod success 01/13/23 09:56:41.799
    Jan 13 09:56:41.800: INFO: Pod "pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f" satisfied condition "Succeeded or Failed"
    Jan 13 09:56:41.805: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 09:56:41.83
    Jan 13 09:56:41.852: INFO: Waiting for pod pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f to disappear
    Jan 13 09:56:41.858: INFO: Pod pod-secrets-28158975-0471-4d69-9c16-71c8ae0f6a2f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:56:41.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6751" for this suite. 01/13/23 09:56:41.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:56:41.895
Jan 13 09:56:41.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 09:56:41.898
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:41.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:41.945
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/13/23 09:56:41.957
STEP: Creating a ResourceQuota 01/13/23 09:56:46.978
STEP: Ensuring resource quota status is calculated 01/13/23 09:56:47.001
STEP: Creating a ReplicationController 01/13/23 09:56:49.009
STEP: Ensuring resource quota status captures replication controller creation 01/13/23 09:56:49.034
STEP: Deleting a ReplicationController 01/13/23 09:56:51.041
STEP: Ensuring resource quota status released usage 01/13/23 09:56:51.059
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 09:56:53.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4799" for this suite. 01/13/23 09:56:53.082
------------------------------
• [SLOW TEST] [11.199 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:56:41.895
    Jan 13 09:56:41.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 09:56:41.898
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:41.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:41.945
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/13/23 09:56:41.957
    STEP: Creating a ResourceQuota 01/13/23 09:56:46.978
    STEP: Ensuring resource quota status is calculated 01/13/23 09:56:47.001
    STEP: Creating a ReplicationController 01/13/23 09:56:49.009
    STEP: Ensuring resource quota status captures replication controller creation 01/13/23 09:56:49.034
    STEP: Deleting a ReplicationController 01/13/23 09:56:51.041
    STEP: Ensuring resource quota status released usage 01/13/23 09:56:51.059
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:56:53.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4799" for this suite. 01/13/23 09:56:53.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:56:53.098
Jan 13 09:56:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 09:56:53.101
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:53.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:53.149
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7245 01/13/23 09:56:53.158
STEP: creating service affinity-clusterip in namespace services-7245 01/13/23 09:56:53.159
STEP: creating replication controller affinity-clusterip in namespace services-7245 01/13/23 09:56:53.19
I0113 09:56:53.210336      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7245, replica count: 3
I0113 09:56:56.262166      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 09:56:59.262663      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 09:56:59.272: INFO: Creating new exec pod
Jan 13 09:56:59.281: INFO: Waiting up to 5m0s for pod "execpod-affinityf8mkx" in namespace "services-7245" to be "running"
Jan 13 09:56:59.293: INFO: Pod "execpod-affinityf8mkx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.193211ms
Jan 13 09:57:01.306: INFO: Pod "execpod-affinityf8mkx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025217451s
Jan 13 09:57:03.303: INFO: Pod "execpod-affinityf8mkx": Phase="Running", Reason="", readiness=true. Elapsed: 4.021632565s
Jan 13 09:57:03.303: INFO: Pod "execpod-affinityf8mkx" satisfied condition "running"
Jan 13 09:57:04.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7245 exec execpod-affinityf8mkx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 13 09:57:05.105: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 13 09:57:05.105: INFO: stdout: ""
Jan 13 09:57:05.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7245 exec execpod-affinityf8mkx -- /bin/sh -x -c nc -v -z -w 2 10.99.127.16 80'
Jan 13 09:57:05.725: INFO: stderr: "+ nc -v -z -w 2 10.99.127.16 80\nConnection to 10.99.127.16 80 port [tcp/http] succeeded!\n"
Jan 13 09:57:05.725: INFO: stdout: ""
Jan 13 09:57:05.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7245 exec execpod-affinityf8mkx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.127.16:80/ ; done'
Jan 13 09:57:06.348: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n"
Jan 13 09:57:06.348: INFO: stdout: "\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29"
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
Jan 13 09:57:06.348: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7245, will wait for the garbage collector to delete the pods 01/13/23 09:57:06.369
Jan 13 09:57:06.516: INFO: Deleting ReplicationController affinity-clusterip took: 77.568006ms
Jan 13 09:57:06.717: INFO: Terminating ReplicationController affinity-clusterip pods took: 201.16736ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 09:57:11.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7245" for this suite. 01/13/23 09:57:11.112
------------------------------
• [SLOW TEST] [18.033 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:56:53.098
    Jan 13 09:56:53.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 09:56:53.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:56:53.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:56:53.149
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7245 01/13/23 09:56:53.158
    STEP: creating service affinity-clusterip in namespace services-7245 01/13/23 09:56:53.159
    STEP: creating replication controller affinity-clusterip in namespace services-7245 01/13/23 09:56:53.19
    I0113 09:56:53.210336      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7245, replica count: 3
    I0113 09:56:56.262166      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 09:56:59.262663      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 09:56:59.272: INFO: Creating new exec pod
    Jan 13 09:56:59.281: INFO: Waiting up to 5m0s for pod "execpod-affinityf8mkx" in namespace "services-7245" to be "running"
    Jan 13 09:56:59.293: INFO: Pod "execpod-affinityf8mkx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.193211ms
    Jan 13 09:57:01.306: INFO: Pod "execpod-affinityf8mkx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025217451s
    Jan 13 09:57:03.303: INFO: Pod "execpod-affinityf8mkx": Phase="Running", Reason="", readiness=true. Elapsed: 4.021632565s
    Jan 13 09:57:03.303: INFO: Pod "execpod-affinityf8mkx" satisfied condition "running"
    Jan 13 09:57:04.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7245 exec execpod-affinityf8mkx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 13 09:57:05.105: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 13 09:57:05.105: INFO: stdout: ""
    Jan 13 09:57:05.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7245 exec execpod-affinityf8mkx -- /bin/sh -x -c nc -v -z -w 2 10.99.127.16 80'
    Jan 13 09:57:05.725: INFO: stderr: "+ nc -v -z -w 2 10.99.127.16 80\nConnection to 10.99.127.16 80 port [tcp/http] succeeded!\n"
    Jan 13 09:57:05.725: INFO: stdout: ""
    Jan 13 09:57:05.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7245 exec execpod-affinityf8mkx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.127.16:80/ ; done'
    Jan 13 09:57:06.348: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.127.16:80/\n"
    Jan 13 09:57:06.348: INFO: stdout: "\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29\naffinity-clusterip-dwj29"
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Received response from host: affinity-clusterip-dwj29
    Jan 13 09:57:06.348: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7245, will wait for the garbage collector to delete the pods 01/13/23 09:57:06.369
    Jan 13 09:57:06.516: INFO: Deleting ReplicationController affinity-clusterip took: 77.568006ms
    Jan 13 09:57:06.717: INFO: Terminating ReplicationController affinity-clusterip pods took: 201.16736ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:57:11.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7245" for this suite. 01/13/23 09:57:11.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:57:11.131
Jan 13 09:57:11.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename gc 01/13/23 09:57:11.133
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:57:11.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:57:11.237
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/13/23 09:57:11.267
STEP: delete the rc 01/13/23 09:57:16.3
STEP: wait for the rc to be deleted 01/13/23 09:57:16.335
Jan 13 09:57:17.529: INFO: 80 pods remaining
Jan 13 09:57:17.529: INFO: 80 pods has nil DeletionTimestamp
Jan 13 09:57:17.529: INFO: 
Jan 13 09:57:18.460: INFO: 72 pods remaining
Jan 13 09:57:18.460: INFO: 72 pods has nil DeletionTimestamp
Jan 13 09:57:18.460: INFO: 
Jan 13 09:57:19.462: INFO: 60 pods remaining
Jan 13 09:57:19.462: INFO: 60 pods has nil DeletionTimestamp
Jan 13 09:57:19.462: INFO: 
Jan 13 09:57:20.592: INFO: 40 pods remaining
Jan 13 09:57:20.592: INFO: 40 pods has nil DeletionTimestamp
Jan 13 09:57:20.592: INFO: 
Jan 13 09:57:21.716: INFO: 34 pods remaining
Jan 13 09:57:21.716: INFO: 34 pods has nil DeletionTimestamp
Jan 13 09:57:21.716: INFO: 
Jan 13 09:57:22.380: INFO: 16 pods remaining
Jan 13 09:57:22.380: INFO: 16 pods has nil DeletionTimestamp
Jan 13 09:57:22.380: INFO: 
Jan 13 09:57:23.354: INFO: 0 pods remaining
Jan 13 09:57:23.354: INFO: 0 pods has nil DeletionTimestamp
Jan 13 09:57:23.354: INFO: 
STEP: Gathering metrics 01/13/23 09:57:24.396
Jan 13 09:57:24.467: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
Jan 13 09:57:24.479: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 12.036675ms
Jan 13 09:57:24.479: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
Jan 13 09:57:24.479: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
Jan 13 09:57:24.904: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 13 09:57:24.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5519" for this suite. 01/13/23 09:57:24.948
------------------------------
• [SLOW TEST] [13.834 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:57:11.131
    Jan 13 09:57:11.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename gc 01/13/23 09:57:11.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:57:11.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:57:11.237
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/13/23 09:57:11.267
    STEP: delete the rc 01/13/23 09:57:16.3
    STEP: wait for the rc to be deleted 01/13/23 09:57:16.335
    Jan 13 09:57:17.529: INFO: 80 pods remaining
    Jan 13 09:57:17.529: INFO: 80 pods has nil DeletionTimestamp
    Jan 13 09:57:17.529: INFO: 
    Jan 13 09:57:18.460: INFO: 72 pods remaining
    Jan 13 09:57:18.460: INFO: 72 pods has nil DeletionTimestamp
    Jan 13 09:57:18.460: INFO: 
    Jan 13 09:57:19.462: INFO: 60 pods remaining
    Jan 13 09:57:19.462: INFO: 60 pods has nil DeletionTimestamp
    Jan 13 09:57:19.462: INFO: 
    Jan 13 09:57:20.592: INFO: 40 pods remaining
    Jan 13 09:57:20.592: INFO: 40 pods has nil DeletionTimestamp
    Jan 13 09:57:20.592: INFO: 
    Jan 13 09:57:21.716: INFO: 34 pods remaining
    Jan 13 09:57:21.716: INFO: 34 pods has nil DeletionTimestamp
    Jan 13 09:57:21.716: INFO: 
    Jan 13 09:57:22.380: INFO: 16 pods remaining
    Jan 13 09:57:22.380: INFO: 16 pods has nil DeletionTimestamp
    Jan 13 09:57:22.380: INFO: 
    Jan 13 09:57:23.354: INFO: 0 pods remaining
    Jan 13 09:57:23.354: INFO: 0 pods has nil DeletionTimestamp
    Jan 13 09:57:23.354: INFO: 
    STEP: Gathering metrics 01/13/23 09:57:24.396
    Jan 13 09:57:24.467: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
    Jan 13 09:57:24.479: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 12.036675ms
    Jan 13 09:57:24.479: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
    Jan 13 09:57:24.479: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
    Jan 13 09:57:24.904: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:57:24.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5519" for this suite. 01/13/23 09:57:24.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:57:24.975
Jan 13 09:57:24.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 09:57:25.003
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:57:25.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:57:25.072
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 09:57:25.23
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:57:27.008
STEP: Deploying the webhook pod 01/13/23 09:57:27.03
STEP: Wait for the deployment to be ready 01/13/23 09:57:27.059
Jan 13 09:57:27.098: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 09:57:29.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:31.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:33.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:35.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:37.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:39.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:41.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:43.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:57:45.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 09:57:47.196
STEP: Verifying the service has paired with the endpoint 01/13/23 09:57:47.236
Jan 13 09:57:48.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/13/23 09:57:48.252
STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:48.253
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/13/23 09:57:48.33
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/13/23 09:57:49.365
STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:49.365
STEP: Having no error when timeout is longer than webhook latency 01/13/23 09:57:50.559
STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:50.559
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/13/23 09:57:55.952
STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:55.953
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 09:58:01.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4581" for this suite. 01/13/23 09:58:01.413
STEP: Destroying namespace "webhook-4581-markers" for this suite. 01/13/23 09:58:01.435
------------------------------
• [SLOW TEST] [36.507 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:57:24.975
    Jan 13 09:57:24.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 09:57:25.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:57:25.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:57:25.072
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 09:57:25.23
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 09:57:27.008
    STEP: Deploying the webhook pod 01/13/23 09:57:27.03
    STEP: Wait for the deployment to be ready 01/13/23 09:57:27.059
    Jan 13 09:57:27.098: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 09:57:29.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:31.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:33.207: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:35.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:37.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:39.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:41.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:43.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 09:57:45.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 9, 57, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 09:57:47.196
    STEP: Verifying the service has paired with the endpoint 01/13/23 09:57:47.236
    Jan 13 09:57:48.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/13/23 09:57:48.252
    STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:48.253
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/13/23 09:57:48.33
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/13/23 09:57:49.365
    STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:49.365
    STEP: Having no error when timeout is longer than webhook latency 01/13/23 09:57:50.559
    STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:50.559
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/13/23 09:57:55.952
    STEP: Registering slow webhook via the AdmissionRegistration API 01/13/23 09:57:55.953
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:58:01.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4581" for this suite. 01/13/23 09:58:01.413
    STEP: Destroying namespace "webhook-4581-markers" for this suite. 01/13/23 09:58:01.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:58:01.481
Jan 13 09:58:01.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename security-context 01/13/23 09:58:01.485
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:58:01.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:58:01.581
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/13/23 09:58:01.609
Jan 13 09:58:01.645: INFO: Waiting up to 5m0s for pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c" in namespace "security-context-3253" to be "Succeeded or Failed"
Jan 13 09:58:01.669: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.747445ms
Jan 13 09:58:03.679: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03340992s
Jan 13 09:58:05.715: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069917711s
Jan 13 09:58:07.678: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032405177s
STEP: Saw pod success 01/13/23 09:58:07.678
Jan 13 09:58:07.678: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c" satisfied condition "Succeeded or Failed"
Jan 13 09:58:07.687: INFO: Trying to get logs from node 10.10.102.31-node pod security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c container test-container: <nil>
STEP: delete the pod 01/13/23 09:58:07.72
Jan 13 09:58:07.770: INFO: Waiting for pod security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c to disappear
Jan 13 09:58:07.775: INFO: Pod security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 13 09:58:07.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-3253" for this suite. 01/13/23 09:58:07.786
------------------------------
• [SLOW TEST] [6.331 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:58:01.481
    Jan 13 09:58:01.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename security-context 01/13/23 09:58:01.485
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:58:01.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:58:01.581
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/13/23 09:58:01.609
    Jan 13 09:58:01.645: INFO: Waiting up to 5m0s for pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c" in namespace "security-context-3253" to be "Succeeded or Failed"
    Jan 13 09:58:01.669: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.747445ms
    Jan 13 09:58:03.679: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03340992s
    Jan 13 09:58:05.715: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069917711s
    Jan 13 09:58:07.678: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032405177s
    STEP: Saw pod success 01/13/23 09:58:07.678
    Jan 13 09:58:07.678: INFO: Pod "security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c" satisfied condition "Succeeded or Failed"
    Jan 13 09:58:07.687: INFO: Trying to get logs from node 10.10.102.31-node pod security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c container test-container: <nil>
    STEP: delete the pod 01/13/23 09:58:07.72
    Jan 13 09:58:07.770: INFO: Waiting for pod security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c to disappear
    Jan 13 09:58:07.775: INFO: Pod security-context-9b8e727b-4657-43a9-b1bd-f59e10ec2a8c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:58:07.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-3253" for this suite. 01/13/23 09:58:07.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:58:07.82
Jan 13 09:58:07.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 09:58:07.83
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:58:07.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:58:07.962
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-fb98f16b-1391-48e9-ac4f-c3bb98c88837 01/13/23 09:58:07.99
STEP: Creating the pod 01/13/23 09:58:08.003
Jan 13 09:58:08.019: INFO: Waiting up to 5m0s for pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a" in namespace "configmap-1054" to be "running and ready"
Jan 13 09:58:08.026: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.338549ms
Jan 13 09:58:08.026: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:58:10.054: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034179694s
Jan 13 09:58:10.054: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:58:12.034: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013818058s
Jan 13 09:58:12.034: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:58:14.038: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Running", Reason="", readiness=true. Elapsed: 6.01772201s
Jan 13 09:58:14.038: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Running (Ready = true)
Jan 13 09:58:14.038: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-fb98f16b-1391-48e9-ac4f-c3bb98c88837 01/13/23 09:58:14.067
STEP: waiting to observe update in volume 01/13/23 09:58:14.084
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 09:59:43.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1054" for this suite. 01/13/23 09:59:43.509
------------------------------
• [SLOW TEST] [95.701 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:58:07.82
    Jan 13 09:58:07.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 09:58:07.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:58:07.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:58:07.962
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-fb98f16b-1391-48e9-ac4f-c3bb98c88837 01/13/23 09:58:07.99
    STEP: Creating the pod 01/13/23 09:58:08.003
    Jan 13 09:58:08.019: INFO: Waiting up to 5m0s for pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a" in namespace "configmap-1054" to be "running and ready"
    Jan 13 09:58:08.026: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.338549ms
    Jan 13 09:58:08.026: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:58:10.054: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034179694s
    Jan 13 09:58:10.054: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:58:12.034: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013818058s
    Jan 13 09:58:12.034: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 09:58:14.038: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a": Phase="Running", Reason="", readiness=true. Elapsed: 6.01772201s
    Jan 13 09:58:14.038: INFO: The phase of Pod pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a is Running (Ready = true)
    Jan 13 09:58:14.038: INFO: Pod "pod-configmaps-57ea184c-5ead-4696-8133-056f63ad970a" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-fb98f16b-1391-48e9-ac4f-c3bb98c88837 01/13/23 09:58:14.067
    STEP: waiting to observe update in volume 01/13/23 09:58:14.084
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 09:59:43.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1054" for this suite. 01/13/23 09:59:43.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 09:59:43.523
Jan 13 09:59:43.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename subpath 01/13/23 09:59:43.526
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:59:43.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:59:43.594
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/13/23 09:59:43.604
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-9qrm 01/13/23 09:59:43.663
STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:59:43.664
Jan 13 09:59:43.779: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9qrm" in namespace "subpath-6822" to be "Succeeded or Failed"
Jan 13 09:59:43.789: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.476352ms
Jan 13 09:59:45.798: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019357501s
Jan 13 09:59:47.797: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 4.018773169s
Jan 13 09:59:49.802: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 6.023228185s
Jan 13 09:59:51.801: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 8.021932381s
Jan 13 09:59:53.815: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 10.036009419s
Jan 13 09:59:55.799: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 12.020077044s
Jan 13 09:59:57.800: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 14.02190079s
Jan 13 09:59:59.808: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 16.029574865s
Jan 13 10:00:01.795: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 18.016250543s
Jan 13 10:00:03.808: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 20.02982937s
Jan 13 10:00:05.805: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 22.026056376s
Jan 13 10:00:07.796: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 24.017714439s
Jan 13 10:00:09.821: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=false. Elapsed: 26.042136044s
Jan 13 10:00:11.799: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.020356081s
STEP: Saw pod success 01/13/23 10:00:11.799
Jan 13 10:00:11.799: INFO: Pod "pod-subpath-test-secret-9qrm" satisfied condition "Succeeded or Failed"
Jan 13 10:00:11.805: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-secret-9qrm container test-container-subpath-secret-9qrm: <nil>
STEP: delete the pod 01/13/23 10:00:11.822
Jan 13 10:00:11.846: INFO: Waiting for pod pod-subpath-test-secret-9qrm to disappear
Jan 13 10:00:11.855: INFO: Pod pod-subpath-test-secret-9qrm no longer exists
STEP: Deleting pod pod-subpath-test-secret-9qrm 01/13/23 10:00:11.857
Jan 13 10:00:11.858: INFO: Deleting pod "pod-subpath-test-secret-9qrm" in namespace "subpath-6822"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 13 10:00:11.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6822" for this suite. 01/13/23 10:00:11.886
------------------------------
• [SLOW TEST] [28.400 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 09:59:43.523
    Jan 13 09:59:43.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename subpath 01/13/23 09:59:43.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 09:59:43.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 09:59:43.594
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/13/23 09:59:43.604
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-9qrm 01/13/23 09:59:43.663
    STEP: Creating a pod to test atomic-volume-subpath 01/13/23 09:59:43.664
    Jan 13 09:59:43.779: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9qrm" in namespace "subpath-6822" to be "Succeeded or Failed"
    Jan 13 09:59:43.789: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.476352ms
    Jan 13 09:59:45.798: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019357501s
    Jan 13 09:59:47.797: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 4.018773169s
    Jan 13 09:59:49.802: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 6.023228185s
    Jan 13 09:59:51.801: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 8.021932381s
    Jan 13 09:59:53.815: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 10.036009419s
    Jan 13 09:59:55.799: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 12.020077044s
    Jan 13 09:59:57.800: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 14.02190079s
    Jan 13 09:59:59.808: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 16.029574865s
    Jan 13 10:00:01.795: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 18.016250543s
    Jan 13 10:00:03.808: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 20.02982937s
    Jan 13 10:00:05.805: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 22.026056376s
    Jan 13 10:00:07.796: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=true. Elapsed: 24.017714439s
    Jan 13 10:00:09.821: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Running", Reason="", readiness=false. Elapsed: 26.042136044s
    Jan 13 10:00:11.799: INFO: Pod "pod-subpath-test-secret-9qrm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.020356081s
    STEP: Saw pod success 01/13/23 10:00:11.799
    Jan 13 10:00:11.799: INFO: Pod "pod-subpath-test-secret-9qrm" satisfied condition "Succeeded or Failed"
    Jan 13 10:00:11.805: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-secret-9qrm container test-container-subpath-secret-9qrm: <nil>
    STEP: delete the pod 01/13/23 10:00:11.822
    Jan 13 10:00:11.846: INFO: Waiting for pod pod-subpath-test-secret-9qrm to disappear
    Jan 13 10:00:11.855: INFO: Pod pod-subpath-test-secret-9qrm no longer exists
    STEP: Deleting pod pod-subpath-test-secret-9qrm 01/13/23 10:00:11.857
    Jan 13 10:00:11.858: INFO: Deleting pod "pod-subpath-test-secret-9qrm" in namespace "subpath-6822"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:00:11.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6822" for this suite. 01/13/23 10:00:11.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:00:11.926
Jan 13 10:00:11.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 10:00:11.929
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:11.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:11.975
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/13/23 10:00:11.985
Jan 13 10:00:12.027: INFO: Waiting up to 5m0s for pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf" in namespace "var-expansion-9985" to be "Succeeded or Failed"
Jan 13 10:00:12.039: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.52796ms
Jan 13 10:00:14.053: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02658719s
Jan 13 10:00:16.050: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022798712s
Jan 13 10:00:18.047: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02010078s
STEP: Saw pod success 01/13/23 10:00:18.047
Jan 13 10:00:18.047: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf" satisfied condition "Succeeded or Failed"
Jan 13 10:00:18.052: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-257a5543-11d3-41e3-aba3-29537db780bf container dapi-container: <nil>
STEP: delete the pod 01/13/23 10:00:18.066
Jan 13 10:00:18.083: INFO: Waiting for pod var-expansion-257a5543-11d3-41e3-aba3-29537db780bf to disappear
Jan 13 10:00:18.087: INFO: Pod var-expansion-257a5543-11d3-41e3-aba3-29537db780bf no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 10:00:18.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9985" for this suite. 01/13/23 10:00:18.094
------------------------------
• [SLOW TEST] [6.178 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:00:11.926
    Jan 13 10:00:11.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 10:00:11.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:11.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:11.975
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/13/23 10:00:11.985
    Jan 13 10:00:12.027: INFO: Waiting up to 5m0s for pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf" in namespace "var-expansion-9985" to be "Succeeded or Failed"
    Jan 13 10:00:12.039: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.52796ms
    Jan 13 10:00:14.053: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02658719s
    Jan 13 10:00:16.050: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022798712s
    Jan 13 10:00:18.047: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02010078s
    STEP: Saw pod success 01/13/23 10:00:18.047
    Jan 13 10:00:18.047: INFO: Pod "var-expansion-257a5543-11d3-41e3-aba3-29537db780bf" satisfied condition "Succeeded or Failed"
    Jan 13 10:00:18.052: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-257a5543-11d3-41e3-aba3-29537db780bf container dapi-container: <nil>
    STEP: delete the pod 01/13/23 10:00:18.066
    Jan 13 10:00:18.083: INFO: Waiting for pod var-expansion-257a5543-11d3-41e3-aba3-29537db780bf to disappear
    Jan 13 10:00:18.087: INFO: Pod var-expansion-257a5543-11d3-41e3-aba3-29537db780bf no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:00:18.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9985" for this suite. 01/13/23 10:00:18.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:00:18.107
Jan 13 10:00:18.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:00:18.111
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:18.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:18.154
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 13 10:00:18.185: INFO: Waiting up to 5m0s for pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae" in namespace "svcaccounts-184" to be "running"
Jan 13 10:00:18.206: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 20.262966ms
Jan 13 10:00:20.220: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03459134s
Jan 13 10:00:22.214: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae": Phase="Running", Reason="", readiness=true. Elapsed: 4.028924225s
Jan 13 10:00:22.215: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae" satisfied condition "running"
STEP: reading a file in the container 01/13/23 10:00:22.215
Jan 13 10:00:22.215: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-184 pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/13/23 10:00:22.755
Jan 13 10:00:22.755: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-184 pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/13/23 10:00:23.118
Jan 13 10:00:23.118: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-184 pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 13 10:00:23.485: INFO: Got root ca configmap in namespace "svcaccounts-184"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 13 10:00:23.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-184" for this suite. 01/13/23 10:00:23.496
------------------------------
• [SLOW TEST] [5.402 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:00:18.107
    Jan 13 10:00:18.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:00:18.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:18.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:18.154
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 13 10:00:18.185: INFO: Waiting up to 5m0s for pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae" in namespace "svcaccounts-184" to be "running"
    Jan 13 10:00:18.206: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 20.262966ms
    Jan 13 10:00:20.220: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03459134s
    Jan 13 10:00:22.214: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae": Phase="Running", Reason="", readiness=true. Elapsed: 4.028924225s
    Jan 13 10:00:22.215: INFO: Pod "pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae" satisfied condition "running"
    STEP: reading a file in the container 01/13/23 10:00:22.215
    Jan 13 10:00:22.215: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-184 pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/13/23 10:00:22.755
    Jan 13 10:00:22.755: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-184 pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/13/23 10:00:23.118
    Jan 13 10:00:23.118: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-184 pod-service-account-949d178c-9a2b-486e-8e71-40fcfdb5f5ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 13 10:00:23.485: INFO: Got root ca configmap in namespace "svcaccounts-184"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:00:23.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-184" for this suite. 01/13/23 10:00:23.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:00:23.511
Jan 13 10:00:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-runtime 01/13/23 10:00:23.513
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:23.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:23.543
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/13/23 10:00:23.548
STEP: wait for the container to reach Succeeded 01/13/23 10:00:23.562
STEP: get the container status 01/13/23 10:00:30.652
STEP: the container should be terminated 01/13/23 10:00:30.689
STEP: the termination message should be set 01/13/23 10:00:30.689
Jan 13 10:00:30.690: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/13/23 10:00:30.69
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 13 10:00:30.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-513" for this suite. 01/13/23 10:00:30.737
------------------------------
• [SLOW TEST] [7.298 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:00:23.511
    Jan 13 10:00:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-runtime 01/13/23 10:00:23.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:23.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:23.543
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/13/23 10:00:23.548
    STEP: wait for the container to reach Succeeded 01/13/23 10:00:23.562
    STEP: get the container status 01/13/23 10:00:30.652
    STEP: the container should be terminated 01/13/23 10:00:30.689
    STEP: the termination message should be set 01/13/23 10:00:30.689
    Jan 13 10:00:30.690: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/13/23 10:00:30.69
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:00:30.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-513" for this suite. 01/13/23 10:00:30.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:00:30.812
Jan 13 10:00:30.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename proxy 01/13/23 10:00:30.814
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:30.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:30.93
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 13 10:00:30.950: INFO: Creating pod...
Jan 13 10:00:30.996: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2569" to be "running"
Jan 13 10:00:31.024: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 27.575073ms
Jan 13 10:00:33.038: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041601969s
Jan 13 10:00:35.039: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.042720294s
Jan 13 10:00:35.039: INFO: Pod "agnhost" satisfied condition "running"
Jan 13 10:00:35.039: INFO: Creating service...
Jan 13 10:00:35.060: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/DELETE
Jan 13 10:00:35.095: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 13 10:00:35.096: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/GET
Jan 13 10:00:35.106: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 13 10:00:35.106: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/HEAD
Jan 13 10:00:35.114: INFO: http.Client request:HEAD | StatusCode:200
Jan 13 10:00:35.114: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 13 10:00:35.152: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 13 10:00:35.152: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/PATCH
Jan 13 10:00:35.177: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 13 10:00:35.177: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/POST
Jan 13 10:00:35.197: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 13 10:00:35.197: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/PUT
Jan 13 10:00:35.214: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 13 10:00:35.214: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/DELETE
Jan 13 10:00:35.232: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 13 10:00:35.232: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/GET
Jan 13 10:00:35.281: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 13 10:00:35.281: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/HEAD
Jan 13 10:00:35.304: INFO: http.Client request:HEAD | StatusCode:200
Jan 13 10:00:35.304: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/OPTIONS
Jan 13 10:00:35.319: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 13 10:00:35.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/PATCH
Jan 13 10:00:35.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 13 10:00:35.352: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/POST
Jan 13 10:00:35.377: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 13 10:00:35.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/PUT
Jan 13 10:00:35.407: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 13 10:00:35.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2569" for this suite. 01/13/23 10:00:35.434
------------------------------
• [4.641 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:00:30.812
    Jan 13 10:00:30.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename proxy 01/13/23 10:00:30.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:30.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:30.93
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 13 10:00:30.950: INFO: Creating pod...
    Jan 13 10:00:30.996: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2569" to be "running"
    Jan 13 10:00:31.024: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 27.575073ms
    Jan 13 10:00:33.038: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041601969s
    Jan 13 10:00:35.039: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.042720294s
    Jan 13 10:00:35.039: INFO: Pod "agnhost" satisfied condition "running"
    Jan 13 10:00:35.039: INFO: Creating service...
    Jan 13 10:00:35.060: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/DELETE
    Jan 13 10:00:35.095: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 13 10:00:35.096: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/GET
    Jan 13 10:00:35.106: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 13 10:00:35.106: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/HEAD
    Jan 13 10:00:35.114: INFO: http.Client request:HEAD | StatusCode:200
    Jan 13 10:00:35.114: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 13 10:00:35.152: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 13 10:00:35.152: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/PATCH
    Jan 13 10:00:35.177: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 13 10:00:35.177: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/POST
    Jan 13 10:00:35.197: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 13 10:00:35.197: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/pods/agnhost/proxy/some/path/with/PUT
    Jan 13 10:00:35.214: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 13 10:00:35.214: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/DELETE
    Jan 13 10:00:35.232: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 13 10:00:35.232: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/GET
    Jan 13 10:00:35.281: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 13 10:00:35.281: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/HEAD
    Jan 13 10:00:35.304: INFO: http.Client request:HEAD | StatusCode:200
    Jan 13 10:00:35.304: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/OPTIONS
    Jan 13 10:00:35.319: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 13 10:00:35.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/PATCH
    Jan 13 10:00:35.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 13 10:00:35.352: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/POST
    Jan 13 10:00:35.377: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 13 10:00:35.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2569/services/test-service/proxy/some/path/with/PUT
    Jan 13 10:00:35.407: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:00:35.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2569" for this suite. 01/13/23 10:00:35.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:00:35.454
Jan 13 10:00:35.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir-wrapper 01/13/23 10:00:35.457
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:35.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:35.625
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/13/23 10:00:35.661
STEP: Creating RC which spawns configmap-volume pods 01/13/23 10:00:36.297
Jan 13 10:00:36.334: INFO: Pod name wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec: Found 0 pods out of 5
Jan 13 10:00:41.375: INFO: Pod name wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/13/23 10:00:41.375
Jan 13 10:00:41.375: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:00:41.382: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.540035ms
Jan 13 10:00:43.398: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022090408s
Jan 13 10:00:45.399: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023814356s
Jan 13 10:00:47.392: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016143607s
Jan 13 10:00:49.397: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021396511s
Jan 13 10:00:51.423: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047862492s
Jan 13 10:00:53.390: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Running", Reason="", readiness=true. Elapsed: 12.014114348s
Jan 13 10:00:53.390: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr" satisfied condition "running"
Jan 13 10:00:53.390: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-5hg2l" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:00:53.403: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-5hg2l": Phase="Running", Reason="", readiness=true. Elapsed: 12.754013ms
Jan 13 10:00:53.403: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-5hg2l" satisfied condition "running"
Jan 13 10:00:53.403: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-gmctd" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:00:53.410: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-gmctd": Phase="Running", Reason="", readiness=true. Elapsed: 7.559484ms
Jan 13 10:00:53.410: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-gmctd" satisfied condition "running"
Jan 13 10:00:53.410: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-q7t5s" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:00:53.417: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-q7t5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.581929ms
Jan 13 10:00:53.417: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-q7t5s" satisfied condition "running"
Jan 13 10:00:53.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-tsrpd" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:00:53.425: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-tsrpd": Phase="Running", Reason="", readiness=true. Elapsed: 8.314962ms
Jan 13 10:00:53.426: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-tsrpd" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec in namespace emptydir-wrapper-5053, will wait for the garbage collector to delete the pods 01/13/23 10:00:53.426
Jan 13 10:00:53.502: INFO: Deleting ReplicationController wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec took: 16.39222ms
Jan 13 10:00:53.603: INFO: Terminating ReplicationController wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec pods took: 100.773399ms
STEP: Creating RC which spawns configmap-volume pods 01/13/23 10:00:58.414
Jan 13 10:00:58.457: INFO: Pod name wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426: Found 0 pods out of 5
Jan 13 10:01:03.488: INFO: Pod name wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/13/23 10:01:03.488
Jan 13 10:01:03.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:03.500: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 12.344055ms
Jan 13 10:01:05.541: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052385832s
Jan 13 10:01:07.536: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048180716s
Jan 13 10:01:09.509: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020989693s
Jan 13 10:01:11.508: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Running", Reason="", readiness=true. Elapsed: 8.020297482s
Jan 13 10:01:11.509: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k" satisfied condition "running"
Jan 13 10:01:11.509: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:11.515: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.860272ms
Jan 13 10:01:13.528: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.019033113s
Jan 13 10:01:13.528: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg" satisfied condition "running"
Jan 13 10:01:13.528: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-c54f6" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:13.537: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-c54f6": Phase="Running", Reason="", readiness=true. Elapsed: 9.109098ms
Jan 13 10:01:13.537: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-c54f6" satisfied condition "running"
Jan 13 10:01:13.537: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-jrh69" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:13.545: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-jrh69": Phase="Running", Reason="", readiness=true. Elapsed: 8.32391ms
Jan 13 10:01:13.546: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-jrh69" satisfied condition "running"
Jan 13 10:01:13.546: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-vsgp5" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:13.554: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-vsgp5": Phase="Running", Reason="", readiness=true. Elapsed: 8.885098ms
Jan 13 10:01:13.555: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-vsgp5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426 in namespace emptydir-wrapper-5053, will wait for the garbage collector to delete the pods 01/13/23 10:01:13.555
Jan 13 10:01:13.704: INFO: Deleting ReplicationController wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426 took: 31.204751ms
Jan 13 10:01:14.005: INFO: Terminating ReplicationController wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426 pods took: 301.130893ms
STEP: Creating RC which spawns configmap-volume pods 01/13/23 10:01:18.617
Jan 13 10:01:18.660: INFO: Pod name wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf: Found 0 pods out of 5
Jan 13 10:01:23.700: INFO: Pod name wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/13/23 10:01:23.7
Jan 13 10:01:23.700: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:23.715: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 15.473028ms
Jan 13 10:01:25.727: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027102602s
Jan 13 10:01:27.734: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033705979s
Jan 13 10:01:29.728: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028028648s
Jan 13 10:01:31.726: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025806604s
Jan 13 10:01:33.736: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03569843s
Jan 13 10:01:35.735: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Running", Reason="", readiness=true. Elapsed: 12.035057841s
Jan 13 10:01:35.737: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z" satisfied condition "running"
Jan 13 10:01:35.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-7gf9g" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:35.757: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-7gf9g": Phase="Running", Reason="", readiness=true. Elapsed: 20.000322ms
Jan 13 10:01:35.757: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-7gf9g" satisfied condition "running"
Jan 13 10:01:35.757: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-kzkqz" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:35.769: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-kzkqz": Phase="Running", Reason="", readiness=true. Elapsed: 12.207553ms
Jan 13 10:01:35.769: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-kzkqz" satisfied condition "running"
Jan 13 10:01:35.769: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-rcv8d" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:35.781: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-rcv8d": Phase="Running", Reason="", readiness=true. Elapsed: 12.344653ms
Jan 13 10:01:35.782: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-rcv8d" satisfied condition "running"
Jan 13 10:01:35.782: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-xvhhs" in namespace "emptydir-wrapper-5053" to be "running"
Jan 13 10:01:35.809: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-xvhhs": Phase="Running", Reason="", readiness=true. Elapsed: 27.093707ms
Jan 13 10:01:35.809: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-xvhhs" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf in namespace emptydir-wrapper-5053, will wait for the garbage collector to delete the pods 01/13/23 10:01:35.809
Jan 13 10:01:35.908: INFO: Deleting ReplicationController wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf took: 23.716724ms
Jan 13 10:01:36.108: INFO: Terminating ReplicationController wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf pods took: 200.391544ms
STEP: Cleaning up the configMaps 01/13/23 10:01:42.409
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:01:43.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5053" for this suite. 01/13/23 10:01:43.468
------------------------------
• [SLOW TEST] [68.030 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:00:35.454
    Jan 13 10:00:35.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir-wrapper 01/13/23 10:00:35.457
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:00:35.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:00:35.625
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/13/23 10:00:35.661
    STEP: Creating RC which spawns configmap-volume pods 01/13/23 10:00:36.297
    Jan 13 10:00:36.334: INFO: Pod name wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec: Found 0 pods out of 5
    Jan 13 10:00:41.375: INFO: Pod name wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/13/23 10:00:41.375
    Jan 13 10:00:41.375: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:00:41.382: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.540035ms
    Jan 13 10:00:43.398: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022090408s
    Jan 13 10:00:45.399: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023814356s
    Jan 13 10:00:47.392: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016143607s
    Jan 13 10:00:49.397: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021396511s
    Jan 13 10:00:51.423: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047862492s
    Jan 13 10:00:53.390: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr": Phase="Running", Reason="", readiness=true. Elapsed: 12.014114348s
    Jan 13 10:00:53.390: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-52gfr" satisfied condition "running"
    Jan 13 10:00:53.390: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-5hg2l" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:00:53.403: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-5hg2l": Phase="Running", Reason="", readiness=true. Elapsed: 12.754013ms
    Jan 13 10:00:53.403: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-5hg2l" satisfied condition "running"
    Jan 13 10:00:53.403: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-gmctd" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:00:53.410: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-gmctd": Phase="Running", Reason="", readiness=true. Elapsed: 7.559484ms
    Jan 13 10:00:53.410: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-gmctd" satisfied condition "running"
    Jan 13 10:00:53.410: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-q7t5s" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:00:53.417: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-q7t5s": Phase="Running", Reason="", readiness=true. Elapsed: 6.581929ms
    Jan 13 10:00:53.417: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-q7t5s" satisfied condition "running"
    Jan 13 10:00:53.417: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-tsrpd" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:00:53.425: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-tsrpd": Phase="Running", Reason="", readiness=true. Elapsed: 8.314962ms
    Jan 13 10:00:53.426: INFO: Pod "wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec-tsrpd" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec in namespace emptydir-wrapper-5053, will wait for the garbage collector to delete the pods 01/13/23 10:00:53.426
    Jan 13 10:00:53.502: INFO: Deleting ReplicationController wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec took: 16.39222ms
    Jan 13 10:00:53.603: INFO: Terminating ReplicationController wrapped-volume-race-3d9aac28-8908-4436-aa0e-0054447541ec pods took: 100.773399ms
    STEP: Creating RC which spawns configmap-volume pods 01/13/23 10:00:58.414
    Jan 13 10:00:58.457: INFO: Pod name wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426: Found 0 pods out of 5
    Jan 13 10:01:03.488: INFO: Pod name wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/13/23 10:01:03.488
    Jan 13 10:01:03.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:03.500: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 12.344055ms
    Jan 13 10:01:05.541: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052385832s
    Jan 13 10:01:07.536: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048180716s
    Jan 13 10:01:09.509: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020989693s
    Jan 13 10:01:11.508: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k": Phase="Running", Reason="", readiness=true. Elapsed: 8.020297482s
    Jan 13 10:01:11.509: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-2f55k" satisfied condition "running"
    Jan 13 10:01:11.509: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:11.515: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.860272ms
    Jan 13 10:01:13.528: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.019033113s
    Jan 13 10:01:13.528: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-b2cxg" satisfied condition "running"
    Jan 13 10:01:13.528: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-c54f6" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:13.537: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-c54f6": Phase="Running", Reason="", readiness=true. Elapsed: 9.109098ms
    Jan 13 10:01:13.537: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-c54f6" satisfied condition "running"
    Jan 13 10:01:13.537: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-jrh69" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:13.545: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-jrh69": Phase="Running", Reason="", readiness=true. Elapsed: 8.32391ms
    Jan 13 10:01:13.546: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-jrh69" satisfied condition "running"
    Jan 13 10:01:13.546: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-vsgp5" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:13.554: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-vsgp5": Phase="Running", Reason="", readiness=true. Elapsed: 8.885098ms
    Jan 13 10:01:13.555: INFO: Pod "wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426-vsgp5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426 in namespace emptydir-wrapper-5053, will wait for the garbage collector to delete the pods 01/13/23 10:01:13.555
    Jan 13 10:01:13.704: INFO: Deleting ReplicationController wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426 took: 31.204751ms
    Jan 13 10:01:14.005: INFO: Terminating ReplicationController wrapped-volume-race-08164c20-4018-4d08-b8a3-d57f1ba06426 pods took: 301.130893ms
    STEP: Creating RC which spawns configmap-volume pods 01/13/23 10:01:18.617
    Jan 13 10:01:18.660: INFO: Pod name wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf: Found 0 pods out of 5
    Jan 13 10:01:23.700: INFO: Pod name wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/13/23 10:01:23.7
    Jan 13 10:01:23.700: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:23.715: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 15.473028ms
    Jan 13 10:01:25.727: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027102602s
    Jan 13 10:01:27.734: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033705979s
    Jan 13 10:01:29.728: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028028648s
    Jan 13 10:01:31.726: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025806604s
    Jan 13 10:01:33.736: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03569843s
    Jan 13 10:01:35.735: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z": Phase="Running", Reason="", readiness=true. Elapsed: 12.035057841s
    Jan 13 10:01:35.737: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-44h7z" satisfied condition "running"
    Jan 13 10:01:35.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-7gf9g" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:35.757: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-7gf9g": Phase="Running", Reason="", readiness=true. Elapsed: 20.000322ms
    Jan 13 10:01:35.757: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-7gf9g" satisfied condition "running"
    Jan 13 10:01:35.757: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-kzkqz" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:35.769: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-kzkqz": Phase="Running", Reason="", readiness=true. Elapsed: 12.207553ms
    Jan 13 10:01:35.769: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-kzkqz" satisfied condition "running"
    Jan 13 10:01:35.769: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-rcv8d" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:35.781: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-rcv8d": Phase="Running", Reason="", readiness=true. Elapsed: 12.344653ms
    Jan 13 10:01:35.782: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-rcv8d" satisfied condition "running"
    Jan 13 10:01:35.782: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-xvhhs" in namespace "emptydir-wrapper-5053" to be "running"
    Jan 13 10:01:35.809: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-xvhhs": Phase="Running", Reason="", readiness=true. Elapsed: 27.093707ms
    Jan 13 10:01:35.809: INFO: Pod "wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf-xvhhs" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf in namespace emptydir-wrapper-5053, will wait for the garbage collector to delete the pods 01/13/23 10:01:35.809
    Jan 13 10:01:35.908: INFO: Deleting ReplicationController wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf took: 23.716724ms
    Jan 13 10:01:36.108: INFO: Terminating ReplicationController wrapped-volume-race-115aca97-807b-41d4-b9e6-eaddb02d66cf pods took: 200.391544ms
    STEP: Cleaning up the configMaps 01/13/23 10:01:42.409
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:01:43.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5053" for this suite. 01/13/23 10:01:43.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:01:43.49
Jan 13 10:01:43.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename events 01/13/23 10:01:43.494
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:01:43.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:01:43.554
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/13/23 10:01:43.577
Jan 13 10:01:43.586: INFO: created test-event-1
Jan 13 10:01:43.608: INFO: created test-event-2
Jan 13 10:01:43.623: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/13/23 10:01:43.623
STEP: delete collection of events 01/13/23 10:01:43.632
Jan 13 10:01:43.632: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/13/23 10:01:43.728
Jan 13 10:01:43.729: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 13 10:01:43.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4419" for this suite. 01/13/23 10:01:43.76
------------------------------
• [0.284 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:01:43.49
    Jan 13 10:01:43.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename events 01/13/23 10:01:43.494
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:01:43.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:01:43.554
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/13/23 10:01:43.577
    Jan 13 10:01:43.586: INFO: created test-event-1
    Jan 13 10:01:43.608: INFO: created test-event-2
    Jan 13 10:01:43.623: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/13/23 10:01:43.623
    STEP: delete collection of events 01/13/23 10:01:43.632
    Jan 13 10:01:43.632: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/13/23 10:01:43.728
    Jan 13 10:01:43.729: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:01:43.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4419" for this suite. 01/13/23 10:01:43.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:01:43.781
Jan 13 10:01:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 10:01:43.785
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:01:43.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:01:43.857
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/13/23 10:01:43.867
STEP: Creating a ResourceQuota 01/13/23 10:01:48.887
STEP: Ensuring resource quota status is calculated 01/13/23 10:01:48.907
STEP: Creating a ReplicaSet 01/13/23 10:01:50.914
STEP: Ensuring resource quota status captures replicaset creation 01/13/23 10:01:50.938
STEP: Deleting a ReplicaSet 01/13/23 10:01:52.955
STEP: Ensuring resource quota status released usage 01/13/23 10:01:52.978
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 10:01:54.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4023" for this suite. 01/13/23 10:01:55.059
------------------------------
• [SLOW TEST] [11.297 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:01:43.781
    Jan 13 10:01:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 10:01:43.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:01:43.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:01:43.857
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/13/23 10:01:43.867
    STEP: Creating a ResourceQuota 01/13/23 10:01:48.887
    STEP: Ensuring resource quota status is calculated 01/13/23 10:01:48.907
    STEP: Creating a ReplicaSet 01/13/23 10:01:50.914
    STEP: Ensuring resource quota status captures replicaset creation 01/13/23 10:01:50.938
    STEP: Deleting a ReplicaSet 01/13/23 10:01:52.955
    STEP: Ensuring resource quota status released usage 01/13/23 10:01:52.978
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:01:54.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4023" for this suite. 01/13/23 10:01:55.059
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:01:55.079
Jan 13 10:01:55.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 10:01:55.082
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:01:55.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:01:55.143
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/13/23 10:01:55.167
Jan 13 10:01:55.196: INFO: Waiting up to 5m0s for pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d" in namespace "downward-api-6178" to be "Succeeded or Failed"
Jan 13 10:01:55.221: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.660881ms
Jan 13 10:01:57.232: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035774036s
Jan 13 10:01:59.229: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033182827s
Jan 13 10:02:01.231: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034833179s
STEP: Saw pod success 01/13/23 10:02:01.231
Jan 13 10:02:01.231: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d" satisfied condition "Succeeded or Failed"
Jan 13 10:02:01.252: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d container dapi-container: <nil>
STEP: delete the pod 01/13/23 10:02:01.312
Jan 13 10:02:01.350: INFO: Waiting for pod downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d to disappear
Jan 13 10:02:01.356: INFO: Pod downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 13 10:02:01.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6178" for this suite. 01/13/23 10:02:01.369
------------------------------
• [SLOW TEST] [6.308 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:01:55.079
    Jan 13 10:01:55.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 10:01:55.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:01:55.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:01:55.143
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/13/23 10:01:55.167
    Jan 13 10:01:55.196: INFO: Waiting up to 5m0s for pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d" in namespace "downward-api-6178" to be "Succeeded or Failed"
    Jan 13 10:01:55.221: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.660881ms
    Jan 13 10:01:57.232: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035774036s
    Jan 13 10:01:59.229: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033182827s
    Jan 13 10:02:01.231: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034833179s
    STEP: Saw pod success 01/13/23 10:02:01.231
    Jan 13 10:02:01.231: INFO: Pod "downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d" satisfied condition "Succeeded or Failed"
    Jan 13 10:02:01.252: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d container dapi-container: <nil>
    STEP: delete the pod 01/13/23 10:02:01.312
    Jan 13 10:02:01.350: INFO: Waiting for pod downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d to disappear
    Jan 13 10:02:01.356: INFO: Pod downward-api-578d9e57-82e9-4df0-81a9-a88badf9349d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:02:01.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6178" for this suite. 01/13/23 10:02:01.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:02:01.389
Jan 13 10:02:01.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 10:02:01.394
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:02:01.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:02:01.461
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/13/23 10:02:01.483
Jan 13 10:02:01.504: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4139" to be "running and ready"
Jan 13 10:02:01.521: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.991231ms
Jan 13 10:02:01.521: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:02:03.559: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0542251s
Jan 13 10:02:03.559: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:02:05.540: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.035565591s
Jan 13 10:02:05.540: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 13 10:02:05.540: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/13/23 10:02:05.554
Jan 13 10:02:05.569: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4139" to be "running and ready"
Jan 13 10:02:05.577: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955775ms
Jan 13 10:02:05.578: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:02:07.585: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015037684s
Jan 13 10:02:07.585: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:02:09.588: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.018102802s
Jan 13 10:02:09.588: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 13 10:02:09.588: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/13/23 10:02:09.595
STEP: delete the pod with lifecycle hook 01/13/23 10:02:09.633
Jan 13 10:02:09.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 13 10:02:09.713: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 13 10:02:11.713: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 13 10:02:11.721: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 13 10:02:13.714: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 13 10:02:13.747: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 13 10:02:13.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4139" for this suite. 01/13/23 10:02:13.79
------------------------------
• [SLOW TEST] [12.442 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:02:01.389
    Jan 13 10:02:01.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 10:02:01.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:02:01.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:02:01.461
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/13/23 10:02:01.483
    Jan 13 10:02:01.504: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4139" to be "running and ready"
    Jan 13 10:02:01.521: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.991231ms
    Jan 13 10:02:01.521: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:02:03.559: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0542251s
    Jan 13 10:02:03.559: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:02:05.540: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.035565591s
    Jan 13 10:02:05.540: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 13 10:02:05.540: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/13/23 10:02:05.554
    Jan 13 10:02:05.569: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4139" to be "running and ready"
    Jan 13 10:02:05.577: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955775ms
    Jan 13 10:02:05.578: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:02:07.585: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015037684s
    Jan 13 10:02:07.585: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:02:09.588: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.018102802s
    Jan 13 10:02:09.588: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 13 10:02:09.588: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/13/23 10:02:09.595
    STEP: delete the pod with lifecycle hook 01/13/23 10:02:09.633
    Jan 13 10:02:09.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 13 10:02:09.713: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 13 10:02:11.713: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 13 10:02:11.721: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 13 10:02:13.714: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 13 10:02:13.747: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:02:13.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4139" for this suite. 01/13/23 10:02:13.79
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:02:13.832
Jan 13 10:02:13.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename csiinlinevolumes 01/13/23 10:02:13.836
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:02:13.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:02:13.955
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/13/23 10:02:13.991
STEP: getting 01/13/23 10:02:14.102
STEP: listing in namespace 01/13/23 10:02:14.127
STEP: patching 01/13/23 10:02:14.144
STEP: deleting 01/13/23 10:02:14.284
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:02:14.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-448" for this suite. 01/13/23 10:02:14.393
------------------------------
• [0.578 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:02:13.832
    Jan 13 10:02:13.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename csiinlinevolumes 01/13/23 10:02:13.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:02:13.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:02:13.955
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/13/23 10:02:13.991
    STEP: getting 01/13/23 10:02:14.102
    STEP: listing in namespace 01/13/23 10:02:14.127
    STEP: patching 01/13/23 10:02:14.144
    STEP: deleting 01/13/23 10:02:14.284
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:02:14.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-448" for this suite. 01/13/23 10:02:14.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:02:14.468
Jan 13 10:02:14.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename cronjob 01/13/23 10:02:14.475
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:02:14.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:02:14.532
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/13/23 10:02:14.546
STEP: Ensuring more than one job is running at a time 01/13/23 10:02:14.566
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/13/23 10:04:00.575
STEP: Removing cronjob 01/13/23 10:04:00.585
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:00.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5049" for this suite. 01/13/23 10:04:00.622
------------------------------
• [SLOW TEST] [106.235 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:02:14.468
    Jan 13 10:02:14.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename cronjob 01/13/23 10:02:14.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:02:14.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:02:14.532
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/13/23 10:02:14.546
    STEP: Ensuring more than one job is running at a time 01/13/23 10:02:14.566
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/13/23 10:04:00.575
    STEP: Removing cronjob 01/13/23 10:04:00.585
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:00.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5049" for this suite. 01/13/23 10:04:00.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:00.706
Jan 13 10:04:00.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename disruption 01/13/23 10:04:00.709
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:00.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:00.849
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:00.871
Jan 13 10:04:00.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename disruption-2 01/13/23 10:04:00.874
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:01.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:01.072
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/13/23 10:04:01.14
STEP: Waiting for the pdb to be processed 01/13/23 10:04:03.174
STEP: Waiting for the pdb to be processed 01/13/23 10:04:03.197
STEP: listing a collection of PDBs across all namespaces 01/13/23 10:04:05.213
STEP: listing a collection of PDBs in namespace disruption-9750 01/13/23 10:04:05.223
STEP: deleting a collection of PDBs 01/13/23 10:04:05.242
STEP: Waiting for the PDB collection to be deleted 01/13/23 10:04:05.27
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:05.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:05.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-5561" for this suite. 01/13/23 10:04:05.32
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9750" for this suite. 01/13/23 10:04:05.337
------------------------------
• [4.652 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:00.706
    Jan 13 10:04:00.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename disruption 01/13/23 10:04:00.709
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:00.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:00.849
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:00.871
    Jan 13 10:04:00.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename disruption-2 01/13/23 10:04:00.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:01.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:01.072
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/13/23 10:04:01.14
    STEP: Waiting for the pdb to be processed 01/13/23 10:04:03.174
    STEP: Waiting for the pdb to be processed 01/13/23 10:04:03.197
    STEP: listing a collection of PDBs across all namespaces 01/13/23 10:04:05.213
    STEP: listing a collection of PDBs in namespace disruption-9750 01/13/23 10:04:05.223
    STEP: deleting a collection of PDBs 01/13/23 10:04:05.242
    STEP: Waiting for the PDB collection to be deleted 01/13/23 10:04:05.27
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:05.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:05.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-5561" for this suite. 01/13/23 10:04:05.32
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9750" for this suite. 01/13/23 10:04:05.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:05.362
Jan 13 10:04:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename containers 01/13/23 10:04:05.364
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:05.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:05.426
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/13/23 10:04:05.44
Jan 13 10:04:05.475: INFO: Waiting up to 5m0s for pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283" in namespace "containers-6860" to be "Succeeded or Failed"
Jan 13 10:04:05.489: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Pending", Reason="", readiness=false. Elapsed: 14.614424ms
Jan 13 10:04:07.499: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023739702s
Jan 13 10:04:09.515: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03999216s
Jan 13 10:04:11.496: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020927012s
STEP: Saw pod success 01/13/23 10:04:11.496
Jan 13 10:04:11.496: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283" satisfied condition "Succeeded or Failed"
Jan 13 10:04:11.504: INFO: Trying to get logs from node 10.10.102.31-node pod client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:04:11.563
Jan 13 10:04:11.582: INFO: Waiting for pod client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283 to disappear
Jan 13 10:04:11.586: INFO: Pod client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:11.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6860" for this suite. 01/13/23 10:04:11.593
------------------------------
• [SLOW TEST] [6.243 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:05.362
    Jan 13 10:04:05.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename containers 01/13/23 10:04:05.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:05.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:05.426
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/13/23 10:04:05.44
    Jan 13 10:04:05.475: INFO: Waiting up to 5m0s for pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283" in namespace "containers-6860" to be "Succeeded or Failed"
    Jan 13 10:04:05.489: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Pending", Reason="", readiness=false. Elapsed: 14.614424ms
    Jan 13 10:04:07.499: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023739702s
    Jan 13 10:04:09.515: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03999216s
    Jan 13 10:04:11.496: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020927012s
    STEP: Saw pod success 01/13/23 10:04:11.496
    Jan 13 10:04:11.496: INFO: Pod "client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283" satisfied condition "Succeeded or Failed"
    Jan 13 10:04:11.504: INFO: Trying to get logs from node 10.10.102.31-node pod client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:04:11.563
    Jan 13 10:04:11.582: INFO: Waiting for pod client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283 to disappear
    Jan 13 10:04:11.586: INFO: Pod client-containers-33963fa3-7b72-4e33-b38b-2944eb1f1283 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:11.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6860" for this suite. 01/13/23 10:04:11.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:11.607
Jan 13 10:04:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:04:11.609
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:11.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:11.637
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:04:11.662
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:04:14.308
STEP: Deploying the webhook pod 01/13/23 10:04:14.327
STEP: Wait for the deployment to be ready 01/13/23 10:04:14.403
Jan 13 10:04:14.464: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:04:16.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:04:18.513
STEP: Verifying the service has paired with the endpoint 01/13/23 10:04:18.538
Jan 13 10:04:19.539: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 13 10:04:19.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/13/23 10:04:20.08
STEP: Creating a custom resource that should be denied by the webhook 01/13/23 10:04:20.18
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/13/23 10:04:22.328
STEP: Updating the custom resource with disallowed data should be denied 01/13/23 10:04:22.348
STEP: Deleting the custom resource should be denied 01/13/23 10:04:22.373
STEP: Remove the offending key and value from the custom resource data 01/13/23 10:04:22.395
STEP: Deleting the updated custom resource should be successful 01/13/23 10:04:22.433
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:23.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2008" for this suite. 01/13/23 10:04:23.265
STEP: Destroying namespace "webhook-2008-markers" for this suite. 01/13/23 10:04:23.301
------------------------------
• [SLOW TEST] [11.718 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:11.607
    Jan 13 10:04:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:04:11.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:11.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:11.637
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:04:11.662
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:04:14.308
    STEP: Deploying the webhook pod 01/13/23 10:04:14.327
    STEP: Wait for the deployment to be ready 01/13/23 10:04:14.403
    Jan 13 10:04:14.464: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:04:16.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 4, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:04:18.513
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:04:18.538
    Jan 13 10:04:19.539: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 13 10:04:19.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/13/23 10:04:20.08
    STEP: Creating a custom resource that should be denied by the webhook 01/13/23 10:04:20.18
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/13/23 10:04:22.328
    STEP: Updating the custom resource with disallowed data should be denied 01/13/23 10:04:22.348
    STEP: Deleting the custom resource should be denied 01/13/23 10:04:22.373
    STEP: Remove the offending key and value from the custom resource data 01/13/23 10:04:22.395
    STEP: Deleting the updated custom resource should be successful 01/13/23 10:04:22.433
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:23.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2008" for this suite. 01/13/23 10:04:23.265
    STEP: Destroying namespace "webhook-2008-markers" for this suite. 01/13/23 10:04:23.301
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:23.327
Jan 13 10:04:23.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:04:23.33
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:23.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:23.456
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-2ee9e676-ccab-4713-8692-a1a9d292f5ed 01/13/23 10:04:23.469
STEP: Creating a pod to test consume secrets 01/13/23 10:04:23.485
Jan 13 10:04:23.525: INFO: Waiting up to 5m0s for pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7" in namespace "secrets-6806" to be "Succeeded or Failed"
Jan 13 10:04:23.543: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.582498ms
Jan 13 10:04:25.560: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034876314s
Jan 13 10:04:27.554: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028298006s
Jan 13 10:04:29.555: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02910571s
STEP: Saw pod success 01/13/23 10:04:29.555
Jan 13 10:04:29.555: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7" satisfied condition "Succeeded or Failed"
Jan 13 10:04:29.565: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7 container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 10:04:29.607
Jan 13 10:04:29.672: INFO: Waiting for pod pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7 to disappear
Jan 13 10:04:29.684: INFO: Pod pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:29.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6806" for this suite. 01/13/23 10:04:29.695
------------------------------
• [SLOW TEST] [6.410 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:23.327
    Jan 13 10:04:23.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:04:23.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:23.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:23.456
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-2ee9e676-ccab-4713-8692-a1a9d292f5ed 01/13/23 10:04:23.469
    STEP: Creating a pod to test consume secrets 01/13/23 10:04:23.485
    Jan 13 10:04:23.525: INFO: Waiting up to 5m0s for pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7" in namespace "secrets-6806" to be "Succeeded or Failed"
    Jan 13 10:04:23.543: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.582498ms
    Jan 13 10:04:25.560: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034876314s
    Jan 13 10:04:27.554: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028298006s
    Jan 13 10:04:29.555: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02910571s
    STEP: Saw pod success 01/13/23 10:04:29.555
    Jan 13 10:04:29.555: INFO: Pod "pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7" satisfied condition "Succeeded or Failed"
    Jan 13 10:04:29.565: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7 container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:04:29.607
    Jan 13 10:04:29.672: INFO: Waiting for pod pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7 to disappear
    Jan 13 10:04:29.684: INFO: Pod pod-secrets-e8f7b97a-3fb8-4694-9ec7-f38f7ec9fcc7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:29.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6806" for this suite. 01/13/23 10:04:29.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:29.742
Jan 13 10:04:29.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename containers 01/13/23 10:04:29.745
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:29.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:29.82
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/13/23 10:04:29.836
Jan 13 10:04:29.857: INFO: Waiting up to 5m0s for pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc" in namespace "containers-3472" to be "Succeeded or Failed"
Jan 13 10:04:29.867: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413511ms
Jan 13 10:04:31.879: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022147434s
Jan 13 10:04:33.876: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019834963s
Jan 13 10:04:35.877: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019966496s
Jan 13 10:04:37.888: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.031281574s
STEP: Saw pod success 01/13/23 10:04:37.888
Jan 13 10:04:37.888: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc" satisfied condition "Succeeded or Failed"
Jan 13 10:04:37.899: INFO: Trying to get logs from node 10.10.102.31-node pod client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:04:37.926
Jan 13 10:04:37.961: INFO: Waiting for pod client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc to disappear
Jan 13 10:04:37.972: INFO: Pod client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:37.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3472" for this suite. 01/13/23 10:04:37.983
------------------------------
• [SLOW TEST] [8.256 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:29.742
    Jan 13 10:04:29.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename containers 01/13/23 10:04:29.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:29.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:29.82
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/13/23 10:04:29.836
    Jan 13 10:04:29.857: INFO: Waiting up to 5m0s for pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc" in namespace "containers-3472" to be "Succeeded or Failed"
    Jan 13 10:04:29.867: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413511ms
    Jan 13 10:04:31.879: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022147434s
    Jan 13 10:04:33.876: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019834963s
    Jan 13 10:04:35.877: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019966496s
    Jan 13 10:04:37.888: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.031281574s
    STEP: Saw pod success 01/13/23 10:04:37.888
    Jan 13 10:04:37.888: INFO: Pod "client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc" satisfied condition "Succeeded or Failed"
    Jan 13 10:04:37.899: INFO: Trying to get logs from node 10.10.102.31-node pod client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:04:37.926
    Jan 13 10:04:37.961: INFO: Waiting for pod client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc to disappear
    Jan 13 10:04:37.972: INFO: Pod client-containers-ab7e4b51-9488-445d-af4d-139e5cf758bc no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:37.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3472" for this suite. 01/13/23 10:04:37.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:38.009
Jan 13 10:04:38.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:04:38.012
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:38.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:38.063
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-b06bd6f3-0e29-4063-9664-1acbfc279a06 01/13/23 10:04:38.07
STEP: Creating a pod to test consume configMaps 01/13/23 10:04:38.079
Jan 13 10:04:38.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5" in namespace "projected-136" to be "Succeeded or Failed"
Jan 13 10:04:38.101: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.424762ms
Jan 13 10:04:40.117: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023529421s
Jan 13 10:04:42.110: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016138026s
Jan 13 10:04:44.110: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016511299s
STEP: Saw pod success 01/13/23 10:04:44.11
Jan 13 10:04:44.111: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5" satisfied condition "Succeeded or Failed"
Jan 13 10:04:44.117: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/13/23 10:04:44.135
Jan 13 10:04:44.180: INFO: Waiting for pod pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5 to disappear
Jan 13 10:04:44.194: INFO: Pod pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:44.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-136" for this suite. 01/13/23 10:04:44.204
------------------------------
• [SLOW TEST] [6.208 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:38.009
    Jan 13 10:04:38.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:04:38.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:38.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:38.063
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-b06bd6f3-0e29-4063-9664-1acbfc279a06 01/13/23 10:04:38.07
    STEP: Creating a pod to test consume configMaps 01/13/23 10:04:38.079
    Jan 13 10:04:38.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5" in namespace "projected-136" to be "Succeeded or Failed"
    Jan 13 10:04:38.101: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.424762ms
    Jan 13 10:04:40.117: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023529421s
    Jan 13 10:04:42.110: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016138026s
    Jan 13 10:04:44.110: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016511299s
    STEP: Saw pod success 01/13/23 10:04:44.11
    Jan 13 10:04:44.111: INFO: Pod "pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5" satisfied condition "Succeeded or Failed"
    Jan 13 10:04:44.117: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:04:44.135
    Jan 13 10:04:44.180: INFO: Waiting for pod pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5 to disappear
    Jan 13 10:04:44.194: INFO: Pod pod-projected-configmaps-77026d1a-c6da-499b-978f-a1415aad8ff5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:44.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-136" for this suite. 01/13/23 10:04:44.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:44.22
Jan 13 10:04:44.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 10:04:44.222
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:44.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:44.297
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-663b3de5-e164-4573-bb70-f35bc1d1db27 01/13/23 10:04:44.302
STEP: Creating a pod to test consume configMaps 01/13/23 10:04:44.31
Jan 13 10:04:44.323: INFO: Waiting up to 5m0s for pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac" in namespace "configmap-6097" to be "Succeeded or Failed"
Jan 13 10:04:44.327: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532491ms
Jan 13 10:04:46.334: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011811992s
Jan 13 10:04:48.337: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013858285s
Jan 13 10:04:50.335: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011961225s
STEP: Saw pod success 01/13/23 10:04:50.335
Jan 13 10:04:50.335: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac" satisfied condition "Succeeded or Failed"
Jan 13 10:04:50.344: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:04:50.377
Jan 13 10:04:50.400: INFO: Waiting for pod pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac to disappear
Jan 13 10:04:50.409: INFO: Pod pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:04:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6097" for this suite. 01/13/23 10:04:50.421
------------------------------
• [SLOW TEST] [6.212 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:44.22
    Jan 13 10:04:44.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 10:04:44.222
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:44.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:44.297
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-663b3de5-e164-4573-bb70-f35bc1d1db27 01/13/23 10:04:44.302
    STEP: Creating a pod to test consume configMaps 01/13/23 10:04:44.31
    Jan 13 10:04:44.323: INFO: Waiting up to 5m0s for pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac" in namespace "configmap-6097" to be "Succeeded or Failed"
    Jan 13 10:04:44.327: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532491ms
    Jan 13 10:04:46.334: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011811992s
    Jan 13 10:04:48.337: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013858285s
    Jan 13 10:04:50.335: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011961225s
    STEP: Saw pod success 01/13/23 10:04:50.335
    Jan 13 10:04:50.335: INFO: Pod "pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac" satisfied condition "Succeeded or Failed"
    Jan 13 10:04:50.344: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:04:50.377
    Jan 13 10:04:50.400: INFO: Waiting for pod pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac to disappear
    Jan 13 10:04:50.409: INFO: Pod pod-configmaps-16ca587a-ab70-4844-a087-85321590c5ac no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:04:50.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6097" for this suite. 01/13/23 10:04:50.421
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:04:50.434
Jan 13 10:04:50.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:04:50.436
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:50.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:50.47
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-365 01/13/23 10:04:50.48
STEP: creating service affinity-nodeport in namespace services-365 01/13/23 10:04:50.48
STEP: creating replication controller affinity-nodeport in namespace services-365 01/13/23 10:04:50.546
I0113 10:04:50.564370      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-365, replica count: 3
I0113 10:04:53.615827      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:04:56.617554      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 10:04:56.671: INFO: Creating new exec pod
Jan 13 10:04:56.693: INFO: Waiting up to 5m0s for pod "execpod-affinityqjt2z" in namespace "services-365" to be "running"
Jan 13 10:04:56.698: INFO: Pod "execpod-affinityqjt2z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.092574ms
Jan 13 10:04:58.714: INFO: Pod "execpod-affinityqjt2z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020927371s
Jan 13 10:05:00.712: INFO: Pod "execpod-affinityqjt2z": Phase="Running", Reason="", readiness=true. Elapsed: 4.019312703s
Jan 13 10:05:00.712: INFO: Pod "execpod-affinityqjt2z" satisfied condition "running"
Jan 13 10:05:01.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 13 10:05:02.154: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 13 10:05:02.154: INFO: stdout: ""
Jan 13 10:05:02.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 10.104.40.58 80'
Jan 13 10:05:02.557: INFO: stderr: "+ nc -v -z -w 2 10.104.40.58 80\nConnection to 10.104.40.58 80 port [tcp/http] succeeded!\n"
Jan 13 10:05:02.557: INFO: stdout: ""
Jan 13 10:05:02.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 32034'
Jan 13 10:05:03.022: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 32034\nConnection to 10.10.102.31 32034 port [tcp/*] succeeded!\n"
Jan 13 10:05:03.022: INFO: stdout: ""
Jan 13 10:05:03.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 32034'
Jan 13 10:05:03.386: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 32034\nConnection to 10.10.102.32 32034 port [tcp/*] succeeded!\n"
Jan 13 10:05:03.386: INFO: stdout: ""
Jan 13 10:05:03.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.102.31:32034/ ; done'
Jan 13 10:05:04.020: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n"
Jan 13 10:05:04.020: INFO: stdout: "\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5"
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
Jan 13 10:05:04.020: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-365, will wait for the garbage collector to delete the pods 01/13/23 10:05:04.045
Jan 13 10:05:04.113: INFO: Deleting ReplicationController affinity-nodeport took: 10.976429ms
Jan 13 10:05:04.214: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.015451ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:05:07.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-365" for this suite. 01/13/23 10:05:07.676
------------------------------
• [SLOW TEST] [17.261 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:04:50.434
    Jan 13 10:04:50.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:04:50.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:04:50.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:04:50.47
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-365 01/13/23 10:04:50.48
    STEP: creating service affinity-nodeport in namespace services-365 01/13/23 10:04:50.48
    STEP: creating replication controller affinity-nodeport in namespace services-365 01/13/23 10:04:50.546
    I0113 10:04:50.564370      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-365, replica count: 3
    I0113 10:04:53.615827      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:04:56.617554      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 10:04:56.671: INFO: Creating new exec pod
    Jan 13 10:04:56.693: INFO: Waiting up to 5m0s for pod "execpod-affinityqjt2z" in namespace "services-365" to be "running"
    Jan 13 10:04:56.698: INFO: Pod "execpod-affinityqjt2z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.092574ms
    Jan 13 10:04:58.714: INFO: Pod "execpod-affinityqjt2z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020927371s
    Jan 13 10:05:00.712: INFO: Pod "execpod-affinityqjt2z": Phase="Running", Reason="", readiness=true. Elapsed: 4.019312703s
    Jan 13 10:05:00.712: INFO: Pod "execpod-affinityqjt2z" satisfied condition "running"
    Jan 13 10:05:01.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 13 10:05:02.154: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 13 10:05:02.154: INFO: stdout: ""
    Jan 13 10:05:02.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 10.104.40.58 80'
    Jan 13 10:05:02.557: INFO: stderr: "+ nc -v -z -w 2 10.104.40.58 80\nConnection to 10.104.40.58 80 port [tcp/http] succeeded!\n"
    Jan 13 10:05:02.557: INFO: stdout: ""
    Jan 13 10:05:02.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 32034'
    Jan 13 10:05:03.022: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 32034\nConnection to 10.10.102.31 32034 port [tcp/*] succeeded!\n"
    Jan 13 10:05:03.022: INFO: stdout: ""
    Jan 13 10:05:03.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 32034'
    Jan 13 10:05:03.386: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 32034\nConnection to 10.10.102.32 32034 port [tcp/*] succeeded!\n"
    Jan 13 10:05:03.386: INFO: stdout: ""
    Jan 13 10:05:03.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-365 exec execpod-affinityqjt2z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.102.31:32034/ ; done'
    Jan 13 10:05:04.020: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32034/\n"
    Jan 13 10:05:04.020: INFO: stdout: "\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5\naffinity-nodeport-6hbc5"
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Received response from host: affinity-nodeport-6hbc5
    Jan 13 10:05:04.020: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-365, will wait for the garbage collector to delete the pods 01/13/23 10:05:04.045
    Jan 13 10:05:04.113: INFO: Deleting ReplicationController affinity-nodeport took: 10.976429ms
    Jan 13 10:05:04.214: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.015451ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:05:07.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-365" for this suite. 01/13/23 10:05:07.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:05:07.702
Jan 13 10:05:07.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 10:05:07.704
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:05:07.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:05:07.783
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 in namespace container-probe-9795 01/13/23 10:05:07.811
Jan 13 10:05:07.830: INFO: Waiting up to 5m0s for pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1" in namespace "container-probe-9795" to be "not pending"
Jan 13 10:05:07.838: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.421249ms
Jan 13 10:05:09.858: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028112725s
Jan 13 10:05:11.850: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019378355s
Jan 13 10:05:13.851: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Running", Reason="", readiness=true. Elapsed: 6.020169869s
Jan 13 10:05:13.851: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1" satisfied condition "not pending"
Jan 13 10:05:13.851: INFO: Started pod busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 in namespace container-probe-9795
STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:05:13.851
Jan 13 10:05:13.864: INFO: Initial restart count of pod busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 is 0
Jan 13 10:06:02.119: INFO: Restart count of pod container-probe-9795/busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 is now 1 (48.254508735s elapsed)
STEP: deleting the pod 01/13/23 10:06:02.119
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:02.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9795" for this suite. 01/13/23 10:06:02.157
------------------------------
• [SLOW TEST] [54.473 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:05:07.702
    Jan 13 10:05:07.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 10:05:07.704
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:05:07.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:05:07.783
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 in namespace container-probe-9795 01/13/23 10:05:07.811
    Jan 13 10:05:07.830: INFO: Waiting up to 5m0s for pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1" in namespace "container-probe-9795" to be "not pending"
    Jan 13 10:05:07.838: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.421249ms
    Jan 13 10:05:09.858: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028112725s
    Jan 13 10:05:11.850: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019378355s
    Jan 13 10:05:13.851: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1": Phase="Running", Reason="", readiness=true. Elapsed: 6.020169869s
    Jan 13 10:05:13.851: INFO: Pod "busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1" satisfied condition "not pending"
    Jan 13 10:05:13.851: INFO: Started pod busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 in namespace container-probe-9795
    STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:05:13.851
    Jan 13 10:05:13.864: INFO: Initial restart count of pod busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 is 0
    Jan 13 10:06:02.119: INFO: Restart count of pod container-probe-9795/busybox-c6c08b43-163d-4fed-96d7-1ae548ed77f1 is now 1 (48.254508735s elapsed)
    STEP: deleting the pod 01/13/23 10:06:02.119
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:02.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9795" for this suite. 01/13/23 10:06:02.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:02.175
Jan 13 10:06:02.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:06:02.177
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:02.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:02.257
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 13 10:06:02.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: creating the pod 01/13/23 10:06:02.284
STEP: submitting the pod to kubernetes 01/13/23 10:06:02.284
Jan 13 10:06:02.305: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833" in namespace "pods-4251" to be "running and ready"
Jan 13 10:06:02.312: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115503ms
Jan 13 10:06:02.312: INFO: The phase of Pod pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:06:04.326: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021092987s
Jan 13 10:06:04.326: INFO: The phase of Pod pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:06:06.321: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833": Phase="Running", Reason="", readiness=true. Elapsed: 4.015523848s
Jan 13 10:06:06.321: INFO: The phase of Pod pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833 is Running (Ready = true)
Jan 13 10:06:06.321: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:06.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4251" for this suite. 01/13/23 10:06:06.565
------------------------------
• [4.401 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:02.175
    Jan 13 10:06:02.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:06:02.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:02.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:02.257
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 13 10:06:02.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: creating the pod 01/13/23 10:06:02.284
    STEP: submitting the pod to kubernetes 01/13/23 10:06:02.284
    Jan 13 10:06:02.305: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833" in namespace "pods-4251" to be "running and ready"
    Jan 13 10:06:02.312: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115503ms
    Jan 13 10:06:02.312: INFO: The phase of Pod pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:06:04.326: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021092987s
    Jan 13 10:06:04.326: INFO: The phase of Pod pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:06:06.321: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833": Phase="Running", Reason="", readiness=true. Elapsed: 4.015523848s
    Jan 13 10:06:06.321: INFO: The phase of Pod pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833 is Running (Ready = true)
    Jan 13 10:06:06.321: INFO: Pod "pod-exec-websocket-40961efc-997d-47e0-9c7b-c0f9938e7833" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:06.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4251" for this suite. 01/13/23 10:06:06.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:06.578
Jan 13 10:06:06.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replicaset 01/13/23 10:06:06.584
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:06.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:06.754
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 13 10:06:06.807: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 13 10:06:11.815: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/13/23 10:06:11.815
STEP: Scaling up "test-rs" replicaset  01/13/23 10:06:11.816
Jan 13 10:06:11.847: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/13/23 10:06:11.847
W0113 10:06:11.865050      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 13 10:06:11.869: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 10:06:11.917: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 10:06:11.968: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 10:06:11.982: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 10:06:14.291: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 2, AvailableReplicas 2
Jan 13 10:06:15.732: INFO: observed Replicaset test-rs in namespace replicaset-8270 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:15.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8270" for this suite. 01/13/23 10:06:15.754
------------------------------
• [SLOW TEST] [9.209 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:06.578
    Jan 13 10:06:06.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replicaset 01/13/23 10:06:06.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:06.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:06.754
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 13 10:06:06.807: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 13 10:06:11.815: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/13/23 10:06:11.815
    STEP: Scaling up "test-rs" replicaset  01/13/23 10:06:11.816
    Jan 13 10:06:11.847: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/13/23 10:06:11.847
    W0113 10:06:11.865050      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 13 10:06:11.869: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
    Jan 13 10:06:11.917: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
    Jan 13 10:06:11.968: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
    Jan 13 10:06:11.982: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 1, AvailableReplicas 1
    Jan 13 10:06:14.291: INFO: observed ReplicaSet test-rs in namespace replicaset-8270 with ReadyReplicas 2, AvailableReplicas 2
    Jan 13 10:06:15.732: INFO: observed Replicaset test-rs in namespace replicaset-8270 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:15.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8270" for this suite. 01/13/23 10:06:15.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:15.792
Jan 13 10:06:15.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:06:15.794
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:15.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:15.841
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-4988 01/13/23 10:06:15.859
STEP: creating service affinity-nodeport-transition in namespace services-4988 01/13/23 10:06:15.859
STEP: creating replication controller affinity-nodeport-transition in namespace services-4988 01/13/23 10:06:15.945
I0113 10:06:15.964153      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4988, replica count: 3
I0113 10:06:19.015702      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:06:22.016846      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 10:06:22.041: INFO: Creating new exec pod
Jan 13 10:06:22.053: INFO: Waiting up to 5m0s for pod "execpod-affinityf6mgp" in namespace "services-4988" to be "running"
Jan 13 10:06:22.068: INFO: Pod "execpod-affinityf6mgp": Phase="Pending", Reason="", readiness=false. Elapsed: 14.390662ms
Jan 13 10:06:24.086: INFO: Pod "execpod-affinityf6mgp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032521609s
Jan 13 10:06:26.079: INFO: Pod "execpod-affinityf6mgp": Phase="Running", Reason="", readiness=true. Elapsed: 4.025502642s
Jan 13 10:06:26.079: INFO: Pod "execpod-affinityf6mgp" satisfied condition "running"
Jan 13 10:06:27.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 13 10:06:27.819: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 13 10:06:27.819: INFO: stdout: ""
Jan 13 10:06:27.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 10.108.65.109 80'
Jan 13 10:06:28.487: INFO: stderr: "+ nc -v -z -w 2 10.108.65.109 80\nConnection to 10.108.65.109 80 port [tcp/http] succeeded!\n"
Jan 13 10:06:28.487: INFO: stdout: ""
Jan 13 10:06:28.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 32365'
Jan 13 10:06:29.025: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 32365\nConnection to 10.10.102.31 32365 port [tcp/*] succeeded!\n"
Jan 13 10:06:29.025: INFO: stdout: ""
Jan 13 10:06:29.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 32365'
Jan 13 10:06:29.487: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 32365\nConnection to 10.10.102.32 32365 port [tcp/*] succeeded!\n"
Jan 13 10:06:29.487: INFO: stdout: ""
Jan 13 10:06:29.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.102.31:32365/ ; done'
Jan 13 10:06:30.452: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n"
Jan 13 10:06:30.452: INFO: stdout: "\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-9r5fq"
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-j4zbn
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-j4zbn
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-ngw2x
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-ngw2x
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-j4zbn
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-j4zbn
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-j4zbn
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-ngw2x
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-ngw2x
Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:30.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.102.31:32365/ ; done'
Jan 13 10:06:31.715: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n"
Jan 13 10:06:31.715: INFO: stdout: "\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq"
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
Jan 13 10:06:31.715: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4988, will wait for the garbage collector to delete the pods 01/13/23 10:06:31.742
Jan 13 10:06:31.860: INFO: Deleting ReplicationController affinity-nodeport-transition took: 39.312181ms
Jan 13 10:06:31.961: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.08668ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:35.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4988" for this suite. 01/13/23 10:06:35.267
------------------------------
• [SLOW TEST] [19.512 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:15.792
    Jan 13 10:06:15.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:06:15.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:15.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:15.841
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-4988 01/13/23 10:06:15.859
    STEP: creating service affinity-nodeport-transition in namespace services-4988 01/13/23 10:06:15.859
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4988 01/13/23 10:06:15.945
    I0113 10:06:15.964153      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4988, replica count: 3
    I0113 10:06:19.015702      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:06:22.016846      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 10:06:22.041: INFO: Creating new exec pod
    Jan 13 10:06:22.053: INFO: Waiting up to 5m0s for pod "execpod-affinityf6mgp" in namespace "services-4988" to be "running"
    Jan 13 10:06:22.068: INFO: Pod "execpod-affinityf6mgp": Phase="Pending", Reason="", readiness=false. Elapsed: 14.390662ms
    Jan 13 10:06:24.086: INFO: Pod "execpod-affinityf6mgp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032521609s
    Jan 13 10:06:26.079: INFO: Pod "execpod-affinityf6mgp": Phase="Running", Reason="", readiness=true. Elapsed: 4.025502642s
    Jan 13 10:06:26.079: INFO: Pod "execpod-affinityf6mgp" satisfied condition "running"
    Jan 13 10:06:27.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 13 10:06:27.819: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 13 10:06:27.819: INFO: stdout: ""
    Jan 13 10:06:27.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 10.108.65.109 80'
    Jan 13 10:06:28.487: INFO: stderr: "+ nc -v -z -w 2 10.108.65.109 80\nConnection to 10.108.65.109 80 port [tcp/http] succeeded!\n"
    Jan 13 10:06:28.487: INFO: stdout: ""
    Jan 13 10:06:28.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 32365'
    Jan 13 10:06:29.025: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 32365\nConnection to 10.10.102.31 32365 port [tcp/*] succeeded!\n"
    Jan 13 10:06:29.025: INFO: stdout: ""
    Jan 13 10:06:29.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 32365'
    Jan 13 10:06:29.487: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 32365\nConnection to 10.10.102.32 32365 port [tcp/*] succeeded!\n"
    Jan 13 10:06:29.487: INFO: stdout: ""
    Jan 13 10:06:29.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.102.31:32365/ ; done'
    Jan 13 10:06:30.452: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n"
    Jan 13 10:06:30.452: INFO: stdout: "\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-j4zbn\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-ngw2x\naffinity-nodeport-transition-9r5fq"
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-j4zbn
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-j4zbn
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-ngw2x
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-ngw2x
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:30.452: INFO: Received response from host: affinity-nodeport-transition-j4zbn
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-j4zbn
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-j4zbn
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-ngw2x
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-ngw2x
    Jan 13 10:06:30.453: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:30.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-4988 exec execpod-affinityf6mgp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.102.31:32365/ ; done'
    Jan 13 10:06:31.715: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.102.31:32365/\n"
    Jan 13 10:06:31.715: INFO: stdout: "\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq\naffinity-nodeport-transition-9r5fq"
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Received response from host: affinity-nodeport-transition-9r5fq
    Jan 13 10:06:31.715: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4988, will wait for the garbage collector to delete the pods 01/13/23 10:06:31.742
    Jan 13 10:06:31.860: INFO: Deleting ReplicationController affinity-nodeport-transition took: 39.312181ms
    Jan 13 10:06:31.961: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.08668ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:35.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4988" for this suite. 01/13/23 10:06:35.267
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:35.306
Jan 13 10:06:35.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 10:06:35.322
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:35.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:35.406
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-21f730b7-8516-4e12-93be-ebd891c33a5e 01/13/23 10:06:35.419
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:35.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9424" for this suite. 01/13/23 10:06:35.444
------------------------------
• [0.173 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:35.306
    Jan 13 10:06:35.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 10:06:35.322
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:35.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:35.406
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-21f730b7-8516-4e12-93be-ebd891c33a5e 01/13/23 10:06:35.419
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:35.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9424" for this suite. 01/13/23 10:06:35.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:35.48
Jan 13 10:06:35.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:06:35.483
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:35.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:35.594
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/13/23 10:06:35.629
Jan 13 10:06:35.668: INFO: Waiting up to 5m0s for pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220" in namespace "projected-6447" to be "running and ready"
Jan 13 10:06:35.685: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220": Phase="Pending", Reason="", readiness=false. Elapsed: 17.477256ms
Jan 13 10:06:35.685: INFO: The phase of Pod annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:06:37.709: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041255986s
Jan 13 10:06:37.709: INFO: The phase of Pod annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:06:39.692: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220": Phase="Running", Reason="", readiness=true. Elapsed: 4.024240911s
Jan 13 10:06:39.692: INFO: The phase of Pod annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220 is Running (Ready = true)
Jan 13 10:06:39.692: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220" satisfied condition "running and ready"
Jan 13 10:06:40.279: INFO: Successfully updated pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:42.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6447" for this suite. 01/13/23 10:06:42.343
------------------------------
• [SLOW TEST] [6.882 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:35.48
    Jan 13 10:06:35.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:06:35.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:35.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:35.594
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/13/23 10:06:35.629
    Jan 13 10:06:35.668: INFO: Waiting up to 5m0s for pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220" in namespace "projected-6447" to be "running and ready"
    Jan 13 10:06:35.685: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220": Phase="Pending", Reason="", readiness=false. Elapsed: 17.477256ms
    Jan 13 10:06:35.685: INFO: The phase of Pod annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:06:37.709: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041255986s
    Jan 13 10:06:37.709: INFO: The phase of Pod annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:06:39.692: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220": Phase="Running", Reason="", readiness=true. Elapsed: 4.024240911s
    Jan 13 10:06:39.692: INFO: The phase of Pod annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220 is Running (Ready = true)
    Jan 13 10:06:39.692: INFO: Pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220" satisfied condition "running and ready"
    Jan 13 10:06:40.279: INFO: Successfully updated pod "annotationupdatee9e1da18-b4ae-46b4-9f31-a5ba1ad8a220"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:42.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6447" for this suite. 01/13/23 10:06:42.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:42.371
Jan 13 10:06:42.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:06:42.379
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:42.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:42.424
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/13/23 10:06:42.435
Jan 13 10:06:42.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 create -f -'
Jan 13 10:06:45.417: INFO: stderr: ""
Jan 13 10:06:45.417: INFO: stdout: "pod/pause created\n"
Jan 13 10:06:45.417: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 13 10:06:45.417: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1416" to be "running and ready"
Jan 13 10:06:45.460: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 42.967718ms
Jan 13 10:06:45.460: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.10.102.31-node' to be 'Running' but was 'Pending'
Jan 13 10:06:47.476: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059142823s
Jan 13 10:06:47.476: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.10.102.31-node' to be 'Running' but was 'Pending'
Jan 13 10:06:49.466: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.049228315s
Jan 13 10:06:49.466: INFO: Pod "pause" satisfied condition "running and ready"
Jan 13 10:06:49.466: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/13/23 10:06:49.466
Jan 13 10:06:49.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 label pods pause testing-label=testing-label-value'
Jan 13 10:06:49.957: INFO: stderr: ""
Jan 13 10:06:49.957: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/13/23 10:06:49.957
Jan 13 10:06:49.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get pod pause -L testing-label'
Jan 13 10:06:50.308: INFO: stderr: ""
Jan 13 10:06:50.308: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/13/23 10:06:50.308
Jan 13 10:06:50.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 label pods pause testing-label-'
Jan 13 10:06:50.717: INFO: stderr: ""
Jan 13 10:06:50.717: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/13/23 10:06:50.717
Jan 13 10:06:50.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get pod pause -L testing-label'
Jan 13 10:06:51.051: INFO: stderr: ""
Jan 13 10:06:51.051: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/13/23 10:06:51.052
Jan 13 10:06:51.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 delete --grace-period=0 --force -f -'
Jan 13 10:06:51.317: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:06:51.317: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 13 10:06:51.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get rc,svc -l name=pause --no-headers'
Jan 13 10:06:51.589: INFO: stderr: "No resources found in kubectl-1416 namespace.\n"
Jan 13 10:06:51.589: INFO: stdout: ""
Jan 13 10:06:51.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 13 10:06:51.799: INFO: stderr: ""
Jan 13 10:06:51.799: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:51.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1416" for this suite. 01/13/23 10:06:51.81
------------------------------
• [SLOW TEST] [9.447 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:42.371
    Jan 13 10:06:42.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:06:42.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:42.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:42.424
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/13/23 10:06:42.435
    Jan 13 10:06:42.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 create -f -'
    Jan 13 10:06:45.417: INFO: stderr: ""
    Jan 13 10:06:45.417: INFO: stdout: "pod/pause created\n"
    Jan 13 10:06:45.417: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 13 10:06:45.417: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1416" to be "running and ready"
    Jan 13 10:06:45.460: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 42.967718ms
    Jan 13 10:06:45.460: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.10.102.31-node' to be 'Running' but was 'Pending'
    Jan 13 10:06:47.476: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059142823s
    Jan 13 10:06:47.476: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.10.102.31-node' to be 'Running' but was 'Pending'
    Jan 13 10:06:49.466: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.049228315s
    Jan 13 10:06:49.466: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 13 10:06:49.466: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/13/23 10:06:49.466
    Jan 13 10:06:49.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 label pods pause testing-label=testing-label-value'
    Jan 13 10:06:49.957: INFO: stderr: ""
    Jan 13 10:06:49.957: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/13/23 10:06:49.957
    Jan 13 10:06:49.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get pod pause -L testing-label'
    Jan 13 10:06:50.308: INFO: stderr: ""
    Jan 13 10:06:50.308: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/13/23 10:06:50.308
    Jan 13 10:06:50.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 label pods pause testing-label-'
    Jan 13 10:06:50.717: INFO: stderr: ""
    Jan 13 10:06:50.717: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/13/23 10:06:50.717
    Jan 13 10:06:50.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get pod pause -L testing-label'
    Jan 13 10:06:51.051: INFO: stderr: ""
    Jan 13 10:06:51.051: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/13/23 10:06:51.052
    Jan 13 10:06:51.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 delete --grace-period=0 --force -f -'
    Jan 13 10:06:51.317: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:06:51.317: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 13 10:06:51.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get rc,svc -l name=pause --no-headers'
    Jan 13 10:06:51.589: INFO: stderr: "No resources found in kubectl-1416 namespace.\n"
    Jan 13 10:06:51.589: INFO: stdout: ""
    Jan 13 10:06:51.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1416 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 13 10:06:51.799: INFO: stderr: ""
    Jan 13 10:06:51.799: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:51.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1416" for this suite. 01/13/23 10:06:51.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:51.819
Jan 13 10:06:51.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename limitrange 01/13/23 10:06:51.822
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:51.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:51.853
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/13/23 10:06:51.859
STEP: Setting up watch 01/13/23 10:06:51.859
STEP: Submitting a LimitRange 01/13/23 10:06:51.964
STEP: Verifying LimitRange creation was observed 01/13/23 10:06:51.973
STEP: Fetching the LimitRange to ensure it has proper values 01/13/23 10:06:51.974
Jan 13 10:06:51.978: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 13 10:06:51.978: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/13/23 10:06:51.978
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/13/23 10:06:51.986
Jan 13 10:06:51.991: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 13 10:06:51.991: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/13/23 10:06:51.991
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/13/23 10:06:51.999
Jan 13 10:06:52.008: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 13 10:06:52.009: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/13/23 10:06:52.009
STEP: Failing to create a Pod with more than max resources 01/13/23 10:06:52.014
STEP: Updating a LimitRange 01/13/23 10:06:52.018
STEP: Verifying LimitRange updating is effective 01/13/23 10:06:52.027
STEP: Creating a Pod with less than former min resources 01/13/23 10:06:54.037
STEP: Failing to create a Pod with more than max resources 01/13/23 10:06:54.054
STEP: Deleting a LimitRange 01/13/23 10:06:54.06
STEP: Verifying the LimitRange was deleted 01/13/23 10:06:54.076
Jan 13 10:06:59.096: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/13/23 10:06:59.096
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:59.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6498" for this suite. 01/13/23 10:06:59.127
------------------------------
• [SLOW TEST] [7.321 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:51.819
    Jan 13 10:06:51.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename limitrange 01/13/23 10:06:51.822
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:51.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:51.853
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/13/23 10:06:51.859
    STEP: Setting up watch 01/13/23 10:06:51.859
    STEP: Submitting a LimitRange 01/13/23 10:06:51.964
    STEP: Verifying LimitRange creation was observed 01/13/23 10:06:51.973
    STEP: Fetching the LimitRange to ensure it has proper values 01/13/23 10:06:51.974
    Jan 13 10:06:51.978: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 13 10:06:51.978: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/13/23 10:06:51.978
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/13/23 10:06:51.986
    Jan 13 10:06:51.991: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 13 10:06:51.991: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/13/23 10:06:51.991
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/13/23 10:06:51.999
    Jan 13 10:06:52.008: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 13 10:06:52.009: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/13/23 10:06:52.009
    STEP: Failing to create a Pod with more than max resources 01/13/23 10:06:52.014
    STEP: Updating a LimitRange 01/13/23 10:06:52.018
    STEP: Verifying LimitRange updating is effective 01/13/23 10:06:52.027
    STEP: Creating a Pod with less than former min resources 01/13/23 10:06:54.037
    STEP: Failing to create a Pod with more than max resources 01/13/23 10:06:54.054
    STEP: Deleting a LimitRange 01/13/23 10:06:54.06
    STEP: Verifying the LimitRange was deleted 01/13/23 10:06:54.076
    Jan 13 10:06:59.096: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/13/23 10:06:59.096
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:59.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6498" for this suite. 01/13/23 10:06:59.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:59.147
Jan 13 10:06:59.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:06:59.15
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:59.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:59.195
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-ae854cd7-3316-48a7-95a6-abed2564bbe6 01/13/23 10:06:59.203
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:06:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6856" for this suite. 01/13/23 10:06:59.216
------------------------------
• [0.082 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:59.147
    Jan 13 10:06:59.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:06:59.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:59.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:59.195
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-ae854cd7-3316-48a7-95a6-abed2564bbe6 01/13/23 10:06:59.203
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:06:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6856" for this suite. 01/13/23 10:06:59.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:06:59.24
Jan 13 10:06:59.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:06:59.244
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:59.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:59.274
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 13 10:06:59.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: creating the pod 01/13/23 10:06:59.282
STEP: submitting the pod to kubernetes 01/13/23 10:06:59.283
Jan 13 10:06:59.297: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075" in namespace "pods-630" to be "running and ready"
Jan 13 10:06:59.304: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289669ms
Jan 13 10:06:59.304: INFO: The phase of Pod pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:07:01.313: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015770889s
Jan 13 10:07:01.313: INFO: The phase of Pod pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:07:03.314: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075": Phase="Running", Reason="", readiness=true. Elapsed: 4.016869864s
Jan 13 10:07:03.314: INFO: The phase of Pod pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075 is Running (Ready = true)
Jan 13 10:07:03.314: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:03.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-630" for this suite. 01/13/23 10:07:03.407
------------------------------
• [4.181 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:06:59.24
    Jan 13 10:06:59.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:06:59.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:06:59.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:06:59.274
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 13 10:06:59.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: creating the pod 01/13/23 10:06:59.282
    STEP: submitting the pod to kubernetes 01/13/23 10:06:59.283
    Jan 13 10:06:59.297: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075" in namespace "pods-630" to be "running and ready"
    Jan 13 10:06:59.304: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289669ms
    Jan 13 10:06:59.304: INFO: The phase of Pod pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:07:01.313: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015770889s
    Jan 13 10:07:01.313: INFO: The phase of Pod pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:07:03.314: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075": Phase="Running", Reason="", readiness=true. Elapsed: 4.016869864s
    Jan 13 10:07:03.314: INFO: The phase of Pod pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075 is Running (Ready = true)
    Jan 13 10:07:03.314: INFO: Pod "pod-logs-websocket-ea37edf3-8907-4525-997a-9f7e1b211075" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:03.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-630" for this suite. 01/13/23 10:07:03.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:03.426
Jan 13 10:07:03.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/13/23 10:07:03.43
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:03.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:03.493
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/13/23 10:07:03.5
STEP: Creating hostNetwork=false pod 01/13/23 10:07:03.5
Jan 13 10:07:03.524: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7326" to be "running and ready"
Jan 13 10:07:03.532: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.986393ms
Jan 13 10:07:03.532: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:07:05.544: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019800356s
Jan 13 10:07:05.544: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:07:07.542: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017745994s
Jan 13 10:07:07.542: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 13 10:07:07.542: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/13/23 10:07:07.55
Jan 13 10:07:07.566: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7326" to be "running and ready"
Jan 13 10:07:07.571: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.246634ms
Jan 13 10:07:07.571: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:07:09.581: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015672869s
Jan 13 10:07:09.581: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:07:11.577: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011859594s
Jan 13 10:07:11.578: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 13 10:07:11.578: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/13/23 10:07:11.582
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/13/23 10:07:11.582
Jan 13 10:07:11.583: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:11.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:11.584: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:11.584: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 13 10:07:11.807: INFO: Exec stderr: ""
Jan 13 10:07:11.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:11.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:11.809: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:11.809: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 13 10:07:12.022: INFO: Exec stderr: ""
Jan 13 10:07:12.022: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:12.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:12.023: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:12.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 13 10:07:12.198: INFO: Exec stderr: ""
Jan 13 10:07:12.198: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:12.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:12.199: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:12.200: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 13 10:07:12.505: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/13/23 10:07:12.505
Jan 13 10:07:12.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:12.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:12.507: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:12.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 13 10:07:12.812: INFO: Exec stderr: ""
Jan 13 10:07:12.813: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:12.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:12.814: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:12.814: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 13 10:07:12.998: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/13/23 10:07:12.998
Jan 13 10:07:12.998: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:12.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:13.000: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:13.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 13 10:07:13.196: INFO: Exec stderr: ""
Jan 13 10:07:13.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:13.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:13.197: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:13.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 13 10:07:13.357: INFO: Exec stderr: ""
Jan 13 10:07:13.357: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:13.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:13.358: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:13.359: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 13 10:07:13.520: INFO: Exec stderr: ""
Jan 13 10:07:13.520: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:07:13.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:07:13.522: INFO: ExecWithOptions: Clientset creation
Jan 13 10:07:13.522: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 13 10:07:13.701: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:13.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7326" for this suite. 01/13/23 10:07:13.718
------------------------------
• [SLOW TEST] [10.307 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:03.426
    Jan 13 10:07:03.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/13/23 10:07:03.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:03.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:03.493
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/13/23 10:07:03.5
    STEP: Creating hostNetwork=false pod 01/13/23 10:07:03.5
    Jan 13 10:07:03.524: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7326" to be "running and ready"
    Jan 13 10:07:03.532: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.986393ms
    Jan 13 10:07:03.532: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:07:05.544: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019800356s
    Jan 13 10:07:05.544: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:07:07.542: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017745994s
    Jan 13 10:07:07.542: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 13 10:07:07.542: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/13/23 10:07:07.55
    Jan 13 10:07:07.566: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7326" to be "running and ready"
    Jan 13 10:07:07.571: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.246634ms
    Jan 13 10:07:07.571: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:07:09.581: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015672869s
    Jan 13 10:07:09.581: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:07:11.577: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011859594s
    Jan 13 10:07:11.578: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 13 10:07:11.578: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/13/23 10:07:11.582
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/13/23 10:07:11.582
    Jan 13 10:07:11.583: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:11.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:11.584: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:11.584: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 13 10:07:11.807: INFO: Exec stderr: ""
    Jan 13 10:07:11.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:11.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:11.809: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:11.809: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 13 10:07:12.022: INFO: Exec stderr: ""
    Jan 13 10:07:12.022: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:12.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:12.023: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:12.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 13 10:07:12.198: INFO: Exec stderr: ""
    Jan 13 10:07:12.198: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:12.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:12.199: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:12.200: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 13 10:07:12.505: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/13/23 10:07:12.505
    Jan 13 10:07:12.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:12.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:12.507: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:12.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 13 10:07:12.812: INFO: Exec stderr: ""
    Jan 13 10:07:12.813: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:12.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:12.814: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:12.814: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 13 10:07:12.998: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/13/23 10:07:12.998
    Jan 13 10:07:12.998: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:12.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:13.000: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:13.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 13 10:07:13.196: INFO: Exec stderr: ""
    Jan 13 10:07:13.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:13.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:13.197: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:13.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 13 10:07:13.357: INFO: Exec stderr: ""
    Jan 13 10:07:13.357: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:13.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:13.358: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:13.359: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 13 10:07:13.520: INFO: Exec stderr: ""
    Jan 13 10:07:13.520: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7326 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:07:13.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:07:13.522: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:07:13.522: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7326/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 13 10:07:13.701: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:13.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-7326" for this suite. 01/13/23 10:07:13.718
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:13.734
Jan 13 10:07:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:07:13.736
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:13.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:13.794
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:07:13.841
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:07:15.612
STEP: Deploying the webhook pod 01/13/23 10:07:15.64
STEP: Wait for the deployment to be ready 01/13/23 10:07:15.729
Jan 13 10:07:15.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:07:17.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:07:19.829
STEP: Verifying the service has paired with the endpoint 01/13/23 10:07:19.929
Jan 13 10:07:20.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 13 10:07:20.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-812-crds.webhook.example.com via the AdmissionRegistration API 01/13/23 10:07:21.459
STEP: Creating a custom resource while v1 is storage version 01/13/23 10:07:21.496
STEP: Patching Custom Resource Definition to set v2 as storage 01/13/23 10:07:23.712
STEP: Patching the custom resource while v2 is storage version 01/13/23 10:07:23.852
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:24.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7333" for this suite. 01/13/23 10:07:24.652
STEP: Destroying namespace "webhook-7333-markers" for this suite. 01/13/23 10:07:24.673
------------------------------
• [SLOW TEST] [10.972 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:13.734
    Jan 13 10:07:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:07:13.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:13.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:13.794
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:07:13.841
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:07:15.612
    STEP: Deploying the webhook pod 01/13/23 10:07:15.64
    STEP: Wait for the deployment to be ready 01/13/23 10:07:15.729
    Jan 13 10:07:15.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:07:17.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 7, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:07:19.829
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:07:19.929
    Jan 13 10:07:20.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 13 10:07:20.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-812-crds.webhook.example.com via the AdmissionRegistration API 01/13/23 10:07:21.459
    STEP: Creating a custom resource while v1 is storage version 01/13/23 10:07:21.496
    STEP: Patching Custom Resource Definition to set v2 as storage 01/13/23 10:07:23.712
    STEP: Patching the custom resource while v2 is storage version 01/13/23 10:07:23.852
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:24.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7333" for this suite. 01/13/23 10:07:24.652
    STEP: Destroying namespace "webhook-7333-markers" for this suite. 01/13/23 10:07:24.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:24.764
Jan 13 10:07:24.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:07:24.767
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:24.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:24.882
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:07:24.897
Jan 13 10:07:24.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46" in namespace "projected-6686" to be "Succeeded or Failed"
Jan 13 10:07:24.930: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Pending", Reason="", readiness=false. Elapsed: 11.896079ms
Jan 13 10:07:26.946: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027541756s
Jan 13 10:07:28.948: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02961408s
Jan 13 10:07:30.948: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029209404s
STEP: Saw pod success 01/13/23 10:07:30.948
Jan 13 10:07:30.948: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46" satisfied condition "Succeeded or Failed"
Jan 13 10:07:30.958: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46 container client-container: <nil>
STEP: delete the pod 01/13/23 10:07:30.989
Jan 13 10:07:31.023: INFO: Waiting for pod downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46 to disappear
Jan 13 10:07:31.039: INFO: Pod downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:31.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6686" for this suite. 01/13/23 10:07:31.053
------------------------------
• [SLOW TEST] [6.313 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:24.764
    Jan 13 10:07:24.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:07:24.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:24.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:24.882
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:07:24.897
    Jan 13 10:07:24.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46" in namespace "projected-6686" to be "Succeeded or Failed"
    Jan 13 10:07:24.930: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Pending", Reason="", readiness=false. Elapsed: 11.896079ms
    Jan 13 10:07:26.946: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027541756s
    Jan 13 10:07:28.948: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02961408s
    Jan 13 10:07:30.948: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029209404s
    STEP: Saw pod success 01/13/23 10:07:30.948
    Jan 13 10:07:30.948: INFO: Pod "downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46" satisfied condition "Succeeded or Failed"
    Jan 13 10:07:30.958: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:07:30.989
    Jan 13 10:07:31.023: INFO: Waiting for pod downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46 to disappear
    Jan 13 10:07:31.039: INFO: Pod downwardapi-volume-51edc72b-8876-49d4-8f8e-579432555a46 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:31.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6686" for this suite. 01/13/23 10:07:31.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:31.077
Jan 13 10:07:31.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:07:31.082
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:31.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:31.126
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7088 01/13/23 10:07:31.141
STEP: changing the ExternalName service to type=ClusterIP 01/13/23 10:07:31.152
STEP: creating replication controller externalname-service in namespace services-7088 01/13/23 10:07:31.193
I0113 10:07:31.211097      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7088, replica count: 2
I0113 10:07:34.262704      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:07:37.264626      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 10:07:37.264: INFO: Creating new exec pod
Jan 13 10:07:37.275: INFO: Waiting up to 5m0s for pod "execpodwcp57" in namespace "services-7088" to be "running"
Jan 13 10:07:37.285: INFO: Pod "execpodwcp57": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57794ms
Jan 13 10:07:39.306: INFO: Pod "execpodwcp57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030534797s
Jan 13 10:07:41.292: INFO: Pod "execpodwcp57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016600038s
Jan 13 10:07:43.292: INFO: Pod "execpodwcp57": Phase="Running", Reason="", readiness=true. Elapsed: 6.016232557s
Jan 13 10:07:43.292: INFO: Pod "execpodwcp57" satisfied condition "running"
Jan 13 10:07:44.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7088 exec execpodwcp57 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 13 10:07:45.118: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 13 10:07:45.118: INFO: stdout: ""
Jan 13 10:07:45.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7088 exec execpodwcp57 -- /bin/sh -x -c nc -v -z -w 2 10.100.36.176 80'
Jan 13 10:07:45.646: INFO: stderr: "+ nc -v -z -w 2 10.100.36.176 80\nConnection to 10.100.36.176 80 port [tcp/http] succeeded!\n"
Jan 13 10:07:45.646: INFO: stdout: ""
Jan 13 10:07:45.646: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:45.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7088" for this suite. 01/13/23 10:07:45.754
------------------------------
• [SLOW TEST] [14.694 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:31.077
    Jan 13 10:07:31.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:07:31.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:31.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:31.126
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7088 01/13/23 10:07:31.141
    STEP: changing the ExternalName service to type=ClusterIP 01/13/23 10:07:31.152
    STEP: creating replication controller externalname-service in namespace services-7088 01/13/23 10:07:31.193
    I0113 10:07:31.211097      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7088, replica count: 2
    I0113 10:07:34.262704      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:07:37.264626      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 10:07:37.264: INFO: Creating new exec pod
    Jan 13 10:07:37.275: INFO: Waiting up to 5m0s for pod "execpodwcp57" in namespace "services-7088" to be "running"
    Jan 13 10:07:37.285: INFO: Pod "execpodwcp57": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57794ms
    Jan 13 10:07:39.306: INFO: Pod "execpodwcp57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030534797s
    Jan 13 10:07:41.292: INFO: Pod "execpodwcp57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016600038s
    Jan 13 10:07:43.292: INFO: Pod "execpodwcp57": Phase="Running", Reason="", readiness=true. Elapsed: 6.016232557s
    Jan 13 10:07:43.292: INFO: Pod "execpodwcp57" satisfied condition "running"
    Jan 13 10:07:44.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7088 exec execpodwcp57 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 13 10:07:45.118: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 13 10:07:45.118: INFO: stdout: ""
    Jan 13 10:07:45.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-7088 exec execpodwcp57 -- /bin/sh -x -c nc -v -z -w 2 10.100.36.176 80'
    Jan 13 10:07:45.646: INFO: stderr: "+ nc -v -z -w 2 10.100.36.176 80\nConnection to 10.100.36.176 80 port [tcp/http] succeeded!\n"
    Jan 13 10:07:45.646: INFO: stdout: ""
    Jan 13 10:07:45.646: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:45.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7088" for this suite. 01/13/23 10:07:45.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:45.783
Jan 13 10:07:45.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:07:45.785
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:45.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:45.828
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/13/23 10:07:45.836
Jan 13 10:07:45.881: INFO: Waiting up to 5m0s for pod "pod-3189b806-845e-4b14-b655-ec008009e164" in namespace "emptydir-1954" to be "Succeeded or Failed"
Jan 13 10:07:45.894: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Pending", Reason="", readiness=false. Elapsed: 12.489593ms
Jan 13 10:07:47.901: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020188394s
Jan 13 10:07:49.920: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039196556s
Jan 13 10:07:51.904: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022648336s
STEP: Saw pod success 01/13/23 10:07:51.904
Jan 13 10:07:51.904: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164" satisfied condition "Succeeded or Failed"
Jan 13 10:07:51.914: INFO: Trying to get logs from node 10.10.102.31-node pod pod-3189b806-845e-4b14-b655-ec008009e164 container test-container: <nil>
STEP: delete the pod 01/13/23 10:07:51.941
Jan 13 10:07:51.972: INFO: Waiting for pod pod-3189b806-845e-4b14-b655-ec008009e164 to disappear
Jan 13 10:07:51.982: INFO: Pod pod-3189b806-845e-4b14-b655-ec008009e164 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:51.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1954" for this suite. 01/13/23 10:07:51.995
------------------------------
• [SLOW TEST] [6.222 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:45.783
    Jan 13 10:07:45.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:07:45.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:45.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:45.828
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/13/23 10:07:45.836
    Jan 13 10:07:45.881: INFO: Waiting up to 5m0s for pod "pod-3189b806-845e-4b14-b655-ec008009e164" in namespace "emptydir-1954" to be "Succeeded or Failed"
    Jan 13 10:07:45.894: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Pending", Reason="", readiness=false. Elapsed: 12.489593ms
    Jan 13 10:07:47.901: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020188394s
    Jan 13 10:07:49.920: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039196556s
    Jan 13 10:07:51.904: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022648336s
    STEP: Saw pod success 01/13/23 10:07:51.904
    Jan 13 10:07:51.904: INFO: Pod "pod-3189b806-845e-4b14-b655-ec008009e164" satisfied condition "Succeeded or Failed"
    Jan 13 10:07:51.914: INFO: Trying to get logs from node 10.10.102.31-node pod pod-3189b806-845e-4b14-b655-ec008009e164 container test-container: <nil>
    STEP: delete the pod 01/13/23 10:07:51.941
    Jan 13 10:07:51.972: INFO: Waiting for pod pod-3189b806-845e-4b14-b655-ec008009e164 to disappear
    Jan 13 10:07:51.982: INFO: Pod pod-3189b806-845e-4b14-b655-ec008009e164 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:51.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1954" for this suite. 01/13/23 10:07:51.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:52.007
Jan 13 10:07:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:07:52.01
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:52.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:52.044
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-12a06c7a-97ce-4d83-888c-f1bde7a756a2 01/13/23 10:07:52.056
STEP: Creating a pod to test consume configMaps 01/13/23 10:07:52.067
Jan 13 10:07:52.084: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb" in namespace "projected-8188" to be "Succeeded or Failed"
Jan 13 10:07:52.102: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.800784ms
Jan 13 10:07:54.111: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027394242s
Jan 13 10:07:56.112: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027984888s
Jan 13 10:07:58.115: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031794892s
STEP: Saw pod success 01/13/23 10:07:58.116
Jan 13 10:07:58.116: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb" satisfied condition "Succeeded or Failed"
Jan 13 10:07:58.123: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:07:58.166
Jan 13 10:07:58.245: INFO: Waiting for pod pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb to disappear
Jan 13 10:07:58.267: INFO: Pod pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:58.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8188" for this suite. 01/13/23 10:07:58.278
------------------------------
• [SLOW TEST] [6.295 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:52.007
    Jan 13 10:07:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:07:52.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:52.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:52.044
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-12a06c7a-97ce-4d83-888c-f1bde7a756a2 01/13/23 10:07:52.056
    STEP: Creating a pod to test consume configMaps 01/13/23 10:07:52.067
    Jan 13 10:07:52.084: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb" in namespace "projected-8188" to be "Succeeded or Failed"
    Jan 13 10:07:52.102: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.800784ms
    Jan 13 10:07:54.111: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027394242s
    Jan 13 10:07:56.112: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027984888s
    Jan 13 10:07:58.115: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031794892s
    STEP: Saw pod success 01/13/23 10:07:58.116
    Jan 13 10:07:58.116: INFO: Pod "pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb" satisfied condition "Succeeded or Failed"
    Jan 13 10:07:58.123: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:07:58.166
    Jan 13 10:07:58.245: INFO: Waiting for pod pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb to disappear
    Jan 13 10:07:58.267: INFO: Pod pod-projected-configmaps-bf4374d8-2f0c-433a-b93c-c5da1523dafb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:58.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8188" for this suite. 01/13/23 10:07:58.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:58.304
Jan 13 10:07:58.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 10:07:58.311
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:58.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:58.375
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:07:58.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2208" for this suite. 01/13/23 10:07:58.501
------------------------------
• [0.215 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:58.304
    Jan 13 10:07:58.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 10:07:58.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:58.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:58.375
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:07:58.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2208" for this suite. 01/13/23 10:07:58.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:07:58.522
Jan 13 10:07:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:07:58.524
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:58.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:58.583
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-8832 01/13/23 10:07:58.604
STEP: creating service affinity-clusterip-transition in namespace services-8832 01/13/23 10:07:58.605
STEP: creating replication controller affinity-clusterip-transition in namespace services-8832 01/13/23 10:07:58.635
I0113 10:07:58.655402      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8832, replica count: 3
I0113 10:08:01.707621      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:08:04.709123      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 10:08:04.727: INFO: Creating new exec pod
Jan 13 10:08:04.737: INFO: Waiting up to 5m0s for pod "execpod-affinitydxmsm" in namespace "services-8832" to be "running"
Jan 13 10:08:04.745: INFO: Pod "execpod-affinitydxmsm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.598273ms
Jan 13 10:08:06.768: INFO: Pod "execpod-affinitydxmsm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031200163s
Jan 13 10:08:08.753: INFO: Pod "execpod-affinitydxmsm": Phase="Running", Reason="", readiness=true. Elapsed: 4.015853951s
Jan 13 10:08:08.753: INFO: Pod "execpod-affinitydxmsm" satisfied condition "running"
Jan 13 10:08:09.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 13 10:08:10.430: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 13 10:08:10.430: INFO: stdout: ""
Jan 13 10:08:10.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c nc -v -z -w 2 10.101.0.190 80'
Jan 13 10:08:11.195: INFO: stderr: "+ nc -v -z -w 2 10.101.0.190 80\nConnection to 10.101.0.190 80 port [tcp/http] succeeded!\n"
Jan 13 10:08:11.195: INFO: stdout: ""
Jan 13 10:08:11.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.0.190:80/ ; done'
Jan 13 10:08:12.121: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n"
Jan 13 10:08:12.121: INFO: stdout: "\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-m49vb\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-m49vb\naffinity-clusterip-transition-gjmt9"
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-m49vb
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-m49vb
Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:12.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.0.190:80/ ; done'
Jan 13 10:08:13.229: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n"
Jan 13 10:08:13.229: INFO: stdout: "\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9"
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
Jan 13 10:08:13.229: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8832, will wait for the garbage collector to delete the pods 01/13/23 10:08:13.254
Jan 13 10:08:13.353: INFO: Deleting ReplicationController affinity-clusterip-transition took: 38.028854ms
Jan 13 10:08:13.556: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 202.907559ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:08:17.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8832" for this suite. 01/13/23 10:08:17.449
------------------------------
• [SLOW TEST] [18.938 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:07:58.522
    Jan 13 10:07:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:07:58.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:07:58.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:07:58.583
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-8832 01/13/23 10:07:58.604
    STEP: creating service affinity-clusterip-transition in namespace services-8832 01/13/23 10:07:58.605
    STEP: creating replication controller affinity-clusterip-transition in namespace services-8832 01/13/23 10:07:58.635
    I0113 10:07:58.655402      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8832, replica count: 3
    I0113 10:08:01.707621      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:08:04.709123      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 10:08:04.727: INFO: Creating new exec pod
    Jan 13 10:08:04.737: INFO: Waiting up to 5m0s for pod "execpod-affinitydxmsm" in namespace "services-8832" to be "running"
    Jan 13 10:08:04.745: INFO: Pod "execpod-affinitydxmsm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.598273ms
    Jan 13 10:08:06.768: INFO: Pod "execpod-affinitydxmsm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031200163s
    Jan 13 10:08:08.753: INFO: Pod "execpod-affinitydxmsm": Phase="Running", Reason="", readiness=true. Elapsed: 4.015853951s
    Jan 13 10:08:08.753: INFO: Pod "execpod-affinitydxmsm" satisfied condition "running"
    Jan 13 10:08:09.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 13 10:08:10.430: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 13 10:08:10.430: INFO: stdout: ""
    Jan 13 10:08:10.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c nc -v -z -w 2 10.101.0.190 80'
    Jan 13 10:08:11.195: INFO: stderr: "+ nc -v -z -w 2 10.101.0.190 80\nConnection to 10.101.0.190 80 port [tcp/http] succeeded!\n"
    Jan 13 10:08:11.195: INFO: stdout: ""
    Jan 13 10:08:11.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.0.190:80/ ; done'
    Jan 13 10:08:12.121: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n"
    Jan 13 10:08:12.121: INFO: stdout: "\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-m49vb\naffinity-clusterip-transition-t7ndx\naffinity-clusterip-transition-m49vb\naffinity-clusterip-transition-gjmt9"
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-m49vb
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-t7ndx
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-m49vb
    Jan 13 10:08:12.121: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:12.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-8832 exec execpod-affinitydxmsm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.0.190:80/ ; done'
    Jan 13 10:08:13.229: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.0.190:80/\n"
    Jan 13 10:08:13.229: INFO: stdout: "\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9\naffinity-clusterip-transition-gjmt9"
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Received response from host: affinity-clusterip-transition-gjmt9
    Jan 13 10:08:13.229: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8832, will wait for the garbage collector to delete the pods 01/13/23 10:08:13.254
    Jan 13 10:08:13.353: INFO: Deleting ReplicationController affinity-clusterip-transition took: 38.028854ms
    Jan 13 10:08:13.556: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 202.907559ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:08:17.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8832" for this suite. 01/13/23 10:08:17.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:08:17.466
Jan 13 10:08:17.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:08:17.469
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:17.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:17.52
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/13/23 10:08:17.531
Jan 13 10:08:17.553: INFO: Waiting up to 5m0s for pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd" in namespace "pods-3491" to be "running and ready"
Jan 13 10:08:17.560: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204121ms
Jan 13 10:08:17.561: INFO: The phase of Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:08:19.580: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026745843s
Jan 13 10:08:19.580: INFO: The phase of Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:08:21.569: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd": Phase="Running", Reason="", readiness=true. Elapsed: 4.016103735s
Jan 13 10:08:21.569: INFO: The phase of Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd is Running (Ready = true)
Jan 13 10:08:21.569: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd" satisfied condition "running and ready"
Jan 13 10:08:21.589: INFO: Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd has hostIP: 10.10.102.31
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 10:08:21.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3491" for this suite. 01/13/23 10:08:21.598
------------------------------
• [4.145 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:08:17.466
    Jan 13 10:08:17.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:08:17.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:17.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:17.52
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/13/23 10:08:17.531
    Jan 13 10:08:17.553: INFO: Waiting up to 5m0s for pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd" in namespace "pods-3491" to be "running and ready"
    Jan 13 10:08:17.560: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204121ms
    Jan 13 10:08:17.561: INFO: The phase of Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:08:19.580: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026745843s
    Jan 13 10:08:19.580: INFO: The phase of Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:08:21.569: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd": Phase="Running", Reason="", readiness=true. Elapsed: 4.016103735s
    Jan 13 10:08:21.569: INFO: The phase of Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd is Running (Ready = true)
    Jan 13 10:08:21.569: INFO: Pod "pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd" satisfied condition "running and ready"
    Jan 13 10:08:21.589: INFO: Pod pod-hostip-5864e83f-8366-436f-b646-df5492ece6bd has hostIP: 10.10.102.31
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:08:21.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3491" for this suite. 01/13/23 10:08:21.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:08:21.612
Jan 13 10:08:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:08:21.615
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:21.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:21.671
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:08:21.704
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:08:24.359
STEP: Deploying the webhook pod 01/13/23 10:08:24.374
STEP: Wait for the deployment to be ready 01/13/23 10:08:24.395
Jan 13 10:08:24.408: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:08:26.493: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:08:28.5
STEP: Verifying the service has paired with the endpoint 01/13/23 10:08:28.516
Jan 13 10:08:29.517: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/13/23 10:08:29.524
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/13/23 10:08:29.529
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/13/23 10:08:29.529
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/13/23 10:08:29.529
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/13/23 10:08:29.533
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/13/23 10:08:29.533
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/13/23 10:08:29.537
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:08:29.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6112" for this suite. 01/13/23 10:08:29.65
STEP: Destroying namespace "webhook-6112-markers" for this suite. 01/13/23 10:08:29.688
------------------------------
• [SLOW TEST] [8.110 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:08:21.612
    Jan 13 10:08:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:08:21.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:21.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:21.671
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:08:21.704
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:08:24.359
    STEP: Deploying the webhook pod 01/13/23 10:08:24.374
    STEP: Wait for the deployment to be ready 01/13/23 10:08:24.395
    Jan 13 10:08:24.408: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:08:26.493: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:08:28.5
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:08:28.516
    Jan 13 10:08:29.517: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/13/23 10:08:29.524
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/13/23 10:08:29.529
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/13/23 10:08:29.529
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/13/23 10:08:29.529
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/13/23 10:08:29.533
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/13/23 10:08:29.533
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/13/23 10:08:29.537
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:08:29.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6112" for this suite. 01/13/23 10:08:29.65
    STEP: Destroying namespace "webhook-6112-markers" for this suite. 01/13/23 10:08:29.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:08:29.724
Jan 13 10:08:29.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:08:29.732
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:29.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:29.847
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/13/23 10:08:29.855
Jan 13 10:08:29.884: INFO: Waiting up to 5m0s for pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d" in namespace "emptydir-2550" to be "Succeeded or Failed"
Jan 13 10:08:29.904: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.55799ms
Jan 13 10:08:31.910: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026081046s
Jan 13 10:08:33.929: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045066739s
Jan 13 10:08:35.910: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026072167s
Jan 13 10:08:37.914: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029920757s
STEP: Saw pod success 01/13/23 10:08:37.914
Jan 13 10:08:37.914: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d" satisfied condition "Succeeded or Failed"
Jan 13 10:08:37.920: INFO: Trying to get logs from node 10.10.102.31-node pod pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d container test-container: <nil>
STEP: delete the pod 01/13/23 10:08:37.958
Jan 13 10:08:37.970: INFO: Waiting for pod pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d to disappear
Jan 13 10:08:37.975: INFO: Pod pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:08:37.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2550" for this suite. 01/13/23 10:08:38.003
------------------------------
• [SLOW TEST] [8.307 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:08:29.724
    Jan 13 10:08:29.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:08:29.732
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:29.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:29.847
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/13/23 10:08:29.855
    Jan 13 10:08:29.884: INFO: Waiting up to 5m0s for pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d" in namespace "emptydir-2550" to be "Succeeded or Failed"
    Jan 13 10:08:29.904: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.55799ms
    Jan 13 10:08:31.910: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026081046s
    Jan 13 10:08:33.929: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045066739s
    Jan 13 10:08:35.910: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026072167s
    Jan 13 10:08:37.914: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029920757s
    STEP: Saw pod success 01/13/23 10:08:37.914
    Jan 13 10:08:37.914: INFO: Pod "pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d" satisfied condition "Succeeded or Failed"
    Jan 13 10:08:37.920: INFO: Trying to get logs from node 10.10.102.31-node pod pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d container test-container: <nil>
    STEP: delete the pod 01/13/23 10:08:37.958
    Jan 13 10:08:37.970: INFO: Waiting for pod pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d to disappear
    Jan 13 10:08:37.975: INFO: Pod pod-57c2095f-57e5-4bce-8fd9-bbf4d6a8bf0d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:08:37.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2550" for this suite. 01/13/23 10:08:38.003
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:08:38.033
Jan 13 10:08:38.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubelet-test 01/13/23 10:08:38.035
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:38.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:38.077
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:08:42.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2109" for this suite. 01/13/23 10:08:42.203
------------------------------
• [4.197 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:08:38.033
    Jan 13 10:08:38.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubelet-test 01/13/23 10:08:38.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:38.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:38.077
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:08:42.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2109" for this suite. 01/13/23 10:08:42.203
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:08:42.23
Jan 13 10:08:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:08:42.232
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:42.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:42.28
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:08:42.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9893" for this suite. 01/13/23 10:08:42.418
------------------------------
• [0.208 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:08:42.23
    Jan 13 10:08:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:08:42.232
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:42.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:42.28
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:08:42.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9893" for this suite. 01/13/23 10:08:42.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:08:42.441
Jan 13 10:08:42.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 10:08:42.443
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:42.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:42.502
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/13/23 10:08:42.528
Jan 13 10:08:42.564: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1020" to be "running and ready"
Jan 13 10:08:42.575: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.809239ms
Jan 13 10:08:42.575: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:08:44.586: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02242525s
Jan 13 10:08:44.586: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:08:46.593: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.029363205s
Jan 13 10:08:46.593: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 13 10:08:46.593: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/13/23 10:08:46.608
Jan 13 10:08:46.621: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1020" to be "running and ready"
Jan 13 10:08:46.629: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.953291ms
Jan 13 10:08:46.629: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:08:48.638: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016539462s
Jan 13 10:08:48.638: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:08:50.640: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.018114974s
Jan 13 10:08:50.640: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 13 10:08:50.640: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/13/23 10:08:50.69
Jan 13 10:08:50.725: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 10:08:50.740: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 10:08:52.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 10:08:52.753: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 10:08:54.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 10:08:54.750: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/13/23 10:08:54.75
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 13 10:08:54.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1020" for this suite. 01/13/23 10:08:54.812
------------------------------
• [SLOW TEST] [12.386 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:08:42.441
    Jan 13 10:08:42.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 10:08:42.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:42.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:42.502
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/13/23 10:08:42.528
    Jan 13 10:08:42.564: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1020" to be "running and ready"
    Jan 13 10:08:42.575: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.809239ms
    Jan 13 10:08:42.575: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:08:44.586: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02242525s
    Jan 13 10:08:44.586: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:08:46.593: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.029363205s
    Jan 13 10:08:46.593: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 13 10:08:46.593: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/13/23 10:08:46.608
    Jan 13 10:08:46.621: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1020" to be "running and ready"
    Jan 13 10:08:46.629: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.953291ms
    Jan 13 10:08:46.629: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:08:48.638: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016539462s
    Jan 13 10:08:48.638: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:08:50.640: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.018114974s
    Jan 13 10:08:50.640: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 13 10:08:50.640: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/13/23 10:08:50.69
    Jan 13 10:08:50.725: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 13 10:08:50.740: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 13 10:08:52.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 13 10:08:52.753: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 13 10:08:54.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 13 10:08:54.750: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/13/23 10:08:54.75
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:08:54.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1020" for this suite. 01/13/23 10:08:54.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:08:54.836
Jan 13 10:08:54.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:08:54.843
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:54.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:54.892
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:08:54.96
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:08:56.656
STEP: Deploying the webhook pod 01/13/23 10:08:56.679
STEP: Wait for the deployment to be ready 01/13/23 10:08:56.724
Jan 13 10:08:56.785: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:08:58.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:09:00.826
STEP: Verifying the service has paired with the endpoint 01/13/23 10:09:00.923
Jan 13 10:09:01.924: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/13/23 10:09:01.937
STEP: create a configmap that should be updated by the webhook 01/13/23 10:09:01.994
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:09:02.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2107" for this suite. 01/13/23 10:09:02.124
STEP: Destroying namespace "webhook-2107-markers" for this suite. 01/13/23 10:09:02.134
------------------------------
• [SLOW TEST] [7.326 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:08:54.836
    Jan 13 10:08:54.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:08:54.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:08:54.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:08:54.892
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:08:54.96
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:08:56.656
    STEP: Deploying the webhook pod 01/13/23 10:08:56.679
    STEP: Wait for the deployment to be ready 01/13/23 10:08:56.724
    Jan 13 10:08:56.785: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:08:58.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 8, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:09:00.826
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:09:00.923
    Jan 13 10:09:01.924: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/13/23 10:09:01.937
    STEP: create a configmap that should be updated by the webhook 01/13/23 10:09:01.994
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:09:02.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2107" for this suite. 01/13/23 10:09:02.124
    STEP: Destroying namespace "webhook-2107-markers" for this suite. 01/13/23 10:09:02.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:09:02.167
Jan 13 10:09:02.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-runtime 01/13/23 10:09:02.17
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:09:02.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:09:02.208
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/13/23 10:09:02.236
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/13/23 10:09:21.49
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/13/23 10:09:21.498
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/13/23 10:09:21.513
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/13/23 10:09:21.513
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/13/23 10:09:21.556
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/13/23 10:09:25.598
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/13/23 10:09:27.619
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/13/23 10:09:27.639
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/13/23 10:09:27.639
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/13/23 10:09:27.71
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/13/23 10:09:28.728
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/13/23 10:09:33.775
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/13/23 10:09:33.787
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/13/23 10:09:33.788
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 13 10:09:33.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1321" for this suite. 01/13/23 10:09:33.85
------------------------------
• [SLOW TEST] [31.703 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:09:02.167
    Jan 13 10:09:02.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-runtime 01/13/23 10:09:02.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:09:02.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:09:02.208
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/13/23 10:09:02.236
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/13/23 10:09:21.49
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/13/23 10:09:21.498
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/13/23 10:09:21.513
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/13/23 10:09:21.513
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/13/23 10:09:21.556
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/13/23 10:09:25.598
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/13/23 10:09:27.619
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/13/23 10:09:27.639
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/13/23 10:09:27.639
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/13/23 10:09:27.71
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/13/23 10:09:28.728
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/13/23 10:09:33.775
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/13/23 10:09:33.787
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/13/23 10:09:33.788
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:09:33.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1321" for this suite. 01/13/23 10:09:33.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:09:33.876
Jan 13 10:09:33.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:09:33.878
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:09:33.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:09:33.913
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:09:33.918
Jan 13 10:09:33.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86" in namespace "projected-9302" to be "Succeeded or Failed"
Jan 13 10:09:33.941: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371166ms
Jan 13 10:09:35.955: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021151238s
Jan 13 10:09:37.952: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Running", Reason="", readiness=false. Elapsed: 4.01822884s
Jan 13 10:09:39.954: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019360983s
STEP: Saw pod success 01/13/23 10:09:39.954
Jan 13 10:09:39.954: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86" satisfied condition "Succeeded or Failed"
Jan 13 10:09:39.967: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86 container client-container: <nil>
STEP: delete the pod 01/13/23 10:09:39.998
Jan 13 10:09:40.039: INFO: Waiting for pod downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86 to disappear
Jan 13 10:09:40.053: INFO: Pod downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:09:40.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9302" for this suite. 01/13/23 10:09:40.066
------------------------------
• [SLOW TEST] [6.209 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:09:33.876
    Jan 13 10:09:33.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:09:33.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:09:33.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:09:33.913
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:09:33.918
    Jan 13 10:09:33.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86" in namespace "projected-9302" to be "Succeeded or Failed"
    Jan 13 10:09:33.941: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371166ms
    Jan 13 10:09:35.955: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021151238s
    Jan 13 10:09:37.952: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Running", Reason="", readiness=false. Elapsed: 4.01822884s
    Jan 13 10:09:39.954: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019360983s
    STEP: Saw pod success 01/13/23 10:09:39.954
    Jan 13 10:09:39.954: INFO: Pod "downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86" satisfied condition "Succeeded or Failed"
    Jan 13 10:09:39.967: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:09:39.998
    Jan 13 10:09:40.039: INFO: Waiting for pod downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86 to disappear
    Jan 13 10:09:40.053: INFO: Pod downwardapi-volume-0125e23f-44ed-4a8a-9e2e-20ca4be6db86 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:09:40.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9302" for this suite. 01/13/23 10:09:40.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:09:40.099
Jan 13 10:09:40.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 10:09:40.103
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:09:40.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:09:40.153
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/13/23 10:09:40.16
Jan 13 10:09:40.185: INFO: Waiting up to 2m0s for pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" in namespace "var-expansion-9367" to be "running"
Jan 13 10:09:40.199: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 13.059165ms
Jan 13 10:09:42.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023279787s
Jan 13 10:09:44.215: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029513087s
Jan 13 10:09:46.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021965259s
Jan 13 10:09:48.213: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026959484s
Jan 13 10:09:50.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026661302s
Jan 13 10:09:52.210: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023955612s
Jan 13 10:09:54.204: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 14.018886734s
Jan 13 10:09:56.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022679953s
Jan 13 10:09:58.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026873262s
Jan 13 10:10:00.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 20.021849843s
Jan 13 10:10:02.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020884854s
Jan 13 10:10:04.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020423056s
Jan 13 10:10:06.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01997603s
Jan 13 10:10:08.239: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 28.053080298s
Jan 13 10:10:10.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 30.021261334s
Jan 13 10:10:12.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 32.022450203s
Jan 13 10:10:14.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023897101s
Jan 13 10:10:16.210: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 36.024642212s
Jan 13 10:10:18.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026817833s
Jan 13 10:10:20.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 40.021994301s
Jan 13 10:10:22.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021297933s
Jan 13 10:10:24.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023449141s
Jan 13 10:10:26.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023609578s
Jan 13 10:10:28.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 48.019160685s
Jan 13 10:10:30.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 50.025801038s
Jan 13 10:10:32.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 52.020667716s
Jan 13 10:10:34.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019825518s
Jan 13 10:10:36.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021105828s
Jan 13 10:10:38.214: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 58.028188574s
Jan 13 10:10:40.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020686349s
Jan 13 10:10:42.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021532825s
Jan 13 10:10:44.245: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.059946288s
Jan 13 10:10:46.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021011696s
Jan 13 10:10:48.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.021344127s
Jan 13 10:10:50.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.021294836s
Jan 13 10:10:52.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.02110854s
Jan 13 10:10:54.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021742529s
Jan 13 10:10:56.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021325572s
Jan 13 10:10:58.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019865147s
Jan 13 10:11:00.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024959948s
Jan 13 10:11:02.215: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029546043s
Jan 13 10:11:04.220: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.034220364s
Jan 13 10:11:06.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.020489845s
Jan 13 10:11:08.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.0252945s
Jan 13 10:11:10.217: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.031799765s
Jan 13 10:11:12.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020294681s
Jan 13 10:11:14.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01989397s
Jan 13 10:11:16.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.021308122s
Jan 13 10:11:18.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022245719s
Jan 13 10:11:20.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018998828s
Jan 13 10:11:22.218: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.032283872s
Jan 13 10:11:24.220: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.034339394s
Jan 13 10:11:26.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025086019s
Jan 13 10:11:28.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.020042622s
Jan 13 10:11:30.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022643226s
Jan 13 10:11:32.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.026080223s
Jan 13 10:11:34.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026858056s
Jan 13 10:11:36.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.021593498s
Jan 13 10:11:38.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018996051s
Jan 13 10:11:40.210: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024138059s
Jan 13 10:11:40.216: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03035447s
STEP: updating the pod 01/13/23 10:11:40.216
Jan 13 10:11:40.756: INFO: Successfully updated pod "var-expansion-97752652-976f-4214-9e6f-54d560203587"
STEP: waiting for pod running 01/13/23 10:11:40.757
Jan 13 10:11:40.757: INFO: Waiting up to 2m0s for pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" in namespace "var-expansion-9367" to be "running"
Jan 13 10:11:40.776: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 18.97121ms
Jan 13 10:11:42.784: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Running", Reason="", readiness=true. Elapsed: 2.027282072s
Jan 13 10:11:42.784: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" satisfied condition "running"
STEP: deleting the pod gracefully 01/13/23 10:11:42.785
Jan 13 10:11:42.785: INFO: Deleting pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" in namespace "var-expansion-9367"
Jan 13 10:11:42.802: INFO: Wait up to 5m0s for pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 10:12:14.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9367" for this suite. 01/13/23 10:12:14.84
------------------------------
• [SLOW TEST] [154.768 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:09:40.099
    Jan 13 10:09:40.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 10:09:40.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:09:40.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:09:40.153
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/13/23 10:09:40.16
    Jan 13 10:09:40.185: INFO: Waiting up to 2m0s for pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" in namespace "var-expansion-9367" to be "running"
    Jan 13 10:09:40.199: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 13.059165ms
    Jan 13 10:09:42.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023279787s
    Jan 13 10:09:44.215: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029513087s
    Jan 13 10:09:46.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021965259s
    Jan 13 10:09:48.213: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026959484s
    Jan 13 10:09:50.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026661302s
    Jan 13 10:09:52.210: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023955612s
    Jan 13 10:09:54.204: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 14.018886734s
    Jan 13 10:09:56.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022679953s
    Jan 13 10:09:58.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026873262s
    Jan 13 10:10:00.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 20.021849843s
    Jan 13 10:10:02.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020884854s
    Jan 13 10:10:04.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020423056s
    Jan 13 10:10:06.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01997603s
    Jan 13 10:10:08.239: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 28.053080298s
    Jan 13 10:10:10.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 30.021261334s
    Jan 13 10:10:12.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 32.022450203s
    Jan 13 10:10:14.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023897101s
    Jan 13 10:10:16.210: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 36.024642212s
    Jan 13 10:10:18.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026817833s
    Jan 13 10:10:20.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 40.021994301s
    Jan 13 10:10:22.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021297933s
    Jan 13 10:10:24.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023449141s
    Jan 13 10:10:26.209: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023609578s
    Jan 13 10:10:28.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 48.019160685s
    Jan 13 10:10:30.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 50.025801038s
    Jan 13 10:10:32.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 52.020667716s
    Jan 13 10:10:34.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019825518s
    Jan 13 10:10:36.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021105828s
    Jan 13 10:10:38.214: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 58.028188574s
    Jan 13 10:10:40.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020686349s
    Jan 13 10:10:42.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021532825s
    Jan 13 10:10:44.245: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.059946288s
    Jan 13 10:10:46.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021011696s
    Jan 13 10:10:48.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.021344127s
    Jan 13 10:10:50.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.021294836s
    Jan 13 10:10:52.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.02110854s
    Jan 13 10:10:54.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021742529s
    Jan 13 10:10:56.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021325572s
    Jan 13 10:10:58.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019865147s
    Jan 13 10:11:00.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024959948s
    Jan 13 10:11:02.215: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029546043s
    Jan 13 10:11:04.220: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.034220364s
    Jan 13 10:11:06.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.020489845s
    Jan 13 10:11:08.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.0252945s
    Jan 13 10:11:10.217: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.031799765s
    Jan 13 10:11:12.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020294681s
    Jan 13 10:11:14.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01989397s
    Jan 13 10:11:16.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.021308122s
    Jan 13 10:11:18.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022245719s
    Jan 13 10:11:20.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.018998828s
    Jan 13 10:11:22.218: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.032283872s
    Jan 13 10:11:24.220: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.034339394s
    Jan 13 10:11:26.211: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025086019s
    Jan 13 10:11:28.206: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.020042622s
    Jan 13 10:11:30.208: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.022643226s
    Jan 13 10:11:32.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.026080223s
    Jan 13 10:11:34.212: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026858056s
    Jan 13 10:11:36.207: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.021593498s
    Jan 13 10:11:38.205: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018996051s
    Jan 13 10:11:40.210: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024138059s
    Jan 13 10:11:40.216: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03035447s
    STEP: updating the pod 01/13/23 10:11:40.216
    Jan 13 10:11:40.756: INFO: Successfully updated pod "var-expansion-97752652-976f-4214-9e6f-54d560203587"
    STEP: waiting for pod running 01/13/23 10:11:40.757
    Jan 13 10:11:40.757: INFO: Waiting up to 2m0s for pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" in namespace "var-expansion-9367" to be "running"
    Jan 13 10:11:40.776: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Pending", Reason="", readiness=false. Elapsed: 18.97121ms
    Jan 13 10:11:42.784: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587": Phase="Running", Reason="", readiness=true. Elapsed: 2.027282072s
    Jan 13 10:11:42.784: INFO: Pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" satisfied condition "running"
    STEP: deleting the pod gracefully 01/13/23 10:11:42.785
    Jan 13 10:11:42.785: INFO: Deleting pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" in namespace "var-expansion-9367"
    Jan 13 10:11:42.802: INFO: Wait up to 5m0s for pod "var-expansion-97752652-976f-4214-9e6f-54d560203587" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:12:14.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9367" for this suite. 01/13/23 10:12:14.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:12:14.872
Jan 13 10:12:14.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename podtemplate 01/13/23 10:12:14.875
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:14.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:14.937
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 13 10:12:15.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3333" for this suite. 01/13/23 10:12:15.049
------------------------------
• [0.191 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:12:14.872
    Jan 13 10:12:14.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename podtemplate 01/13/23 10:12:14.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:14.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:14.937
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:12:15.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3333" for this suite. 01/13/23 10:12:15.049
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:12:15.069
Jan 13 10:12:15.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 10:12:15.075
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:15.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:15.111
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-8ba6cdc3-f12b-4f8d-bbbe-570f40bad4a4 01/13/23 10:12:15.122
STEP: Creating a pod to test consume configMaps 01/13/23 10:12:15.132
Jan 13 10:12:15.153: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c" in namespace "configmap-7741" to be "Succeeded or Failed"
Jan 13 10:12:15.162: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.979021ms
Jan 13 10:12:17.172: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019292824s
Jan 13 10:12:19.170: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017426973s
Jan 13 10:12:21.174: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020935073s
Jan 13 10:12:23.169: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016423751s
STEP: Saw pod success 01/13/23 10:12:23.17
Jan 13 10:12:23.170: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c" satisfied condition "Succeeded or Failed"
Jan 13 10:12:23.176: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:12:23.225
Jan 13 10:12:23.244: INFO: Waiting for pod pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c to disappear
Jan 13 10:12:23.251: INFO: Pod pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:12:23.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7741" for this suite. 01/13/23 10:12:23.259
------------------------------
• [SLOW TEST] [8.202 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:12:15.069
    Jan 13 10:12:15.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 10:12:15.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:15.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:15.111
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-8ba6cdc3-f12b-4f8d-bbbe-570f40bad4a4 01/13/23 10:12:15.122
    STEP: Creating a pod to test consume configMaps 01/13/23 10:12:15.132
    Jan 13 10:12:15.153: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c" in namespace "configmap-7741" to be "Succeeded or Failed"
    Jan 13 10:12:15.162: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.979021ms
    Jan 13 10:12:17.172: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019292824s
    Jan 13 10:12:19.170: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017426973s
    Jan 13 10:12:21.174: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020935073s
    Jan 13 10:12:23.169: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.016423751s
    STEP: Saw pod success 01/13/23 10:12:23.17
    Jan 13 10:12:23.170: INFO: Pod "pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c" satisfied condition "Succeeded or Failed"
    Jan 13 10:12:23.176: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:12:23.225
    Jan 13 10:12:23.244: INFO: Waiting for pod pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c to disappear
    Jan 13 10:12:23.251: INFO: Pod pod-configmaps-bd1508e1-d105-46b0-85ee-51b19c62bf7c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:12:23.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7741" for this suite. 01/13/23 10:12:23.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:12:23.273
Jan 13 10:12:23.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:12:23.279
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:23.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:23.311
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-9775a90b-347f-451a-9aa5-a76e0e7f0cf8 01/13/23 10:12:23.321
STEP: Creating a pod to test consume secrets 01/13/23 10:12:23.333
Jan 13 10:12:23.351: INFO: Waiting up to 5m0s for pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1" in namespace "secrets-2763" to be "Succeeded or Failed"
Jan 13 10:12:23.356: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.824667ms
Jan 13 10:12:25.375: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024702428s
Jan 13 10:12:27.367: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016457538s
Jan 13 10:12:29.369: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018481196s
STEP: Saw pod success 01/13/23 10:12:29.369
Jan 13 10:12:29.369: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1" satisfied condition "Succeeded or Failed"
Jan 13 10:12:29.378: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1 container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 10:12:29.399
Jan 13 10:12:29.455: INFO: Waiting for pod pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1 to disappear
Jan 13 10:12:29.462: INFO: Pod pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:12:29.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2763" for this suite. 01/13/23 10:12:29.478
------------------------------
• [SLOW TEST] [6.232 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:12:23.273
    Jan 13 10:12:23.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:12:23.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:23.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:23.311
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-9775a90b-347f-451a-9aa5-a76e0e7f0cf8 01/13/23 10:12:23.321
    STEP: Creating a pod to test consume secrets 01/13/23 10:12:23.333
    Jan 13 10:12:23.351: INFO: Waiting up to 5m0s for pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1" in namespace "secrets-2763" to be "Succeeded or Failed"
    Jan 13 10:12:23.356: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.824667ms
    Jan 13 10:12:25.375: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024702428s
    Jan 13 10:12:27.367: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016457538s
    Jan 13 10:12:29.369: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018481196s
    STEP: Saw pod success 01/13/23 10:12:29.369
    Jan 13 10:12:29.369: INFO: Pod "pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1" satisfied condition "Succeeded or Failed"
    Jan 13 10:12:29.378: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1 container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:12:29.399
    Jan 13 10:12:29.455: INFO: Waiting for pod pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1 to disappear
    Jan 13 10:12:29.462: INFO: Pod pod-secrets-e86fd8ad-2fef-4eb5-a303-6d97b89ce3e1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:12:29.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2763" for this suite. 01/13/23 10:12:29.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:12:29.508
Jan 13 10:12:29.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename security-context-test 01/13/23 10:12:29.511
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:29.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:29.563
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 13 10:12:29.606: INFO: Waiting up to 5m0s for pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45" in namespace "security-context-test-2900" to be "Succeeded or Failed"
Jan 13 10:12:29.621: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 15.319213ms
Jan 13 10:12:31.630: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024153705s
Jan 13 10:12:33.628: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022521858s
Jan 13 10:12:35.637: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030925107s
Jan 13 10:12:37.633: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026896596s
Jan 13 10:12:37.633: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 13 10:12:37.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2900" for this suite. 01/13/23 10:12:37.645
------------------------------
• [SLOW TEST] [8.155 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:12:29.508
    Jan 13 10:12:29.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename security-context-test 01/13/23 10:12:29.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:29.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:29.563
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 13 10:12:29.606: INFO: Waiting up to 5m0s for pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45" in namespace "security-context-test-2900" to be "Succeeded or Failed"
    Jan 13 10:12:29.621: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 15.319213ms
    Jan 13 10:12:31.630: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024153705s
    Jan 13 10:12:33.628: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022521858s
    Jan 13 10:12:35.637: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030925107s
    Jan 13 10:12:37.633: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026896596s
    Jan 13 10:12:37.633: INFO: Pod "busybox-user-65534-79c07544-81e9-40ad-a734-a147c43f6b45" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:12:37.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2900" for this suite. 01/13/23 10:12:37.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:12:37.666
Jan 13 10:12:37.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename daemonsets 01/13/23 10:12:37.669
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:37.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:37.73
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan 13 10:12:37.844: INFO: Create a RollingUpdate DaemonSet
Jan 13 10:12:37.892: INFO: Check that daemon pods launch on every node of the cluster
Jan 13 10:12:37.924: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:37.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:12:37.935: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 10:12:38.971: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:38.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:12:38.981: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 10:12:39.945: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:39.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:12:39.954: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 10:12:40.951: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:40.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:12:40.959: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 10:12:41.955: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:41.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 10:12:41.965: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 13 10:12:41.965: INFO: Update the DaemonSet to trigger a rollout
Jan 13 10:12:41.981: INFO: Updating DaemonSet daemon-set
Jan 13 10:12:47.027: INFO: Roll back the DaemonSet before rollout is complete
Jan 13 10:12:47.054: INFO: Updating DaemonSet daemon-set
Jan 13 10:12:47.054: INFO: Make sure DaemonSet rollback is complete
Jan 13 10:12:47.066: INFO: Wrong image for pod: daemon-set-jbzjv. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan 13 10:12:47.066: INFO: Pod daemon-set-jbzjv is not available
Jan 13 10:12:47.076: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:48.097: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:49.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:50.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:51.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:52.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:53.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:54.201: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:55.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:56.091: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:57.132: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:58.116: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:12:59.101: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:00.118: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:01.114: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:02.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:03.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:04.099: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:05.110: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:06.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:07.120: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:08.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:09.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:10.111: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:11.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:12.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:13.109: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:14.102: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:15.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:16.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:17.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:18.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:19.117: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:20.093: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:21.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:22.110: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:23.103: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:24.103: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:25.099: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:26.093: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:27.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:28.090: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:29.109: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:30.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:31.113: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:32.091: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:33.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:34.105: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:35.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:36.090: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:37.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:38.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:39.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:40.109: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:41.099: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:42.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:43.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:44.120: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:45.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:46.091: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:47.104: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:48.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:49.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:50.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:51.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:52.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:53.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:13:54.087: INFO: Pod daemon-set-97lgf is not available
Jan 13 10:13:54.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/13/23 10:13:54.111
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1742, will wait for the garbage collector to delete the pods 01/13/23 10:13:54.111
Jan 13 10:13:54.181: INFO: Deleting DaemonSet.extensions daemon-set took: 15.638293ms
Jan 13 10:13:54.282: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.631564ms
Jan 13 10:13:56.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:13:56.689: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 13 10:13:56.697: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"591445"},"items":null}

Jan 13 10:13:56.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"591445"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:13:56.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1742" for this suite. 01/13/23 10:13:56.776
------------------------------
• [SLOW TEST] [79.121 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:12:37.666
    Jan 13 10:12:37.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename daemonsets 01/13/23 10:12:37.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:12:37.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:12:37.73
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan 13 10:12:37.844: INFO: Create a RollingUpdate DaemonSet
    Jan 13 10:12:37.892: INFO: Check that daemon pods launch on every node of the cluster
    Jan 13 10:12:37.924: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:37.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:12:37.935: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 10:12:38.971: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:38.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:12:38.981: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 10:12:39.945: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:39.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:12:39.954: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 10:12:40.951: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:40.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:12:40.959: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 10:12:41.955: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:41.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 10:12:41.965: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 13 10:12:41.965: INFO: Update the DaemonSet to trigger a rollout
    Jan 13 10:12:41.981: INFO: Updating DaemonSet daemon-set
    Jan 13 10:12:47.027: INFO: Roll back the DaemonSet before rollout is complete
    Jan 13 10:12:47.054: INFO: Updating DaemonSet daemon-set
    Jan 13 10:12:47.054: INFO: Make sure DaemonSet rollback is complete
    Jan 13 10:12:47.066: INFO: Wrong image for pod: daemon-set-jbzjv. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan 13 10:12:47.066: INFO: Pod daemon-set-jbzjv is not available
    Jan 13 10:12:47.076: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:48.097: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:49.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:50.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:51.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:52.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:53.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:54.201: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:55.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:56.091: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:57.132: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:58.116: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:12:59.101: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:00.118: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:01.114: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:02.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:03.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:04.099: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:05.110: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:06.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:07.120: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:08.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:09.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:10.111: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:11.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:12.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:13.109: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:14.102: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:15.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:16.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:17.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:18.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:19.117: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:20.093: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:21.100: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:22.110: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:23.103: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:24.103: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:25.099: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:26.093: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:27.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:28.090: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:29.109: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:30.095: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:31.113: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:32.091: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:33.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:34.105: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:35.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:36.090: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:37.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:38.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:39.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:40.109: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:41.099: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:42.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:43.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:44.120: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:45.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:46.091: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:47.104: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:48.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:49.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:50.092: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:51.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:52.094: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:53.096: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:13:54.087: INFO: Pod daemon-set-97lgf is not available
    Jan 13 10:13:54.098: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/13/23 10:13:54.111
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1742, will wait for the garbage collector to delete the pods 01/13/23 10:13:54.111
    Jan 13 10:13:54.181: INFO: Deleting DaemonSet.extensions daemon-set took: 15.638293ms
    Jan 13 10:13:54.282: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.631564ms
    Jan 13 10:13:56.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:13:56.689: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 13 10:13:56.697: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"591445"},"items":null}

    Jan 13 10:13:56.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"591445"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:13:56.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1742" for this suite. 01/13/23 10:13:56.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:13:56.787
Jan 13 10:13:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 10:13:56.79
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:13:56.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:13:56.833
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87 in namespace container-probe-8199 01/13/23 10:13:56.84
Jan 13 10:13:56.858: INFO: Waiting up to 5m0s for pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87" in namespace "container-probe-8199" to be "not pending"
Jan 13 10:13:56.863: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.650535ms
Jan 13 10:13:58.872: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014019907s
Jan 13 10:14:00.880: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87": Phase="Running", Reason="", readiness=true. Elapsed: 4.022487241s
Jan 13 10:14:00.880: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87" satisfied condition "not pending"
Jan 13 10:14:00.880: INFO: Started pod liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87 in namespace container-probe-8199
STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:14:00.88
Jan 13 10:14:00.891: INFO: Initial restart count of pod liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87 is 0
STEP: deleting the pod 01/13/23 10:18:02.173
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:02.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8199" for this suite. 01/13/23 10:18:02.245
------------------------------
• [SLOW TEST] [245.480 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:13:56.787
    Jan 13 10:13:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 10:13:56.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:13:56.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:13:56.833
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87 in namespace container-probe-8199 01/13/23 10:13:56.84
    Jan 13 10:13:56.858: INFO: Waiting up to 5m0s for pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87" in namespace "container-probe-8199" to be "not pending"
    Jan 13 10:13:56.863: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87": Phase="Pending", Reason="", readiness=false. Elapsed: 5.650535ms
    Jan 13 10:13:58.872: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014019907s
    Jan 13 10:14:00.880: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87": Phase="Running", Reason="", readiness=true. Elapsed: 4.022487241s
    Jan 13 10:14:00.880: INFO: Pod "liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87" satisfied condition "not pending"
    Jan 13 10:14:00.880: INFO: Started pod liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87 in namespace container-probe-8199
    STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:14:00.88
    Jan 13 10:14:00.891: INFO: Initial restart count of pod liveness-47ea9e1a-4e77-486f-b8cf-739fe637eb87 is 0
    STEP: deleting the pod 01/13/23 10:18:02.173
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:02.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8199" for this suite. 01/13/23 10:18:02.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:02.269
Jan 13 10:18:02.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 10:18:02.271
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:02.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:02.321
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/13/23 10:18:02.332
STEP: Creating a ResourceQuota 01/13/23 10:18:07.339
STEP: Ensuring resource quota status is calculated 01/13/23 10:18:07.353
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:09.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3185" for this suite. 01/13/23 10:18:09.375
------------------------------
• [SLOW TEST] [7.130 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:02.269
    Jan 13 10:18:02.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 10:18:02.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:02.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:02.321
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/13/23 10:18:02.332
    STEP: Creating a ResourceQuota 01/13/23 10:18:07.339
    STEP: Ensuring resource quota status is calculated 01/13/23 10:18:07.353
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:09.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3185" for this suite. 01/13/23 10:18:09.375
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:09.4
Jan 13 10:18:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:18:09.403
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:09.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:09.443
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-8e6217b2-b71a-4c0c-83ff-a91eee0de23d 01/13/23 10:18:09.454
STEP: Creating secret with name secret-projected-all-test-volume-54d65b4f-bae0-4939-9313-9352e603dcb3 01/13/23 10:18:09.467
STEP: Creating a pod to test Check all projections for projected volume plugin 01/13/23 10:18:09.48
Jan 13 10:18:09.509: INFO: Waiting up to 5m0s for pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3" in namespace "projected-4157" to be "Succeeded or Failed"
Jan 13 10:18:09.521: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.848824ms
Jan 13 10:18:11.527: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018214688s
Jan 13 10:18:13.528: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019046646s
Jan 13 10:18:15.529: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020786114s
STEP: Saw pod success 01/13/23 10:18:15.53
Jan 13 10:18:15.530: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3" satisfied condition "Succeeded or Failed"
Jan 13 10:18:15.539: INFO: Trying to get logs from node 10.10.102.31-node pod projected-volume-bbed454d-e898-461f-a687-4786086b61f3 container projected-all-volume-test: <nil>
STEP: delete the pod 01/13/23 10:18:15.593
Jan 13 10:18:15.620: INFO: Waiting for pod projected-volume-bbed454d-e898-461f-a687-4786086b61f3 to disappear
Jan 13 10:18:15.627: INFO: Pod projected-volume-bbed454d-e898-461f-a687-4786086b61f3 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:15.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4157" for this suite. 01/13/23 10:18:15.642
------------------------------
• [SLOW TEST] [6.253 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:09.4
    Jan 13 10:18:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:18:09.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:09.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:09.443
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-8e6217b2-b71a-4c0c-83ff-a91eee0de23d 01/13/23 10:18:09.454
    STEP: Creating secret with name secret-projected-all-test-volume-54d65b4f-bae0-4939-9313-9352e603dcb3 01/13/23 10:18:09.467
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/13/23 10:18:09.48
    Jan 13 10:18:09.509: INFO: Waiting up to 5m0s for pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3" in namespace "projected-4157" to be "Succeeded or Failed"
    Jan 13 10:18:09.521: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.848824ms
    Jan 13 10:18:11.527: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018214688s
    Jan 13 10:18:13.528: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019046646s
    Jan 13 10:18:15.529: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020786114s
    STEP: Saw pod success 01/13/23 10:18:15.53
    Jan 13 10:18:15.530: INFO: Pod "projected-volume-bbed454d-e898-461f-a687-4786086b61f3" satisfied condition "Succeeded or Failed"
    Jan 13 10:18:15.539: INFO: Trying to get logs from node 10.10.102.31-node pod projected-volume-bbed454d-e898-461f-a687-4786086b61f3 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:18:15.593
    Jan 13 10:18:15.620: INFO: Waiting for pod projected-volume-bbed454d-e898-461f-a687-4786086b61f3 to disappear
    Jan 13 10:18:15.627: INFO: Pod projected-volume-bbed454d-e898-461f-a687-4786086b61f3 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:15.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4157" for this suite. 01/13/23 10:18:15.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:15.672
Jan 13 10:18:15.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sysctl 01/13/23 10:18:15.675
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:15.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:15.713
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/13/23 10:18:15.724
STEP: Watching for error events or started pod 01/13/23 10:18:15.754
STEP: Waiting for pod completion 01/13/23 10:18:19.764
Jan 13 10:18:19.765: INFO: Waiting up to 3m0s for pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418" in namespace "sysctl-8115" to be "completed"
Jan 13 10:18:19.778: INFO: Pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418": Phase="Pending", Reason="", readiness=false. Elapsed: 12.770698ms
Jan 13 10:18:21.788: INFO: Pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023475435s
Jan 13 10:18:21.788: INFO: Pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/13/23 10:18:21.797
STEP: Getting logs from the pod 01/13/23 10:18:21.797
STEP: Checking that the sysctl is actually updated 01/13/23 10:18:21.83
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:21.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8115" for this suite. 01/13/23 10:18:21.844
------------------------------
• [SLOW TEST] [6.196 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:15.672
    Jan 13 10:18:15.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sysctl 01/13/23 10:18:15.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:15.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:15.713
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/13/23 10:18:15.724
    STEP: Watching for error events or started pod 01/13/23 10:18:15.754
    STEP: Waiting for pod completion 01/13/23 10:18:19.764
    Jan 13 10:18:19.765: INFO: Waiting up to 3m0s for pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418" in namespace "sysctl-8115" to be "completed"
    Jan 13 10:18:19.778: INFO: Pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418": Phase="Pending", Reason="", readiness=false. Elapsed: 12.770698ms
    Jan 13 10:18:21.788: INFO: Pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023475435s
    Jan 13 10:18:21.788: INFO: Pod "sysctl-b7a79c7b-b77a-41b9-8b95-4e3d2eb8e418" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/13/23 10:18:21.797
    STEP: Getting logs from the pod 01/13/23 10:18:21.797
    STEP: Checking that the sysctl is actually updated 01/13/23 10:18:21.83
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:21.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8115" for this suite. 01/13/23 10:18:21.844
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:21.869
Jan 13 10:18:21.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:18:21.871
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:21.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:21.932
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-e010f8e0-7c82-4174-9e44-8c51e8e46221 01/13/23 10:18:21.95
STEP: Creating a pod to test consume secrets 01/13/23 10:18:21.967
Jan 13 10:18:21.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363" in namespace "projected-588" to be "Succeeded or Failed"
Jan 13 10:18:21.992: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Pending", Reason="", readiness=false. Elapsed: 7.623659ms
Jan 13 10:18:23.999: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014852468s
Jan 13 10:18:25.999: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01459541s
Jan 13 10:18:28.000: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015107s
STEP: Saw pod success 01/13/23 10:18:28
Jan 13 10:18:28.000: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363" satisfied condition "Succeeded or Failed"
Jan 13 10:18:28.007: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/13/23 10:18:28.049
Jan 13 10:18:28.074: INFO: Waiting for pod pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363 to disappear
Jan 13 10:18:28.089: INFO: Pod pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:28.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-588" for this suite. 01/13/23 10:18:28.099
------------------------------
• [SLOW TEST] [6.240 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:21.869
    Jan 13 10:18:21.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:18:21.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:21.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:21.932
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-e010f8e0-7c82-4174-9e44-8c51e8e46221 01/13/23 10:18:21.95
    STEP: Creating a pod to test consume secrets 01/13/23 10:18:21.967
    Jan 13 10:18:21.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363" in namespace "projected-588" to be "Succeeded or Failed"
    Jan 13 10:18:21.992: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Pending", Reason="", readiness=false. Elapsed: 7.623659ms
    Jan 13 10:18:23.999: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014852468s
    Jan 13 10:18:25.999: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01459541s
    Jan 13 10:18:28.000: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015107s
    STEP: Saw pod success 01/13/23 10:18:28
    Jan 13 10:18:28.000: INFO: Pod "pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363" satisfied condition "Succeeded or Failed"
    Jan 13 10:18:28.007: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:18:28.049
    Jan 13 10:18:28.074: INFO: Waiting for pod pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363 to disappear
    Jan 13 10:18:28.089: INFO: Pod pod-projected-secrets-2271d44e-3582-4891-aa22-1754fcb40363 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:28.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-588" for this suite. 01/13/23 10:18:28.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:28.119
Jan 13 10:18:28.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:18:28.122
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:28.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:28.155
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/13/23 10:18:28.187
STEP: waiting for available Endpoint 01/13/23 10:18:28.197
STEP: listing all Endpoints 01/13/23 10:18:28.201
STEP: updating the Endpoint 01/13/23 10:18:28.216
STEP: fetching the Endpoint 01/13/23 10:18:28.229
STEP: patching the Endpoint 01/13/23 10:18:28.235
STEP: fetching the Endpoint 01/13/23 10:18:28.247
STEP: deleting the Endpoint by Collection 01/13/23 10:18:28.254
STEP: waiting for Endpoint deletion 01/13/23 10:18:28.264
STEP: fetching the Endpoint 01/13/23 10:18:28.268
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:28.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5501" for this suite. 01/13/23 10:18:28.282
------------------------------
• [0.174 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:28.119
    Jan 13 10:18:28.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:18:28.122
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:28.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:28.155
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/13/23 10:18:28.187
    STEP: waiting for available Endpoint 01/13/23 10:18:28.197
    STEP: listing all Endpoints 01/13/23 10:18:28.201
    STEP: updating the Endpoint 01/13/23 10:18:28.216
    STEP: fetching the Endpoint 01/13/23 10:18:28.229
    STEP: patching the Endpoint 01/13/23 10:18:28.235
    STEP: fetching the Endpoint 01/13/23 10:18:28.247
    STEP: deleting the Endpoint by Collection 01/13/23 10:18:28.254
    STEP: waiting for Endpoint deletion 01/13/23 10:18:28.264
    STEP: fetching the Endpoint 01/13/23 10:18:28.268
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:28.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5501" for this suite. 01/13/23 10:18:28.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:28.296
Jan 13 10:18:28.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 10:18:28.299
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:28.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:28.342
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:18:28.352
Jan 13 10:18:28.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e" in namespace "downward-api-5852" to be "Succeeded or Failed"
Jan 13 10:18:28.376: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.54654ms
Jan 13 10:18:30.384: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017019769s
Jan 13 10:18:32.393: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026539707s
Jan 13 10:18:34.383: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016442081s
STEP: Saw pod success 01/13/23 10:18:34.383
Jan 13 10:18:34.384: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e" satisfied condition "Succeeded or Failed"
Jan 13 10:18:34.390: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e container client-container: <nil>
STEP: delete the pod 01/13/23 10:18:34.427
Jan 13 10:18:34.463: INFO: Waiting for pod downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e to disappear
Jan 13 10:18:34.472: INFO: Pod downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:34.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5852" for this suite. 01/13/23 10:18:34.486
------------------------------
• [SLOW TEST] [6.200 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:28.296
    Jan 13 10:18:28.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 10:18:28.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:28.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:28.342
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:18:28.352
    Jan 13 10:18:28.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e" in namespace "downward-api-5852" to be "Succeeded or Failed"
    Jan 13 10:18:28.376: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.54654ms
    Jan 13 10:18:30.384: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017019769s
    Jan 13 10:18:32.393: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026539707s
    Jan 13 10:18:34.383: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016442081s
    STEP: Saw pod success 01/13/23 10:18:34.383
    Jan 13 10:18:34.384: INFO: Pod "downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e" satisfied condition "Succeeded or Failed"
    Jan 13 10:18:34.390: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e container client-container: <nil>
    STEP: delete the pod 01/13/23 10:18:34.427
    Jan 13 10:18:34.463: INFO: Waiting for pod downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e to disappear
    Jan 13 10:18:34.472: INFO: Pod downwardapi-volume-660c8a09-e0ae-42c9-902f-f433dddc8f9e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:34.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5852" for this suite. 01/13/23 10:18:34.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:34.501
Jan 13 10:18:34.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:18:34.504
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:34.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:34.556
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:18:34.599
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:18:36.722
STEP: Deploying the webhook pod 01/13/23 10:18:36.744
STEP: Wait for the deployment to be ready 01/13/23 10:18:36.768
Jan 13 10:18:36.785: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 13 10:18:38.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:18:40.822
STEP: Verifying the service has paired with the endpoint 01/13/23 10:18:40.847
Jan 13 10:18:41.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/13/23 10:18:42.016
STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:18:42.117
STEP: Deleting the collection of validation webhooks 01/13/23 10:18:42.201
STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:18:42.376
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:18:42.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4426" for this suite. 01/13/23 10:18:42.581
STEP: Destroying namespace "webhook-4426-markers" for this suite. 01/13/23 10:18:42.622
------------------------------
• [SLOW TEST] [8.139 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:34.501
    Jan 13 10:18:34.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:18:34.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:34.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:34.556
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:18:34.599
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:18:36.722
    STEP: Deploying the webhook pod 01/13/23 10:18:36.744
    STEP: Wait for the deployment to be ready 01/13/23 10:18:36.768
    Jan 13 10:18:36.785: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 13 10:18:38.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 18, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:18:40.822
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:18:40.847
    Jan 13 10:18:41.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/13/23 10:18:42.016
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:18:42.117
    STEP: Deleting the collection of validation webhooks 01/13/23 10:18:42.201
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:18:42.376
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:18:42.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4426" for this suite. 01/13/23 10:18:42.581
    STEP: Destroying namespace "webhook-4426-markers" for this suite. 01/13/23 10:18:42.622
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:18:42.641
Jan 13 10:18:42.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename cronjob 01/13/23 10:18:42.645
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:42.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:42.698
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/13/23 10:18:42.71
STEP: Ensuring a job is scheduled 01/13/23 10:18:42.733
STEP: Ensuring exactly one is scheduled 01/13/23 10:19:00.74
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/13/23 10:19:00.747
STEP: Ensuring no more jobs are scheduled 01/13/23 10:19:00.753
STEP: Removing cronjob 01/13/23 10:24:00.793
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 13 10:24:00.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9593" for this suite. 01/13/23 10:24:00.833
------------------------------
• [SLOW TEST] [318.245 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:18:42.641
    Jan 13 10:18:42.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename cronjob 01/13/23 10:18:42.645
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:18:42.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:18:42.698
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/13/23 10:18:42.71
    STEP: Ensuring a job is scheduled 01/13/23 10:18:42.733
    STEP: Ensuring exactly one is scheduled 01/13/23 10:19:00.74
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/13/23 10:19:00.747
    STEP: Ensuring no more jobs are scheduled 01/13/23 10:19:00.753
    STEP: Removing cronjob 01/13/23 10:24:00.793
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:24:00.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9593" for this suite. 01/13/23 10:24:00.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:24:00.893
Jan 13 10:24:00.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:24:00.898
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:01.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:01.139
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-lnxx4"  01/13/23 10:24:01.154
Jan 13 10:24:01.187: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-lnxx4"  01/13/23 10:24:01.187
Jan 13 10:24:01.223: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 13 10:24:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1884" for this suite. 01/13/23 10:24:01.244
------------------------------
• [0.371 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:24:00.893
    Jan 13 10:24:00.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:24:00.898
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:01.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:01.139
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-lnxx4"  01/13/23 10:24:01.154
    Jan 13 10:24:01.187: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-lnxx4"  01/13/23 10:24:01.187
    Jan 13 10:24:01.223: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:24:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1884" for this suite. 01/13/23 10:24:01.244
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:24:01.265
Jan 13 10:24:01.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:24:01.27
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:01.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:01.318
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/13/23 10:24:01.333
Jan 13 10:24:01.334: INFO: namespace kubectl-6358
Jan 13 10:24:01.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 create -f -'
Jan 13 10:24:04.140: INFO: stderr: ""
Jan 13 10:24:04.140: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/13/23 10:24:04.14
Jan 13 10:24:05.153: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:24:05.153: INFO: Found 0 / 1
Jan 13 10:24:06.152: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:24:06.152: INFO: Found 0 / 1
Jan 13 10:24:07.151: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:24:07.151: INFO: Found 1 / 1
Jan 13 10:24:07.151: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 13 10:24:07.160: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:24:07.160: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 13 10:24:07.160: INFO: wait on agnhost-primary startup in kubectl-6358 
Jan 13 10:24:07.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 logs agnhost-primary-l9t67 agnhost-primary'
Jan 13 10:24:07.614: INFO: stderr: ""
Jan 13 10:24:07.615: INFO: stdout: "Paused\n"
STEP: exposing RC 01/13/23 10:24:07.615
Jan 13 10:24:07.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 13 10:24:07.997: INFO: stderr: ""
Jan 13 10:24:07.997: INFO: stdout: "service/rm2 exposed\n"
Jan 13 10:24:08.019: INFO: Service rm2 in namespace kubectl-6358 found.
STEP: exposing service 01/13/23 10:24:10.056
Jan 13 10:24:10.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 13 10:24:10.495: INFO: stderr: ""
Jan 13 10:24:10.496: INFO: stdout: "service/rm3 exposed\n"
Jan 13 10:24:10.512: INFO: Service rm3 in namespace kubectl-6358 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:24:12.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6358" for this suite. 01/13/23 10:24:12.55
------------------------------
• [SLOW TEST] [11.321 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:24:01.265
    Jan 13 10:24:01.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:24:01.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:01.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:01.318
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/13/23 10:24:01.333
    Jan 13 10:24:01.334: INFO: namespace kubectl-6358
    Jan 13 10:24:01.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 create -f -'
    Jan 13 10:24:04.140: INFO: stderr: ""
    Jan 13 10:24:04.140: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/13/23 10:24:04.14
    Jan 13 10:24:05.153: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:24:05.153: INFO: Found 0 / 1
    Jan 13 10:24:06.152: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:24:06.152: INFO: Found 0 / 1
    Jan 13 10:24:07.151: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:24:07.151: INFO: Found 1 / 1
    Jan 13 10:24:07.151: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 13 10:24:07.160: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:24:07.160: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 13 10:24:07.160: INFO: wait on agnhost-primary startup in kubectl-6358 
    Jan 13 10:24:07.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 logs agnhost-primary-l9t67 agnhost-primary'
    Jan 13 10:24:07.614: INFO: stderr: ""
    Jan 13 10:24:07.615: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/13/23 10:24:07.615
    Jan 13 10:24:07.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 13 10:24:07.997: INFO: stderr: ""
    Jan 13 10:24:07.997: INFO: stdout: "service/rm2 exposed\n"
    Jan 13 10:24:08.019: INFO: Service rm2 in namespace kubectl-6358 found.
    STEP: exposing service 01/13/23 10:24:10.056
    Jan 13 10:24:10.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6358 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 13 10:24:10.495: INFO: stderr: ""
    Jan 13 10:24:10.496: INFO: stdout: "service/rm3 exposed\n"
    Jan 13 10:24:10.512: INFO: Service rm3 in namespace kubectl-6358 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:24:12.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6358" for this suite. 01/13/23 10:24:12.55
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:24:12.587
Jan 13 10:24:12.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:24:12.591
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:12.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:12.736
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:24:12.819
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:24:13.398
STEP: Deploying the webhook pod 01/13/23 10:24:13.462
STEP: Wait for the deployment to be ready 01/13/23 10:24:13.516
Jan 13 10:24:13.552: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:24:15.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:24:17.59
STEP: Verifying the service has paired with the endpoint 01/13/23 10:24:17.633
Jan 13 10:24:18.633: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/13/23 10:24:18.64
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/13/23 10:24:18.685
STEP: Creating a configMap that should not be mutated 01/13/23 10:24:18.698
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/13/23 10:24:18.719
STEP: Creating a configMap that should be mutated 01/13/23 10:24:18.735
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:24:18.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3459" for this suite. 01/13/23 10:24:18.923
STEP: Destroying namespace "webhook-3459-markers" for this suite. 01/13/23 10:24:18.944
------------------------------
• [SLOW TEST] [6.373 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:24:12.587
    Jan 13 10:24:12.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:24:12.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:12.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:12.736
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:24:12.819
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:24:13.398
    STEP: Deploying the webhook pod 01/13/23 10:24:13.462
    STEP: Wait for the deployment to be ready 01/13/23 10:24:13.516
    Jan 13 10:24:13.552: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:24:15.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 24, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:24:17.59
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:24:17.633
    Jan 13 10:24:18.633: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/13/23 10:24:18.64
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/13/23 10:24:18.685
    STEP: Creating a configMap that should not be mutated 01/13/23 10:24:18.698
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/13/23 10:24:18.719
    STEP: Creating a configMap that should be mutated 01/13/23 10:24:18.735
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:24:18.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3459" for this suite. 01/13/23 10:24:18.923
    STEP: Destroying namespace "webhook-3459-markers" for this suite. 01/13/23 10:24:18.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:24:18.966
Jan 13 10:24:18.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:24:18.97
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:19.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:19.038
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:24:19.045
Jan 13 10:24:19.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66" in namespace "projected-3878" to be "Succeeded or Failed"
Jan 13 10:24:19.073: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 13.187306ms
Jan 13 10:24:21.085: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025713396s
Jan 13 10:24:23.080: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01990387s
Jan 13 10:24:25.109: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049093087s
Jan 13 10:24:27.080: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020090452s
STEP: Saw pod success 01/13/23 10:24:27.08
Jan 13 10:24:27.080: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66" satisfied condition "Succeeded or Failed"
Jan 13 10:24:27.086: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66 container client-container: <nil>
STEP: delete the pod 01/13/23 10:24:27.101
Jan 13 10:24:27.118: INFO: Waiting for pod downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66 to disappear
Jan 13 10:24:27.125: INFO: Pod downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:24:27.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3878" for this suite. 01/13/23 10:24:27.137
------------------------------
• [SLOW TEST] [8.197 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:24:18.966
    Jan 13 10:24:18.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:24:18.97
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:19.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:19.038
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:24:19.045
    Jan 13 10:24:19.060: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66" in namespace "projected-3878" to be "Succeeded or Failed"
    Jan 13 10:24:19.073: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 13.187306ms
    Jan 13 10:24:21.085: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025713396s
    Jan 13 10:24:23.080: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01990387s
    Jan 13 10:24:25.109: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049093087s
    Jan 13 10:24:27.080: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020090452s
    STEP: Saw pod success 01/13/23 10:24:27.08
    Jan 13 10:24:27.080: INFO: Pod "downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66" satisfied condition "Succeeded or Failed"
    Jan 13 10:24:27.086: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:24:27.101
    Jan 13 10:24:27.118: INFO: Waiting for pod downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66 to disappear
    Jan 13 10:24:27.125: INFO: Pod downwardapi-volume-72c23913-07da-40ca-8a0f-22ce67d45e66 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:24:27.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3878" for this suite. 01/13/23 10:24:27.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:24:27.167
Jan 13 10:24:27.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-pred 01/13/23 10:24:27.17
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:27.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:27.195
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 13 10:24:27.202: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 10:24:27.215: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 10:24:27.222: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.31-node before test
Jan 13 10:24:27.240: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 10:24:27.240: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 10:24:27.240: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
Jan 13 10:24:27.240: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 10:24:27.240: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:24:27.240: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:24:27.240: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 10:24:27.240: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.32-node before test
Jan 13 10:24:27.263: INFO: calico-kube-controllers-7bdbfc669-lhkk6 from kube-system started at 2023-01-13 09:33:43 +0000 UTC (1 container statuses recorded)
Jan 13 10:24:27.263: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 13 10:24:27.263: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 10:24:27.263: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 10:24:27.263: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
Jan 13 10:24:27.264: INFO: 	Container coredns ready: true, restart count 0
Jan 13 10:24:27.264: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
Jan 13 10:24:27.264: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 10:24:27.264: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
Jan 13 10:24:27.264: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 10:24:27.264: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:24:27.264: INFO: 	Container e2e ready: true, restart count 0
Jan 13 10:24:27.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:24:27.264: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:24:27.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:24:27.264: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/13/23 10:24:27.264
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1739d74e2d85810c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/13/23 10:24:27.323
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:24:28.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4256" for this suite. 01/13/23 10:24:28.323
------------------------------
• [1.165 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:24:27.167
    Jan 13 10:24:27.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-pred 01/13/23 10:24:27.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:27.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:27.195
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 13 10:24:27.202: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 13 10:24:27.215: INFO: Waiting for terminating namespaces to be deleted...
    Jan 13 10:24:27.222: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.31-node before test
    Jan 13 10:24:27.240: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 10:24:27.240: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 10:24:27.240: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
    Jan 13 10:24:27.240: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 10:24:27.240: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:24:27.240: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:24:27.240: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 13 10:24:27.240: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.32-node before test
    Jan 13 10:24:27.263: INFO: calico-kube-controllers-7bdbfc669-lhkk6 from kube-system started at 2023-01-13 09:33:43 +0000 UTC (1 container statuses recorded)
    Jan 13 10:24:27.263: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 13 10:24:27.263: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 10:24:27.263: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 10:24:27.263: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
    Jan 13 10:24:27.264: INFO: 	Container coredns ready: true, restart count 0
    Jan 13 10:24:27.264: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
    Jan 13 10:24:27.264: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 10:24:27.264: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
    Jan 13 10:24:27.264: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 13 10:24:27.264: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:24:27.264: INFO: 	Container e2e ready: true, restart count 0
    Jan 13 10:24:27.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:24:27.264: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:24:27.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:24:27.264: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/13/23 10:24:27.264
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1739d74e2d85810c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/13/23 10:24:27.323
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:24:28.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4256" for this suite. 01/13/23 10:24:28.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:24:28.348
Jan 13 10:24:28.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 10:24:28.351
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:28.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:28.392
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2966 01/13/23 10:24:28.406
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/13/23 10:24:28.414
Jan 13 10:24:28.441: INFO: Found 0 stateful pods, waiting for 3
Jan 13 10:24:38.451: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:24:38.451: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:24:38.451: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 13 10:24:48.453: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:24:48.453: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:24:48.453: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/13/23 10:24:48.51
Jan 13 10:24:48.555: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/13/23 10:24:48.555
STEP: Not applying an update when the partition is greater than the number of replicas 01/13/23 10:24:58.602
STEP: Performing a canary update 01/13/23 10:24:58.603
Jan 13 10:24:58.635: INFO: Updating stateful set ss2
Jan 13 10:24:58.664: INFO: Waiting for Pod statefulset-2966/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/13/23 10:25:08.679
Jan 13 10:25:08.786: INFO: Found 2 stateful pods, waiting for 3
Jan 13 10:25:18.797: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:25:18.797: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:25:18.797: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/13/23 10:25:18.813
Jan 13 10:25:18.845: INFO: Updating stateful set ss2
Jan 13 10:25:18.859: INFO: Waiting for Pod statefulset-2966/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan 13 10:25:28.934: INFO: Updating stateful set ss2
Jan 13 10:25:28.989: INFO: Waiting for StatefulSet statefulset-2966/ss2 to complete update
Jan 13 10:25:28.990: INFO: Waiting for Pod statefulset-2966/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 10:25:39.008: INFO: Deleting all statefulset in ns statefulset-2966
Jan 13 10:25:39.014: INFO: Scaling statefulset ss2 to 0
Jan 13 10:25:49.051: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 10:25:49.057: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:25:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2966" for this suite. 01/13/23 10:25:49.131
------------------------------
• [SLOW TEST] [80.814 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:24:28.348
    Jan 13 10:24:28.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 10:24:28.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:24:28.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:24:28.392
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2966 01/13/23 10:24:28.406
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/13/23 10:24:28.414
    Jan 13 10:24:28.441: INFO: Found 0 stateful pods, waiting for 3
    Jan 13 10:24:38.451: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:24:38.451: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:24:38.451: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Jan 13 10:24:48.453: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:24:48.453: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:24:48.453: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/13/23 10:24:48.51
    Jan 13 10:24:48.555: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/13/23 10:24:48.555
    STEP: Not applying an update when the partition is greater than the number of replicas 01/13/23 10:24:58.602
    STEP: Performing a canary update 01/13/23 10:24:58.603
    Jan 13 10:24:58.635: INFO: Updating stateful set ss2
    Jan 13 10:24:58.664: INFO: Waiting for Pod statefulset-2966/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/13/23 10:25:08.679
    Jan 13 10:25:08.786: INFO: Found 2 stateful pods, waiting for 3
    Jan 13 10:25:18.797: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:25:18.797: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:25:18.797: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/13/23 10:25:18.813
    Jan 13 10:25:18.845: INFO: Updating stateful set ss2
    Jan 13 10:25:18.859: INFO: Waiting for Pod statefulset-2966/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan 13 10:25:28.934: INFO: Updating stateful set ss2
    Jan 13 10:25:28.989: INFO: Waiting for StatefulSet statefulset-2966/ss2 to complete update
    Jan 13 10:25:28.990: INFO: Waiting for Pod statefulset-2966/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 10:25:39.008: INFO: Deleting all statefulset in ns statefulset-2966
    Jan 13 10:25:39.014: INFO: Scaling statefulset ss2 to 0
    Jan 13 10:25:49.051: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 10:25:49.057: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:25:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2966" for this suite. 01/13/23 10:25:49.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:25:49.167
Jan 13 10:25:49.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replication-controller 01/13/23 10:25:49.171
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:25:49.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:25:49.262
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1 01/13/23 10:25:49.272
Jan 13 10:25:49.291: INFO: Pod name my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1: Found 0 pods out of 1
Jan 13 10:25:54.297: INFO: Pod name my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1: Found 1 pods out of 1
Jan 13 10:25:54.297: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1" are running
Jan 13 10:25:54.297: INFO: Waiting up to 5m0s for pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7" in namespace "replication-controller-9154" to be "running"
Jan 13 10:25:54.307: INFO: Pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7": Phase="Running", Reason="", readiness=true. Elapsed: 9.306532ms
Jan 13 10:25:54.307: INFO: Pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7" satisfied condition "running"
Jan 13 10:25:54.307: INFO: Pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:49 +0000 UTC Reason: Message:}])
Jan 13 10:25:54.307: INFO: Trying to dial the pod
Jan 13 10:25:59.336: INFO: Controller my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1: Got expected result from replica 1 [my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7]: "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 13 10:25:59.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9154" for this suite. 01/13/23 10:25:59.349
------------------------------
• [SLOW TEST] [10.197 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:25:49.167
    Jan 13 10:25:49.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replication-controller 01/13/23 10:25:49.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:25:49.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:25:49.262
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1 01/13/23 10:25:49.272
    Jan 13 10:25:49.291: INFO: Pod name my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1: Found 0 pods out of 1
    Jan 13 10:25:54.297: INFO: Pod name my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1: Found 1 pods out of 1
    Jan 13 10:25:54.297: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1" are running
    Jan 13 10:25:54.297: INFO: Waiting up to 5m0s for pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7" in namespace "replication-controller-9154" to be "running"
    Jan 13 10:25:54.307: INFO: Pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7": Phase="Running", Reason="", readiness=true. Elapsed: 9.306532ms
    Jan 13 10:25:54.307: INFO: Pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7" satisfied condition "running"
    Jan 13 10:25:54.307: INFO: Pod "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-13 10:25:49 +0000 UTC Reason: Message:}])
    Jan 13 10:25:54.307: INFO: Trying to dial the pod
    Jan 13 10:25:59.336: INFO: Controller my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1: Got expected result from replica 1 [my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7]: "my-hostname-basic-299c7c7d-5b28-4c78-8d27-a1a338bd96b1-4bmr7", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:25:59.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9154" for this suite. 01/13/23 10:25:59.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:25:59.365
Jan 13 10:25:59.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:25:59.368
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:25:59.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:25:59.442
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:25:59.452
Jan 13 10:25:59.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d" in namespace "projected-837" to be "Succeeded or Failed"
Jan 13 10:25:59.488: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.766556ms
Jan 13 10:26:01.506: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033965912s
Jan 13 10:26:03.505: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032161503s
Jan 13 10:26:05.528: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055495791s
Jan 13 10:26:07.495: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02229119s
STEP: Saw pod success 01/13/23 10:26:07.495
Jan 13 10:26:07.495: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d" satisfied condition "Succeeded or Failed"
Jan 13 10:26:07.499: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d container client-container: <nil>
STEP: delete the pod 01/13/23 10:26:07.539
Jan 13 10:26:07.557: INFO: Waiting for pod downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d to disappear
Jan 13 10:26:07.565: INFO: Pod downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:26:07.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-837" for this suite. 01/13/23 10:26:07.574
------------------------------
• [SLOW TEST] [8.239 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:25:59.365
    Jan 13 10:25:59.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:25:59.368
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:25:59.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:25:59.442
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:25:59.452
    Jan 13 10:25:59.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d" in namespace "projected-837" to be "Succeeded or Failed"
    Jan 13 10:25:59.488: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.766556ms
    Jan 13 10:26:01.506: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033965912s
    Jan 13 10:26:03.505: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032161503s
    Jan 13 10:26:05.528: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055495791s
    Jan 13 10:26:07.495: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02229119s
    STEP: Saw pod success 01/13/23 10:26:07.495
    Jan 13 10:26:07.495: INFO: Pod "downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d" satisfied condition "Succeeded or Failed"
    Jan 13 10:26:07.499: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d container client-container: <nil>
    STEP: delete the pod 01/13/23 10:26:07.539
    Jan 13 10:26:07.557: INFO: Waiting for pod downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d to disappear
    Jan 13 10:26:07.565: INFO: Pod downwardapi-volume-916715d7-8289-4df0-b3b8-a2e2ec9dbd4d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:26:07.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-837" for this suite. 01/13/23 10:26:07.574
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:26:07.605
Jan 13 10:26:07.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:26:07.608
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:26:07.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:26:07.663
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1065 01/13/23 10:26:07.67
STEP: creating replication controller nodeport-test in namespace services-1065 01/13/23 10:26:07.697
I0113 10:26:07.712369      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1065, replica count: 2
I0113 10:26:10.763146      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 10:26:10.763: INFO: Creating new exec pod
Jan 13 10:26:10.782: INFO: Waiting up to 5m0s for pod "execpod45wdz" in namespace "services-1065" to be "running"
Jan 13 10:26:10.802: INFO: Pod "execpod45wdz": Phase="Pending", Reason="", readiness=false. Elapsed: 20.515157ms
Jan 13 10:26:12.837: INFO: Pod "execpod45wdz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055395588s
Jan 13 10:26:14.824: INFO: Pod "execpod45wdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.041727341s
Jan 13 10:26:14.824: INFO: Pod "execpod45wdz" satisfied condition "running"
Jan 13 10:26:15.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 13 10:26:16.374: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 13 10:26:16.374: INFO: stdout: ""
Jan 13 10:26:16.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 10.100.24.6 80'
Jan 13 10:26:16.823: INFO: stderr: "+ nc -v -z -w 2 10.100.24.6 80\nConnection to 10.100.24.6 80 port [tcp/http] succeeded!\n"
Jan 13 10:26:16.823: INFO: stdout: ""
Jan 13 10:26:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 30329'
Jan 13 10:26:17.273: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 30329\nConnection to 10.10.102.31 30329 port [tcp/*] succeeded!\n"
Jan 13 10:26:17.273: INFO: stdout: ""
Jan 13 10:26:17.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 30329'
Jan 13 10:26:17.737: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 30329\nConnection to 10.10.102.32 30329 port [tcp/*] succeeded!\n"
Jan 13 10:26:17.737: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:26:17.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1065" for this suite. 01/13/23 10:26:17.75
------------------------------
• [SLOW TEST] [10.157 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:26:07.605
    Jan 13 10:26:07.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:26:07.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:26:07.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:26:07.663
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1065 01/13/23 10:26:07.67
    STEP: creating replication controller nodeport-test in namespace services-1065 01/13/23 10:26:07.697
    I0113 10:26:07.712369      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1065, replica count: 2
    I0113 10:26:10.763146      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 10:26:10.763: INFO: Creating new exec pod
    Jan 13 10:26:10.782: INFO: Waiting up to 5m0s for pod "execpod45wdz" in namespace "services-1065" to be "running"
    Jan 13 10:26:10.802: INFO: Pod "execpod45wdz": Phase="Pending", Reason="", readiness=false. Elapsed: 20.515157ms
    Jan 13 10:26:12.837: INFO: Pod "execpod45wdz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055395588s
    Jan 13 10:26:14.824: INFO: Pod "execpod45wdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.041727341s
    Jan 13 10:26:14.824: INFO: Pod "execpod45wdz" satisfied condition "running"
    Jan 13 10:26:15.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 13 10:26:16.374: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 13 10:26:16.374: INFO: stdout: ""
    Jan 13 10:26:16.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 10.100.24.6 80'
    Jan 13 10:26:16.823: INFO: stderr: "+ nc -v -z -w 2 10.100.24.6 80\nConnection to 10.100.24.6 80 port [tcp/http] succeeded!\n"
    Jan 13 10:26:16.823: INFO: stdout: ""
    Jan 13 10:26:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 10.10.102.31 30329'
    Jan 13 10:26:17.273: INFO: stderr: "+ nc -v -z -w 2 10.10.102.31 30329\nConnection to 10.10.102.31 30329 port [tcp/*] succeeded!\n"
    Jan 13 10:26:17.273: INFO: stdout: ""
    Jan 13 10:26:17.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-1065 exec execpod45wdz -- /bin/sh -x -c nc -v -z -w 2 10.10.102.32 30329'
    Jan 13 10:26:17.737: INFO: stderr: "+ nc -v -z -w 2 10.10.102.32 30329\nConnection to 10.10.102.32 30329 port [tcp/*] succeeded!\n"
    Jan 13 10:26:17.737: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:26:17.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1065" for this suite. 01/13/23 10:26:17.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:26:17.768
Jan 13 10:26:17.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-pred 01/13/23 10:26:17.774
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:26:17.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:26:17.82
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 13 10:26:17.824: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 10:26:17.840: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 10:26:17.846: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.31-node before test
Jan 13 10:26:17.864: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.864: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 10:26:17.864: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.864: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 10:26:17.864: INFO: execpod45wdz from services-1065 started at 2023-01-13 10:26:10 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.864: INFO: 	Container agnhost-container ready: true, restart count 0
Jan 13 10:26:17.864: INFO: nodeport-test-765qc from services-1065 started at 2023-01-13 10:26:07 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.864: INFO: 	Container nodeport-test ready: true, restart count 0
Jan 13 10:26:17.864: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:26:17.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:26:17.864: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 10:26:17.864: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.32-node before test
Jan 13 10:26:17.879: INFO: calico-kube-controllers-7bdbfc669-lhkk6 from kube-system started at 2023-01-13 09:33:43 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 13 10:26:17.880: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 10:26:17.880: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container coredns ready: true, restart count 0
Jan 13 10:26:17.880: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 10:26:17.880: INFO: nodeport-test-77kcz from services-1065 started at 2023-01-13 10:26:07 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container nodeport-test ready: true, restart count 0
Jan 13 10:26:17.880: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 10:26:17.880: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container e2e ready: true, restart count 0
Jan 13 10:26:17.880: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:26:17.880: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:26:17.880: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:26:17.880: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node 10.10.102.31-node 01/13/23 10:26:17.946
STEP: verifying the node has the label node 10.10.102.32-node 01/13/23 10:26:18.051
Jan 13 10:26:18.129: INFO: Pod calico-kube-controllers-7bdbfc669-lhkk6 requesting resource cpu=0m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod calico-node-rhspb requesting resource cpu=250m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod calico-node-zddvc requesting resource cpu=250m on Node 10.10.102.31-node
Jan 13 10:26:18.129: INFO: Pod coredns-5bbd96d687-qk6p8 requesting resource cpu=100m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod kube-proxy-hpcvx requesting resource cpu=0m on Node 10.10.102.31-node
Jan 13 10:26:18.129: INFO: Pod kube-proxy-tjxsm requesting resource cpu=0m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod execpod45wdz requesting resource cpu=0m on Node 10.10.102.31-node
Jan 13 10:26:18.129: INFO: Pod nodeport-test-765qc requesting resource cpu=0m on Node 10.10.102.31-node
Jan 13 10:26:18.129: INFO: Pod nodeport-test-77kcz requesting resource cpu=0m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod sonobuoy-e2e-job-3d6873fba06b4897 requesting resource cpu=0m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk requesting resource cpu=0m on Node 10.10.102.32-node
Jan 13 10:26:18.129: INFO: Pod sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 requesting resource cpu=0m on Node 10.10.102.31-node
STEP: Starting Pods to consume most of the cluster CPU. 01/13/23 10:26:18.129
Jan 13 10:26:18.129: INFO: Creating a pod which consumes cpu=2625m on Node 10.10.102.31-node
Jan 13 10:26:18.169: INFO: Creating a pod which consumes cpu=2555m on Node 10.10.102.32-node
Jan 13 10:26:18.195: INFO: Waiting up to 5m0s for pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda" in namespace "sched-pred-4664" to be "running"
Jan 13 10:26:18.257: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda": Phase="Pending", Reason="", readiness=false. Elapsed: 61.735248ms
Jan 13 10:26:20.276: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080834424s
Jan 13 10:26:22.267: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda": Phase="Running", Reason="", readiness=true. Elapsed: 4.071835344s
Jan 13 10:26:22.267: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda" satisfied condition "running"
Jan 13 10:26:22.267: INFO: Waiting up to 5m0s for pod "filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac" in namespace "sched-pred-4664" to be "running"
Jan 13 10:26:22.274: INFO: Pod "filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac": Phase="Running", Reason="", readiness=true. Elapsed: 6.968633ms
Jan 13 10:26:22.275: INFO: Pod "filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/13/23 10:26:22.275
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d767fdd86bf7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4664/filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda to 10.10.102.31-node] 01/13/23 10:26:22.282
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d76870ee66c6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/13/23 10:26:22.282
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d7687506a56a], Reason = [Created], Message = [Created container filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda] 01/13/23 10:26:22.282
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d76891ae17ff], Reason = [Started], Message = [Started container filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda] 01/13/23 10:26:22.282
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d76800faaec6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4664/filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac to 10.10.102.32-node] 01/13/23 10:26:22.283
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d76867a797dc], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/13/23 10:26:22.283
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d7686aab5700], Reason = [Created], Message = [Created container filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac] 01/13/23 10:26:22.283
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d768855c9387], Reason = [Started], Message = [Started container filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac] 01/13/23 10:26:22.283
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1739d768f35bfcb1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 01/13/23 10:26:22.309
STEP: removing the label node off the node 10.10.102.31-node 01/13/23 10:26:23.304
STEP: verifying the node doesn't have the label node 01/13/23 10:26:23.336
STEP: removing the label node off the node 10.10.102.32-node 01/13/23 10:26:23.352
STEP: verifying the node doesn't have the label node 01/13/23 10:26:23.419
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:26:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4664" for this suite. 01/13/23 10:26:23.504
------------------------------
• [SLOW TEST] [5.771 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:26:17.768
    Jan 13 10:26:17.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-pred 01/13/23 10:26:17.774
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:26:17.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:26:17.82
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 13 10:26:17.824: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 13 10:26:17.840: INFO: Waiting for terminating namespaces to be deleted...
    Jan 13 10:26:17.846: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.31-node before test
    Jan 13 10:26:17.864: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.864: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 10:26:17.864: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.864: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 10:26:17.864: INFO: execpod45wdz from services-1065 started at 2023-01-13 10:26:10 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.864: INFO: 	Container agnhost-container ready: true, restart count 0
    Jan 13 10:26:17.864: INFO: nodeport-test-765qc from services-1065 started at 2023-01-13 10:26:07 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.864: INFO: 	Container nodeport-test ready: true, restart count 0
    Jan 13 10:26:17.864: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:26:17.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:26:17.864: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 13 10:26:17.864: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.32-node before test
    Jan 13 10:26:17.879: INFO: calico-kube-controllers-7bdbfc669-lhkk6 from kube-system started at 2023-01-13 09:33:43 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 13 10:26:17.880: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 10:26:17.880: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container coredns ready: true, restart count 0
    Jan 13 10:26:17.880: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 10:26:17.880: INFO: nodeport-test-77kcz from services-1065 started at 2023-01-13 10:26:07 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container nodeport-test ready: true, restart count 0
    Jan 13 10:26:17.880: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 13 10:26:17.880: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container e2e ready: true, restart count 0
    Jan 13 10:26:17.880: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:26:17.880: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:26:17.880: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:26:17.880: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node 10.10.102.31-node 01/13/23 10:26:17.946
    STEP: verifying the node has the label node 10.10.102.32-node 01/13/23 10:26:18.051
    Jan 13 10:26:18.129: INFO: Pod calico-kube-controllers-7bdbfc669-lhkk6 requesting resource cpu=0m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod calico-node-rhspb requesting resource cpu=250m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod calico-node-zddvc requesting resource cpu=250m on Node 10.10.102.31-node
    Jan 13 10:26:18.129: INFO: Pod coredns-5bbd96d687-qk6p8 requesting resource cpu=100m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod kube-proxy-hpcvx requesting resource cpu=0m on Node 10.10.102.31-node
    Jan 13 10:26:18.129: INFO: Pod kube-proxy-tjxsm requesting resource cpu=0m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod execpod45wdz requesting resource cpu=0m on Node 10.10.102.31-node
    Jan 13 10:26:18.129: INFO: Pod nodeport-test-765qc requesting resource cpu=0m on Node 10.10.102.31-node
    Jan 13 10:26:18.129: INFO: Pod nodeport-test-77kcz requesting resource cpu=0m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod sonobuoy-e2e-job-3d6873fba06b4897 requesting resource cpu=0m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk requesting resource cpu=0m on Node 10.10.102.32-node
    Jan 13 10:26:18.129: INFO: Pod sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 requesting resource cpu=0m on Node 10.10.102.31-node
    STEP: Starting Pods to consume most of the cluster CPU. 01/13/23 10:26:18.129
    Jan 13 10:26:18.129: INFO: Creating a pod which consumes cpu=2625m on Node 10.10.102.31-node
    Jan 13 10:26:18.169: INFO: Creating a pod which consumes cpu=2555m on Node 10.10.102.32-node
    Jan 13 10:26:18.195: INFO: Waiting up to 5m0s for pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda" in namespace "sched-pred-4664" to be "running"
    Jan 13 10:26:18.257: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda": Phase="Pending", Reason="", readiness=false. Elapsed: 61.735248ms
    Jan 13 10:26:20.276: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080834424s
    Jan 13 10:26:22.267: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda": Phase="Running", Reason="", readiness=true. Elapsed: 4.071835344s
    Jan 13 10:26:22.267: INFO: Pod "filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda" satisfied condition "running"
    Jan 13 10:26:22.267: INFO: Waiting up to 5m0s for pod "filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac" in namespace "sched-pred-4664" to be "running"
    Jan 13 10:26:22.274: INFO: Pod "filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac": Phase="Running", Reason="", readiness=true. Elapsed: 6.968633ms
    Jan 13 10:26:22.275: INFO: Pod "filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/13/23 10:26:22.275
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d767fdd86bf7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4664/filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda to 10.10.102.31-node] 01/13/23 10:26:22.282
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d76870ee66c6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/13/23 10:26:22.282
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d7687506a56a], Reason = [Created], Message = [Created container filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda] 01/13/23 10:26:22.282
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda.1739d76891ae17ff], Reason = [Started], Message = [Started container filler-pod-3f70ef62-98a8-40b7-b412-34132387ddda] 01/13/23 10:26:22.282
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d76800faaec6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4664/filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac to 10.10.102.32-node] 01/13/23 10:26:22.283
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d76867a797dc], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/13/23 10:26:22.283
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d7686aab5700], Reason = [Created], Message = [Created container filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac] 01/13/23 10:26:22.283
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac.1739d768855c9387], Reason = [Started], Message = [Started container filler-pod-87e656c2-d270-48b2-adc6-0b4b523c41ac] 01/13/23 10:26:22.283
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1739d768f35bfcb1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 01/13/23 10:26:22.309
    STEP: removing the label node off the node 10.10.102.31-node 01/13/23 10:26:23.304
    STEP: verifying the node doesn't have the label node 01/13/23 10:26:23.336
    STEP: removing the label node off the node 10.10.102.32-node 01/13/23 10:26:23.352
    STEP: verifying the node doesn't have the label node 01/13/23 10:26:23.419
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:26:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4664" for this suite. 01/13/23 10:26:23.504
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:26:23.54
Jan 13 10:26:23.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-preemption 01/13/23 10:26:23.545
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:26:23.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:26:23.662
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 13 10:26:23.753: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 10:27:23.877: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:23.913
Jan 13 10:27:23.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-preemption-path 01/13/23 10:27:23.916
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:23.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:23.993
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Jan 13 10:27:24.045: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 13 10:27:24.082: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:24.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-4836" for this suite. 01/13/23 10:27:24.306
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4127" for this suite. 01/13/23 10:27:24.323
------------------------------
• [SLOW TEST] [60.806 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:26:23.54
    Jan 13 10:26:23.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-preemption 01/13/23 10:26:23.545
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:26:23.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:26:23.662
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 13 10:26:23.753: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 13 10:27:23.877: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:23.913
    Jan 13 10:27:23.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-preemption-path 01/13/23 10:27:23.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:23.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:23.993
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Jan 13 10:27:24.045: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 13 10:27:24.082: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:24.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-4836" for this suite. 01/13/23 10:27:24.306
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4127" for this suite. 01/13/23 10:27:24.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:24.36
Jan 13 10:27:24.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:27:24.363
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:24.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:24.415
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/13/23 10:27:24.422
Jan 13 10:27:24.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-3330 api-versions'
Jan 13 10:27:24.673: INFO: stderr: ""
Jan 13 10:27:24.673: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:24.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3330" for this suite. 01/13/23 10:27:24.682
------------------------------
• [0.341 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:24.36
    Jan 13 10:27:24.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:27:24.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:24.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:24.415
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/13/23 10:27:24.422
    Jan 13 10:27:24.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-3330 api-versions'
    Jan 13 10:27:24.673: INFO: stderr: ""
    Jan 13 10:27:24.673: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:24.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3330" for this suite. 01/13/23 10:27:24.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:24.704
Jan 13 10:27:24.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replication-controller 01/13/23 10:27:24.706
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:24.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:24.744
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/13/23 10:27:24.758
STEP: waiting for RC to be added 01/13/23 10:27:24.771
STEP: waiting for available Replicas 01/13/23 10:27:24.772
STEP: patching ReplicationController 01/13/23 10:27:26.911
STEP: waiting for RC to be modified 01/13/23 10:27:26.945
STEP: patching ReplicationController status 01/13/23 10:27:26.945
STEP: waiting for RC to be modified 01/13/23 10:27:26.973
STEP: waiting for available Replicas 01/13/23 10:27:26.983
STEP: fetching ReplicationController status 01/13/23 10:27:26.998
STEP: patching ReplicationController scale 01/13/23 10:27:27.009
STEP: waiting for RC to be modified 01/13/23 10:27:27.025
STEP: waiting for ReplicationController's scale to be the max amount 01/13/23 10:27:27.025
STEP: fetching ReplicationController; ensuring that it's patched 01/13/23 10:27:29.501
STEP: updating ReplicationController status 01/13/23 10:27:29.511
STEP: waiting for RC to be modified 01/13/23 10:27:29.522
STEP: listing all ReplicationControllers 01/13/23 10:27:29.523
STEP: checking that ReplicationController has expected values 01/13/23 10:27:29.531
STEP: deleting ReplicationControllers by collection 01/13/23 10:27:29.532
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/13/23 10:27:29.545
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:29.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5744" for this suite. 01/13/23 10:27:29.634
------------------------------
• [4.956 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:24.704
    Jan 13 10:27:24.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replication-controller 01/13/23 10:27:24.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:24.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:24.744
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/13/23 10:27:24.758
    STEP: waiting for RC to be added 01/13/23 10:27:24.771
    STEP: waiting for available Replicas 01/13/23 10:27:24.772
    STEP: patching ReplicationController 01/13/23 10:27:26.911
    STEP: waiting for RC to be modified 01/13/23 10:27:26.945
    STEP: patching ReplicationController status 01/13/23 10:27:26.945
    STEP: waiting for RC to be modified 01/13/23 10:27:26.973
    STEP: waiting for available Replicas 01/13/23 10:27:26.983
    STEP: fetching ReplicationController status 01/13/23 10:27:26.998
    STEP: patching ReplicationController scale 01/13/23 10:27:27.009
    STEP: waiting for RC to be modified 01/13/23 10:27:27.025
    STEP: waiting for ReplicationController's scale to be the max amount 01/13/23 10:27:27.025
    STEP: fetching ReplicationController; ensuring that it's patched 01/13/23 10:27:29.501
    STEP: updating ReplicationController status 01/13/23 10:27:29.511
    STEP: waiting for RC to be modified 01/13/23 10:27:29.522
    STEP: listing all ReplicationControllers 01/13/23 10:27:29.523
    STEP: checking that ReplicationController has expected values 01/13/23 10:27:29.531
    STEP: deleting ReplicationControllers by collection 01/13/23 10:27:29.532
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/13/23 10:27:29.545
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:29.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5744" for this suite. 01/13/23 10:27:29.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:29.662
Jan 13 10:27:29.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:27:29.665
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:29.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:29.729
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/13/23 10:27:29.735
Jan 13 10:27:29.763: INFO: Waiting up to 5m0s for pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16" in namespace "svcaccounts-478" to be "Succeeded or Failed"
Jan 13 10:27:29.786: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Pending", Reason="", readiness=false. Elapsed: 23.239363ms
Jan 13 10:27:31.795: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032727662s
Jan 13 10:27:33.795: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Running", Reason="", readiness=false. Elapsed: 4.032713285s
Jan 13 10:27:35.796: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Running", Reason="", readiness=false. Elapsed: 6.033876102s
Jan 13 10:27:37.811: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.048905673s
STEP: Saw pod success 01/13/23 10:27:37.812
Jan 13 10:27:37.812: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16" satisfied condition "Succeeded or Failed"
Jan 13 10:27:37.829: INFO: Trying to get logs from node 10.10.102.31-node pod test-pod-727a91a0-6aae-4608-886f-5a36ca118b16 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:27:37.918
Jan 13 10:27:37.960: INFO: Waiting for pod test-pod-727a91a0-6aae-4608-886f-5a36ca118b16 to disappear
Jan 13 10:27:37.968: INFO: Pod test-pod-727a91a0-6aae-4608-886f-5a36ca118b16 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:37.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-478" for this suite. 01/13/23 10:27:37.978
------------------------------
• [SLOW TEST] [8.327 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:29.662
    Jan 13 10:27:29.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:27:29.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:29.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:29.729
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/13/23 10:27:29.735
    Jan 13 10:27:29.763: INFO: Waiting up to 5m0s for pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16" in namespace "svcaccounts-478" to be "Succeeded or Failed"
    Jan 13 10:27:29.786: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Pending", Reason="", readiness=false. Elapsed: 23.239363ms
    Jan 13 10:27:31.795: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032727662s
    Jan 13 10:27:33.795: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Running", Reason="", readiness=false. Elapsed: 4.032713285s
    Jan 13 10:27:35.796: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Running", Reason="", readiness=false. Elapsed: 6.033876102s
    Jan 13 10:27:37.811: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.048905673s
    STEP: Saw pod success 01/13/23 10:27:37.812
    Jan 13 10:27:37.812: INFO: Pod "test-pod-727a91a0-6aae-4608-886f-5a36ca118b16" satisfied condition "Succeeded or Failed"
    Jan 13 10:27:37.829: INFO: Trying to get logs from node 10.10.102.31-node pod test-pod-727a91a0-6aae-4608-886f-5a36ca118b16 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:27:37.918
    Jan 13 10:27:37.960: INFO: Waiting for pod test-pod-727a91a0-6aae-4608-886f-5a36ca118b16 to disappear
    Jan 13 10:27:37.968: INFO: Pod test-pod-727a91a0-6aae-4608-886f-5a36ca118b16 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:37.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-478" for this suite. 01/13/23 10:27:37.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:37.991
Jan 13 10:27:37.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-pred 01/13/23 10:27:37.997
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:38.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:38.045
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 13 10:27:38.061: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 10:27:38.092: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 10:27:38.101: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.31-node before test
Jan 13 10:27:38.126: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 10:27:38.126: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 10:27:38.126: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
Jan 13 10:27:38.126: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 10:27:38.126: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:27:38.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:27:38.126: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 10:27:38.126: INFO: 
Logging pods the apiserver thinks is on node 10.10.102.32-node before test
Jan 13 10:27:38.147: INFO: calico-kube-controllers-7bdbfc669-lhkk6 from kube-system started at 2023-01-13 09:33:43 +0000 UTC (1 container statuses recorded)
Jan 13 10:27:38.147: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 13 10:27:38.147: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
Jan 13 10:27:38.147: INFO: 	Container calico-node ready: true, restart count 2
Jan 13 10:27:38.147: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
Jan 13 10:27:38.148: INFO: 	Container coredns ready: true, restart count 0
Jan 13 10:27:38.148: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
Jan 13 10:27:38.148: INFO: 	Container kube-proxy ready: true, restart count 2
Jan 13 10:27:38.148: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
Jan 13 10:27:38.148: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 10:27:38.148: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:27:38.148: INFO: 	Container e2e ready: true, restart count 0
Jan 13 10:27:38.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:27:38.148: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
Jan 13 10:27:38.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 10:27:38.148: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/13/23 10:27:38.148
Jan 13 10:27:38.169: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-252" to be "running"
Jan 13 10:27:38.177: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.865303ms
Jan 13 10:27:40.189: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019174439s
Jan 13 10:27:42.187: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.017345095s
Jan 13 10:27:42.187: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/13/23 10:27:42.195
STEP: Trying to apply a random label on the found node. 01/13/23 10:27:42.24
STEP: verifying the node has the label kubernetes.io/e2e-043e7f52-40f3-434c-812a-b16da94ddd4b 42 01/13/23 10:27:42.289
STEP: Trying to relaunch the pod, now with labels. 01/13/23 10:27:42.327
Jan 13 10:27:42.349: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-252" to be "not pending"
Jan 13 10:27:42.366: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 16.593861ms
Jan 13 10:27:44.374: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025019763s
Jan 13 10:27:46.375: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.025477004s
Jan 13 10:27:46.375: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-043e7f52-40f3-434c-812a-b16da94ddd4b off the node 10.10.102.31-node 01/13/23 10:27:46.379
STEP: verifying the node doesn't have the label kubernetes.io/e2e-043e7f52-40f3-434c-812a-b16da94ddd4b 01/13/23 10:27:46.408
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:46.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-252" for this suite. 01/13/23 10:27:46.431
------------------------------
• [SLOW TEST] [8.455 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:37.991
    Jan 13 10:27:37.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-pred 01/13/23 10:27:37.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:38.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:38.045
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 13 10:27:38.061: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 13 10:27:38.092: INFO: Waiting for terminating namespaces to be deleted...
    Jan 13 10:27:38.101: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.31-node before test
    Jan 13 10:27:38.126: INFO: calico-node-zddvc from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 10:27:38.126: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 10:27:38.126: INFO: kube-proxy-hpcvx from kube-system started at 2023-01-09 09:13:48 +0000 UTC (1 container statuses recorded)
    Jan 13 10:27:38.126: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 10:27:38.126: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-qmf79 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:27:38.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:27:38.126: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 13 10:27:38.126: INFO: 
    Logging pods the apiserver thinks is on node 10.10.102.32-node before test
    Jan 13 10:27:38.147: INFO: calico-kube-controllers-7bdbfc669-lhkk6 from kube-system started at 2023-01-13 09:33:43 +0000 UTC (1 container statuses recorded)
    Jan 13 10:27:38.147: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 13 10:27:38.147: INFO: calico-node-rhspb from kube-system started at 2023-01-10 07:15:34 +0000 UTC (1 container statuses recorded)
    Jan 13 10:27:38.147: INFO: 	Container calico-node ready: true, restart count 2
    Jan 13 10:27:38.147: INFO: coredns-5bbd96d687-qk6p8 from kube-system started at 2023-01-13 09:05:53 +0000 UTC (1 container statuses recorded)
    Jan 13 10:27:38.148: INFO: 	Container coredns ready: true, restart count 0
    Jan 13 10:27:38.148: INFO: kube-proxy-tjxsm from kube-system started at 2023-01-09 09:31:24 +0000 UTC (1 container statuses recorded)
    Jan 13 10:27:38.148: INFO: 	Container kube-proxy ready: true, restart count 2
    Jan 13 10:27:38.148: INFO: sonobuoy from sonobuoy started at 2023-01-13 09:03:27 +0000 UTC (1 container statuses recorded)
    Jan 13 10:27:38.148: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 13 10:27:38.148: INFO: sonobuoy-e2e-job-3d6873fba06b4897 from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:27:38.148: INFO: 	Container e2e ready: true, restart count 0
    Jan 13 10:27:38.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:27:38.148: INFO: sonobuoy-systemd-logs-daemon-set-e02266d812b8486d-9s6bk from sonobuoy started at 2023-01-13 09:03:30 +0000 UTC (2 container statuses recorded)
    Jan 13 10:27:38.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 13 10:27:38.148: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/13/23 10:27:38.148
    Jan 13 10:27:38.169: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-252" to be "running"
    Jan 13 10:27:38.177: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.865303ms
    Jan 13 10:27:40.189: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019174439s
    Jan 13 10:27:42.187: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.017345095s
    Jan 13 10:27:42.187: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/13/23 10:27:42.195
    STEP: Trying to apply a random label on the found node. 01/13/23 10:27:42.24
    STEP: verifying the node has the label kubernetes.io/e2e-043e7f52-40f3-434c-812a-b16da94ddd4b 42 01/13/23 10:27:42.289
    STEP: Trying to relaunch the pod, now with labels. 01/13/23 10:27:42.327
    Jan 13 10:27:42.349: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-252" to be "not pending"
    Jan 13 10:27:42.366: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 16.593861ms
    Jan 13 10:27:44.374: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025019763s
    Jan 13 10:27:46.375: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.025477004s
    Jan 13 10:27:46.375: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-043e7f52-40f3-434c-812a-b16da94ddd4b off the node 10.10.102.31-node 01/13/23 10:27:46.379
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-043e7f52-40f3-434c-812a-b16da94ddd4b 01/13/23 10:27:46.408
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:46.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-252" for this suite. 01/13/23 10:27:46.431
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:46.447
Jan 13 10:27:46.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:27:46.453
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:46.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:46.489
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-4f96b7af-26c5-45e9-ba5e-16e7400d1120 01/13/23 10:27:46.553
STEP: Creating a pod to test consume secrets 01/13/23 10:27:46.569
Jan 13 10:27:46.591: INFO: Waiting up to 5m0s for pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502" in namespace "secrets-2002" to be "Succeeded or Failed"
Jan 13 10:27:46.597: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089014ms
Jan 13 10:27:48.608: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016843269s
Jan 13 10:27:50.605: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013390803s
Jan 13 10:27:52.616: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024950519s
Jan 13 10:27:54.607: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.015834887s
STEP: Saw pod success 01/13/23 10:27:54.607
Jan 13 10:27:54.608: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502" satisfied condition "Succeeded or Failed"
Jan 13 10:27:54.620: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502 container secret-volume-test: <nil>
STEP: delete the pod 01/13/23 10:27:54.66
Jan 13 10:27:54.680: INFO: Waiting for pod pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502 to disappear
Jan 13 10:27:54.685: INFO: Pod pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:54.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2002" for this suite. 01/13/23 10:27:54.695
STEP: Destroying namespace "secret-namespace-9947" for this suite. 01/13/23 10:27:54.711
------------------------------
• [SLOW TEST] [8.282 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:46.447
    Jan 13 10:27:46.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:27:46.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:46.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:46.489
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-4f96b7af-26c5-45e9-ba5e-16e7400d1120 01/13/23 10:27:46.553
    STEP: Creating a pod to test consume secrets 01/13/23 10:27:46.569
    Jan 13 10:27:46.591: INFO: Waiting up to 5m0s for pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502" in namespace "secrets-2002" to be "Succeeded or Failed"
    Jan 13 10:27:46.597: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089014ms
    Jan 13 10:27:48.608: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016843269s
    Jan 13 10:27:50.605: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013390803s
    Jan 13 10:27:52.616: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024950519s
    Jan 13 10:27:54.607: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.015834887s
    STEP: Saw pod success 01/13/23 10:27:54.607
    Jan 13 10:27:54.608: INFO: Pod "pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502" satisfied condition "Succeeded or Failed"
    Jan 13 10:27:54.620: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502 container secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:27:54.66
    Jan 13 10:27:54.680: INFO: Waiting for pod pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502 to disappear
    Jan 13 10:27:54.685: INFO: Pod pod-secrets-7612427f-455d-47ef-b3e7-3d97fd421502 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:54.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2002" for this suite. 01/13/23 10:27:54.695
    STEP: Destroying namespace "secret-namespace-9947" for this suite. 01/13/23 10:27:54.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:54.731
Jan 13 10:27:54.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 10:27:54.734
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:54.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:54.829
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 13 10:27:54.839: INFO: Creating deployment "test-recreate-deployment"
Jan 13 10:27:54.850: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 13 10:27:54.873: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 13 10:27:56.887: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 13 10:27:56.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:27:58.908: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 13 10:27:58.951: INFO: Updating deployment test-recreate-deployment
Jan 13 10:27:58.951: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 10:27:59.176: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9270  3c0aed60-64b7-4d8c-a1c1-707de950132e 594126 2 2023-01-13 10:27:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-13 10:27:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e3038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-13 10:27:59 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-13 10:27:59 +0000 UTC,LastTransitionTime:2023-01-13 10:27:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 13 10:27:59.192: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9270  7d5ba600-33cc-4f0f-a9a5-7aeab07330a7 594122 1 2023-01-13 10:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3c0aed60-64b7-4d8c-a1c1-707de950132e 0xc004c89130 0xc004c89131}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0aed60-64b7-4d8c-a1c1-707de950132e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c891c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:27:59.192: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 13 10:27:59.193: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9270  cba99316-9cec-4787-a72b-369df3047699 594114 2 2023-01-13 10:27:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3c0aed60-64b7-4d8c-a1c1-707de950132e 0xc004c89017 0xc004c89018}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:27:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0aed60-64b7-4d8c-a1c1-707de950132e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c890c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:27:59.202: INFO: Pod "test-recreate-deployment-cff6dc657-7ns2l" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-7ns2l test-recreate-deployment-cff6dc657- deployment-9270  eb60a45a-1488-4d77-bb52-de07f160faba 594125 0 2023-01-13 10:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7d5ba600-33cc-4f0f-a9a5-7aeab07330a7 0xc0049d6e50 0xc0049d6e51}] [] [{kube-controller-manager Update v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d5ba600-33cc-4f0f-a9a5-7aeab07330a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85kqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85kqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 10:27:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 10:27:59.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9270" for this suite. 01/13/23 10:27:59.223
------------------------------
• [4.502 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:54.731
    Jan 13 10:27:54.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 10:27:54.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:54.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:54.829
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 13 10:27:54.839: INFO: Creating deployment "test-recreate-deployment"
    Jan 13 10:27:54.850: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 13 10:27:54.873: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 13 10:27:56.887: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 13 10:27:56.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 27, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:27:58.908: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 13 10:27:58.951: INFO: Updating deployment test-recreate-deployment
    Jan 13 10:27:58.951: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 10:27:59.176: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9270  3c0aed60-64b7-4d8c-a1c1-707de950132e 594126 2 2023-01-13 10:27:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-13 10:27:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e3038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-13 10:27:59 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-13 10:27:59 +0000 UTC,LastTransitionTime:2023-01-13 10:27:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 13 10:27:59.192: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9270  7d5ba600-33cc-4f0f-a9a5-7aeab07330a7 594122 1 2023-01-13 10:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3c0aed60-64b7-4d8c-a1c1-707de950132e 0xc004c89130 0xc004c89131}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0aed60-64b7-4d8c-a1c1-707de950132e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c891c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:27:59.192: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 13 10:27:59.193: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9270  cba99316-9cec-4787-a72b-369df3047699 594114 2 2023-01-13 10:27:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3c0aed60-64b7-4d8c-a1c1-707de950132e 0xc004c89017 0xc004c89018}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:27:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c0aed60-64b7-4d8c-a1c1-707de950132e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c890c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:27:59.202: INFO: Pod "test-recreate-deployment-cff6dc657-7ns2l" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-7ns2l test-recreate-deployment-cff6dc657- deployment-9270  eb60a45a-1488-4d77-bb52-de07f160faba 594125 0 2023-01-13 10:27:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7d5ba600-33cc-4f0f-a9a5-7aeab07330a7 0xc0049d6e50 0xc0049d6e51}] [] [{kube-controller-manager Update v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d5ba600-33cc-4f0f-a9a5-7aeab07330a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 10:27:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85kqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85kqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:27:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:,StartTime:2023-01-13 10:27:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:27:59.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9270" for this suite. 01/13/23 10:27:59.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:27:59.241
Jan 13 10:27:59.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:27:59.245
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:59.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:59.282
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:27:59.316
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:28:02.162
STEP: Deploying the webhook pod 01/13/23 10:28:02.184
STEP: Wait for the deployment to be ready 01/13/23 10:28:02.216
Jan 13 10:28:02.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:28:04.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:28:06.335
STEP: Verifying the service has paired with the endpoint 01/13/23 10:28:06.368
Jan 13 10:28:07.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/13/23 10:28:07.382
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/13/23 10:28:07.44
STEP: Creating a dummy validating-webhook-configuration object 01/13/23 10:28:07.47
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/13/23 10:28:07.497
STEP: Creating a dummy mutating-webhook-configuration object 01/13/23 10:28:07.512
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/13/23 10:28:07.529
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:28:07.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7823" for this suite. 01/13/23 10:28:07.631
STEP: Destroying namespace "webhook-7823-markers" for this suite. 01/13/23 10:28:07.65
------------------------------
• [SLOW TEST] [8.430 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:27:59.241
    Jan 13 10:27:59.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:27:59.245
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:27:59.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:27:59.282
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:27:59.316
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:28:02.162
    STEP: Deploying the webhook pod 01/13/23 10:28:02.184
    STEP: Wait for the deployment to be ready 01/13/23 10:28:02.216
    Jan 13 10:28:02.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:28:04.325: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 28, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:28:06.335
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:28:06.368
    Jan 13 10:28:07.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/13/23 10:28:07.382
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/13/23 10:28:07.44
    STEP: Creating a dummy validating-webhook-configuration object 01/13/23 10:28:07.47
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/13/23 10:28:07.497
    STEP: Creating a dummy mutating-webhook-configuration object 01/13/23 10:28:07.512
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/13/23 10:28:07.529
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:28:07.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7823" for this suite. 01/13/23 10:28:07.631
    STEP: Destroying namespace "webhook-7823-markers" for this suite. 01/13/23 10:28:07.65
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:28:07.672
Jan 13 10:28:07.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 10:28:07.675
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:07.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:07.719
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/13/23 10:28:07.732
Jan 13 10:28:07.751: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-19" to be "running and ready"
Jan 13 10:28:07.758: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.635003ms
Jan 13 10:28:07.758: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:28:09.827: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075170277s
Jan 13 10:28:09.827: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:28:11.773: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.021794289s
Jan 13 10:28:11.773: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 13 10:28:11.773: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/13/23 10:28:11.779
Jan 13 10:28:11.792: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-19" to be "running and ready"
Jan 13 10:28:11.800: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.331384ms
Jan 13 10:28:11.800: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:28:13.814: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02221062s
Jan 13 10:28:13.814: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:28:15.809: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.017065317s
Jan 13 10:28:15.809: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 13 10:28:15.809: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/13/23 10:28:15.819
STEP: delete the pod with lifecycle hook 01/13/23 10:28:15.836
Jan 13 10:28:15.847: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 10:28:15.855: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 10:28:17.856: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 10:28:17.870: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 10:28:19.855: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 10:28:19.876: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 13 10:28:19.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-19" for this suite. 01/13/23 10:28:19.894
------------------------------
• [SLOW TEST] [12.243 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:28:07.672
    Jan 13 10:28:07.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/13/23 10:28:07.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:07.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:07.719
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/13/23 10:28:07.732
    Jan 13 10:28:07.751: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-19" to be "running and ready"
    Jan 13 10:28:07.758: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.635003ms
    Jan 13 10:28:07.758: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:28:09.827: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075170277s
    Jan 13 10:28:09.827: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:28:11.773: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.021794289s
    Jan 13 10:28:11.773: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 13 10:28:11.773: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/13/23 10:28:11.779
    Jan 13 10:28:11.792: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-19" to be "running and ready"
    Jan 13 10:28:11.800: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.331384ms
    Jan 13 10:28:11.800: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:28:13.814: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02221062s
    Jan 13 10:28:13.814: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:28:15.809: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.017065317s
    Jan 13 10:28:15.809: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 13 10:28:15.809: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/13/23 10:28:15.819
    STEP: delete the pod with lifecycle hook 01/13/23 10:28:15.836
    Jan 13 10:28:15.847: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 13 10:28:15.855: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 13 10:28:17.856: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 13 10:28:17.870: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 13 10:28:19.855: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 13 10:28:19.876: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:28:19.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-19" for this suite. 01/13/23 10:28:19.894
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:28:19.917
Jan 13 10:28:19.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:28:19.92
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:19.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:19.994
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-d3ed048c-7d3a-423e-a076-c70c7109c817 01/13/23 10:28:20.008
STEP: Creating a pod to test consume configMaps 01/13/23 10:28:20.028
Jan 13 10:28:20.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e" in namespace "projected-61" to be "Succeeded or Failed"
Jan 13 10:28:20.061: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.583321ms
Jan 13 10:28:22.083: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031965074s
Jan 13 10:28:24.080: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029631931s
Jan 13 10:28:26.068: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016919933s
Jan 13 10:28:28.089: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037885162s
STEP: Saw pod success 01/13/23 10:28:28.089
Jan 13 10:28:28.089: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e" satisfied condition "Succeeded or Failed"
Jan 13 10:28:28.103: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:28:28.132
Jan 13 10:28:28.161: INFO: Waiting for pod pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e to disappear
Jan 13 10:28:28.169: INFO: Pod pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:28:28.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-61" for this suite. 01/13/23 10:28:28.181
------------------------------
• [SLOW TEST] [8.280 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:28:19.917
    Jan 13 10:28:19.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:28:19.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:19.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:19.994
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-d3ed048c-7d3a-423e-a076-c70c7109c817 01/13/23 10:28:20.008
    STEP: Creating a pod to test consume configMaps 01/13/23 10:28:20.028
    Jan 13 10:28:20.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e" in namespace "projected-61" to be "Succeeded or Failed"
    Jan 13 10:28:20.061: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.583321ms
    Jan 13 10:28:22.083: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031965074s
    Jan 13 10:28:24.080: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029631931s
    Jan 13 10:28:26.068: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016919933s
    Jan 13 10:28:28.089: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037885162s
    STEP: Saw pod success 01/13/23 10:28:28.089
    Jan 13 10:28:28.089: INFO: Pod "pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e" satisfied condition "Succeeded or Failed"
    Jan 13 10:28:28.103: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:28:28.132
    Jan 13 10:28:28.161: INFO: Waiting for pod pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e to disappear
    Jan 13 10:28:28.169: INFO: Pod pod-projected-configmaps-3e3aaf7d-8357-4374-8b31-32995c34739e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:28:28.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-61" for this suite. 01/13/23 10:28:28.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:28:28.201
Jan 13 10:28:28.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:28:28.204
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:28.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:28.234
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/13/23 10:28:28.24
Jan 13 10:28:28.240: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-5398 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/13/23 10:28:28.433
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:28:28.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5398" for this suite. 01/13/23 10:28:28.466
------------------------------
• [0.275 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:28:28.201
    Jan 13 10:28:28.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:28:28.204
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:28.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:28.234
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/13/23 10:28:28.24
    Jan 13 10:28:28.240: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-5398 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/13/23 10:28:28.433
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:28:28.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5398" for this suite. 01/13/23 10:28:28.466
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:28:28.477
Jan 13 10:28:28.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubelet-test 01/13/23 10:28:28.48
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:28.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:28.504
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 13 10:28:28.523: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e" in namespace "kubelet-test-1798" to be "running and ready"
Jan 13 10:28:28.528: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.978231ms
Jan 13 10:28:28.528: INFO: The phase of Pod busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:28:30.560: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037449819s
Jan 13 10:28:30.560: INFO: The phase of Pod busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:28:32.538: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e": Phase="Running", Reason="", readiness=true. Elapsed: 4.015520653s
Jan 13 10:28:32.538: INFO: The phase of Pod busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e is Running (Ready = true)
Jan 13 10:28:32.538: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:28:32.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1798" for this suite. 01/13/23 10:28:32.599
------------------------------
• [4.140 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:28:28.477
    Jan 13 10:28:28.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubelet-test 01/13/23 10:28:28.48
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:28.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:28.504
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 13 10:28:28.523: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e" in namespace "kubelet-test-1798" to be "running and ready"
    Jan 13 10:28:28.528: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.978231ms
    Jan 13 10:28:28.528: INFO: The phase of Pod busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:28:30.560: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037449819s
    Jan 13 10:28:30.560: INFO: The phase of Pod busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:28:32.538: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e": Phase="Running", Reason="", readiness=true. Elapsed: 4.015520653s
    Jan 13 10:28:32.538: INFO: The phase of Pod busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e is Running (Ready = true)
    Jan 13 10:28:32.538: INFO: Pod "busybox-scheduling-fbffea47-9d7b-4469-8e28-963c9f43232e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:28:32.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1798" for this suite. 01/13/23 10:28:32.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:28:32.631
Jan 13 10:28:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubelet-test 01/13/23 10:28:32.634
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:32.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:32.685
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/13/23 10:28:32.749
Jan 13 10:28:32.749: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450" in namespace "kubelet-test-5809" to be "completed"
Jan 13 10:28:32.764: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 15.176097ms
Jan 13 10:28:34.775: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02548315s
Jan 13 10:28:36.771: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021490239s
Jan 13 10:28:38.773: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024146836s
Jan 13 10:28:40.798: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04910497s
Jan 13 10:28:40.799: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:28:40.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5809" for this suite. 01/13/23 10:28:40.875
------------------------------
• [SLOW TEST] [8.263 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:28:32.631
    Jan 13 10:28:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubelet-test 01/13/23 10:28:32.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:32.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:32.685
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/13/23 10:28:32.749
    Jan 13 10:28:32.749: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450" in namespace "kubelet-test-5809" to be "completed"
    Jan 13 10:28:32.764: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 15.176097ms
    Jan 13 10:28:34.775: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02548315s
    Jan 13 10:28:36.771: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021490239s
    Jan 13 10:28:38.773: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024146836s
    Jan 13 10:28:40.798: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04910497s
    Jan 13 10:28:40.799: INFO: Pod "agnhost-host-aliases1da6473e-eb83-4a59-8193-cc740977b450" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:28:40.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5809" for this suite. 01/13/23 10:28:40.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:28:40.899
Jan 13 10:28:40.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename daemonsets 01/13/23 10:28:40.902
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:40.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:40.975
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/13/23 10:28:41.07
STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 10:28:41.107
Jan 13 10:28:41.119: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:28:41.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:28:41.129: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 10:28:42.142: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:28:42.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:28:42.155: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 10:28:43.138: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:28:43.148: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:28:43.148: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
Jan 13 10:28:44.146: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 10:28:44.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 13 10:28:44.157: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/13/23 10:28:44.166
Jan 13 10:28:44.177: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/13/23 10:28:44.177
Jan 13 10:28:44.218: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/13/23 10:28:44.218
Jan 13 10:28:44.227: INFO: Observed &DaemonSet event: ADDED
Jan 13 10:28:44.228: INFO: Observed &DaemonSet event: MODIFIED
Jan 13 10:28:44.228: INFO: Observed &DaemonSet event: MODIFIED
Jan 13 10:28:44.229: INFO: Observed &DaemonSet event: MODIFIED
Jan 13 10:28:44.229: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 13 10:28:44.229: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/13/23 10:28:44.229
STEP: watching for the daemon set status to be patched 01/13/23 10:28:44.254
Jan 13 10:28:44.261: INFO: Observed &DaemonSet event: ADDED
Jan 13 10:28:44.262: INFO: Observed &DaemonSet event: MODIFIED
Jan 13 10:28:44.262: INFO: Observed &DaemonSet event: MODIFIED
Jan 13 10:28:44.263: INFO: Observed &DaemonSet event: MODIFIED
Jan 13 10:28:44.263: INFO: Observed daemon set daemon-set in namespace daemonsets-8644 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 13 10:28:44.263: INFO: Observed &DaemonSet event: MODIFIED
Jan 13 10:28:44.263: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 13 10:28:44.263: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/13/23 10:28:44.271
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8644, will wait for the garbage collector to delete the pods 01/13/23 10:28:44.272
Jan 13 10:28:44.358: INFO: Deleting DaemonSet.extensions daemon-set took: 15.271919ms
Jan 13 10:28:44.459: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.048143ms
Jan 13 10:28:48.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 13 10:28:48.269: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 13 10:28:48.275: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"594526"},"items":null}

Jan 13 10:28:48.281: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"594526"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:28:48.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8644" for this suite. 01/13/23 10:28:48.331
------------------------------
• [SLOW TEST] [7.456 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:28:40.899
    Jan 13 10:28:40.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename daemonsets 01/13/23 10:28:40.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:40.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:40.975
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/13/23 10:28:41.07
    STEP: Check that daemon pods launch on every node of the cluster. 01/13/23 10:28:41.107
    Jan 13 10:28:41.119: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:28:41.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:28:41.129: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 10:28:42.142: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:28:42.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:28:42.155: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 10:28:43.138: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:28:43.148: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:28:43.148: INFO: Node 10.10.102.31-node is running 0 daemon pod, expected 1
    Jan 13 10:28:44.146: INFO: DaemonSet pods can't tolerate node 10.10.102.30-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 13 10:28:44.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 13 10:28:44.157: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/13/23 10:28:44.166
    Jan 13 10:28:44.177: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/13/23 10:28:44.177
    Jan 13 10:28:44.218: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/13/23 10:28:44.218
    Jan 13 10:28:44.227: INFO: Observed &DaemonSet event: ADDED
    Jan 13 10:28:44.228: INFO: Observed &DaemonSet event: MODIFIED
    Jan 13 10:28:44.228: INFO: Observed &DaemonSet event: MODIFIED
    Jan 13 10:28:44.229: INFO: Observed &DaemonSet event: MODIFIED
    Jan 13 10:28:44.229: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 13 10:28:44.229: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/13/23 10:28:44.229
    STEP: watching for the daemon set status to be patched 01/13/23 10:28:44.254
    Jan 13 10:28:44.261: INFO: Observed &DaemonSet event: ADDED
    Jan 13 10:28:44.262: INFO: Observed &DaemonSet event: MODIFIED
    Jan 13 10:28:44.262: INFO: Observed &DaemonSet event: MODIFIED
    Jan 13 10:28:44.263: INFO: Observed &DaemonSet event: MODIFIED
    Jan 13 10:28:44.263: INFO: Observed daemon set daemon-set in namespace daemonsets-8644 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 13 10:28:44.263: INFO: Observed &DaemonSet event: MODIFIED
    Jan 13 10:28:44.263: INFO: Found daemon set daemon-set in namespace daemonsets-8644 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 13 10:28:44.263: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/13/23 10:28:44.271
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8644, will wait for the garbage collector to delete the pods 01/13/23 10:28:44.272
    Jan 13 10:28:44.358: INFO: Deleting DaemonSet.extensions daemon-set took: 15.271919ms
    Jan 13 10:28:44.459: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.048143ms
    Jan 13 10:28:48.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 13 10:28:48.269: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 13 10:28:48.275: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"594526"},"items":null}

    Jan 13 10:28:48.281: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"594526"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:28:48.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8644" for this suite. 01/13/23 10:28:48.331
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:28:48.356
Jan 13 10:28:48.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 10:28:48.359
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:48.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:48.449
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/13/23 10:28:48.456
STEP: Creating a ResourceQuota 01/13/23 10:28:53.466
STEP: Ensuring resource quota status is calculated 01/13/23 10:28:53.479
STEP: Creating a Pod that fits quota 01/13/23 10:28:55.486
STEP: Ensuring ResourceQuota status captures the pod usage 01/13/23 10:28:55.517
STEP: Not allowing a pod to be created that exceeds remaining quota 01/13/23 10:28:57.524
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/13/23 10:28:57.529
STEP: Ensuring a pod cannot update its resource requirements 01/13/23 10:28:57.534
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/13/23 10:28:57.543
STEP: Deleting the pod 01/13/23 10:28:59.551
STEP: Ensuring resource quota status released the pod usage 01/13/23 10:28:59.581
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:01.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2564" for this suite. 01/13/23 10:29:01.597
------------------------------
• [SLOW TEST] [13.252 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:28:48.356
    Jan 13 10:28:48.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 10:28:48.359
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:28:48.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:28:48.449
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/13/23 10:28:48.456
    STEP: Creating a ResourceQuota 01/13/23 10:28:53.466
    STEP: Ensuring resource quota status is calculated 01/13/23 10:28:53.479
    STEP: Creating a Pod that fits quota 01/13/23 10:28:55.486
    STEP: Ensuring ResourceQuota status captures the pod usage 01/13/23 10:28:55.517
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/13/23 10:28:57.524
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/13/23 10:28:57.529
    STEP: Ensuring a pod cannot update its resource requirements 01/13/23 10:28:57.534
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/13/23 10:28:57.543
    STEP: Deleting the pod 01/13/23 10:28:59.551
    STEP: Ensuring resource quota status released the pod usage 01/13/23 10:28:59.581
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:01.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2564" for this suite. 01/13/23 10:29:01.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:01.613
Jan 13 10:29:01.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 10:29:01.617
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:01.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:01.652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 13 10:29:01.660: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 13 10:29:01.677: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 13 10:29:06.691: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/13/23 10:29:06.691
Jan 13 10:29:06.692: INFO: Creating deployment "test-rolling-update-deployment"
Jan 13 10:29:06.705: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 13 10:29:06.729: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 13 10:29:08.742: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 13 10:29:08.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:29:10.782: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 10:29:10.829: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3978  b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9 594655 1 2023-01-13 10:29:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-13 10:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042193b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 10:29:06 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-13 10:29:09 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 13 10:29:10.837: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3978  38b8ee7a-10cb-4486-8a90-bd2bfb35194c 594641 1 2023-01-13 10:29:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9 0xc003571f57 0xc003571f58}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001db8008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:29:10.837: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 13 10:29:10.837: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3978  15799720-3d54-4ae5-97eb-c232af5771aa 594654 2 2023-01-13 10:29:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9 0xc003571e17 0xc003571e18}] [] [{e2e.test Update apps/v1 2023-01-13 10:29:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003571ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:29:10.851: INFO: Pod "test-rolling-update-deployment-7549d9f46d-9lzdz" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-9lzdz test-rolling-update-deployment-7549d9f46d- deployment-3978  61566954-4841-4b0d-beee-28f9c6b9bbbb 594640 0 2023-01-13 10:29:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:13142bb2d88ab3a64f4754d704cbeb84d2f9edeaed1bbd45eb0f2a9adb87a80b cni.projectcalico.org/podIP:10.244.27.228/32 cni.projectcalico.org/podIPs:10.244.27.228/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 38b8ee7a-10cb-4486-8a90-bd2bfb35194c 0xc001db8487 0xc001db8488}] [] [{kube-controller-manager Update v1 2023-01-13 10:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38b8ee7a-10cb-4486-8a90-bd2bfb35194c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk9x5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk9x5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.228,StartTime:2023-01-13 10:29:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker://sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:docker://acd9fd4979722e8245cbb005438cd5dfbb9ce2874a9cd934851daa1c63c7af1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:10.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3978" for this suite. 01/13/23 10:29:10.868
------------------------------
• [SLOW TEST] [9.271 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:01.613
    Jan 13 10:29:01.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 10:29:01.617
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:01.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:01.652
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 13 10:29:01.660: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 13 10:29:01.677: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 13 10:29:06.691: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/13/23 10:29:06.691
    Jan 13 10:29:06.692: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 13 10:29:06.705: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 13 10:29:06.729: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 13 10:29:08.742: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 13 10:29:08.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:29:10.782: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 10:29:10.829: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3978  b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9 594655 1 2023-01-13 10:29:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-13 10:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042193b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 10:29:06 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-13 10:29:09 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 13 10:29:10.837: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3978  38b8ee7a-10cb-4486-8a90-bd2bfb35194c 594641 1 2023-01-13 10:29:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9 0xc003571f57 0xc003571f58}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001db8008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:29:10.837: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 13 10:29:10.837: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3978  15799720-3d54-4ae5-97eb-c232af5771aa 594654 2 2023-01-13 10:29:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9 0xc003571e17 0xc003571e18}] [] [{e2e.test Update apps/v1 2023-01-13 10:29:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9d1c3d6-ff1c-4dae-8c91-5d9bb0237ea9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003571ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:29:10.851: INFO: Pod "test-rolling-update-deployment-7549d9f46d-9lzdz" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-9lzdz test-rolling-update-deployment-7549d9f46d- deployment-3978  61566954-4841-4b0d-beee-28f9c6b9bbbb 594640 0 2023-01-13 10:29:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:13142bb2d88ab3a64f4754d704cbeb84d2f9edeaed1bbd45eb0f2a9adb87a80b cni.projectcalico.org/podIP:10.244.27.228/32 cni.projectcalico.org/podIPs:10.244.27.228/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 38b8ee7a-10cb-4486-8a90-bd2bfb35194c 0xc001db8487 0xc001db8488}] [] [{kube-controller-manager Update v1 2023-01-13 10:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38b8ee7a-10cb-4486-8a90-bd2bfb35194c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk9x5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk9x5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.228,StartTime:2023-01-13 10:29:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker://sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:docker://acd9fd4979722e8245cbb005438cd5dfbb9ce2874a9cd934851daa1c63c7af1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:10.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3978" for this suite. 01/13/23 10:29:10.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:10.898
Jan 13 10:29:10.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:29:10.901
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:10.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:10.959
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/13/23 10:29:10.992
Jan 13 10:29:11.046: INFO: Waiting up to 5m0s for pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f" in namespace "emptydir-8401" to be "Succeeded or Failed"
Jan 13 10:29:11.056: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.585899ms
Jan 13 10:29:13.068: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022489932s
Jan 13 10:29:15.082: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036023822s
Jan 13 10:29:17.065: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019537347s
Jan 13 10:29:19.063: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017131526s
STEP: Saw pod success 01/13/23 10:29:19.063
Jan 13 10:29:19.063: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f" satisfied condition "Succeeded or Failed"
Jan 13 10:29:19.081: INFO: Trying to get logs from node 10.10.102.31-node pod pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f container test-container: <nil>
STEP: delete the pod 01/13/23 10:29:19.107
Jan 13 10:29:19.133: INFO: Waiting for pod pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f to disappear
Jan 13 10:29:19.139: INFO: Pod pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:19.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8401" for this suite. 01/13/23 10:29:19.149
------------------------------
• [SLOW TEST] [8.261 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:10.898
    Jan 13 10:29:10.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:29:10.901
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:10.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:10.959
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/13/23 10:29:10.992
    Jan 13 10:29:11.046: INFO: Waiting up to 5m0s for pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f" in namespace "emptydir-8401" to be "Succeeded or Failed"
    Jan 13 10:29:11.056: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.585899ms
    Jan 13 10:29:13.068: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022489932s
    Jan 13 10:29:15.082: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036023822s
    Jan 13 10:29:17.065: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019537347s
    Jan 13 10:29:19.063: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017131526s
    STEP: Saw pod success 01/13/23 10:29:19.063
    Jan 13 10:29:19.063: INFO: Pod "pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f" satisfied condition "Succeeded or Failed"
    Jan 13 10:29:19.081: INFO: Trying to get logs from node 10.10.102.31-node pod pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f container test-container: <nil>
    STEP: delete the pod 01/13/23 10:29:19.107
    Jan 13 10:29:19.133: INFO: Waiting for pod pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f to disappear
    Jan 13 10:29:19.139: INFO: Pod pod-369bef51-9f1d-4f03-b7e0-0aab730cf25f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:19.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8401" for this suite. 01/13/23 10:29:19.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:19.161
Jan 13 10:29:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:29:19.165
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:19.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:19.2
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/13/23 10:29:19.206
Jan 13 10:29:19.220: INFO: Waiting up to 5m0s for pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7" in namespace "emptydir-9687" to be "Succeeded or Failed"
Jan 13 10:29:19.225: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.082893ms
Jan 13 10:29:21.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013264104s
Jan 13 10:29:23.234: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013645398s
Jan 13 10:29:25.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012516209s
Jan 13 10:29:27.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013140079s
STEP: Saw pod success 01/13/23 10:29:27.233
Jan 13 10:29:27.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7" satisfied condition "Succeeded or Failed"
Jan 13 10:29:27.238: INFO: Trying to get logs from node 10.10.102.31-node pod pod-cc3c3aca-da34-4704-b3f1-b452259221e7 container test-container: <nil>
STEP: delete the pod 01/13/23 10:29:27.263
Jan 13 10:29:27.285: INFO: Waiting for pod pod-cc3c3aca-da34-4704-b3f1-b452259221e7 to disappear
Jan 13 10:29:27.291: INFO: Pod pod-cc3c3aca-da34-4704-b3f1-b452259221e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9687" for this suite. 01/13/23 10:29:27.301
------------------------------
• [SLOW TEST] [8.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:19.161
    Jan 13 10:29:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:29:19.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:19.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:19.2
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/13/23 10:29:19.206
    Jan 13 10:29:19.220: INFO: Waiting up to 5m0s for pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7" in namespace "emptydir-9687" to be "Succeeded or Failed"
    Jan 13 10:29:19.225: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.082893ms
    Jan 13 10:29:21.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013264104s
    Jan 13 10:29:23.234: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013645398s
    Jan 13 10:29:25.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012516209s
    Jan 13 10:29:27.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013140079s
    STEP: Saw pod success 01/13/23 10:29:27.233
    Jan 13 10:29:27.233: INFO: Pod "pod-cc3c3aca-da34-4704-b3f1-b452259221e7" satisfied condition "Succeeded or Failed"
    Jan 13 10:29:27.238: INFO: Trying to get logs from node 10.10.102.31-node pod pod-cc3c3aca-da34-4704-b3f1-b452259221e7 container test-container: <nil>
    STEP: delete the pod 01/13/23 10:29:27.263
    Jan 13 10:29:27.285: INFO: Waiting for pod pod-cc3c3aca-da34-4704-b3f1-b452259221e7 to disappear
    Jan 13 10:29:27.291: INFO: Pod pod-cc3c3aca-da34-4704-b3f1-b452259221e7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9687" for this suite. 01/13/23 10:29:27.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:27.315
Jan 13 10:29:27.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 10:29:27.317
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:27.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:27.359
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 13 10:29:27.369: INFO: Creating simple deployment test-new-deployment
Jan 13 10:29:27.404: INFO: deployment "test-new-deployment" doesn't have the required revision set
Jan 13 10:29:29.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 01/13/23 10:29:31.467
STEP: updating a scale subresource 01/13/23 10:29:31.473
STEP: verifying the deployment Spec.Replicas was modified 01/13/23 10:29:31.484
STEP: Patch a scale subresource 01/13/23 10:29:31.498
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 10:29:31.564: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2363  0f157d2b-a822-455b-8b58-fa02d84a3d4c 594804 3 2023-01-13 10:29:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-13 10:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000bff018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 10:29:30 +0000 UTC,LastTransitionTime:2023-01-13 10:29:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-13 10:29:30 +0000 UTC,LastTransitionTime:2023-01-13 10:29:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 13 10:29:31.582: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2363  361dc4cd-c9c8-4a01-8b4f-a383e1e4d733 594813 3 2023-01-13 10:29:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0f157d2b-a822-455b-8b58-fa02d84a3d4c 0xc000bffd47 0xc000bffd48}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f157d2b-a822-455b-8b58-fa02d84a3d4c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005602018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:29:31.596: INFO: Pod "test-new-deployment-7f5969cbc7-b9pmm" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-b9pmm test-new-deployment-7f5969cbc7- deployment-2363  8414d467-26e9-45d6-b0fa-aef0a0751b10 594812 0 2023-01-13 10:29:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 361dc4cd-c9c8-4a01-8b4f-a383e1e4d733 0xc004e78587 0xc004e78588}] [] [{kube-controller-manager Update v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"361dc4cd-c9c8-4a01-8b4f-a383e1e4d733\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7gx9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7gx9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:,StartTime:2023-01-13 10:29:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 10:29:31.596: INFO: Pod "test-new-deployment-7f5969cbc7-p7k2p" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-p7k2p test-new-deployment-7f5969cbc7- deployment-2363  364207c0-b8ab-4291-bcdd-f97165746ad3 594797 0 2023-01-13 10:29:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1a3786fe0f317d127b48973dd52adf2d20f54c6c276cf0cc53f5ef6e2fd3a7c1 cni.projectcalico.org/podIP:10.244.27.233/32 cni.projectcalico.org/podIPs:10.244.27.233/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 361dc4cd-c9c8-4a01-8b4f-a383e1e4d733 0xc004e78767 0xc004e78768}] [] [{kube-controller-manager Update v1 2023-01-13 10:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"361dc4cd-c9c8-4a01-8b4f-a383e1e4d733\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:29:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:29:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wh45l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wh45l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.233,StartTime:2023-01-13 10:29:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:29:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://7a0aaa4c7595904685699566bcfb467abcdef564c99fbae39f385bd31a636ca7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:31.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2363" for this suite. 01/13/23 10:29:31.607
------------------------------
• [4.322 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:27.315
    Jan 13 10:29:27.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 10:29:27.317
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:27.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:27.359
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 13 10:29:27.369: INFO: Creating simple deployment test-new-deployment
    Jan 13 10:29:27.404: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Jan 13 10:29:29.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 29, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 01/13/23 10:29:31.467
    STEP: updating a scale subresource 01/13/23 10:29:31.473
    STEP: verifying the deployment Spec.Replicas was modified 01/13/23 10:29:31.484
    STEP: Patch a scale subresource 01/13/23 10:29:31.498
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 10:29:31.564: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2363  0f157d2b-a822-455b-8b58-fa02d84a3d4c 594804 3 2023-01-13 10:29:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-13 10:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000bff018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 10:29:30 +0000 UTC,LastTransitionTime:2023-01-13 10:29:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-13 10:29:30 +0000 UTC,LastTransitionTime:2023-01-13 10:29:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 13 10:29:31.582: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2363  361dc4cd-c9c8-4a01-8b4f-a383e1e4d733 594813 3 2023-01-13 10:29:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0f157d2b-a822-455b-8b58-fa02d84a3d4c 0xc000bffd47 0xc000bffd48}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f157d2b-a822-455b-8b58-fa02d84a3d4c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005602018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:29:31.596: INFO: Pod "test-new-deployment-7f5969cbc7-b9pmm" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-b9pmm test-new-deployment-7f5969cbc7- deployment-2363  8414d467-26e9-45d6-b0fa-aef0a0751b10 594812 0 2023-01-13 10:29:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 361dc4cd-c9c8-4a01-8b4f-a383e1e4d733 0xc004e78587 0xc004e78588}] [] [{kube-controller-manager Update v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"361dc4cd-c9c8-4a01-8b4f-a383e1e4d733\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-13 10:29:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7gx9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7gx9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:,StartTime:2023-01-13 10:29:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 13 10:29:31.596: INFO: Pod "test-new-deployment-7f5969cbc7-p7k2p" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-p7k2p test-new-deployment-7f5969cbc7- deployment-2363  364207c0-b8ab-4291-bcdd-f97165746ad3 594797 0 2023-01-13 10:29:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1a3786fe0f317d127b48973dd52adf2d20f54c6c276cf0cc53f5ef6e2fd3a7c1 cni.projectcalico.org/podIP:10.244.27.233/32 cni.projectcalico.org/podIPs:10.244.27.233/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 361dc4cd-c9c8-4a01-8b4f-a383e1e4d733 0xc004e78767 0xc004e78768}] [] [{kube-controller-manager Update v1 2023-01-13 10:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"361dc4cd-c9c8-4a01-8b4f-a383e1e4d733\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:29:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:29:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wh45l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wh45l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:29:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.233,StartTime:2023-01-13 10:29:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:29:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://7a0aaa4c7595904685699566bcfb467abcdef564c99fbae39f385bd31a636ca7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:31.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2363" for this suite. 01/13/23 10:29:31.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:31.649
Jan 13 10:29:31.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:29:31.652
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:31.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:31.687
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/13/23 10:29:31.694
STEP: submitting the pod to kubernetes 01/13/23 10:29:31.695
STEP: verifying QOS class is set on the pod 01/13/23 10:29:31.71
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:31.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-197" for this suite. 01/13/23 10:29:31.737
------------------------------
• [0.102 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:31.649
    Jan 13 10:29:31.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:29:31.652
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:31.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:31.687
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/13/23 10:29:31.694
    STEP: submitting the pod to kubernetes 01/13/23 10:29:31.695
    STEP: verifying QOS class is set on the pod 01/13/23 10:29:31.71
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:31.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-197" for this suite. 01/13/23 10:29:31.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:31.756
Jan 13 10:29:31.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:29:31.76
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:31.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:31.794
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:29:31.801
Jan 13 10:29:31.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc" in namespace "projected-7419" to be "Succeeded or Failed"
Jan 13 10:29:31.820: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.183988ms
Jan 13 10:29:33.831: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015355907s
Jan 13 10:29:35.830: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014878193s
Jan 13 10:29:37.833: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017411002s
STEP: Saw pod success 01/13/23 10:29:37.833
Jan 13 10:29:37.833: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc" satisfied condition "Succeeded or Failed"
Jan 13 10:29:37.842: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc container client-container: <nil>
STEP: delete the pod 01/13/23 10:29:37.912
Jan 13 10:29:37.971: INFO: Waiting for pod downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc to disappear
Jan 13 10:29:37.977: INFO: Pod downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:37.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7419" for this suite. 01/13/23 10:29:37.987
------------------------------
• [SLOW TEST] [6.241 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:31.756
    Jan 13 10:29:31.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:29:31.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:31.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:31.794
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:29:31.801
    Jan 13 10:29:31.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc" in namespace "projected-7419" to be "Succeeded or Failed"
    Jan 13 10:29:31.820: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.183988ms
    Jan 13 10:29:33.831: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015355907s
    Jan 13 10:29:35.830: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014878193s
    Jan 13 10:29:37.833: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017411002s
    STEP: Saw pod success 01/13/23 10:29:37.833
    Jan 13 10:29:37.833: INFO: Pod "downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc" satisfied condition "Succeeded or Failed"
    Jan 13 10:29:37.842: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc container client-container: <nil>
    STEP: delete the pod 01/13/23 10:29:37.912
    Jan 13 10:29:37.971: INFO: Waiting for pod downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc to disappear
    Jan 13 10:29:37.977: INFO: Pod downwardapi-volume-95edd678-5356-453b-b60d-c72552ab19bc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:37.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7419" for this suite. 01/13/23 10:29:37.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:37.998
Jan 13 10:29:37.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 10:29:38.001
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:38.04
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/13/23 10:29:38.046
STEP: Getting a ResourceQuota 01/13/23 10:29:38.053
STEP: Listing all ResourceQuotas with LabelSelector 01/13/23 10:29:38.062
STEP: Patching the ResourceQuota 01/13/23 10:29:38.068
STEP: Deleting a Collection of ResourceQuotas 01/13/23 10:29:38.082
STEP: Verifying the deleted ResourceQuota 01/13/23 10:29:38.1
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:38.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9650" for this suite. 01/13/23 10:29:38.12
------------------------------
• [0.139 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:37.998
    Jan 13 10:29:37.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 10:29:38.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:38.04
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/13/23 10:29:38.046
    STEP: Getting a ResourceQuota 01/13/23 10:29:38.053
    STEP: Listing all ResourceQuotas with LabelSelector 01/13/23 10:29:38.062
    STEP: Patching the ResourceQuota 01/13/23 10:29:38.068
    STEP: Deleting a Collection of ResourceQuotas 01/13/23 10:29:38.082
    STEP: Verifying the deleted ResourceQuota 01/13/23 10:29:38.1
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:38.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9650" for this suite. 01/13/23 10:29:38.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:38.14
Jan 13 10:29:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:29:38.143
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:38.189
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7246" for this suite. 01/13/23 10:29:38.236
------------------------------
• [0.117 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:38.14
    Jan 13 10:29:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:29:38.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:38.189
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7246" for this suite. 01/13/23 10:29:38.236
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:38.259
Jan 13 10:29:38.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename namespaces 01/13/23 10:29:38.262
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:38.308
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/13/23 10:29:38.338
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.385
STEP: Creating a service in the namespace 01/13/23 10:29:38.392
STEP: Deleting the namespace 01/13/23 10:29:38.416
STEP: Waiting for the namespace to be removed. 01/13/23 10:29:38.439
STEP: Recreating the namespace 01/13/23 10:29:44.462
STEP: Verifying there is no service in the namespace 01/13/23 10:29:44.522
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:44.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1492" for this suite. 01/13/23 10:29:44.558
STEP: Destroying namespace "nsdeletetest-2263" for this suite. 01/13/23 10:29:44.575
Jan 13 10:29:44.582: INFO: Namespace nsdeletetest-2263 was already deleted
STEP: Destroying namespace "nsdeletetest-4575" for this suite. 01/13/23 10:29:44.582
------------------------------
• [SLOW TEST] [6.333 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:38.259
    Jan 13 10:29:38.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename namespaces 01/13/23 10:29:38.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:38.308
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/13/23 10:29:38.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:38.385
    STEP: Creating a service in the namespace 01/13/23 10:29:38.392
    STEP: Deleting the namespace 01/13/23 10:29:38.416
    STEP: Waiting for the namespace to be removed. 01/13/23 10:29:38.439
    STEP: Recreating the namespace 01/13/23 10:29:44.462
    STEP: Verifying there is no service in the namespace 01/13/23 10:29:44.522
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:44.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1492" for this suite. 01/13/23 10:29:44.558
    STEP: Destroying namespace "nsdeletetest-2263" for this suite. 01/13/23 10:29:44.575
    Jan 13 10:29:44.582: INFO: Namespace nsdeletetest-2263 was already deleted
    STEP: Destroying namespace "nsdeletetest-4575" for this suite. 01/13/23 10:29:44.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:44.594
Jan 13 10:29:44.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 10:29:44.597
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:44.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:44.641
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:29:44.649
Jan 13 10:29:44.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871" in namespace "downward-api-8328" to be "Succeeded or Failed"
Jan 13 10:29:44.678: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400803ms
Jan 13 10:29:46.724: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051758519s
Jan 13 10:29:48.687: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014780297s
Jan 13 10:29:50.687: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014540403s
STEP: Saw pod success 01/13/23 10:29:50.687
Jan 13 10:29:50.687: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871" satisfied condition "Succeeded or Failed"
Jan 13 10:29:50.697: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871 container client-container: <nil>
STEP: delete the pod 01/13/23 10:29:50.716
Jan 13 10:29:50.754: INFO: Waiting for pod downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871 to disappear
Jan 13 10:29:50.765: INFO: Pod downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 10:29:50.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8328" for this suite. 01/13/23 10:29:50.774
------------------------------
• [SLOW TEST] [6.199 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:44.594
    Jan 13 10:29:44.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 10:29:44.597
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:44.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:44.641
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:29:44.649
    Jan 13 10:29:44.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871" in namespace "downward-api-8328" to be "Succeeded or Failed"
    Jan 13 10:29:44.678: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Pending", Reason="", readiness=false. Elapsed: 5.400803ms
    Jan 13 10:29:46.724: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051758519s
    Jan 13 10:29:48.687: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014780297s
    Jan 13 10:29:50.687: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014540403s
    STEP: Saw pod success 01/13/23 10:29:50.687
    Jan 13 10:29:50.687: INFO: Pod "downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871" satisfied condition "Succeeded or Failed"
    Jan 13 10:29:50.697: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:29:50.716
    Jan 13 10:29:50.754: INFO: Waiting for pod downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871 to disappear
    Jan 13 10:29:50.765: INFO: Pod downwardapi-volume-e3fb56b4-a948-4e11-b30f-4d5cbe70a871 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:29:50.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8328" for this suite. 01/13/23 10:29:50.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:29:50.797
Jan 13 10:29:50.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-preemption 01/13/23 10:29:50.799
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:50.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:50.838
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 13 10:29:50.876: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 10:30:50.940: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:30:50.947
Jan 13 10:30:50.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sched-preemption-path 01/13/23 10:30:50.95
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:30:50.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:30:50.991
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 01/13/23 10:30:50.999
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/13/23 10:30:50.999
Jan 13 10:30:51.016: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2087" to be "running"
Jan 13 10:30:51.022: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.324873ms
Jan 13 10:30:53.031: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015318754s
Jan 13 10:30:55.033: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.017342377s
Jan 13 10:30:55.033: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/13/23 10:30:55.039
Jan 13 10:30:55.058: INFO: found a healthy node: 10.10.102.31-node
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Jan 13 10:31:07.248: INFO: pods created so far: [1 1 1]
Jan 13 10:31:07.248: INFO: length of pods created so far: 3
Jan 13 10:31:11.265: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 13 10:31:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:31:18.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2087" for this suite. 01/13/23 10:31:18.448
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-718" for this suite. 01/13/23 10:31:18.458
------------------------------
• [SLOW TEST] [87.677 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:29:50.797
    Jan 13 10:29:50.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-preemption 01/13/23 10:29:50.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:29:50.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:29:50.838
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 13 10:29:50.876: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 13 10:30:50.940: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:30:50.947
    Jan 13 10:30:50.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sched-preemption-path 01/13/23 10:30:50.95
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:30:50.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:30:50.991
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 01/13/23 10:30:50.999
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/13/23 10:30:50.999
    Jan 13 10:30:51.016: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2087" to be "running"
    Jan 13 10:30:51.022: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.324873ms
    Jan 13 10:30:53.031: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015318754s
    Jan 13 10:30:55.033: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.017342377s
    Jan 13 10:30:55.033: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/13/23 10:30:55.039
    Jan 13 10:30:55.058: INFO: found a healthy node: 10.10.102.31-node
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Jan 13 10:31:07.248: INFO: pods created so far: [1 1 1]
    Jan 13 10:31:07.248: INFO: length of pods created so far: 3
    Jan 13 10:31:11.265: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:31:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:31:18.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2087" for this suite. 01/13/23 10:31:18.448
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-718" for this suite. 01/13/23 10:31:18.458
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:31:18.474
Jan 13 10:31:18.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:31:18.479
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:18.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:18.524
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-0c9365fc-4968-42ac-9ef6-87f16485ed5a 01/13/23 10:31:18.532
STEP: Creating a pod to test consume secrets 01/13/23 10:31:18.541
Jan 13 10:31:18.562: INFO: Waiting up to 5m0s for pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d" in namespace "secrets-2146" to be "Succeeded or Failed"
Jan 13 10:31:18.569: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.212626ms
Jan 13 10:31:20.579: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017371705s
Jan 13 10:31:22.584: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02172989s
Jan 13 10:31:24.580: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018142048s
Jan 13 10:31:26.581: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018502381s
STEP: Saw pod success 01/13/23 10:31:26.581
Jan 13 10:31:26.581: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d" satisfied condition "Succeeded or Failed"
Jan 13 10:31:26.592: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d container secret-env-test: <nil>
STEP: delete the pod 01/13/23 10:31:26.64
Jan 13 10:31:26.670: INFO: Waiting for pod pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d to disappear
Jan 13 10:31:26.679: INFO: Pod pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:31:26.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2146" for this suite. 01/13/23 10:31:26.688
------------------------------
• [SLOW TEST] [8.227 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:31:18.474
    Jan 13 10:31:18.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:31:18.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:18.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:18.524
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-0c9365fc-4968-42ac-9ef6-87f16485ed5a 01/13/23 10:31:18.532
    STEP: Creating a pod to test consume secrets 01/13/23 10:31:18.541
    Jan 13 10:31:18.562: INFO: Waiting up to 5m0s for pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d" in namespace "secrets-2146" to be "Succeeded or Failed"
    Jan 13 10:31:18.569: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.212626ms
    Jan 13 10:31:20.579: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017371705s
    Jan 13 10:31:22.584: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02172989s
    Jan 13 10:31:24.580: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018142048s
    Jan 13 10:31:26.581: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018502381s
    STEP: Saw pod success 01/13/23 10:31:26.581
    Jan 13 10:31:26.581: INFO: Pod "pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d" satisfied condition "Succeeded or Failed"
    Jan 13 10:31:26.592: INFO: Trying to get logs from node 10.10.102.31-node pod pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d container secret-env-test: <nil>
    STEP: delete the pod 01/13/23 10:31:26.64
    Jan 13 10:31:26.670: INFO: Waiting for pod pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d to disappear
    Jan 13 10:31:26.679: INFO: Pod pod-secrets-abd5ef49-1937-4466-b3b9-cd3e6ee3a98d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:31:26.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2146" for this suite. 01/13/23 10:31:26.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:31:26.703
Jan 13 10:31:26.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:31:26.706
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:26.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:26.737
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:31:26.765
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:31:28.521
STEP: Deploying the webhook pod 01/13/23 10:31:28.534
STEP: Wait for the deployment to be ready 01/13/23 10:31:28.561
Jan 13 10:31:28.576: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:31:30.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:31:32.628
STEP: Verifying the service has paired with the endpoint 01/13/23 10:31:32.667
Jan 13 10:31:33.669: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 13 10:31:33.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4-crds.webhook.example.com via the AdmissionRegistration API 01/13/23 10:31:34.199
STEP: Creating a custom resource that should be mutated by the webhook 01/13/23 10:31:34.24
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:31:37.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4545" for this suite. 01/13/23 10:31:37.265
STEP: Destroying namespace "webhook-4545-markers" for this suite. 01/13/23 10:31:37.286
------------------------------
• [SLOW TEST] [10.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:31:26.703
    Jan 13 10:31:26.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:31:26.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:26.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:26.737
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:31:26.765
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:31:28.521
    STEP: Deploying the webhook pod 01/13/23 10:31:28.534
    STEP: Wait for the deployment to be ready 01/13/23 10:31:28.561
    Jan 13 10:31:28.576: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:31:30.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:31:32.628
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:31:32.667
    Jan 13 10:31:33.669: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 13 10:31:33.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4-crds.webhook.example.com via the AdmissionRegistration API 01/13/23 10:31:34.199
    STEP: Creating a custom resource that should be mutated by the webhook 01/13/23 10:31:34.24
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:31:37.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4545" for this suite. 01/13/23 10:31:37.265
    STEP: Destroying namespace "webhook-4545-markers" for this suite. 01/13/23 10:31:37.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:31:37.304
Jan 13 10:31:37.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:31:37.309
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:37.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:37.354
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:31:37.427
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:31:39.415
STEP: Deploying the webhook pod 01/13/23 10:31:39.427
STEP: Wait for the deployment to be ready 01/13/23 10:31:39.464
Jan 13 10:31:39.508: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:31:41.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:31:43.546
STEP: Verifying the service has paired with the endpoint 01/13/23 10:31:43.571
Jan 13 10:31:44.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/13/23 10:31:44.665
STEP: Creating a configMap that should be mutated 01/13/23 10:31:44.699
STEP: Deleting the collection of validation webhooks 01/13/23 10:31:44.848
STEP: Creating a configMap that should not be mutated 01/13/23 10:31:45.02
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:31:45.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6511" for this suite. 01/13/23 10:31:45.248
STEP: Destroying namespace "webhook-6511-markers" for this suite. 01/13/23 10:31:45.277
------------------------------
• [SLOW TEST] [8.004 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:31:37.304
    Jan 13 10:31:37.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:31:37.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:37.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:37.354
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:31:37.427
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:31:39.415
    STEP: Deploying the webhook pod 01/13/23 10:31:39.427
    STEP: Wait for the deployment to be ready 01/13/23 10:31:39.464
    Jan 13 10:31:39.508: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:31:41.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 31, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:31:43.546
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:31:43.571
    Jan 13 10:31:44.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/13/23 10:31:44.665
    STEP: Creating a configMap that should be mutated 01/13/23 10:31:44.699
    STEP: Deleting the collection of validation webhooks 01/13/23 10:31:44.848
    STEP: Creating a configMap that should not be mutated 01/13/23 10:31:45.02
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:31:45.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6511" for this suite. 01/13/23 10:31:45.248
    STEP: Destroying namespace "webhook-6511-markers" for this suite. 01/13/23 10:31:45.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:31:45.309
Jan 13 10:31:45.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:31:45.311
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:45.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:45.349
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 13 10:31:45.396: INFO: created pod
Jan 13 10:31:45.396: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5559" to be "Succeeded or Failed"
Jan 13 10:31:45.403: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988985ms
Jan 13 10:31:47.417: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021461396s
Jan 13 10:31:49.411: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015276631s
Jan 13 10:31:51.414: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018143656s
Jan 13 10:31:53.410: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014894304s
STEP: Saw pod success 01/13/23 10:31:53.411
Jan 13 10:31:53.411: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 13 10:32:23.412: INFO: polling logs
Jan 13 10:32:23.440: INFO: Pod logs: 
I0113 10:31:47.682237       1 log.go:198] OK: Got token
I0113 10:31:47.682412       1 log.go:198] validating with in-cluster discovery
I0113 10:31:47.685119       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0113 10:31:47.685591       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5559:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673606505, NotBefore:1673605905, IssuedAt:1673605905, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5559", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03ab16f5-2fbf-492c-a3f9-f5fc11b11dcf"}}}
I0113 10:31:47.893246       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0113 10:31:47.916975       1 log.go:198] OK: Validated signature on JWT
I0113 10:31:47.917820       1 log.go:198] OK: Got valid claims from token!
I0113 10:31:47.917892       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5559:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673606505, NotBefore:1673605905, IssuedAt:1673605905, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5559", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03ab16f5-2fbf-492c-a3f9-f5fc11b11dcf"}}}

Jan 13 10:32:23.440: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 13 10:32:23.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5559" for this suite. 01/13/23 10:32:23.462
------------------------------
• [SLOW TEST] [38.167 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:31:45.309
    Jan 13 10:31:45.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:31:45.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:31:45.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:31:45.349
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 13 10:31:45.396: INFO: created pod
    Jan 13 10:31:45.396: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5559" to be "Succeeded or Failed"
    Jan 13 10:31:45.403: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988985ms
    Jan 13 10:31:47.417: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021461396s
    Jan 13 10:31:49.411: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015276631s
    Jan 13 10:31:51.414: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018143656s
    Jan 13 10:31:53.410: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014894304s
    STEP: Saw pod success 01/13/23 10:31:53.411
    Jan 13 10:31:53.411: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 13 10:32:23.412: INFO: polling logs
    Jan 13 10:32:23.440: INFO: Pod logs: 
    I0113 10:31:47.682237       1 log.go:198] OK: Got token
    I0113 10:31:47.682412       1 log.go:198] validating with in-cluster discovery
    I0113 10:31:47.685119       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0113 10:31:47.685591       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5559:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673606505, NotBefore:1673605905, IssuedAt:1673605905, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5559", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03ab16f5-2fbf-492c-a3f9-f5fc11b11dcf"}}}
    I0113 10:31:47.893246       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0113 10:31:47.916975       1 log.go:198] OK: Validated signature on JWT
    I0113 10:31:47.917820       1 log.go:198] OK: Got valid claims from token!
    I0113 10:31:47.917892       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5559:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673606505, NotBefore:1673605905, IssuedAt:1673605905, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5559", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"03ab16f5-2fbf-492c-a3f9-f5fc11b11dcf"}}}

    Jan 13 10:32:23.440: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:32:23.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5559" for this suite. 01/13/23 10:32:23.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:32:23.481
Jan 13 10:32:23.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename containers 01/13/23 10:32:23.485
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:23.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:23.535
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 13 10:32:23.567: INFO: Waiting up to 5m0s for pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b" in namespace "containers-1804" to be "running"
Jan 13 10:32:23.576: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.436577ms
Jan 13 10:32:25.597: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02983174s
Jan 13 10:32:27.583: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b": Phase="Running", Reason="", readiness=true. Elapsed: 4.016354581s
Jan 13 10:32:27.583: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 13 10:32:27.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1804" for this suite. 01/13/23 10:32:27.615
------------------------------
• [4.157 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:32:23.481
    Jan 13 10:32:23.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename containers 01/13/23 10:32:23.485
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:23.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:23.535
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 13 10:32:23.567: INFO: Waiting up to 5m0s for pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b" in namespace "containers-1804" to be "running"
    Jan 13 10:32:23.576: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.436577ms
    Jan 13 10:32:25.597: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02983174s
    Jan 13 10:32:27.583: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b": Phase="Running", Reason="", readiness=true. Elapsed: 4.016354581s
    Jan 13 10:32:27.583: INFO: Pod "client-containers-3cbb1bc3-8a20-4e3c-b360-4f8e67f0c46b" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:32:27.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1804" for this suite. 01/13/23 10:32:27.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:32:27.641
Jan 13 10:32:27.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename proxy 01/13/23 10:32:27.644
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:27.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:27.705
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/13/23 10:32:27.758
STEP: creating replication controller proxy-service-5sdg6 in namespace proxy-2699 01/13/23 10:32:27.758
I0113 10:32:27.808582      20 runners.go:193] Created replication controller with name: proxy-service-5sdg6, namespace: proxy-2699, replica count: 1
I0113 10:32:28.859813      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:32:29.860865      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:32:30.862082      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0113 10:32:31.863167      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 10:32:31.872: INFO: setup took 4.154739163s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/13/23 10:32:31.872
Jan 13 10:32:31.888: INFO: (0) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 14.114441ms)
Jan 13 10:32:31.888: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 15.097628ms)
Jan 13 10:32:31.889: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 15.817241ms)
Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 38.797981ms)
Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 39.157051ms)
Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 38.692718ms)
Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 38.626852ms)
Jan 13 10:32:31.913: INFO: (0) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 40.340174ms)
Jan 13 10:32:31.915: INFO: (0) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 41.650915ms)
Jan 13 10:32:31.918: INFO: (0) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 44.095382ms)
Jan 13 10:32:31.918: INFO: (0) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 44.259772ms)
Jan 13 10:32:31.918: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 44.158846ms)
Jan 13 10:32:31.922: INFO: (0) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 48.838215ms)
Jan 13 10:32:31.922: INFO: (0) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 47.624548ms)
Jan 13 10:32:31.935: INFO: (0) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 61.462844ms)
Jan 13 10:32:31.936: INFO: (0) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 61.41183ms)
Jan 13 10:32:31.948: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.933782ms)
Jan 13 10:32:31.948: INFO: (1) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 12.656091ms)
Jan 13 10:32:31.949: INFO: (1) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 12.199012ms)
Jan 13 10:32:31.949: INFO: (1) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 12.752364ms)
Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 15.849084ms)
Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 16.42422ms)
Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 16.592672ms)
Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.18111ms)
Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 16.720455ms)
Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 17.463374ms)
Jan 13 10:32:31.954: INFO: (1) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 16.967704ms)
Jan 13 10:32:31.954: INFO: (1) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 17.267638ms)
Jan 13 10:32:31.956: INFO: (1) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 19.090599ms)
Jan 13 10:32:31.956: INFO: (1) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 19.420396ms)
Jan 13 10:32:31.956: INFO: (1) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 19.82017ms)
Jan 13 10:32:31.957: INFO: (1) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 19.773523ms)
Jan 13 10:32:31.969: INFO: (2) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 11.817054ms)
Jan 13 10:32:31.969: INFO: (2) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 11.934541ms)
Jan 13 10:32:31.973: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 15.859592ms)
Jan 13 10:32:31.977: INFO: (2) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 18.65027ms)
Jan 13 10:32:31.977: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 18.658517ms)
Jan 13 10:32:31.978: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 19.450069ms)
Jan 13 10:32:31.978: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 19.313363ms)
Jan 13 10:32:31.978: INFO: (2) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 20.358757ms)
Jan 13 10:32:31.979: INFO: (2) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 21.642743ms)
Jan 13 10:32:31.979: INFO: (2) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 21.115118ms)
Jan 13 10:32:31.982: INFO: (2) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 24.759035ms)
Jan 13 10:32:31.982: INFO: (2) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 24.996202ms)
Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 24.306715ms)
Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 24.915442ms)
Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 25.043549ms)
Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 24.916867ms)
Jan 13 10:32:31.996: INFO: (3) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 12.588601ms)
Jan 13 10:32:31.996: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 13.677959ms)
Jan 13 10:32:31.997: INFO: (3) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 13.137218ms)
Jan 13 10:32:31.998: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.72856ms)
Jan 13 10:32:31.998: INFO: (3) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 14.772016ms)
Jan 13 10:32:31.998: INFO: (3) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 14.690714ms)
Jan 13 10:32:31.999: INFO: (3) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 14.893518ms)
Jan 13 10:32:31.999: INFO: (3) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 15.307142ms)
Jan 13 10:32:31.999: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 15.014796ms)
Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 17.415567ms)
Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 17.491322ms)
Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 18.211153ms)
Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 17.865398ms)
Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 17.852715ms)
Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 18.098164ms)
Jan 13 10:32:32.002: INFO: (3) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 17.732561ms)
Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.540858ms)
Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 11.12658ms)
Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 10.990273ms)
Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 12.334225ms)
Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.714688ms)
Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 11.171028ms)
Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 13.143011ms)
Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 11.693205ms)
Jan 13 10:32:32.018: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 15.150114ms)
Jan 13 10:32:32.018: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.521921ms)
Jan 13 10:32:32.018: INFO: (4) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 14.803999ms)
Jan 13 10:32:32.019: INFO: (4) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 17.03017ms)
Jan 13 10:32:32.020: INFO: (4) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 16.623618ms)
Jan 13 10:32:32.020: INFO: (4) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 18.167477ms)
Jan 13 10:32:32.020: INFO: (4) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 17.869845ms)
Jan 13 10:32:32.022: INFO: (4) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 19.233035ms)
Jan 13 10:32:32.036: INFO: (5) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 13.246238ms)
Jan 13 10:32:32.036: INFO: (5) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 13.138682ms)
Jan 13 10:32:32.036: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 13.449846ms)
Jan 13 10:32:32.040: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 16.876782ms)
Jan 13 10:32:32.040: INFO: (5) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 17.359793ms)
Jan 13 10:32:32.040: INFO: (5) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 16.736776ms)
Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 17.920549ms)
Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.79945ms)
Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 18.053483ms)
Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 17.786858ms)
Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 18.617274ms)
Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 17.927854ms)
Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 20.063702ms)
Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 19.887009ms)
Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 19.889936ms)
Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 20.196473ms)
Jan 13 10:32:32.056: INFO: (6) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 12.353826ms)
Jan 13 10:32:32.056: INFO: (6) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 12.272521ms)
Jan 13 10:32:32.056: INFO: (6) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 12.975093ms)
Jan 13 10:32:32.061: INFO: (6) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 17.722306ms)
Jan 13 10:32:32.061: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.896642ms)
Jan 13 10:32:32.061: INFO: (6) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.296047ms)
Jan 13 10:32:32.062: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 17.969258ms)
Jan 13 10:32:32.062: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 17.461057ms)
Jan 13 10:32:32.063: INFO: (6) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 18.604131ms)
Jan 13 10:32:32.063: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 18.538661ms)
Jan 13 10:32:32.066: INFO: (6) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 22.689998ms)
Jan 13 10:32:32.067: INFO: (6) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 22.276848ms)
Jan 13 10:32:32.067: INFO: (6) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 22.769022ms)
Jan 13 10:32:32.068: INFO: (6) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 24.653445ms)
Jan 13 10:32:32.068: INFO: (6) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 23.928682ms)
Jan 13 10:32:32.068: INFO: (6) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 24.200948ms)
Jan 13 10:32:32.085: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.126806ms)
Jan 13 10:32:32.085: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 16.655837ms)
Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 16.86678ms)
Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.608795ms)
Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 17.373746ms)
Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 17.468779ms)
Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 18.648815ms)
Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 18.487747ms)
Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 18.928469ms)
Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 19.415336ms)
Jan 13 10:32:32.091: INFO: (7) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 21.887293ms)
Jan 13 10:32:32.091: INFO: (7) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 22.790573ms)
Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 22.688321ms)
Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 22.700078ms)
Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 22.888813ms)
Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 23.633355ms)
Jan 13 10:32:32.101: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 8.686291ms)
Jan 13 10:32:32.101: INFO: (8) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 9.191203ms)
Jan 13 10:32:32.102: INFO: (8) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 8.911337ms)
Jan 13 10:32:32.102: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 8.872811ms)
Jan 13 10:32:32.103: INFO: (8) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 10.511439ms)
Jan 13 10:32:32.106: INFO: (8) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 12.834714ms)
Jan 13 10:32:32.107: INFO: (8) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.09503ms)
Jan 13 10:32:32.107: INFO: (8) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 14.321208ms)
Jan 13 10:32:32.108: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 14.100035ms)
Jan 13 10:32:32.108: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.689036ms)
Jan 13 10:32:32.113: INFO: (8) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 19.981419ms)
Jan 13 10:32:32.113: INFO: (8) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 20.26648ms)
Jan 13 10:32:32.113: INFO: (8) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 20.663934ms)
Jan 13 10:32:32.114: INFO: (8) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 20.246327ms)
Jan 13 10:32:32.114: INFO: (8) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 20.18844ms)
Jan 13 10:32:32.114: INFO: (8) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 20.256294ms)
Jan 13 10:32:32.135: INFO: (9) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 21.2551ms)
Jan 13 10:32:32.138: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 23.170334ms)
Jan 13 10:32:32.140: INFO: (9) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.08769ms)
Jan 13 10:32:32.140: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 24.371162ms)
Jan 13 10:32:32.140: INFO: (9) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 25.600689ms)
Jan 13 10:32:32.143: INFO: (9) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 28.482648ms)
Jan 13 10:32:32.143: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 27.593835ms)
Jan 13 10:32:32.143: INFO: (9) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 29.282117ms)
Jan 13 10:32:32.144: INFO: (9) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 28.840732ms)
Jan 13 10:32:32.145: INFO: (9) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 29.961416ms)
Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 31.035754ms)
Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 30.783442ms)
Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 30.840301ms)
Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 32.247872ms)
Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 31.384211ms)
Jan 13 10:32:32.148: INFO: (9) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 32.396413ms)
Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 21.441272ms)
Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 21.591509ms)
Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 21.115183ms)
Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 22.420315ms)
Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 21.744916ms)
Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 21.369855ms)
Jan 13 10:32:32.171: INFO: (10) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 22.10471ms)
Jan 13 10:32:32.172: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 22.737746ms)
Jan 13 10:32:32.172: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 23.671647ms)
Jan 13 10:32:32.176: INFO: (10) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 27.50218ms)
Jan 13 10:32:32.177: INFO: (10) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 29.038095ms)
Jan 13 10:32:32.177: INFO: (10) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 28.600593ms)
Jan 13 10:32:32.178: INFO: (10) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 29.071328ms)
Jan 13 10:32:32.178: INFO: (10) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 29.224263ms)
Jan 13 10:32:32.185: INFO: (10) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 36.492499ms)
Jan 13 10:32:32.185: INFO: (10) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 37.135619ms)
Jan 13 10:32:32.207: INFO: (11) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 21.391394ms)
Jan 13 10:32:32.224: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 36.973016ms)
Jan 13 10:32:32.224: INFO: (11) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 36.562664ms)
Jan 13 10:32:32.225: INFO: (11) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 38.162581ms)
Jan 13 10:32:32.227: INFO: (11) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 39.693664ms)
Jan 13 10:32:32.233: INFO: (11) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 46.226206ms)
Jan 13 10:32:32.233: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 45.790997ms)
Jan 13 10:32:32.234: INFO: (11) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 45.960112ms)
Jan 13 10:32:32.234: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 46.918939ms)
Jan 13 10:32:32.234: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 46.449036ms)
Jan 13 10:32:32.235: INFO: (11) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 49.211843ms)
Jan 13 10:32:32.236: INFO: (11) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 50.07773ms)
Jan 13 10:32:32.236: INFO: (11) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 49.551982ms)
Jan 13 10:32:32.238: INFO: (11) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 51.756559ms)
Jan 13 10:32:32.238: INFO: (11) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 51.163642ms)
Jan 13 10:32:32.242: INFO: (11) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 56.723828ms)
Jan 13 10:32:32.255: INFO: (12) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.855687ms)
Jan 13 10:32:32.255: INFO: (12) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 12.226573ms)
Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 49.988682ms)
Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 49.177371ms)
Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 49.885696ms)
Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 49.590301ms)
Jan 13 10:32:32.295: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 50.554782ms)
Jan 13 10:32:32.295: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 51.520057ms)
Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 50.966672ms)
Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 51.125936ms)
Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 51.25145ms)
Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 51.771279ms)
Jan 13 10:32:32.297: INFO: (12) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 52.587032ms)
Jan 13 10:32:32.297: INFO: (12) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 52.525818ms)
Jan 13 10:32:32.302: INFO: (12) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 57.76921ms)
Jan 13 10:32:32.303: INFO: (12) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 58.187459ms)
Jan 13 10:32:32.315: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 12.621795ms)
Jan 13 10:32:32.319: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 16.368943ms)
Jan 13 10:32:32.320: INFO: (13) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 16.87723ms)
Jan 13 10:32:32.328: INFO: (13) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 24.195147ms)
Jan 13 10:32:32.329: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.595799ms)
Jan 13 10:32:32.329: INFO: (13) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 24.744017ms)
Jan 13 10:32:32.330: INFO: (13) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 26.518218ms)
Jan 13 10:32:32.330: INFO: (13) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 26.517654ms)
Jan 13 10:32:32.330: INFO: (13) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 25.98134ms)
Jan 13 10:32:32.331: INFO: (13) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 28.519676ms)
Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 27.123119ms)
Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 27.513642ms)
Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 28.030745ms)
Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 28.355957ms)
Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 28.372642ms)
Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 27.675102ms)
Jan 13 10:32:32.343: INFO: (14) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 10.10351ms)
Jan 13 10:32:32.343: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 10.385061ms)
Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 32.125669ms)
Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 31.658026ms)
Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 31.936326ms)
Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 32.254058ms)
Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 32.169103ms)
Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 32.302094ms)
Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 33.170279ms)
Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 32.137939ms)
Jan 13 10:32:32.373: INFO: (14) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 39.119192ms)
Jan 13 10:32:32.381: INFO: (14) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 47.395067ms)
Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 49.056138ms)
Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 48.940542ms)
Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 49.044195ms)
Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 49.069028ms)
Jan 13 10:32:32.400: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.871879ms)
Jan 13 10:32:32.401: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.106891ms)
Jan 13 10:32:32.401: INFO: (15) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 18.22746ms)
Jan 13 10:32:32.406: INFO: (15) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 21.676552ms)
Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 24.7283ms)
Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 25.395449ms)
Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 25.09263ms)
Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 25.391663ms)
Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 25.331827ms)
Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.976451ms)
Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 37.499508ms)
Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 37.412037ms)
Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 38.034162ms)
Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 38.906087ms)
Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 38.239318ms)
Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 37.812143ms)
Jan 13 10:32:32.438: INFO: (16) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 15.9452ms)
Jan 13 10:32:32.438: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 15.988843ms)
Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 20.871429ms)
Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 20.764409ms)
Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 20.888864ms)
Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 21.505606ms)
Jan 13 10:32:32.445: INFO: (16) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 21.850697ms)
Jan 13 10:32:32.445: INFO: (16) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 22.607753ms)
Jan 13 10:32:32.445: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 22.587989ms)
Jan 13 10:32:32.447: INFO: (16) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 23.612761ms)
Jan 13 10:32:32.448: INFO: (16) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 24.753495ms)
Jan 13 10:32:32.448: INFO: (16) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 24.751816ms)
Jan 13 10:32:32.448: INFO: (16) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 25.313967ms)
Jan 13 10:32:32.459: INFO: (16) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 36.3933ms)
Jan 13 10:32:32.460: INFO: (16) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 36.846088ms)
Jan 13 10:32:32.460: INFO: (16) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 37.147549ms)
Jan 13 10:32:32.480: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 19.965832ms)
Jan 13 10:32:32.500: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 38.447999ms)
Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 39.909925ms)
Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 38.715357ms)
Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 39.89166ms)
Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 39.496048ms)
Jan 13 10:32:32.512: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 50.701357ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 52.005711ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 52.111269ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 51.378566ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 51.501809ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 51.306952ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 51.650646ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 51.7874ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 51.123412ms)
Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 51.46749ms)
Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 50.964774ms)
Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 51.15998ms)
Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 50.978547ms)
Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 51.755166ms)
Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 52.11556ms)
Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 52.251023ms)
Jan 13 10:32:32.566: INFO: (18) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 52.691956ms)
Jan 13 10:32:32.566: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 53.255416ms)
Jan 13 10:32:32.566: INFO: (18) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 52.748944ms)
Jan 13 10:32:32.567: INFO: (18) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 53.060525ms)
Jan 13 10:32:32.577: INFO: (18) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 63.529561ms)
Jan 13 10:32:32.578: INFO: (18) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 64.026905ms)
Jan 13 10:32:32.578: INFO: (18) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 64.285627ms)
Jan 13 10:32:32.581: INFO: (18) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 67.656864ms)
Jan 13 10:32:32.581: INFO: (18) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 67.280526ms)
Jan 13 10:32:32.582: INFO: (18) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 68.632579ms)
Jan 13 10:32:32.606: INFO: (19) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 23.440899ms)
Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.096557ms)
Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 24.776574ms)
Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 24.321751ms)
Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 24.738725ms)
Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 25.388583ms)
Jan 13 10:32:32.608: INFO: (19) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 25.444707ms)
Jan 13 10:32:32.608: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 25.447047ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 27.003523ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 27.240815ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 27.380356ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 27.420903ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 26.99395ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 27.362255ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 27.177918ms)
Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 27.486352ms)
STEP: deleting ReplicationController proxy-service-5sdg6 in namespace proxy-2699, will wait for the garbage collector to delete the pods 01/13/23 10:32:32.61
Jan 13 10:32:32.681: INFO: Deleting ReplicationController proxy-service-5sdg6 took: 11.662566ms
Jan 13 10:32:32.782: INFO: Terminating ReplicationController proxy-service-5sdg6 pods took: 100.666533ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 13 10:32:37.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2699" for this suite. 01/13/23 10:32:37.413
------------------------------
• [SLOW TEST] [9.813 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:32:27.641
    Jan 13 10:32:27.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename proxy 01/13/23 10:32:27.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:27.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:27.705
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/13/23 10:32:27.758
    STEP: creating replication controller proxy-service-5sdg6 in namespace proxy-2699 01/13/23 10:32:27.758
    I0113 10:32:27.808582      20 runners.go:193] Created replication controller with name: proxy-service-5sdg6, namespace: proxy-2699, replica count: 1
    I0113 10:32:28.859813      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:32:29.860865      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:32:30.862082      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0113 10:32:31.863167      20 runners.go:193] proxy-service-5sdg6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 10:32:31.872: INFO: setup took 4.154739163s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/13/23 10:32:31.872
    Jan 13 10:32:31.888: INFO: (0) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 14.114441ms)
    Jan 13 10:32:31.888: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 15.097628ms)
    Jan 13 10:32:31.889: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 15.817241ms)
    Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 38.797981ms)
    Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 39.157051ms)
    Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 38.692718ms)
    Jan 13 10:32:31.912: INFO: (0) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 38.626852ms)
    Jan 13 10:32:31.913: INFO: (0) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 40.340174ms)
    Jan 13 10:32:31.915: INFO: (0) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 41.650915ms)
    Jan 13 10:32:31.918: INFO: (0) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 44.095382ms)
    Jan 13 10:32:31.918: INFO: (0) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 44.259772ms)
    Jan 13 10:32:31.918: INFO: (0) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 44.158846ms)
    Jan 13 10:32:31.922: INFO: (0) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 48.838215ms)
    Jan 13 10:32:31.922: INFO: (0) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 47.624548ms)
    Jan 13 10:32:31.935: INFO: (0) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 61.462844ms)
    Jan 13 10:32:31.936: INFO: (0) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 61.41183ms)
    Jan 13 10:32:31.948: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.933782ms)
    Jan 13 10:32:31.948: INFO: (1) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 12.656091ms)
    Jan 13 10:32:31.949: INFO: (1) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 12.199012ms)
    Jan 13 10:32:31.949: INFO: (1) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 12.752364ms)
    Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 15.849084ms)
    Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 16.42422ms)
    Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 16.592672ms)
    Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.18111ms)
    Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 16.720455ms)
    Jan 13 10:32:31.953: INFO: (1) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 17.463374ms)
    Jan 13 10:32:31.954: INFO: (1) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 16.967704ms)
    Jan 13 10:32:31.954: INFO: (1) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 17.267638ms)
    Jan 13 10:32:31.956: INFO: (1) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 19.090599ms)
    Jan 13 10:32:31.956: INFO: (1) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 19.420396ms)
    Jan 13 10:32:31.956: INFO: (1) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 19.82017ms)
    Jan 13 10:32:31.957: INFO: (1) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 19.773523ms)
    Jan 13 10:32:31.969: INFO: (2) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 11.817054ms)
    Jan 13 10:32:31.969: INFO: (2) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 11.934541ms)
    Jan 13 10:32:31.973: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 15.859592ms)
    Jan 13 10:32:31.977: INFO: (2) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 18.65027ms)
    Jan 13 10:32:31.977: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 18.658517ms)
    Jan 13 10:32:31.978: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 19.450069ms)
    Jan 13 10:32:31.978: INFO: (2) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 19.313363ms)
    Jan 13 10:32:31.978: INFO: (2) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 20.358757ms)
    Jan 13 10:32:31.979: INFO: (2) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 21.642743ms)
    Jan 13 10:32:31.979: INFO: (2) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 21.115118ms)
    Jan 13 10:32:31.982: INFO: (2) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 24.759035ms)
    Jan 13 10:32:31.982: INFO: (2) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 24.996202ms)
    Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 24.306715ms)
    Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 24.915442ms)
    Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 25.043549ms)
    Jan 13 10:32:31.983: INFO: (2) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 24.916867ms)
    Jan 13 10:32:31.996: INFO: (3) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 12.588601ms)
    Jan 13 10:32:31.996: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 13.677959ms)
    Jan 13 10:32:31.997: INFO: (3) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 13.137218ms)
    Jan 13 10:32:31.998: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.72856ms)
    Jan 13 10:32:31.998: INFO: (3) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 14.772016ms)
    Jan 13 10:32:31.998: INFO: (3) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 14.690714ms)
    Jan 13 10:32:31.999: INFO: (3) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 14.893518ms)
    Jan 13 10:32:31.999: INFO: (3) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 15.307142ms)
    Jan 13 10:32:31.999: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 15.014796ms)
    Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 17.415567ms)
    Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 17.491322ms)
    Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 18.211153ms)
    Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 17.865398ms)
    Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 17.852715ms)
    Jan 13 10:32:32.001: INFO: (3) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 18.098164ms)
    Jan 13 10:32:32.002: INFO: (3) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 17.732561ms)
    Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.540858ms)
    Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 11.12658ms)
    Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 10.990273ms)
    Jan 13 10:32:32.014: INFO: (4) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 12.334225ms)
    Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.714688ms)
    Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 11.171028ms)
    Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 13.143011ms)
    Jan 13 10:32:32.015: INFO: (4) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 11.693205ms)
    Jan 13 10:32:32.018: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 15.150114ms)
    Jan 13 10:32:32.018: INFO: (4) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.521921ms)
    Jan 13 10:32:32.018: INFO: (4) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 14.803999ms)
    Jan 13 10:32:32.019: INFO: (4) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 17.03017ms)
    Jan 13 10:32:32.020: INFO: (4) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 16.623618ms)
    Jan 13 10:32:32.020: INFO: (4) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 18.167477ms)
    Jan 13 10:32:32.020: INFO: (4) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 17.869845ms)
    Jan 13 10:32:32.022: INFO: (4) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 19.233035ms)
    Jan 13 10:32:32.036: INFO: (5) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 13.246238ms)
    Jan 13 10:32:32.036: INFO: (5) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 13.138682ms)
    Jan 13 10:32:32.036: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 13.449846ms)
    Jan 13 10:32:32.040: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 16.876782ms)
    Jan 13 10:32:32.040: INFO: (5) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 17.359793ms)
    Jan 13 10:32:32.040: INFO: (5) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 16.736776ms)
    Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 17.920549ms)
    Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.79945ms)
    Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 18.053483ms)
    Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 17.786858ms)
    Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 18.617274ms)
    Jan 13 10:32:32.041: INFO: (5) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 17.927854ms)
    Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 20.063702ms)
    Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 19.887009ms)
    Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 19.889936ms)
    Jan 13 10:32:32.043: INFO: (5) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 20.196473ms)
    Jan 13 10:32:32.056: INFO: (6) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 12.353826ms)
    Jan 13 10:32:32.056: INFO: (6) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 12.272521ms)
    Jan 13 10:32:32.056: INFO: (6) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 12.975093ms)
    Jan 13 10:32:32.061: INFO: (6) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 17.722306ms)
    Jan 13 10:32:32.061: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.896642ms)
    Jan 13 10:32:32.061: INFO: (6) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.296047ms)
    Jan 13 10:32:32.062: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 17.969258ms)
    Jan 13 10:32:32.062: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 17.461057ms)
    Jan 13 10:32:32.063: INFO: (6) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 18.604131ms)
    Jan 13 10:32:32.063: INFO: (6) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 18.538661ms)
    Jan 13 10:32:32.066: INFO: (6) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 22.689998ms)
    Jan 13 10:32:32.067: INFO: (6) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 22.276848ms)
    Jan 13 10:32:32.067: INFO: (6) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 22.769022ms)
    Jan 13 10:32:32.068: INFO: (6) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 24.653445ms)
    Jan 13 10:32:32.068: INFO: (6) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 23.928682ms)
    Jan 13 10:32:32.068: INFO: (6) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 24.200948ms)
    Jan 13 10:32:32.085: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.126806ms)
    Jan 13 10:32:32.085: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 16.655837ms)
    Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 16.86678ms)
    Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.608795ms)
    Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 17.373746ms)
    Jan 13 10:32:32.086: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 17.468779ms)
    Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 18.648815ms)
    Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 18.487747ms)
    Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 18.928469ms)
    Jan 13 10:32:32.088: INFO: (7) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 19.415336ms)
    Jan 13 10:32:32.091: INFO: (7) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 21.887293ms)
    Jan 13 10:32:32.091: INFO: (7) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 22.790573ms)
    Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 22.688321ms)
    Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 22.700078ms)
    Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 22.888813ms)
    Jan 13 10:32:32.092: INFO: (7) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 23.633355ms)
    Jan 13 10:32:32.101: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 8.686291ms)
    Jan 13 10:32:32.101: INFO: (8) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 9.191203ms)
    Jan 13 10:32:32.102: INFO: (8) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 8.911337ms)
    Jan 13 10:32:32.102: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 8.872811ms)
    Jan 13 10:32:32.103: INFO: (8) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 10.511439ms)
    Jan 13 10:32:32.106: INFO: (8) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 12.834714ms)
    Jan 13 10:32:32.107: INFO: (8) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.09503ms)
    Jan 13 10:32:32.107: INFO: (8) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 14.321208ms)
    Jan 13 10:32:32.108: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 14.100035ms)
    Jan 13 10:32:32.108: INFO: (8) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 14.689036ms)
    Jan 13 10:32:32.113: INFO: (8) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 19.981419ms)
    Jan 13 10:32:32.113: INFO: (8) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 20.26648ms)
    Jan 13 10:32:32.113: INFO: (8) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 20.663934ms)
    Jan 13 10:32:32.114: INFO: (8) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 20.246327ms)
    Jan 13 10:32:32.114: INFO: (8) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 20.18844ms)
    Jan 13 10:32:32.114: INFO: (8) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 20.256294ms)
    Jan 13 10:32:32.135: INFO: (9) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 21.2551ms)
    Jan 13 10:32:32.138: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 23.170334ms)
    Jan 13 10:32:32.140: INFO: (9) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.08769ms)
    Jan 13 10:32:32.140: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 24.371162ms)
    Jan 13 10:32:32.140: INFO: (9) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 25.600689ms)
    Jan 13 10:32:32.143: INFO: (9) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 28.482648ms)
    Jan 13 10:32:32.143: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 27.593835ms)
    Jan 13 10:32:32.143: INFO: (9) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 29.282117ms)
    Jan 13 10:32:32.144: INFO: (9) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 28.840732ms)
    Jan 13 10:32:32.145: INFO: (9) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 29.961416ms)
    Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 31.035754ms)
    Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 30.783442ms)
    Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 30.840301ms)
    Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 32.247872ms)
    Jan 13 10:32:32.146: INFO: (9) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 31.384211ms)
    Jan 13 10:32:32.148: INFO: (9) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 32.396413ms)
    Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 21.441272ms)
    Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 21.591509ms)
    Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 21.115183ms)
    Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 22.420315ms)
    Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 21.744916ms)
    Jan 13 10:32:32.170: INFO: (10) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 21.369855ms)
    Jan 13 10:32:32.171: INFO: (10) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 22.10471ms)
    Jan 13 10:32:32.172: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 22.737746ms)
    Jan 13 10:32:32.172: INFO: (10) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 23.671647ms)
    Jan 13 10:32:32.176: INFO: (10) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 27.50218ms)
    Jan 13 10:32:32.177: INFO: (10) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 29.038095ms)
    Jan 13 10:32:32.177: INFO: (10) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 28.600593ms)
    Jan 13 10:32:32.178: INFO: (10) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 29.071328ms)
    Jan 13 10:32:32.178: INFO: (10) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 29.224263ms)
    Jan 13 10:32:32.185: INFO: (10) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 36.492499ms)
    Jan 13 10:32:32.185: INFO: (10) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 37.135619ms)
    Jan 13 10:32:32.207: INFO: (11) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 21.391394ms)
    Jan 13 10:32:32.224: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 36.973016ms)
    Jan 13 10:32:32.224: INFO: (11) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 36.562664ms)
    Jan 13 10:32:32.225: INFO: (11) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 38.162581ms)
    Jan 13 10:32:32.227: INFO: (11) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 39.693664ms)
    Jan 13 10:32:32.233: INFO: (11) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 46.226206ms)
    Jan 13 10:32:32.233: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 45.790997ms)
    Jan 13 10:32:32.234: INFO: (11) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 45.960112ms)
    Jan 13 10:32:32.234: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 46.918939ms)
    Jan 13 10:32:32.234: INFO: (11) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 46.449036ms)
    Jan 13 10:32:32.235: INFO: (11) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 49.211843ms)
    Jan 13 10:32:32.236: INFO: (11) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 50.07773ms)
    Jan 13 10:32:32.236: INFO: (11) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 49.551982ms)
    Jan 13 10:32:32.238: INFO: (11) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 51.756559ms)
    Jan 13 10:32:32.238: INFO: (11) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 51.163642ms)
    Jan 13 10:32:32.242: INFO: (11) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 56.723828ms)
    Jan 13 10:32:32.255: INFO: (12) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 11.855687ms)
    Jan 13 10:32:32.255: INFO: (12) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 12.226573ms)
    Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 49.988682ms)
    Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 49.177371ms)
    Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 49.885696ms)
    Jan 13 10:32:32.293: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 49.590301ms)
    Jan 13 10:32:32.295: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 50.554782ms)
    Jan 13 10:32:32.295: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 51.520057ms)
    Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 50.966672ms)
    Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 51.125936ms)
    Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 51.25145ms)
    Jan 13 10:32:32.296: INFO: (12) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 51.771279ms)
    Jan 13 10:32:32.297: INFO: (12) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 52.587032ms)
    Jan 13 10:32:32.297: INFO: (12) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 52.525818ms)
    Jan 13 10:32:32.302: INFO: (12) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 57.76921ms)
    Jan 13 10:32:32.303: INFO: (12) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 58.187459ms)
    Jan 13 10:32:32.315: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 12.621795ms)
    Jan 13 10:32:32.319: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 16.368943ms)
    Jan 13 10:32:32.320: INFO: (13) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 16.87723ms)
    Jan 13 10:32:32.328: INFO: (13) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 24.195147ms)
    Jan 13 10:32:32.329: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.595799ms)
    Jan 13 10:32:32.329: INFO: (13) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 24.744017ms)
    Jan 13 10:32:32.330: INFO: (13) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 26.518218ms)
    Jan 13 10:32:32.330: INFO: (13) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 26.517654ms)
    Jan 13 10:32:32.330: INFO: (13) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 25.98134ms)
    Jan 13 10:32:32.331: INFO: (13) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 28.519676ms)
    Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 27.123119ms)
    Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 27.513642ms)
    Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 28.030745ms)
    Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 28.355957ms)
    Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 28.372642ms)
    Jan 13 10:32:32.332: INFO: (13) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 27.675102ms)
    Jan 13 10:32:32.343: INFO: (14) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 10.10351ms)
    Jan 13 10:32:32.343: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 10.385061ms)
    Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 32.125669ms)
    Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 31.658026ms)
    Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 31.936326ms)
    Jan 13 10:32:32.365: INFO: (14) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 32.254058ms)
    Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 32.169103ms)
    Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 32.302094ms)
    Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 33.170279ms)
    Jan 13 10:32:32.366: INFO: (14) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 32.137939ms)
    Jan 13 10:32:32.373: INFO: (14) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 39.119192ms)
    Jan 13 10:32:32.381: INFO: (14) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 47.395067ms)
    Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 49.056138ms)
    Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 48.940542ms)
    Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 49.044195ms)
    Jan 13 10:32:32.383: INFO: (14) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 49.069028ms)
    Jan 13 10:32:32.400: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 16.871879ms)
    Jan 13 10:32:32.401: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 17.106891ms)
    Jan 13 10:32:32.401: INFO: (15) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 18.22746ms)
    Jan 13 10:32:32.406: INFO: (15) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 21.676552ms)
    Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 24.7283ms)
    Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 25.395449ms)
    Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 25.09263ms)
    Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 25.391663ms)
    Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 25.331827ms)
    Jan 13 10:32:32.409: INFO: (15) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.976451ms)
    Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 37.499508ms)
    Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 37.412037ms)
    Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 38.034162ms)
    Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 38.906087ms)
    Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 38.239318ms)
    Jan 13 10:32:32.422: INFO: (15) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 37.812143ms)
    Jan 13 10:32:32.438: INFO: (16) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 15.9452ms)
    Jan 13 10:32:32.438: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 15.988843ms)
    Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 20.871429ms)
    Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 20.764409ms)
    Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 20.888864ms)
    Jan 13 10:32:32.444: INFO: (16) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 21.505606ms)
    Jan 13 10:32:32.445: INFO: (16) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 21.850697ms)
    Jan 13 10:32:32.445: INFO: (16) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 22.607753ms)
    Jan 13 10:32:32.445: INFO: (16) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 22.587989ms)
    Jan 13 10:32:32.447: INFO: (16) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 23.612761ms)
    Jan 13 10:32:32.448: INFO: (16) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 24.753495ms)
    Jan 13 10:32:32.448: INFO: (16) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 24.751816ms)
    Jan 13 10:32:32.448: INFO: (16) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 25.313967ms)
    Jan 13 10:32:32.459: INFO: (16) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 36.3933ms)
    Jan 13 10:32:32.460: INFO: (16) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 36.846088ms)
    Jan 13 10:32:32.460: INFO: (16) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 37.147549ms)
    Jan 13 10:32:32.480: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 19.965832ms)
    Jan 13 10:32:32.500: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 38.447999ms)
    Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 39.909925ms)
    Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 38.715357ms)
    Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 39.89166ms)
    Jan 13 10:32:32.501: INFO: (17) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 39.496048ms)
    Jan 13 10:32:32.512: INFO: (17) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 50.701357ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 52.005711ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 52.111269ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 51.378566ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 51.501809ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 51.306952ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 51.650646ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 51.7874ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 51.123412ms)
    Jan 13 10:32:32.513: INFO: (17) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 51.46749ms)
    Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 50.964774ms)
    Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 51.15998ms)
    Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 50.978547ms)
    Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 51.755166ms)
    Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 52.11556ms)
    Jan 13 10:32:32.565: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 52.251023ms)
    Jan 13 10:32:32.566: INFO: (18) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 52.691956ms)
    Jan 13 10:32:32.566: INFO: (18) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 53.255416ms)
    Jan 13 10:32:32.566: INFO: (18) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 52.748944ms)
    Jan 13 10:32:32.567: INFO: (18) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 53.060525ms)
    Jan 13 10:32:32.577: INFO: (18) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 63.529561ms)
    Jan 13 10:32:32.578: INFO: (18) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 64.026905ms)
    Jan 13 10:32:32.578: INFO: (18) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 64.285627ms)
    Jan 13 10:32:32.581: INFO: (18) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 67.656864ms)
    Jan 13 10:32:32.581: INFO: (18) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 67.280526ms)
    Jan 13 10:32:32.582: INFO: (18) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 68.632579ms)
    Jan 13 10:32:32.606: INFO: (19) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname1/proxy/: foo (200; 23.440899ms)
    Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 24.096557ms)
    Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">test<... (200; 24.776574ms)
    Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 24.321751ms)
    Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:162/proxy/: bar (200; 24.738725ms)
    Jan 13 10:32:32.607: INFO: (19) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:462/proxy/: tls qux (200; 25.388583ms)
    Jan 13 10:32:32.608: INFO: (19) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:443/proxy/tlsrewritem... (200; 25.444707ms)
    Jan 13 10:32:32.608: INFO: (19) /api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/proxy-service-5sdg6-s8ccq/proxy/rewriteme">test</a> (200; 25.447047ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname2/proxy/: bar (200; 27.003523ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/http:proxy-service-5sdg6:portname1/proxy/: foo (200; 27.240815ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:1080/proxy/rewriteme">... (200; 27.380356ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/pods/http:proxy-service-5sdg6-s8ccq:160/proxy/: foo (200; 27.420903ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname2/proxy/: tls qux (200; 26.99395ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/pods/https:proxy-service-5sdg6-s8ccq:460/proxy/: tls baz (200; 27.362255ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/https:proxy-service-5sdg6:tlsportname1/proxy/: tls baz (200; 27.177918ms)
    Jan 13 10:32:32.610: INFO: (19) /api/v1/namespaces/proxy-2699/services/proxy-service-5sdg6:portname2/proxy/: bar (200; 27.486352ms)
    STEP: deleting ReplicationController proxy-service-5sdg6 in namespace proxy-2699, will wait for the garbage collector to delete the pods 01/13/23 10:32:32.61
    Jan 13 10:32:32.681: INFO: Deleting ReplicationController proxy-service-5sdg6 took: 11.662566ms
    Jan 13 10:32:32.782: INFO: Terminating ReplicationController proxy-service-5sdg6 pods took: 100.666533ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:32:37.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2699" for this suite. 01/13/23 10:32:37.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:32:37.457
Jan 13 10:32:37.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 10:32:37.461
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:37.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:37.559
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 13 10:32:37.608: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 13 10:32:42.622: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/13/23 10:32:42.622
Jan 13 10:32:42.622: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/13/23 10:32:42.654
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 10:32:42.702: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1883  896d2855-128c-46a0-9892-89c4f112de0f 595827 1 2023-01-13 10:32:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-13 10:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a1758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 13 10:32:42.714: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 13 10:32:42.714: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 13 10:32:42.715: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1883  73857d7d-0ac4-4a62-91cc-8cbfef5de39b 595828 1 2023-01-13 10:32:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 896d2855-128c-46a0-9892-89c4f112de0f 0xc0050a1ab7 0xc0050a1ab8}] [] [{e2e.test Update apps/v1 2023-01-13 10:32:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:32:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-13 10:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"896d2855-128c-46a0-9892-89c4f112de0f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0050a1b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:32:42.734: INFO: Pod "test-cleanup-controller-74k6j" is available:
&Pod{ObjectMeta:{test-cleanup-controller-74k6j test-cleanup-controller- deployment-1883  b0a8029f-dfbd-42c3-bd6d-e046a59e4455 595822 0 2023-01-13 10:32:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b174fb1f4f5f6e085d81ee07ccf0adab2b21c3847620377be824ebc36cb036ec cni.projectcalico.org/podIP:10.244.27.230/32 cni.projectcalico.org/podIPs:10.244.27.230/32] [{apps/v1 ReplicaSet test-cleanup-controller 73857d7d-0ac4-4a62-91cc-8cbfef5de39b 0xc0050a1e97 0xc0050a1e98}] [] [{kube-controller-manager Update v1 2023-01-13 10:32:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73857d7d-0ac4-4a62-91cc-8cbfef5de39b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:32:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxphd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxphd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.230,StartTime:2023-01-13 10:32:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:32:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://5c186d3d340c15bf89dcfba751d6fe0a1190fd8d60e2d3aeb74a5c037693c61d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 10:32:42.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1883" for this suite. 01/13/23 10:32:42.756
------------------------------
• [SLOW TEST] [5.322 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:32:37.457
    Jan 13 10:32:37.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 10:32:37.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:37.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:37.559
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 13 10:32:37.608: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 13 10:32:42.622: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/13/23 10:32:42.622
    Jan 13 10:32:42.622: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/13/23 10:32:42.654
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 10:32:42.702: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1883  896d2855-128c-46a0-9892-89c4f112de0f 595827 1 2023-01-13 10:32:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-13 10:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a1758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 13 10:32:42.714: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 13 10:32:42.714: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 13 10:32:42.715: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1883  73857d7d-0ac4-4a62-91cc-8cbfef5de39b 595828 1 2023-01-13 10:32:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 896d2855-128c-46a0-9892-89c4f112de0f 0xc0050a1ab7 0xc0050a1ab8}] [] [{e2e.test Update apps/v1 2023-01-13 10:32:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:32:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-13 10:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"896d2855-128c-46a0-9892-89c4f112de0f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0050a1b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:32:42.734: INFO: Pod "test-cleanup-controller-74k6j" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-74k6j test-cleanup-controller- deployment-1883  b0a8029f-dfbd-42c3-bd6d-e046a59e4455 595822 0 2023-01-13 10:32:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b174fb1f4f5f6e085d81ee07ccf0adab2b21c3847620377be824ebc36cb036ec cni.projectcalico.org/podIP:10.244.27.230/32 cni.projectcalico.org/podIPs:10.244.27.230/32] [{apps/v1 ReplicaSet test-cleanup-controller 73857d7d-0ac4-4a62-91cc-8cbfef5de39b 0xc0050a1e97 0xc0050a1e98}] [] [{kube-controller-manager Update v1 2023-01-13 10:32:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73857d7d-0ac4-4a62-91cc-8cbfef5de39b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:32:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxphd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxphd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:32:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.230,StartTime:2023-01-13 10:32:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:32:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://5c186d3d340c15bf89dcfba751d6fe0a1190fd8d60e2d3aeb74a5c037693c61d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:32:42.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1883" for this suite. 01/13/23 10:32:42.756
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:32:42.78
Jan 13 10:32:42.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:32:42.782
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:42.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:42.846
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/13/23 10:32:42.854
STEP: setting up watch 01/13/23 10:32:42.855
STEP: submitting the pod to kubernetes 01/13/23 10:32:42.961
STEP: verifying the pod is in kubernetes 01/13/23 10:32:42.983
STEP: verifying pod creation was observed 01/13/23 10:32:42.998
Jan 13 10:32:42.998: INFO: Waiting up to 5m0s for pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119" in namespace "pods-6798" to be "running"
Jan 13 10:32:43.011: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119": Phase="Pending", Reason="", readiness=false. Elapsed: 12.725016ms
Jan 13 10:32:45.027: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028420036s
Jan 13 10:32:47.020: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119": Phase="Running", Reason="", readiness=true. Elapsed: 4.021862757s
Jan 13 10:32:47.020: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119" satisfied condition "running"
STEP: deleting the pod gracefully 01/13/23 10:32:47.028
STEP: verifying pod deletion was observed 01/13/23 10:32:47.05
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 10:32:50.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6798" for this suite. 01/13/23 10:32:50.865
------------------------------
• [SLOW TEST] [8.104 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:32:42.78
    Jan 13 10:32:42.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:32:42.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:42.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:42.846
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/13/23 10:32:42.854
    STEP: setting up watch 01/13/23 10:32:42.855
    STEP: submitting the pod to kubernetes 01/13/23 10:32:42.961
    STEP: verifying the pod is in kubernetes 01/13/23 10:32:42.983
    STEP: verifying pod creation was observed 01/13/23 10:32:42.998
    Jan 13 10:32:42.998: INFO: Waiting up to 5m0s for pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119" in namespace "pods-6798" to be "running"
    Jan 13 10:32:43.011: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119": Phase="Pending", Reason="", readiness=false. Elapsed: 12.725016ms
    Jan 13 10:32:45.027: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028420036s
    Jan 13 10:32:47.020: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119": Phase="Running", Reason="", readiness=true. Elapsed: 4.021862757s
    Jan 13 10:32:47.020: INFO: Pod "pod-submit-remove-a69cf04a-0992-4ba8-b23c-5d9a6eda5119" satisfied condition "running"
    STEP: deleting the pod gracefully 01/13/23 10:32:47.028
    STEP: verifying pod deletion was observed 01/13/23 10:32:47.05
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:32:50.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6798" for this suite. 01/13/23 10:32:50.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:32:50.885
Jan 13 10:32:50.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:32:50.888
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:50.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:50.923
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/13/23 10:32:50.929
Jan 13 10:32:50.940: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c" in namespace "emptydir-1327" to be "running"
Jan 13 10:32:50.945: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720958ms
Jan 13 10:32:52.952: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01226465s
Jan 13 10:32:54.952: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012010985s
Jan 13 10:32:56.953: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Running", Reason="", readiness=false. Elapsed: 6.013367442s
Jan 13 10:32:56.953: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/13/23 10:32:56.953
Jan 13 10:32:56.954: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1327 PodName:pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:32:56.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:32:56.956: INFO: ExecWithOptions: Clientset creation
Jan 13 10:32:56.956: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1327/pods/pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 13 10:32:57.203: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:32:57.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1327" for this suite. 01/13/23 10:32:57.213
------------------------------
• [SLOW TEST] [6.339 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:32:50.885
    Jan 13 10:32:50.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:32:50.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:50.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:50.923
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/13/23 10:32:50.929
    Jan 13 10:32:50.940: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c" in namespace "emptydir-1327" to be "running"
    Jan 13 10:32:50.945: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720958ms
    Jan 13 10:32:52.952: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01226465s
    Jan 13 10:32:54.952: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012010985s
    Jan 13 10:32:56.953: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c": Phase="Running", Reason="", readiness=false. Elapsed: 6.013367442s
    Jan 13 10:32:56.953: INFO: Pod "pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/13/23 10:32:56.953
    Jan 13 10:32:56.954: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1327 PodName:pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:32:56.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:32:56.956: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:32:56.956: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1327/pods/pod-sharedvolume-29c9b016-14e3-4501-9d31-deed97511a6c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 13 10:32:57.203: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:32:57.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1327" for this suite. 01/13/23 10:32:57.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:32:57.227
Jan 13 10:32:57.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:32:57.23
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:57.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:57.265
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:32:57.272
Jan 13 10:32:57.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0" in namespace "projected-1625" to be "Succeeded or Failed"
Jan 13 10:32:57.303: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.099024ms
Jan 13 10:32:59.313: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017136503s
Jan 13 10:33:01.320: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023447588s
Jan 13 10:33:03.311: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014464442s
Jan 13 10:33:05.322: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02615043s
STEP: Saw pod success 01/13/23 10:33:05.322
Jan 13 10:33:05.323: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0" satisfied condition "Succeeded or Failed"
Jan 13 10:33:05.330: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0 container client-container: <nil>
STEP: delete the pod 01/13/23 10:33:05.352
Jan 13 10:33:05.402: INFO: Waiting for pod downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0 to disappear
Jan 13 10:33:05.411: INFO: Pod downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:33:05.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1625" for this suite. 01/13/23 10:33:05.434
------------------------------
• [SLOW TEST] [8.227 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:32:57.227
    Jan 13 10:32:57.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:32:57.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:32:57.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:32:57.265
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:32:57.272
    Jan 13 10:32:57.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0" in namespace "projected-1625" to be "Succeeded or Failed"
    Jan 13 10:32:57.303: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.099024ms
    Jan 13 10:32:59.313: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017136503s
    Jan 13 10:33:01.320: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023447588s
    Jan 13 10:33:03.311: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014464442s
    Jan 13 10:33:05.322: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02615043s
    STEP: Saw pod success 01/13/23 10:33:05.322
    Jan 13 10:33:05.323: INFO: Pod "downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0" satisfied condition "Succeeded or Failed"
    Jan 13 10:33:05.330: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:33:05.352
    Jan 13 10:33:05.402: INFO: Waiting for pod downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0 to disappear
    Jan 13 10:33:05.411: INFO: Pod downwardapi-volume-21f369dc-a590-44de-8277-a4b406280ba0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:33:05.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1625" for this suite. 01/13/23 10:33:05.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:33:05.456
Jan 13 10:33:05.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename sysctl 01/13/23 10:33:05.458
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:05.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:05.5
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/13/23 10:33:05.51
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:33:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7855" for this suite. 01/13/23 10:33:05.535
------------------------------
• [0.099 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:33:05.456
    Jan 13 10:33:05.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename sysctl 01/13/23 10:33:05.458
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:05.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:05.5
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/13/23 10:33:05.51
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:33:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7855" for this suite. 01/13/23 10:33:05.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:33:05.556
Jan 13 10:33:05.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:33:05.558
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:05.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:05.604
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-4fbf148d-a48c-4847-81bc-694e09f752a5 01/13/23 10:33:05.634
STEP: Creating secret with name s-test-opt-upd-1f540c85-c2e2-4152-b177-2ffda7277767 01/13/23 10:33:05.65
STEP: Creating the pod 01/13/23 10:33:05.66
Jan 13 10:33:05.695: INFO: Waiting up to 5m0s for pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638" in namespace "secrets-8217" to be "running and ready"
Jan 13 10:33:05.710: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638": Phase="Pending", Reason="", readiness=false. Elapsed: 14.935501ms
Jan 13 10:33:05.710: INFO: The phase of Pod pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:33:07.716: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021662528s
Jan 13 10:33:07.716: INFO: The phase of Pod pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:33:09.726: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638": Phase="Running", Reason="", readiness=true. Elapsed: 4.031740589s
Jan 13 10:33:09.727: INFO: The phase of Pod pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638 is Running (Ready = true)
Jan 13 10:33:09.727: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-4fbf148d-a48c-4847-81bc-694e09f752a5 01/13/23 10:33:09.844
STEP: Updating secret s-test-opt-upd-1f540c85-c2e2-4152-b177-2ffda7277767 01/13/23 10:33:09.854
STEP: Creating secret with name s-test-opt-create-ce111268-6a58-4939-bd19-159abb2c50b5 01/13/23 10:33:09.863
STEP: waiting to observe update in volume 01/13/23 10:33:09.871
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:33:14.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8217" for this suite. 01/13/23 10:33:14.058
------------------------------
• [SLOW TEST] [8.572 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:33:05.556
    Jan 13 10:33:05.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:33:05.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:05.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:05.604
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-4fbf148d-a48c-4847-81bc-694e09f752a5 01/13/23 10:33:05.634
    STEP: Creating secret with name s-test-opt-upd-1f540c85-c2e2-4152-b177-2ffda7277767 01/13/23 10:33:05.65
    STEP: Creating the pod 01/13/23 10:33:05.66
    Jan 13 10:33:05.695: INFO: Waiting up to 5m0s for pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638" in namespace "secrets-8217" to be "running and ready"
    Jan 13 10:33:05.710: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638": Phase="Pending", Reason="", readiness=false. Elapsed: 14.935501ms
    Jan 13 10:33:05.710: INFO: The phase of Pod pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:33:07.716: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021662528s
    Jan 13 10:33:07.716: INFO: The phase of Pod pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:33:09.726: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638": Phase="Running", Reason="", readiness=true. Elapsed: 4.031740589s
    Jan 13 10:33:09.727: INFO: The phase of Pod pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638 is Running (Ready = true)
    Jan 13 10:33:09.727: INFO: Pod "pod-secrets-8e260a09-b09e-41a5-a3a7-518bdc55b638" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-4fbf148d-a48c-4847-81bc-694e09f752a5 01/13/23 10:33:09.844
    STEP: Updating secret s-test-opt-upd-1f540c85-c2e2-4152-b177-2ffda7277767 01/13/23 10:33:09.854
    STEP: Creating secret with name s-test-opt-create-ce111268-6a58-4939-bd19-159abb2c50b5 01/13/23 10:33:09.863
    STEP: waiting to observe update in volume 01/13/23 10:33:09.871
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:33:14.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8217" for this suite. 01/13/23 10:33:14.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:33:14.131
Jan 13 10:33:14.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename namespaces 01/13/23 10:33:14.134
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:14.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:14.295
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-1795" 01/13/23 10:33:14.303
Jan 13 10:33:14.338: INFO: Namespace "namespaces-1795" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5655d73b-a226-4012-b01d-7e72ee24020b", "kubernetes.io/metadata.name":"namespaces-1795", "namespaces-1795":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:33:14.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1795" for this suite. 01/13/23 10:33:14.375
------------------------------
• [0.278 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:33:14.131
    Jan 13 10:33:14.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename namespaces 01/13/23 10:33:14.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:14.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:14.295
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-1795" 01/13/23 10:33:14.303
    Jan 13 10:33:14.338: INFO: Namespace "namespaces-1795" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5655d73b-a226-4012-b01d-7e72ee24020b", "kubernetes.io/metadata.name":"namespaces-1795", "namespaces-1795":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:33:14.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1795" for this suite. 01/13/23 10:33:14.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:33:14.413
Jan 13 10:33:14.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename namespaces 01/13/23 10:33:14.416
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:14.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:14.47
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/13/23 10:33:14.489
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:14.522
STEP: Creating a pod in the namespace 01/13/23 10:33:14.544
STEP: Waiting for the pod to have running status 01/13/23 10:33:14.598
Jan 13 10:33:14.599: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2494" to be "running"
Jan 13 10:33:14.606: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.217742ms
Jan 13 10:33:16.616: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017453802s
Jan 13 10:33:18.615: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016524975s
Jan 13 10:33:18.615: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/13/23 10:33:18.615
STEP: Waiting for the namespace to be removed. 01/13/23 10:33:18.626
STEP: Recreating the namespace 01/13/23 10:33:30.64
STEP: Verifying there are no pods in the namespace 01/13/23 10:33:30.684
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:33:30.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7525" for this suite. 01/13/23 10:33:30.737
STEP: Destroying namespace "nsdeletetest-2494" for this suite. 01/13/23 10:33:30.758
Jan 13 10:33:30.772: INFO: Namespace nsdeletetest-2494 was already deleted
STEP: Destroying namespace "nsdeletetest-1708" for this suite. 01/13/23 10:33:30.772
------------------------------
• [SLOW TEST] [16.395 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:33:14.413
    Jan 13 10:33:14.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename namespaces 01/13/23 10:33:14.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:14.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:14.47
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/13/23 10:33:14.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:14.522
    STEP: Creating a pod in the namespace 01/13/23 10:33:14.544
    STEP: Waiting for the pod to have running status 01/13/23 10:33:14.598
    Jan 13 10:33:14.599: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2494" to be "running"
    Jan 13 10:33:14.606: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.217742ms
    Jan 13 10:33:16.616: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017453802s
    Jan 13 10:33:18.615: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016524975s
    Jan 13 10:33:18.615: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/13/23 10:33:18.615
    STEP: Waiting for the namespace to be removed. 01/13/23 10:33:18.626
    STEP: Recreating the namespace 01/13/23 10:33:30.64
    STEP: Verifying there are no pods in the namespace 01/13/23 10:33:30.684
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:33:30.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7525" for this suite. 01/13/23 10:33:30.737
    STEP: Destroying namespace "nsdeletetest-2494" for this suite. 01/13/23 10:33:30.758
    Jan 13 10:33:30.772: INFO: Namespace nsdeletetest-2494 was already deleted
    STEP: Destroying namespace "nsdeletetest-1708" for this suite. 01/13/23 10:33:30.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:33:30.811
Jan 13 10:33:30.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 10:33:30.814
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:30.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:30.851
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-1e620903-2113-4852-9d0c-690d04c8b62b 01/13/23 10:33:30.861
STEP: Creating a pod to test consume configMaps 01/13/23 10:33:30.874
Jan 13 10:33:30.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e" in namespace "configmap-7108" to be "Succeeded or Failed"
Jan 13 10:33:30.989: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.969004ms
Jan 13 10:33:32.996: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051323592s
Jan 13 10:33:35.004: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059666186s
Jan 13 10:33:36.998: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053951383s
STEP: Saw pod success 01/13/23 10:33:36.998
Jan 13 10:33:36.998: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e" satisfied condition "Succeeded or Failed"
Jan 13 10:33:37.006: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e container configmap-volume-test: <nil>
STEP: delete the pod 01/13/23 10:33:37.032
Jan 13 10:33:37.068: INFO: Waiting for pod pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e to disappear
Jan 13 10:33:37.086: INFO: Pod pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:33:37.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7108" for this suite. 01/13/23 10:33:37.106
------------------------------
• [SLOW TEST] [6.314 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:33:30.811
    Jan 13 10:33:30.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 10:33:30.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:30.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:30.851
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-1e620903-2113-4852-9d0c-690d04c8b62b 01/13/23 10:33:30.861
    STEP: Creating a pod to test consume configMaps 01/13/23 10:33:30.874
    Jan 13 10:33:30.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e" in namespace "configmap-7108" to be "Succeeded or Failed"
    Jan 13 10:33:30.989: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.969004ms
    Jan 13 10:33:32.996: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051323592s
    Jan 13 10:33:35.004: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059666186s
    Jan 13 10:33:36.998: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053951383s
    STEP: Saw pod success 01/13/23 10:33:36.998
    Jan 13 10:33:36.998: INFO: Pod "pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e" satisfied condition "Succeeded or Failed"
    Jan 13 10:33:37.006: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e container configmap-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:33:37.032
    Jan 13 10:33:37.068: INFO: Waiting for pod pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e to disappear
    Jan 13 10:33:37.086: INFO: Pod pod-configmaps-7d5a3750-c570-4b51-aa1e-a2c260a14f8e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:33:37.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7108" for this suite. 01/13/23 10:33:37.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:33:37.126
Jan 13 10:33:37.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:33:37.13
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:37.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:37.17
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-de7ea6db-b4d0-4610-aba5-2ac7dceaeb39 01/13/23 10:33:37.194
STEP: Creating the pod 01/13/23 10:33:37.205
Jan 13 10:33:37.226: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e" in namespace "projected-1183" to be "running and ready"
Jan 13 10:33:37.233: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913454ms
Jan 13 10:33:37.233: INFO: The phase of Pod pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:33:39.245: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018435964s
Jan 13 10:33:39.245: INFO: The phase of Pod pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:33:41.246: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e": Phase="Running", Reason="", readiness=true. Elapsed: 4.020159548s
Jan 13 10:33:41.246: INFO: The phase of Pod pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e is Running (Ready = true)
Jan 13 10:33:41.246: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-de7ea6db-b4d0-4610-aba5-2ac7dceaeb39 01/13/23 10:33:41.281
STEP: waiting to observe update in volume 01/13/23 10:33:41.311
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:33:45.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1183" for this suite. 01/13/23 10:33:45.411
------------------------------
• [SLOW TEST] [8.305 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:33:37.126
    Jan 13 10:33:37.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:33:37.13
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:37.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:37.17
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-de7ea6db-b4d0-4610-aba5-2ac7dceaeb39 01/13/23 10:33:37.194
    STEP: Creating the pod 01/13/23 10:33:37.205
    Jan 13 10:33:37.226: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e" in namespace "projected-1183" to be "running and ready"
    Jan 13 10:33:37.233: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913454ms
    Jan 13 10:33:37.233: INFO: The phase of Pod pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:33:39.245: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018435964s
    Jan 13 10:33:39.245: INFO: The phase of Pod pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:33:41.246: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e": Phase="Running", Reason="", readiness=true. Elapsed: 4.020159548s
    Jan 13 10:33:41.246: INFO: The phase of Pod pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e is Running (Ready = true)
    Jan 13 10:33:41.246: INFO: Pod "pod-projected-configmaps-38017748-acc9-4626-a86e-8f6383b2ea7e" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-de7ea6db-b4d0-4610-aba5-2ac7dceaeb39 01/13/23 10:33:41.281
    STEP: waiting to observe update in volume 01/13/23 10:33:41.311
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:33:45.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1183" for this suite. 01/13/23 10:33:45.411
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:33:45.433
Jan 13 10:33:45.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 10:33:45.436
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:45.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:45.518
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 10:34:45.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6827" for this suite. 01/13/23 10:34:45.6
------------------------------
• [SLOW TEST] [60.179 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:33:45.433
    Jan 13 10:33:45.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 10:33:45.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:33:45.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:33:45.518
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:34:45.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6827" for this suite. 01/13/23 10:34:45.6
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:34:45.613
Jan 13 10:34:45.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename endpointslicemirroring 01/13/23 10:34:45.614
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:34:45.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:34:45.647
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/13/23 10:34:45.672
Jan 13 10:34:45.690: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/13/23 10:34:47.698
Jan 13 10:34:47.713: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/13/23 10:34:49.723
Jan 13 10:34:49.755: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 13 10:34:51.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6047" for this suite. 01/13/23 10:34:51.783
------------------------------
• [SLOW TEST] [6.196 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:34:45.613
    Jan 13 10:34:45.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename endpointslicemirroring 01/13/23 10:34:45.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:34:45.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:34:45.647
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/13/23 10:34:45.672
    Jan 13 10:34:45.690: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/13/23 10:34:47.698
    Jan 13 10:34:47.713: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/13/23 10:34:49.723
    Jan 13 10:34:49.755: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:34:51.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6047" for this suite. 01/13/23 10:34:51.783
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:34:51.809
Jan 13 10:34:51.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename proxy 01/13/23 10:34:51.811
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:34:51.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:34:51.866
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 13 10:34:51.872: INFO: Creating pod...
Jan 13 10:34:51.888: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5299" to be "running"
Jan 13 10:34:51.900: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.003149ms
Jan 13 10:34:53.925: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037601944s
Jan 13 10:34:55.907: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.019153732s
Jan 13 10:34:55.907: INFO: Pod "agnhost" satisfied condition "running"
Jan 13 10:34:55.907: INFO: Creating service...
Jan 13 10:34:55.931: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=DELETE
Jan 13 10:34:55.940: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 13 10:34:55.940: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=OPTIONS
Jan 13 10:34:55.947: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 13 10:34:55.947: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=PATCH
Jan 13 10:34:55.969: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 13 10:34:55.969: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=POST
Jan 13 10:34:55.990: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 13 10:34:55.990: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=PUT
Jan 13 10:34:55.996: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 13 10:34:55.996: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 13 10:34:56.004: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 13 10:34:56.004: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 13 10:34:56.012: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 13 10:34:56.012: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 13 10:34:56.019: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 13 10:34:56.019: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=POST
Jan 13 10:34:56.026: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 13 10:34:56.026: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=PUT
Jan 13 10:34:56.032: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 13 10:34:56.033: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=GET
Jan 13 10:34:56.036: INFO: http.Client request:GET StatusCode:301
Jan 13 10:34:56.036: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=GET
Jan 13 10:34:56.041: INFO: http.Client request:GET StatusCode:301
Jan 13 10:34:56.041: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=HEAD
Jan 13 10:34:56.043: INFO: http.Client request:HEAD StatusCode:301
Jan 13 10:34:56.043: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 13 10:34:56.049: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 13 10:34:56.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5299" for this suite. 01/13/23 10:34:56.057
------------------------------
• [4.259 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:34:51.809
    Jan 13 10:34:51.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename proxy 01/13/23 10:34:51.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:34:51.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:34:51.866
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 13 10:34:51.872: INFO: Creating pod...
    Jan 13 10:34:51.888: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5299" to be "running"
    Jan 13 10:34:51.900: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.003149ms
    Jan 13 10:34:53.925: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037601944s
    Jan 13 10:34:55.907: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.019153732s
    Jan 13 10:34:55.907: INFO: Pod "agnhost" satisfied condition "running"
    Jan 13 10:34:55.907: INFO: Creating service...
    Jan 13 10:34:55.931: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=DELETE
    Jan 13 10:34:55.940: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 13 10:34:55.940: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=OPTIONS
    Jan 13 10:34:55.947: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 13 10:34:55.947: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=PATCH
    Jan 13 10:34:55.969: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 13 10:34:55.969: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=POST
    Jan 13 10:34:55.990: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 13 10:34:55.990: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=PUT
    Jan 13 10:34:55.996: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 13 10:34:55.996: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 13 10:34:56.004: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 13 10:34:56.004: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 13 10:34:56.012: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 13 10:34:56.012: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 13 10:34:56.019: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 13 10:34:56.019: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=POST
    Jan 13 10:34:56.026: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 13 10:34:56.026: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 13 10:34:56.032: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 13 10:34:56.033: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=GET
    Jan 13 10:34:56.036: INFO: http.Client request:GET StatusCode:301
    Jan 13 10:34:56.036: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=GET
    Jan 13 10:34:56.041: INFO: http.Client request:GET StatusCode:301
    Jan 13 10:34:56.041: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/pods/agnhost/proxy?method=HEAD
    Jan 13 10:34:56.043: INFO: http.Client request:HEAD StatusCode:301
    Jan 13 10:34:56.043: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5299/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 13 10:34:56.049: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:34:56.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5299" for this suite. 01/13/23 10:34:56.057
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:34:56.07
Jan 13 10:34:56.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:34:56.072
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:34:56.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:34:56.111
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-be0441cf-6c55-49d5-8636-10ea9a2b6da6 01/13/23 10:34:56.121
STEP: Creating a pod to test consume secrets 01/13/23 10:34:56.128
Jan 13 10:34:56.143: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b" in namespace "projected-6039" to be "Succeeded or Failed"
Jan 13 10:34:56.149: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.233376ms
Jan 13 10:34:58.156: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012914046s
Jan 13 10:35:00.157: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013691212s
Jan 13 10:35:02.155: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011850514s
Jan 13 10:35:04.174: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030279217s
STEP: Saw pod success 01/13/23 10:35:04.174
Jan 13 10:35:04.174: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b" satisfied condition "Succeeded or Failed"
Jan 13 10:35:04.181: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b container projected-secret-volume-test: <nil>
STEP: delete the pod 01/13/23 10:35:04.237
Jan 13 10:35:04.317: INFO: Waiting for pod pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b to disappear
Jan 13 10:35:04.326: INFO: Pod pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 13 10:35:04.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6039" for this suite. 01/13/23 10:35:04.343
------------------------------
• [SLOW TEST] [8.310 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:34:56.07
    Jan 13 10:34:56.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:34:56.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:34:56.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:34:56.111
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-be0441cf-6c55-49d5-8636-10ea9a2b6da6 01/13/23 10:34:56.121
    STEP: Creating a pod to test consume secrets 01/13/23 10:34:56.128
    Jan 13 10:34:56.143: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b" in namespace "projected-6039" to be "Succeeded or Failed"
    Jan 13 10:34:56.149: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.233376ms
    Jan 13 10:34:58.156: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012914046s
    Jan 13 10:35:00.157: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013691212s
    Jan 13 10:35:02.155: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011850514s
    Jan 13 10:35:04.174: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030279217s
    STEP: Saw pod success 01/13/23 10:35:04.174
    Jan 13 10:35:04.174: INFO: Pod "pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b" satisfied condition "Succeeded or Failed"
    Jan 13 10:35:04.181: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:35:04.237
    Jan 13 10:35:04.317: INFO: Waiting for pod pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b to disappear
    Jan 13 10:35:04.326: INFO: Pod pod-projected-secrets-986c9e55-3d1a-44e2-ab4a-55821aa9b05b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:35:04.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6039" for this suite. 01/13/23 10:35:04.343
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:35:04.381
Jan 13 10:35:04.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 10:35:04.384
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:04.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:04.448
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/13/23 10:35:04.454
STEP: Creating a ResourceQuota 01/13/23 10:35:09.464
STEP: Ensuring resource quota status is calculated 01/13/23 10:35:09.497
STEP: Creating a Service 01/13/23 10:35:11.504
STEP: Creating a NodePort Service 01/13/23 10:35:11.555
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/13/23 10:35:11.6
STEP: Ensuring resource quota status captures service creation 01/13/23 10:35:11.64
STEP: Deleting Services 01/13/23 10:35:13.649
STEP: Ensuring resource quota status released usage 01/13/23 10:35:13.737
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 10:35:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6022" for this suite. 01/13/23 10:35:15.781
------------------------------
• [SLOW TEST] [11.441 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:35:04.381
    Jan 13 10:35:04.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 10:35:04.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:04.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:04.448
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/13/23 10:35:04.454
    STEP: Creating a ResourceQuota 01/13/23 10:35:09.464
    STEP: Ensuring resource quota status is calculated 01/13/23 10:35:09.497
    STEP: Creating a Service 01/13/23 10:35:11.504
    STEP: Creating a NodePort Service 01/13/23 10:35:11.555
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/13/23 10:35:11.6
    STEP: Ensuring resource quota status captures service creation 01/13/23 10:35:11.64
    STEP: Deleting Services 01/13/23 10:35:13.649
    STEP: Ensuring resource quota status released usage 01/13/23 10:35:13.737
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:35:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6022" for this suite. 01/13/23 10:35:15.781
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:35:15.822
Jan 13 10:35:15.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 10:35:15.826
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:15.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:15.902
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-1423d85d-25a9-4834-b241-1561347863ab 01/13/23 10:35:15.924
STEP: Creating a pod to test consume configMaps 01/13/23 10:35:15.964
Jan 13 10:35:15.998: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4" in namespace "configmap-5401" to be "Succeeded or Failed"
Jan 13 10:35:16.013: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.002422ms
Jan 13 10:35:18.021: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02299227s
Jan 13 10:35:20.024: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025751388s
Jan 13 10:35:22.020: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021948074s
STEP: Saw pod success 01/13/23 10:35:22.021
Jan 13 10:35:22.021: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4" satisfied condition "Succeeded or Failed"
Jan 13 10:35:22.028: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:35:22.048
Jan 13 10:35:22.064: INFO: Waiting for pod pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4 to disappear
Jan 13 10:35:22.069: INFO: Pod pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:35:22.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5401" for this suite. 01/13/23 10:35:22.075
------------------------------
• [SLOW TEST] [6.262 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:35:15.822
    Jan 13 10:35:15.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 10:35:15.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:15.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:15.902
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-1423d85d-25a9-4834-b241-1561347863ab 01/13/23 10:35:15.924
    STEP: Creating a pod to test consume configMaps 01/13/23 10:35:15.964
    Jan 13 10:35:15.998: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4" in namespace "configmap-5401" to be "Succeeded or Failed"
    Jan 13 10:35:16.013: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.002422ms
    Jan 13 10:35:18.021: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02299227s
    Jan 13 10:35:20.024: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025751388s
    Jan 13 10:35:22.020: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021948074s
    STEP: Saw pod success 01/13/23 10:35:22.021
    Jan 13 10:35:22.021: INFO: Pod "pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4" satisfied condition "Succeeded or Failed"
    Jan 13 10:35:22.028: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:35:22.048
    Jan 13 10:35:22.064: INFO: Waiting for pod pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4 to disappear
    Jan 13 10:35:22.069: INFO: Pod pod-configmaps-4b6a2a00-c00d-4cb8-8229-98022fbe67c4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:35:22.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5401" for this suite. 01/13/23 10:35:22.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:35:22.086
Jan 13 10:35:22.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename endpointslice 01/13/23 10:35:22.089
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:22.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:22.129
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/13/23 10:35:22.14
STEP: getting /apis/discovery.k8s.io 01/13/23 10:35:22.149
STEP: getting /apis/discovery.k8s.iov1 01/13/23 10:35:22.153
STEP: creating 01/13/23 10:35:22.157
STEP: getting 01/13/23 10:35:22.195
STEP: listing 01/13/23 10:35:22.204
STEP: watching 01/13/23 10:35:22.223
Jan 13 10:35:22.223: INFO: starting watch
STEP: cluster-wide listing 01/13/23 10:35:22.229
STEP: cluster-wide watching 01/13/23 10:35:22.237
Jan 13 10:35:22.237: INFO: starting watch
STEP: patching 01/13/23 10:35:22.241
STEP: updating 01/13/23 10:35:22.255
Jan 13 10:35:22.274: INFO: waiting for watch events with expected annotations
Jan 13 10:35:22.274: INFO: saw patched and updated annotations
STEP: deleting 01/13/23 10:35:22.274
STEP: deleting a collection 01/13/23 10:35:22.317
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 13 10:35:22.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6973" for this suite. 01/13/23 10:35:22.411
------------------------------
• [0.350 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:35:22.086
    Jan 13 10:35:22.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename endpointslice 01/13/23 10:35:22.089
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:22.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:22.129
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/13/23 10:35:22.14
    STEP: getting /apis/discovery.k8s.io 01/13/23 10:35:22.149
    STEP: getting /apis/discovery.k8s.iov1 01/13/23 10:35:22.153
    STEP: creating 01/13/23 10:35:22.157
    STEP: getting 01/13/23 10:35:22.195
    STEP: listing 01/13/23 10:35:22.204
    STEP: watching 01/13/23 10:35:22.223
    Jan 13 10:35:22.223: INFO: starting watch
    STEP: cluster-wide listing 01/13/23 10:35:22.229
    STEP: cluster-wide watching 01/13/23 10:35:22.237
    Jan 13 10:35:22.237: INFO: starting watch
    STEP: patching 01/13/23 10:35:22.241
    STEP: updating 01/13/23 10:35:22.255
    Jan 13 10:35:22.274: INFO: waiting for watch events with expected annotations
    Jan 13 10:35:22.274: INFO: saw patched and updated annotations
    STEP: deleting 01/13/23 10:35:22.274
    STEP: deleting a collection 01/13/23 10:35:22.317
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:35:22.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6973" for this suite. 01/13/23 10:35:22.411
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:35:22.436
Jan 13 10:35:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename ephemeral-containers-test 01/13/23 10:35:22.439
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:22.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:22.567
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/13/23 10:35:22.593
Jan 13 10:35:22.621: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2446" to be "running and ready"
Jan 13 10:35:22.642: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.473949ms
Jan 13 10:35:22.642: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:35:24.653: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031203746s
Jan 13 10:35:24.653: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:35:26.661: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.039215876s
Jan 13 10:35:26.661: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 13 10:35:26.661: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/13/23 10:35:26.682
Jan 13 10:35:26.723: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2446" to be "container debugger running"
Jan 13 10:35:26.734: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.379973ms
Jan 13 10:35:28.745: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020953615s
Jan 13 10:35:30.759: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.035262527s
Jan 13 10:35:30.759: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/13/23 10:35:30.759
Jan 13 10:35:30.759: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2446 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:35:30.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:35:30.761: INFO: ExecWithOptions: Clientset creation
Jan 13 10:35:30.761: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2446/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 13 10:35:30.991: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:35:31.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2446" for this suite. 01/13/23 10:35:31.103
------------------------------
• [SLOW TEST] [8.693 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:35:22.436
    Jan 13 10:35:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/13/23 10:35:22.439
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:22.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:22.567
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/13/23 10:35:22.593
    Jan 13 10:35:22.621: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2446" to be "running and ready"
    Jan 13 10:35:22.642: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.473949ms
    Jan 13 10:35:22.642: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:35:24.653: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031203746s
    Jan 13 10:35:24.653: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:35:26.661: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.039215876s
    Jan 13 10:35:26.661: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 13 10:35:26.661: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/13/23 10:35:26.682
    Jan 13 10:35:26.723: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2446" to be "container debugger running"
    Jan 13 10:35:26.734: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.379973ms
    Jan 13 10:35:28.745: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020953615s
    Jan 13 10:35:30.759: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.035262527s
    Jan 13 10:35:30.759: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/13/23 10:35:30.759
    Jan 13 10:35:30.759: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2446 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:35:30.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:35:30.761: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:35:30.761: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2446/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 13 10:35:30.991: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:35:31.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2446" for this suite. 01/13/23 10:35:31.103
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:35:31.131
Jan 13 10:35:31.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename disruption 01/13/23 10:35:31.133
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:31.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:31.229
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/13/23 10:35:31.288
STEP: Waiting for the pdb to be processed 01/13/23 10:35:31.314
STEP: updating the pdb 01/13/23 10:35:33.33
STEP: Waiting for the pdb to be processed 01/13/23 10:35:33.352
STEP: patching the pdb 01/13/23 10:35:33.367
STEP: Waiting for the pdb to be processed 01/13/23 10:35:33.386
STEP: Waiting for the pdb to be deleted 01/13/23 10:35:35.425
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 13 10:35:35.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2254" for this suite. 01/13/23 10:35:35.451
------------------------------
• [4.344 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:35:31.131
    Jan 13 10:35:31.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename disruption 01/13/23 10:35:31.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:31.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:31.229
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/13/23 10:35:31.288
    STEP: Waiting for the pdb to be processed 01/13/23 10:35:31.314
    STEP: updating the pdb 01/13/23 10:35:33.33
    STEP: Waiting for the pdb to be processed 01/13/23 10:35:33.352
    STEP: patching the pdb 01/13/23 10:35:33.367
    STEP: Waiting for the pdb to be processed 01/13/23 10:35:33.386
    STEP: Waiting for the pdb to be deleted 01/13/23 10:35:35.425
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:35:35.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2254" for this suite. 01/13/23 10:35:35.451
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:35:35.476
Jan 13 10:35:35.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename lease-test 01/13/23 10:35:35.479
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:35.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:35.587
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 13 10:35:35.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-8522" for this suite. 01/13/23 10:35:35.918
------------------------------
• [0.465 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:35:35.476
    Jan 13 10:35:35.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename lease-test 01/13/23 10:35:35.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:35.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:35.587
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:35:35.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-8522" for this suite. 01/13/23 10:35:35.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:35:35.942
Jan 13 10:35:35.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:35:35.944
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:35.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:35.989
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/13/23 10:35:36.002
Jan 13 10:35:36.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 create -f -'
Jan 13 10:35:39.012: INFO: stderr: ""
Jan 13 10:35:39.012: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 10:35:39.012
Jan 13 10:35:39.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 10:35:39.250: INFO: stderr: ""
Jan 13 10:35:39.250: INFO: stdout: "update-demo-nautilus-k7lkv update-demo-nautilus-ngfrd "
Jan 13 10:35:39.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-k7lkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 10:35:39.517: INFO: stderr: ""
Jan 13 10:35:39.517: INFO: stdout: ""
Jan 13 10:35:39.517: INFO: update-demo-nautilus-k7lkv is created but not running
Jan 13 10:35:44.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 10:35:44.752: INFO: stderr: ""
Jan 13 10:35:44.752: INFO: stdout: "update-demo-nautilus-k7lkv update-demo-nautilus-ngfrd "
Jan 13 10:35:44.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-k7lkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 10:35:45.122: INFO: stderr: ""
Jan 13 10:35:45.122: INFO: stdout: "true"
Jan 13 10:35:45.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-k7lkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 10:35:45.569: INFO: stderr: ""
Jan 13 10:35:45.569: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 13 10:35:45.569: INFO: validating pod update-demo-nautilus-k7lkv
Jan 13 10:35:45.595: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 10:35:45.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 10:35:45.595: INFO: update-demo-nautilus-k7lkv is verified up and running
Jan 13 10:35:45.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 10:35:46.012: INFO: stderr: ""
Jan 13 10:35:46.012: INFO: stdout: "true"
Jan 13 10:35:46.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 10:35:46.306: INFO: stderr: ""
Jan 13 10:35:46.306: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 13 10:35:46.306: INFO: validating pod update-demo-nautilus-ngfrd
Jan 13 10:35:46.320: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 10:35:46.320: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 10:35:46.320: INFO: update-demo-nautilus-ngfrd is verified up and running
STEP: scaling down the replication controller 01/13/23 10:35:46.32
Jan 13 10:35:46.324: INFO: scanned /root for discovery docs: <nil>
Jan 13 10:35:46.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 13 10:35:47.673: INFO: stderr: ""
Jan 13 10:35:47.673: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 10:35:47.673
Jan 13 10:35:47.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 10:35:47.981: INFO: stderr: ""
Jan 13 10:35:47.981: INFO: stdout: "update-demo-nautilus-k7lkv update-demo-nautilus-ngfrd "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/13/23 10:35:47.981
Jan 13 10:35:52.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 10:35:53.425: INFO: stderr: ""
Jan 13 10:35:53.425: INFO: stdout: "update-demo-nautilus-ngfrd "
Jan 13 10:35:53.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 10:35:53.683: INFO: stderr: ""
Jan 13 10:35:53.683: INFO: stdout: "true"
Jan 13 10:35:53.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 10:35:53.903: INFO: stderr: ""
Jan 13 10:35:53.903: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 13 10:35:53.903: INFO: validating pod update-demo-nautilus-ngfrd
Jan 13 10:35:53.909: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 10:35:53.909: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 10:35:53.909: INFO: update-demo-nautilus-ngfrd is verified up and running
STEP: scaling up the replication controller 01/13/23 10:35:53.909
Jan 13 10:35:53.912: INFO: scanned /root for discovery docs: <nil>
Jan 13 10:35:53.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 13 10:35:55.266: INFO: stderr: ""
Jan 13 10:35:55.267: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 10:35:55.267
Jan 13 10:35:55.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 10:35:55.819: INFO: stderr: ""
Jan 13 10:35:55.820: INFO: stdout: "update-demo-nautilus-db92z update-demo-nautilus-ngfrd "
Jan 13 10:35:55.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-db92z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 10:35:56.146: INFO: stderr: ""
Jan 13 10:35:56.146: INFO: stdout: ""
Jan 13 10:35:56.147: INFO: update-demo-nautilus-db92z is created but not running
Jan 13 10:36:01.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 10:36:01.430: INFO: stderr: ""
Jan 13 10:36:01.430: INFO: stdout: "update-demo-nautilus-db92z update-demo-nautilus-ngfrd "
Jan 13 10:36:01.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-db92z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 10:36:01.699: INFO: stderr: ""
Jan 13 10:36:01.699: INFO: stdout: "true"
Jan 13 10:36:01.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-db92z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 10:36:01.965: INFO: stderr: ""
Jan 13 10:36:01.965: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 13 10:36:01.965: INFO: validating pod update-demo-nautilus-db92z
Jan 13 10:36:01.977: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 10:36:01.977: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 10:36:01.977: INFO: update-demo-nautilus-db92z is verified up and running
Jan 13 10:36:01.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 10:36:02.306: INFO: stderr: ""
Jan 13 10:36:02.306: INFO: stdout: "true"
Jan 13 10:36:02.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 10:36:02.683: INFO: stderr: ""
Jan 13 10:36:02.683: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 13 10:36:02.683: INFO: validating pod update-demo-nautilus-ngfrd
Jan 13 10:36:02.696: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 10:36:02.696: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 10:36:02.696: INFO: update-demo-nautilus-ngfrd is verified up and running
STEP: using delete to clean up resources 01/13/23 10:36:02.696
Jan 13 10:36:02.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 delete --grace-period=0 --force -f -'
Jan 13 10:36:03.462: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:36:03.462: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 13 10:36:03.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get rc,svc -l name=update-demo --no-headers'
Jan 13 10:36:03.976: INFO: stderr: "No resources found in kubectl-1936 namespace.\n"
Jan 13 10:36:03.976: INFO: stdout: ""
Jan 13 10:36:03.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 13 10:36:04.280: INFO: stderr: ""
Jan 13 10:36:04.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:36:04.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1936" for this suite. 01/13/23 10:36:04.291
------------------------------
• [SLOW TEST] [28.365 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:35:35.942
    Jan 13 10:35:35.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:35:35.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:35:35.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:35:35.989
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/13/23 10:35:36.002
    Jan 13 10:35:36.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 create -f -'
    Jan 13 10:35:39.012: INFO: stderr: ""
    Jan 13 10:35:39.012: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 10:35:39.012
    Jan 13 10:35:39.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 10:35:39.250: INFO: stderr: ""
    Jan 13 10:35:39.250: INFO: stdout: "update-demo-nautilus-k7lkv update-demo-nautilus-ngfrd "
    Jan 13 10:35:39.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-k7lkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 10:35:39.517: INFO: stderr: ""
    Jan 13 10:35:39.517: INFO: stdout: ""
    Jan 13 10:35:39.517: INFO: update-demo-nautilus-k7lkv is created but not running
    Jan 13 10:35:44.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 10:35:44.752: INFO: stderr: ""
    Jan 13 10:35:44.752: INFO: stdout: "update-demo-nautilus-k7lkv update-demo-nautilus-ngfrd "
    Jan 13 10:35:44.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-k7lkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 10:35:45.122: INFO: stderr: ""
    Jan 13 10:35:45.122: INFO: stdout: "true"
    Jan 13 10:35:45.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-k7lkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 13 10:35:45.569: INFO: stderr: ""
    Jan 13 10:35:45.569: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 13 10:35:45.569: INFO: validating pod update-demo-nautilus-k7lkv
    Jan 13 10:35:45.595: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 13 10:35:45.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 13 10:35:45.595: INFO: update-demo-nautilus-k7lkv is verified up and running
    Jan 13 10:35:45.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 10:35:46.012: INFO: stderr: ""
    Jan 13 10:35:46.012: INFO: stdout: "true"
    Jan 13 10:35:46.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 13 10:35:46.306: INFO: stderr: ""
    Jan 13 10:35:46.306: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 13 10:35:46.306: INFO: validating pod update-demo-nautilus-ngfrd
    Jan 13 10:35:46.320: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 13 10:35:46.320: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 13 10:35:46.320: INFO: update-demo-nautilus-ngfrd is verified up and running
    STEP: scaling down the replication controller 01/13/23 10:35:46.32
    Jan 13 10:35:46.324: INFO: scanned /root for discovery docs: <nil>
    Jan 13 10:35:46.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 13 10:35:47.673: INFO: stderr: ""
    Jan 13 10:35:47.673: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 10:35:47.673
    Jan 13 10:35:47.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 10:35:47.981: INFO: stderr: ""
    Jan 13 10:35:47.981: INFO: stdout: "update-demo-nautilus-k7lkv update-demo-nautilus-ngfrd "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/13/23 10:35:47.981
    Jan 13 10:35:52.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 10:35:53.425: INFO: stderr: ""
    Jan 13 10:35:53.425: INFO: stdout: "update-demo-nautilus-ngfrd "
    Jan 13 10:35:53.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 10:35:53.683: INFO: stderr: ""
    Jan 13 10:35:53.683: INFO: stdout: "true"
    Jan 13 10:35:53.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 13 10:35:53.903: INFO: stderr: ""
    Jan 13 10:35:53.903: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 13 10:35:53.903: INFO: validating pod update-demo-nautilus-ngfrd
    Jan 13 10:35:53.909: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 13 10:35:53.909: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 13 10:35:53.909: INFO: update-demo-nautilus-ngfrd is verified up and running
    STEP: scaling up the replication controller 01/13/23 10:35:53.909
    Jan 13 10:35:53.912: INFO: scanned /root for discovery docs: <nil>
    Jan 13 10:35:53.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 13 10:35:55.266: INFO: stderr: ""
    Jan 13 10:35:55.267: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/13/23 10:35:55.267
    Jan 13 10:35:55.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 10:35:55.819: INFO: stderr: ""
    Jan 13 10:35:55.820: INFO: stdout: "update-demo-nautilus-db92z update-demo-nautilus-ngfrd "
    Jan 13 10:35:55.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-db92z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 10:35:56.146: INFO: stderr: ""
    Jan 13 10:35:56.146: INFO: stdout: ""
    Jan 13 10:35:56.147: INFO: update-demo-nautilus-db92z is created but not running
    Jan 13 10:36:01.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 13 10:36:01.430: INFO: stderr: ""
    Jan 13 10:36:01.430: INFO: stdout: "update-demo-nautilus-db92z update-demo-nautilus-ngfrd "
    Jan 13 10:36:01.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-db92z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 10:36:01.699: INFO: stderr: ""
    Jan 13 10:36:01.699: INFO: stdout: "true"
    Jan 13 10:36:01.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-db92z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 13 10:36:01.965: INFO: stderr: ""
    Jan 13 10:36:01.965: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 13 10:36:01.965: INFO: validating pod update-demo-nautilus-db92z
    Jan 13 10:36:01.977: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 13 10:36:01.977: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 13 10:36:01.977: INFO: update-demo-nautilus-db92z is verified up and running
    Jan 13 10:36:01.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 13 10:36:02.306: INFO: stderr: ""
    Jan 13 10:36:02.306: INFO: stdout: "true"
    Jan 13 10:36:02.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods update-demo-nautilus-ngfrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 13 10:36:02.683: INFO: stderr: ""
    Jan 13 10:36:02.683: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 13 10:36:02.683: INFO: validating pod update-demo-nautilus-ngfrd
    Jan 13 10:36:02.696: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 13 10:36:02.696: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 13 10:36:02.696: INFO: update-demo-nautilus-ngfrd is verified up and running
    STEP: using delete to clean up resources 01/13/23 10:36:02.696
    Jan 13 10:36:02.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 delete --grace-period=0 --force -f -'
    Jan 13 10:36:03.462: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:36:03.462: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 13 10:36:03.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get rc,svc -l name=update-demo --no-headers'
    Jan 13 10:36:03.976: INFO: stderr: "No resources found in kubectl-1936 namespace.\n"
    Jan 13 10:36:03.976: INFO: stdout: ""
    Jan 13 10:36:03.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1936 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 13 10:36:04.280: INFO: stderr: ""
    Jan 13 10:36:04.280: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:36:04.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1936" for this suite. 01/13/23 10:36:04.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:36:04.326
Jan 13 10:36:04.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:36:04.329
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:36:04.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:36:04.376
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 13 10:36:04.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/13/23 10:36:08.607
Jan 13 10:36:08.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 create -f -'
Jan 13 10:36:11.441: INFO: stderr: ""
Jan 13 10:36:11.441: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 13 10:36:11.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 delete e2e-test-crd-publish-openapi-7038-crds test-cr'
Jan 13 10:36:11.733: INFO: stderr: ""
Jan 13 10:36:11.733: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 13 10:36:11.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 apply -f -'
Jan 13 10:36:12.555: INFO: stderr: ""
Jan 13 10:36:12.555: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 13 10:36:12.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 delete e2e-test-crd-publish-openapi-7038-crds test-cr'
Jan 13 10:36:13.137: INFO: stderr: ""
Jan 13 10:36:13.137: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/13/23 10:36:13.137
Jan 13 10:36:13.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 explain e2e-test-crd-publish-openapi-7038-crds'
Jan 13 10:36:13.833: INFO: stderr: ""
Jan 13 10:36:13.833: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7038-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:36:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6832" for this suite. 01/13/23 10:36:16.561
------------------------------
• [SLOW TEST] [12.255 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:36:04.326
    Jan 13 10:36:04.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:36:04.329
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:36:04.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:36:04.376
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 13 10:36:04.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/13/23 10:36:08.607
    Jan 13 10:36:08.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 create -f -'
    Jan 13 10:36:11.441: INFO: stderr: ""
    Jan 13 10:36:11.441: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 13 10:36:11.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 delete e2e-test-crd-publish-openapi-7038-crds test-cr'
    Jan 13 10:36:11.733: INFO: stderr: ""
    Jan 13 10:36:11.733: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 13 10:36:11.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 apply -f -'
    Jan 13 10:36:12.555: INFO: stderr: ""
    Jan 13 10:36:12.555: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 13 10:36:12.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 --namespace=crd-publish-openapi-6832 delete e2e-test-crd-publish-openapi-7038-crds test-cr'
    Jan 13 10:36:13.137: INFO: stderr: ""
    Jan 13 10:36:13.137: INFO: stdout: "e2e-test-crd-publish-openapi-7038-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/13/23 10:36:13.137
    Jan 13 10:36:13.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-6832 explain e2e-test-crd-publish-openapi-7038-crds'
    Jan 13 10:36:13.833: INFO: stderr: ""
    Jan 13 10:36:13.833: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7038-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:36:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6832" for this suite. 01/13/23 10:36:16.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:36:16.591
Jan 13 10:36:16.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 10:36:16.593
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:36:16.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:36:16.67
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/13/23 10:36:16.695
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
 01/13/23 10:36:16.712
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
 01/13/23 10:36:16.713
STEP: creating a pod to probe DNS 01/13/23 10:36:16.713
STEP: submitting the pod to kubernetes 01/13/23 10:36:16.713
Jan 13 10:36:16.738: INFO: Waiting up to 15m0s for pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664" in namespace "dns-15" to be "running"
Jan 13 10:36:16.744: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664": Phase="Pending", Reason="", readiness=false. Elapsed: 6.566339ms
Jan 13 10:36:18.753: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015233656s
Jan 13 10:36:20.753: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664": Phase="Running", Reason="", readiness=true. Elapsed: 4.015236084s
Jan 13 10:36:20.753: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664" satisfied condition "running"
STEP: retrieving the pod 01/13/23 10:36:20.753
STEP: looking for the results for each expected name from probers 01/13/23 10:36:20.76
Jan 13 10:36:20.783: INFO: DNS probes using dns-test-0b955266-6f18-4081-9c26-0f49afbff664 succeeded

STEP: deleting the pod 01/13/23 10:36:20.783
STEP: changing the externalName to bar.example.com 01/13/23 10:36:20.81
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
 01/13/23 10:36:20.83
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
 01/13/23 10:36:20.831
STEP: creating a second pod to probe DNS 01/13/23 10:36:20.831
STEP: submitting the pod to kubernetes 01/13/23 10:36:20.831
Jan 13 10:36:20.855: INFO: Waiting up to 15m0s for pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d" in namespace "dns-15" to be "running"
Jan 13 10:36:20.866: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.858202ms
Jan 13 10:36:22.880: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024649926s
Jan 13 10:36:24.877: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d": Phase="Running", Reason="", readiness=true. Elapsed: 4.0223615s
Jan 13 10:36:24.877: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d" satisfied condition "running"
STEP: retrieving the pod 01/13/23 10:36:24.878
STEP: looking for the results for each expected name from probers 01/13/23 10:36:24.893
Jan 13 10:36:24.911: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 10:36:24.930: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local]

Jan 13 10:36:29.938: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 10:36:29.946: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local]

Jan 13 10:36:34.945: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 10:36:34.952: INFO: File jessie_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 10:36:34.952: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local jessie_udp@dns-test-service-3.dns-15.svc.cluster.local]

Jan 13 10:36:39.941: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 10:36:39.952: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local]

Jan 13 10:36:44.957: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 10:36:44.990: INFO: File jessie_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 10:36:44.991: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local jessie_udp@dns-test-service-3.dns-15.svc.cluster.local]

Jan 13 10:36:49.948: INFO: DNS probes using dns-test-c00e2659-8645-4651-b48d-f7035f03a55d succeeded

STEP: deleting the pod 01/13/23 10:36:49.948
STEP: changing the service to type=ClusterIP 01/13/23 10:36:49.98
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
 01/13/23 10:36:50.021
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
 01/13/23 10:36:50.021
STEP: creating a third pod to probe DNS 01/13/23 10:36:50.021
STEP: submitting the pod to kubernetes 01/13/23 10:36:50.027
Jan 13 10:36:50.061: INFO: Waiting up to 15m0s for pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b" in namespace "dns-15" to be "running"
Jan 13 10:36:50.066: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.722678ms
Jan 13 10:36:52.076: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014868387s
Jan 13 10:36:54.092: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b": Phase="Running", Reason="", readiness=true. Elapsed: 4.03095705s
Jan 13 10:36:54.092: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b" satisfied condition "running"
STEP: retrieving the pod 01/13/23 10:36:54.092
STEP: looking for the results for each expected name from probers 01/13/23 10:36:54.106
Jan 13 10:36:54.190: INFO: DNS probes using dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b succeeded

STEP: deleting the pod 01/13/23 10:36:54.19
STEP: deleting the test externalName service 01/13/23 10:36:54.229
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 10:36:54.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-15" for this suite. 01/13/23 10:36:54.269
------------------------------
• [SLOW TEST] [37.700 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:36:16.591
    Jan 13 10:36:16.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 10:36:16.593
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:36:16.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:36:16.67
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/13/23 10:36:16.695
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
     01/13/23 10:36:16.712
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
     01/13/23 10:36:16.713
    STEP: creating a pod to probe DNS 01/13/23 10:36:16.713
    STEP: submitting the pod to kubernetes 01/13/23 10:36:16.713
    Jan 13 10:36:16.738: INFO: Waiting up to 15m0s for pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664" in namespace "dns-15" to be "running"
    Jan 13 10:36:16.744: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664": Phase="Pending", Reason="", readiness=false. Elapsed: 6.566339ms
    Jan 13 10:36:18.753: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015233656s
    Jan 13 10:36:20.753: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664": Phase="Running", Reason="", readiness=true. Elapsed: 4.015236084s
    Jan 13 10:36:20.753: INFO: Pod "dns-test-0b955266-6f18-4081-9c26-0f49afbff664" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 10:36:20.753
    STEP: looking for the results for each expected name from probers 01/13/23 10:36:20.76
    Jan 13 10:36:20.783: INFO: DNS probes using dns-test-0b955266-6f18-4081-9c26-0f49afbff664 succeeded

    STEP: deleting the pod 01/13/23 10:36:20.783
    STEP: changing the externalName to bar.example.com 01/13/23 10:36:20.81
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
     01/13/23 10:36:20.83
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
     01/13/23 10:36:20.831
    STEP: creating a second pod to probe DNS 01/13/23 10:36:20.831
    STEP: submitting the pod to kubernetes 01/13/23 10:36:20.831
    Jan 13 10:36:20.855: INFO: Waiting up to 15m0s for pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d" in namespace "dns-15" to be "running"
    Jan 13 10:36:20.866: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.858202ms
    Jan 13 10:36:22.880: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024649926s
    Jan 13 10:36:24.877: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d": Phase="Running", Reason="", readiness=true. Elapsed: 4.0223615s
    Jan 13 10:36:24.877: INFO: Pod "dns-test-c00e2659-8645-4651-b48d-f7035f03a55d" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 10:36:24.878
    STEP: looking for the results for each expected name from probers 01/13/23 10:36:24.893
    Jan 13 10:36:24.911: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 13 10:36:24.930: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local]

    Jan 13 10:36:29.938: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 13 10:36:29.946: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local]

    Jan 13 10:36:34.945: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 13 10:36:34.952: INFO: File jessie_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 13 10:36:34.952: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local jessie_udp@dns-test-service-3.dns-15.svc.cluster.local]

    Jan 13 10:36:39.941: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 13 10:36:39.952: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local]

    Jan 13 10:36:44.957: INFO: File wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 13 10:36:44.990: INFO: File jessie_udp@dns-test-service-3.dns-15.svc.cluster.local from pod  dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 13 10:36:44.991: INFO: Lookups using dns-15/dns-test-c00e2659-8645-4651-b48d-f7035f03a55d failed for: [wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local jessie_udp@dns-test-service-3.dns-15.svc.cluster.local]

    Jan 13 10:36:49.948: INFO: DNS probes using dns-test-c00e2659-8645-4651-b48d-f7035f03a55d succeeded

    STEP: deleting the pod 01/13/23 10:36:49.948
    STEP: changing the service to type=ClusterIP 01/13/23 10:36:49.98
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
     01/13/23 10:36:50.021
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-15.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-15.svc.cluster.local; sleep 1; done
     01/13/23 10:36:50.021
    STEP: creating a third pod to probe DNS 01/13/23 10:36:50.021
    STEP: submitting the pod to kubernetes 01/13/23 10:36:50.027
    Jan 13 10:36:50.061: INFO: Waiting up to 15m0s for pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b" in namespace "dns-15" to be "running"
    Jan 13 10:36:50.066: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.722678ms
    Jan 13 10:36:52.076: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014868387s
    Jan 13 10:36:54.092: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b": Phase="Running", Reason="", readiness=true. Elapsed: 4.03095705s
    Jan 13 10:36:54.092: INFO: Pod "dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 10:36:54.092
    STEP: looking for the results for each expected name from probers 01/13/23 10:36:54.106
    Jan 13 10:36:54.190: INFO: DNS probes using dns-test-d0a7f0cb-5000-412c-a1a4-ec2aac13d26b succeeded

    STEP: deleting the pod 01/13/23 10:36:54.19
    STEP: deleting the test externalName service 01/13/23 10:36:54.229
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:36:54.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-15" for this suite. 01/13/23 10:36:54.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:36:54.295
Jan 13 10:36:54.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:36:54.298
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:36:54.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:36:54.335
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/13/23 10:36:54.344
Jan 13 10:36:54.398: INFO: Waiting up to 5m0s for pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651" in namespace "emptydir-6689" to be "Succeeded or Failed"
Jan 13 10:36:54.434: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Pending", Reason="", readiness=false. Elapsed: 36.234746ms
Jan 13 10:36:56.449: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051269971s
Jan 13 10:36:58.442: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043837228s
Jan 13 10:37:00.466: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068551975s
STEP: Saw pod success 01/13/23 10:37:00.466
Jan 13 10:37:00.467: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651" satisfied condition "Succeeded or Failed"
Jan 13 10:37:00.475: INFO: Trying to get logs from node 10.10.102.31-node pod pod-b140613d-312f-4d6c-af4e-7bb01583e651 container test-container: <nil>
STEP: delete the pod 01/13/23 10:37:00.507
Jan 13 10:37:00.531: INFO: Waiting for pod pod-b140613d-312f-4d6c-af4e-7bb01583e651 to disappear
Jan 13 10:37:00.561: INFO: Pod pod-b140613d-312f-4d6c-af4e-7bb01583e651 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:37:00.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6689" for this suite. 01/13/23 10:37:00.576
------------------------------
• [SLOW TEST] [6.332 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:36:54.295
    Jan 13 10:36:54.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:36:54.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:36:54.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:36:54.335
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/13/23 10:36:54.344
    Jan 13 10:36:54.398: INFO: Waiting up to 5m0s for pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651" in namespace "emptydir-6689" to be "Succeeded or Failed"
    Jan 13 10:36:54.434: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Pending", Reason="", readiness=false. Elapsed: 36.234746ms
    Jan 13 10:36:56.449: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051269971s
    Jan 13 10:36:58.442: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043837228s
    Jan 13 10:37:00.466: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068551975s
    STEP: Saw pod success 01/13/23 10:37:00.466
    Jan 13 10:37:00.467: INFO: Pod "pod-b140613d-312f-4d6c-af4e-7bb01583e651" satisfied condition "Succeeded or Failed"
    Jan 13 10:37:00.475: INFO: Trying to get logs from node 10.10.102.31-node pod pod-b140613d-312f-4d6c-af4e-7bb01583e651 container test-container: <nil>
    STEP: delete the pod 01/13/23 10:37:00.507
    Jan 13 10:37:00.531: INFO: Waiting for pod pod-b140613d-312f-4d6c-af4e-7bb01583e651 to disappear
    Jan 13 10:37:00.561: INFO: Pod pod-b140613d-312f-4d6c-af4e-7bb01583e651 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:37:00.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6689" for this suite. 01/13/23 10:37:00.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:37:00.629
Jan 13 10:37:00.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-runtime 01/13/23 10:37:00.636
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:37:00.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:37:00.677
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/13/23 10:37:00.688
STEP: wait for the container to reach Succeeded 01/13/23 10:37:00.718
STEP: get the container status 01/13/23 10:37:07.826
STEP: the container should be terminated 01/13/23 10:37:07.832
STEP: the termination message should be set 01/13/23 10:37:07.832
Jan 13 10:37:07.832: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/13/23 10:37:07.832
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 13 10:37:07.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-248" for this suite. 01/13/23 10:37:07.868
------------------------------
• [SLOW TEST] [7.253 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:37:00.629
    Jan 13 10:37:00.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-runtime 01/13/23 10:37:00.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:37:00.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:37:00.677
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/13/23 10:37:00.688
    STEP: wait for the container to reach Succeeded 01/13/23 10:37:00.718
    STEP: get the container status 01/13/23 10:37:07.826
    STEP: the container should be terminated 01/13/23 10:37:07.832
    STEP: the termination message should be set 01/13/23 10:37:07.832
    Jan 13 10:37:07.832: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/13/23 10:37:07.832
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:37:07.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-248" for this suite. 01/13/23 10:37:07.868
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:37:07.883
Jan 13 10:37:07.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:37:07.886
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:37:07.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:37:07.947
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/13/23 10:37:07.961
STEP: watching for the Service to be added 01/13/23 10:37:07.979
Jan 13 10:37:07.983: INFO: Found Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 13 10:37:07.983: INFO: Service test-service-lfkbr created
STEP: Getting /status 01/13/23 10:37:07.983
Jan 13 10:37:07.996: INFO: Service test-service-lfkbr has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/13/23 10:37:07.996
STEP: watching for the Service to be patched 01/13/23 10:37:08.024
Jan 13 10:37:08.028: INFO: observed Service test-service-lfkbr in namespace services-8268 with annotations: map[] & LoadBalancer: {[]}
Jan 13 10:37:08.028: INFO: Found Service test-service-lfkbr in namespace services-8268 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 13 10:37:08.028: INFO: Service test-service-lfkbr has service status patched
STEP: updating the ServiceStatus 01/13/23 10:37:08.028
Jan 13 10:37:08.059: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/13/23 10:37:08.059
Jan 13 10:37:08.064: INFO: Observed Service test-service-lfkbr in namespace services-8268 with annotations: map[] & Conditions: {[]}
Jan 13 10:37:08.064: INFO: Observed event: &Service{ObjectMeta:{test-service-lfkbr  services-8268  c981e3c6-006a-4364-8522-1c876e095fd2 597161 0 2023-01-13 10:37:07 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-13 10:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-13 10:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.103.17.212,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.103.17.212],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 13 10:37:08.064: INFO: Found Service test-service-lfkbr in namespace services-8268 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 13 10:37:08.064: INFO: Service test-service-lfkbr has service status updated
STEP: patching the service 01/13/23 10:37:08.064
STEP: watching for the Service to be patched 01/13/23 10:37:08.08
Jan 13 10:37:08.083: INFO: observed Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true]
Jan 13 10:37:08.083: INFO: observed Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true]
Jan 13 10:37:08.083: INFO: observed Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true]
Jan 13 10:37:08.083: INFO: Found Service test-service-lfkbr in namespace services-8268 with labels: map[test-service:patched test-service-static:true]
Jan 13 10:37:08.083: INFO: Service test-service-lfkbr patched
STEP: deleting the service 01/13/23 10:37:08.083
STEP: watching for the Service to be deleted 01/13/23 10:37:08.105
Jan 13 10:37:08.109: INFO: Observed event: ADDED
Jan 13 10:37:08.109: INFO: Observed event: MODIFIED
Jan 13 10:37:08.109: INFO: Observed event: MODIFIED
Jan 13 10:37:08.109: INFO: Observed event: MODIFIED
Jan 13 10:37:08.109: INFO: Found Service test-service-lfkbr in namespace services-8268 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 13 10:37:08.109: INFO: Service test-service-lfkbr deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:37:08.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8268" for this suite. 01/13/23 10:37:08.118
------------------------------
• [0.245 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:37:07.883
    Jan 13 10:37:07.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:37:07.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:37:07.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:37:07.947
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/13/23 10:37:07.961
    STEP: watching for the Service to be added 01/13/23 10:37:07.979
    Jan 13 10:37:07.983: INFO: Found Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 13 10:37:07.983: INFO: Service test-service-lfkbr created
    STEP: Getting /status 01/13/23 10:37:07.983
    Jan 13 10:37:07.996: INFO: Service test-service-lfkbr has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/13/23 10:37:07.996
    STEP: watching for the Service to be patched 01/13/23 10:37:08.024
    Jan 13 10:37:08.028: INFO: observed Service test-service-lfkbr in namespace services-8268 with annotations: map[] & LoadBalancer: {[]}
    Jan 13 10:37:08.028: INFO: Found Service test-service-lfkbr in namespace services-8268 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 13 10:37:08.028: INFO: Service test-service-lfkbr has service status patched
    STEP: updating the ServiceStatus 01/13/23 10:37:08.028
    Jan 13 10:37:08.059: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/13/23 10:37:08.059
    Jan 13 10:37:08.064: INFO: Observed Service test-service-lfkbr in namespace services-8268 with annotations: map[] & Conditions: {[]}
    Jan 13 10:37:08.064: INFO: Observed event: &Service{ObjectMeta:{test-service-lfkbr  services-8268  c981e3c6-006a-4364-8522-1c876e095fd2 597161 0 2023-01-13 10:37:07 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-13 10:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-13 10:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.103.17.212,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.103.17.212],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 13 10:37:08.064: INFO: Found Service test-service-lfkbr in namespace services-8268 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 13 10:37:08.064: INFO: Service test-service-lfkbr has service status updated
    STEP: patching the service 01/13/23 10:37:08.064
    STEP: watching for the Service to be patched 01/13/23 10:37:08.08
    Jan 13 10:37:08.083: INFO: observed Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true]
    Jan 13 10:37:08.083: INFO: observed Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true]
    Jan 13 10:37:08.083: INFO: observed Service test-service-lfkbr in namespace services-8268 with labels: map[test-service-static:true]
    Jan 13 10:37:08.083: INFO: Found Service test-service-lfkbr in namespace services-8268 with labels: map[test-service:patched test-service-static:true]
    Jan 13 10:37:08.083: INFO: Service test-service-lfkbr patched
    STEP: deleting the service 01/13/23 10:37:08.083
    STEP: watching for the Service to be deleted 01/13/23 10:37:08.105
    Jan 13 10:37:08.109: INFO: Observed event: ADDED
    Jan 13 10:37:08.109: INFO: Observed event: MODIFIED
    Jan 13 10:37:08.109: INFO: Observed event: MODIFIED
    Jan 13 10:37:08.109: INFO: Observed event: MODIFIED
    Jan 13 10:37:08.109: INFO: Found Service test-service-lfkbr in namespace services-8268 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 13 10:37:08.109: INFO: Service test-service-lfkbr deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:37:08.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8268" for this suite. 01/13/23 10:37:08.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:37:08.13
Jan 13 10:37:08.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 10:37:08.135
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:37:08.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:37:08.164
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8123 01/13/23 10:37:08.171
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/13/23 10:37:08.187
Jan 13 10:37:08.218: INFO: Found 0 stateful pods, waiting for 3
Jan 13 10:37:18.229: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:37:18.229: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:37:18.229: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 10:37:18.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 10:37:18.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 10:37:18.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 10:37:18.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/13/23 10:37:28.703
Jan 13 10:37:28.735: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/13/23 10:37:28.735
STEP: Updating Pods in reverse ordinal order 01/13/23 10:37:38.77
Jan 13 10:37:38.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 10:37:39.243: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 10:37:39.243: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 10:37:39.243: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/13/23 10:37:59.294
Jan 13 10:37:59.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 10:37:59.757: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 10:37:59.757: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 10:37:59.757: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 10:38:09.850: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/13/23 10:38:19.927
Jan 13 10:38:19.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 10:38:20.435: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 10:38:20.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 10:38:20.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 10:38:40.513: INFO: Deleting all statefulset in ns statefulset-8123
Jan 13 10:38:40.544: INFO: Scaling statefulset ss2 to 0
Jan 13 10:38:50.641: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 10:38:50.647: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:38:50.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8123" for this suite. 01/13/23 10:38:50.694
------------------------------
• [SLOW TEST] [102.581 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:37:08.13
    Jan 13 10:37:08.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 10:37:08.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:37:08.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:37:08.164
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8123 01/13/23 10:37:08.171
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/13/23 10:37:08.187
    Jan 13 10:37:08.218: INFO: Found 0 stateful pods, waiting for 3
    Jan 13 10:37:18.229: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:37:18.229: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:37:18.229: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 13 10:37:18.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 10:37:18.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 10:37:18.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 10:37:18.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/13/23 10:37:28.703
    Jan 13 10:37:28.735: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/13/23 10:37:28.735
    STEP: Updating Pods in reverse ordinal order 01/13/23 10:37:38.77
    Jan 13 10:37:38.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 10:37:39.243: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 13 10:37:39.243: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 10:37:39.243: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/13/23 10:37:59.294
    Jan 13 10:37:59.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 13 10:37:59.757: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 13 10:37:59.757: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 13 10:37:59.757: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 13 10:38:09.850: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/13/23 10:38:19.927
    Jan 13 10:38:19.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=statefulset-8123 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 13 10:38:20.435: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 13 10:38:20.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 13 10:38:20.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 10:38:40.513: INFO: Deleting all statefulset in ns statefulset-8123
    Jan 13 10:38:40.544: INFO: Scaling statefulset ss2 to 0
    Jan 13 10:38:50.641: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 10:38:50.647: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:38:50.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8123" for this suite. 01/13/23 10:38:50.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:38:50.719
Jan 13 10:38:50.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:38:50.722
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:38:50.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:38:50.761
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:38:50.768
Jan 13 10:38:50.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d" in namespace "projected-6333" to be "Succeeded or Failed"
Jan 13 10:38:50.790: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888863ms
Jan 13 10:38:52.797: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011977966s
Jan 13 10:38:54.800: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014977262s
Jan 13 10:38:56.798: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013158827s
STEP: Saw pod success 01/13/23 10:38:56.798
Jan 13 10:38:56.798: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d" satisfied condition "Succeeded or Failed"
Jan 13 10:38:56.811: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d container client-container: <nil>
STEP: delete the pod 01/13/23 10:38:56.856
Jan 13 10:38:56.878: INFO: Waiting for pod downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d to disappear
Jan 13 10:38:56.883: INFO: Pod downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:38:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6333" for this suite. 01/13/23 10:38:56.896
------------------------------
• [SLOW TEST] [6.207 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:38:50.719
    Jan 13 10:38:50.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:38:50.722
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:38:50.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:38:50.761
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:38:50.768
    Jan 13 10:38:50.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d" in namespace "projected-6333" to be "Succeeded or Failed"
    Jan 13 10:38:50.790: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888863ms
    Jan 13 10:38:52.797: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011977966s
    Jan 13 10:38:54.800: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014977262s
    Jan 13 10:38:56.798: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013158827s
    STEP: Saw pod success 01/13/23 10:38:56.798
    Jan 13 10:38:56.798: INFO: Pod "downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d" satisfied condition "Succeeded or Failed"
    Jan 13 10:38:56.811: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d container client-container: <nil>
    STEP: delete the pod 01/13/23 10:38:56.856
    Jan 13 10:38:56.878: INFO: Waiting for pod downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d to disappear
    Jan 13 10:38:56.883: INFO: Pod downwardapi-volume-ca249378-4773-444a-83a3-5b67c0479e5d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:38:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6333" for this suite. 01/13/23 10:38:56.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:38:56.927
Jan 13 10:38:56.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename hostport 01/13/23 10:38:56.931
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:38:56.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:38:56.976
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/13/23 10:38:56.99
Jan 13 10:38:57.053: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5949" to be "running and ready"
Jan 13 10:38:57.058: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.843568ms
Jan 13 10:38:57.059: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:38:59.070: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017230922s
Jan 13 10:38:59.070: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:01.072: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.019798267s
Jan 13 10:39:01.073: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 13 10:39:01.073: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.10.102.31 on the node which pod1 resides and expect scheduled 01/13/23 10:39:01.073
Jan 13 10:39:01.109: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5949" to be "running and ready"
Jan 13 10:39:01.136: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 27.417502ms
Jan 13 10:39:01.136: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:03.143: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034414788s
Jan 13 10:39:03.143: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:05.144: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034797137s
Jan 13 10:39:05.144: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 13 10:39:05.144: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.10.102.31 but use UDP protocol on the node which pod2 resides 01/13/23 10:39:05.144
Jan 13 10:39:05.155: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5949" to be "running and ready"
Jan 13 10:39:05.162: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.546586ms
Jan 13 10:39:05.162: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:07.170: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01473103s
Jan 13 10:39:07.170: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:09.175: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.019590171s
Jan 13 10:39:09.175: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 13 10:39:09.175: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 13 10:39:09.217: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5949" to be "running and ready"
Jan 13 10:39:09.225: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.533069ms
Jan 13 10:39:09.225: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:11.239: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021926261s
Jan 13 10:39:11.239: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:13.232: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 4.014998955s
Jan 13 10:39:13.232: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 13 10:39:13.232: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/13/23 10:39:13.236
Jan 13 10:39:13.236: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.10.102.31 http://127.0.0.1:54323/hostname] Namespace:hostport-5949 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:39:13.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:39:13.238: INFO: ExecWithOptions: Clientset creation
Jan 13 10:39:13.238: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5949/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.10.102.31+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.102.31, port: 54323 01/13/23 10:39:13.455
Jan 13 10:39:13.455: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.10.102.31:54323/hostname] Namespace:hostport-5949 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:39:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:39:13.457: INFO: ExecWithOptions: Clientset creation
Jan 13 10:39:13.457: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5949/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.10.102.31%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.102.31, port: 54323 UDP 01/13/23 10:39:13.663
Jan 13 10:39:13.664: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.10.102.31 54323] Namespace:hostport-5949 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:39:13.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:39:13.665: INFO: ExecWithOptions: Clientset creation
Jan 13 10:39:13.665: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5949/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.10.102.31+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 13 10:39:18.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-5949" for this suite. 01/13/23 10:39:18.894
------------------------------
• [SLOW TEST] [21.975 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:38:56.927
    Jan 13 10:38:56.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename hostport 01/13/23 10:38:56.931
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:38:56.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:38:56.976
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/13/23 10:38:56.99
    Jan 13 10:38:57.053: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5949" to be "running and ready"
    Jan 13 10:38:57.058: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.843568ms
    Jan 13 10:38:57.059: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:38:59.070: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017230922s
    Jan 13 10:38:59.070: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:01.072: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.019798267s
    Jan 13 10:39:01.073: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 13 10:39:01.073: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.10.102.31 on the node which pod1 resides and expect scheduled 01/13/23 10:39:01.073
    Jan 13 10:39:01.109: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5949" to be "running and ready"
    Jan 13 10:39:01.136: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 27.417502ms
    Jan 13 10:39:01.136: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:03.143: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034414788s
    Jan 13 10:39:03.143: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:05.144: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034797137s
    Jan 13 10:39:05.144: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 13 10:39:05.144: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.10.102.31 but use UDP protocol on the node which pod2 resides 01/13/23 10:39:05.144
    Jan 13 10:39:05.155: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5949" to be "running and ready"
    Jan 13 10:39:05.162: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.546586ms
    Jan 13 10:39:05.162: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:07.170: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01473103s
    Jan 13 10:39:07.170: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:09.175: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.019590171s
    Jan 13 10:39:09.175: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 13 10:39:09.175: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 13 10:39:09.217: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5949" to be "running and ready"
    Jan 13 10:39:09.225: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.533069ms
    Jan 13 10:39:09.225: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:11.239: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021926261s
    Jan 13 10:39:11.239: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:13.232: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 4.014998955s
    Jan 13 10:39:13.232: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 13 10:39:13.232: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/13/23 10:39:13.236
    Jan 13 10:39:13.236: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.10.102.31 http://127.0.0.1:54323/hostname] Namespace:hostport-5949 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:39:13.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:39:13.238: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:39:13.238: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5949/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.10.102.31+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.102.31, port: 54323 01/13/23 10:39:13.455
    Jan 13 10:39:13.455: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.10.102.31:54323/hostname] Namespace:hostport-5949 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:39:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:39:13.457: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:39:13.457: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5949/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.10.102.31%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.102.31, port: 54323 UDP 01/13/23 10:39:13.663
    Jan 13 10:39:13.664: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.10.102.31 54323] Namespace:hostport-5949 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:39:13.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:39:13.665: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:39:13.665: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5949/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.10.102.31+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:39:18.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-5949" for this suite. 01/13/23 10:39:18.894
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:39:18.904
Jan 13 10:39:18.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:39:18.907
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:18.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:18.928
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/13/23 10:39:18.935
Jan 13 10:39:18.952: INFO: Waiting up to 5m0s for pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90" in namespace "emptydir-2105" to be "Succeeded or Failed"
Jan 13 10:39:18.960: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698943ms
Jan 13 10:39:20.967: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015407431s
Jan 13 10:39:22.969: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017301343s
Jan 13 10:39:24.968: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016426837s
STEP: Saw pod success 01/13/23 10:39:24.968
Jan 13 10:39:24.969: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90" satisfied condition "Succeeded or Failed"
Jan 13 10:39:24.975: INFO: Trying to get logs from node 10.10.102.31-node pod pod-243823e1-74a2-4928-8e5b-ac253e282f90 container test-container: <nil>
STEP: delete the pod 01/13/23 10:39:25.04
Jan 13 10:39:25.062: INFO: Waiting for pod pod-243823e1-74a2-4928-8e5b-ac253e282f90 to disappear
Jan 13 10:39:25.067: INFO: Pod pod-243823e1-74a2-4928-8e5b-ac253e282f90 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:39:25.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2105" for this suite. 01/13/23 10:39:25.09
------------------------------
• [SLOW TEST] [6.212 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:39:18.904
    Jan 13 10:39:18.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:39:18.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:18.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:18.928
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/13/23 10:39:18.935
    Jan 13 10:39:18.952: INFO: Waiting up to 5m0s for pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90" in namespace "emptydir-2105" to be "Succeeded or Failed"
    Jan 13 10:39:18.960: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698943ms
    Jan 13 10:39:20.967: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015407431s
    Jan 13 10:39:22.969: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017301343s
    Jan 13 10:39:24.968: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016426837s
    STEP: Saw pod success 01/13/23 10:39:24.968
    Jan 13 10:39:24.969: INFO: Pod "pod-243823e1-74a2-4928-8e5b-ac253e282f90" satisfied condition "Succeeded or Failed"
    Jan 13 10:39:24.975: INFO: Trying to get logs from node 10.10.102.31-node pod pod-243823e1-74a2-4928-8e5b-ac253e282f90 container test-container: <nil>
    STEP: delete the pod 01/13/23 10:39:25.04
    Jan 13 10:39:25.062: INFO: Waiting for pod pod-243823e1-74a2-4928-8e5b-ac253e282f90 to disappear
    Jan 13 10:39:25.067: INFO: Pod pod-243823e1-74a2-4928-8e5b-ac253e282f90 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:39:25.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2105" for this suite. 01/13/23 10:39:25.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:39:25.124
Jan 13 10:39:25.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pod-network-test 01/13/23 10:39:25.126
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:25.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:25.173
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7596 01/13/23 10:39:25.186
STEP: creating a selector 01/13/23 10:39:25.186
STEP: Creating the service pods in kubernetes 01/13/23 10:39:25.186
Jan 13 10:39:25.186: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 10:39:25.264: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7596" to be "running and ready"
Jan 13 10:39:25.281: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.271042ms
Jan 13 10:39:25.281: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:27.293: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029167362s
Jan 13 10:39:27.293: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:39:29.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025262445s
Jan 13 10:39:29.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:31.290: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026084124s
Jan 13 10:39:31.290: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:33.288: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024572744s
Jan 13 10:39:33.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:35.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.036005734s
Jan 13 10:39:35.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:37.290: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025907139s
Jan 13 10:39:37.290: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:39.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024711215s
Jan 13 10:39:39.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:41.290: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026274044s
Jan 13 10:39:41.290: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:43.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.024682558s
Jan 13 10:39:43.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:45.307: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.043208771s
Jan 13 10:39:45.307: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:39:47.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025541159s
Jan 13 10:39:47.290: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 13 10:39:47.290: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 13 10:39:47.296: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7596" to be "running and ready"
Jan 13 10:39:47.303: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.861795ms
Jan 13 10:39:47.303: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 13 10:39:47.303: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/13/23 10:39:47.309
Jan 13 10:39:47.374: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7596" to be "running"
Jan 13 10:39:47.390: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.885972ms
Jan 13 10:39:49.400: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025949449s
Jan 13 10:39:51.397: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022843467s
Jan 13 10:39:51.397: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 13 10:39:51.402: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7596" to be "running"
Jan 13 10:39:51.409: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.253234ms
Jan 13 10:39:51.409: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 13 10:39:51.415: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 10:39:51.415: INFO: Going to poll 10.244.27.229 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 13 10:39:51.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.27.229:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7596 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:39:51.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:39:51.427: INFO: ExecWithOptions: Clientset creation
Jan 13 10:39:51.427: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7596/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.27.229%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 13 10:39:51.634: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 13 10:39:51.634: INFO: Going to poll 10.244.169.5 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 13 10:39:51.640: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.169.5:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7596 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:39:51.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:39:51.642: INFO: ExecWithOptions: Clientset creation
Jan 13 10:39:51.642: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7596/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.169.5%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 13 10:39:51.934: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 13 10:39:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7596" for this suite. 01/13/23 10:39:51.943
------------------------------
• [SLOW TEST] [26.829 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:39:25.124
    Jan 13 10:39:25.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pod-network-test 01/13/23 10:39:25.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:25.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:25.173
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7596 01/13/23 10:39:25.186
    STEP: creating a selector 01/13/23 10:39:25.186
    STEP: Creating the service pods in kubernetes 01/13/23 10:39:25.186
    Jan 13 10:39:25.186: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 13 10:39:25.264: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7596" to be "running and ready"
    Jan 13 10:39:25.281: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.271042ms
    Jan 13 10:39:25.281: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:27.293: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029167362s
    Jan 13 10:39:27.293: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:39:29.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025262445s
    Jan 13 10:39:29.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:31.290: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026084124s
    Jan 13 10:39:31.290: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:33.288: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024572744s
    Jan 13 10:39:33.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:35.300: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.036005734s
    Jan 13 10:39:35.300: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:37.290: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025907139s
    Jan 13 10:39:37.290: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:39.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024711215s
    Jan 13 10:39:39.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:41.290: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026274044s
    Jan 13 10:39:41.290: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:43.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.024682558s
    Jan 13 10:39:43.289: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:45.307: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.043208771s
    Jan 13 10:39:45.307: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:39:47.289: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025541159s
    Jan 13 10:39:47.290: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 13 10:39:47.290: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 13 10:39:47.296: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7596" to be "running and ready"
    Jan 13 10:39:47.303: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.861795ms
    Jan 13 10:39:47.303: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 13 10:39:47.303: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/13/23 10:39:47.309
    Jan 13 10:39:47.374: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7596" to be "running"
    Jan 13 10:39:47.390: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.885972ms
    Jan 13 10:39:49.400: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025949449s
    Jan 13 10:39:51.397: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022843467s
    Jan 13 10:39:51.397: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 13 10:39:51.402: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7596" to be "running"
    Jan 13 10:39:51.409: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.253234ms
    Jan 13 10:39:51.409: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 13 10:39:51.415: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 13 10:39:51.415: INFO: Going to poll 10.244.27.229 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 13 10:39:51.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.27.229:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7596 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:39:51.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:39:51.427: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:39:51.427: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7596/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.27.229%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 13 10:39:51.634: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 13 10:39:51.634: INFO: Going to poll 10.244.169.5 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 13 10:39:51.640: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.169.5:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7596 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:39:51.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:39:51.642: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:39:51.642: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7596/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.169.5%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 13 10:39:51.934: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:39:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7596" for this suite. 01/13/23 10:39:51.943
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:39:51.955
Jan 13 10:39:51.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename events 01/13/23 10:39:51.958
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:51.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:51.985
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/13/23 10:39:51.992
STEP: listing all events in all namespaces 01/13/23 10:39:51.999
STEP: patching the test event 01/13/23 10:39:52.008
STEP: fetching the test event 01/13/23 10:39:52.018
STEP: updating the test event 01/13/23 10:39:52.022
STEP: getting the test event 01/13/23 10:39:52.043
STEP: deleting the test event 01/13/23 10:39:52.05
STEP: listing all events in all namespaces 01/13/23 10:39:52.06
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 13 10:39:52.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3522" for this suite. 01/13/23 10:39:52.076
------------------------------
• [0.133 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:39:51.955
    Jan 13 10:39:51.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename events 01/13/23 10:39:51.958
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:51.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:51.985
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/13/23 10:39:51.992
    STEP: listing all events in all namespaces 01/13/23 10:39:51.999
    STEP: patching the test event 01/13/23 10:39:52.008
    STEP: fetching the test event 01/13/23 10:39:52.018
    STEP: updating the test event 01/13/23 10:39:52.022
    STEP: getting the test event 01/13/23 10:39:52.043
    STEP: deleting the test event 01/13/23 10:39:52.05
    STEP: listing all events in all namespaces 01/13/23 10:39:52.06
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:39:52.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3522" for this suite. 01/13/23 10:39:52.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:39:52.092
Jan 13 10:39:52.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:39:52.095
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:52.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:52.13
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/13/23 10:39:52.14
Jan 13 10:39:52.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 13 10:39:52.396: INFO: stderr: ""
Jan 13 10:39:52.396: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/13/23 10:39:52.396
Jan 13 10:39:52.396: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 13 10:39:52.397: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8557" to be "running and ready, or succeeded"
Jan 13 10:39:52.408: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.374751ms
Jan 13 10:39:52.408: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.10.102.31-node' to be 'Running' but was 'Pending'
Jan 13 10:39:54.422: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024946741s
Jan 13 10:39:54.422: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.10.102.31-node' to be 'Running' but was 'Pending'
Jan 13 10:39:56.417: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.020443974s
Jan 13 10:39:56.417: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 13 10:39:56.417: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/13/23 10:39:56.417
Jan 13 10:39:56.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator'
Jan 13 10:39:56.695: INFO: stderr: ""
Jan 13 10:39:56.695: INFO: stdout: "I0113 10:39:54.634415       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/zm7 409\nI0113 10:39:54.835097       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/66q4 567\nI0113 10:39:55.034494       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/swzx 351\nI0113 10:39:55.235111       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/fr8 314\nI0113 10:39:55.434611       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/p67 505\nI0113 10:39:55.635203       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/4jp 410\nI0113 10:39:55.834647       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/vh49 340\nI0113 10:39:56.035326       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zzm 431\nI0113 10:39:56.235101       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hz6 358\nI0113 10:39:56.436161       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/c69 502\nI0113 10:39:56.634636       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/rcff 585\n"
STEP: limiting log lines 01/13/23 10:39:56.695
Jan 13 10:39:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --tail=1'
Jan 13 10:39:56.921: INFO: stderr: ""
Jan 13 10:39:56.921: INFO: stdout: "I0113 10:39:56.838317       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/z58 550\n"
Jan 13 10:39:56.921: INFO: got output "I0113 10:39:56.838317       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/z58 550\n"
STEP: limiting log bytes 01/13/23 10:39:56.921
Jan 13 10:39:56.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --limit-bytes=1'
Jan 13 10:39:57.170: INFO: stderr: ""
Jan 13 10:39:57.170: INFO: stdout: "I"
Jan 13 10:39:57.170: INFO: got output "I"
STEP: exposing timestamps 01/13/23 10:39:57.17
Jan 13 10:39:57.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 13 10:39:57.451: INFO: stderr: ""
Jan 13 10:39:57.451: INFO: stdout: "2023-01-13T10:39:57.438917495Z I0113 10:39:57.438168       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/zrrc 216\n"
Jan 13 10:39:57.451: INFO: got output "2023-01-13T10:39:57.438917495Z I0113 10:39:57.438168       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/zrrc 216\n"
STEP: restricting to a time range 01/13/23 10:39:57.451
Jan 13 10:39:59.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --since=1s'
Jan 13 10:40:00.150: INFO: stderr: ""
Jan 13 10:40:00.151: INFO: stdout: "I0113 10:39:59.234568       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/df8b 251\nI0113 10:39:59.435381       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/gzr 557\nI0113 10:39:59.635005       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/ctrw 240\nI0113 10:39:59.835651       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/lbp 489\nI0113 10:40:00.035226       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/hcqv 595\n"
Jan 13 10:40:00.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --since=24h'
Jan 13 10:40:00.378: INFO: stderr: ""
Jan 13 10:40:00.378: INFO: stdout: "I0113 10:39:54.634415       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/zm7 409\nI0113 10:39:54.835097       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/66q4 567\nI0113 10:39:55.034494       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/swzx 351\nI0113 10:39:55.235111       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/fr8 314\nI0113 10:39:55.434611       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/p67 505\nI0113 10:39:55.635203       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/4jp 410\nI0113 10:39:55.834647       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/vh49 340\nI0113 10:39:56.035326       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zzm 431\nI0113 10:39:56.235101       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hz6 358\nI0113 10:39:56.436161       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/c69 502\nI0113 10:39:56.634636       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/rcff 585\nI0113 10:39:56.838317       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/z58 550\nI0113 10:39:57.035060       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/czt 375\nI0113 10:39:57.235670       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/w29m 225\nI0113 10:39:57.438168       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/zrrc 216\nI0113 10:39:57.634939       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/7jxx 309\nI0113 10:39:57.834822       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/stbd 296\nI0113 10:39:58.035354       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bfxs 518\nI0113 10:39:58.235277       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/m26j 456\nI0113 10:39:58.435840       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/mdr 490\nI0113 10:39:58.635447       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/qn7 492\nI0113 10:39:58.835437       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/m7x 598\nI0113 10:39:59.035153       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/cff 538\nI0113 10:39:59.234568       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/df8b 251\nI0113 10:39:59.435381       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/gzr 557\nI0113 10:39:59.635005       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/ctrw 240\nI0113 10:39:59.835651       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/lbp 489\nI0113 10:40:00.035226       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/hcqv 595\nI0113 10:40:00.234666       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/2mpq 348\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 13 10:40:00.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 delete pod logs-generator'
Jan 13 10:40:02.598: INFO: stderr: ""
Jan 13 10:40:02.599: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:40:02.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8557" for this suite. 01/13/23 10:40:02.62
------------------------------
• [SLOW TEST] [10.543 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:39:52.092
    Jan 13 10:39:52.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:39:52.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:39:52.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:39:52.13
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/13/23 10:39:52.14
    Jan 13 10:39:52.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 13 10:39:52.396: INFO: stderr: ""
    Jan 13 10:39:52.396: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/13/23 10:39:52.396
    Jan 13 10:39:52.396: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 13 10:39:52.397: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8557" to be "running and ready, or succeeded"
    Jan 13 10:39:52.408: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.374751ms
    Jan 13 10:39:52.408: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.10.102.31-node' to be 'Running' but was 'Pending'
    Jan 13 10:39:54.422: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024946741s
    Jan 13 10:39:54.422: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.10.102.31-node' to be 'Running' but was 'Pending'
    Jan 13 10:39:56.417: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.020443974s
    Jan 13 10:39:56.417: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 13 10:39:56.417: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/13/23 10:39:56.417
    Jan 13 10:39:56.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator'
    Jan 13 10:39:56.695: INFO: stderr: ""
    Jan 13 10:39:56.695: INFO: stdout: "I0113 10:39:54.634415       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/zm7 409\nI0113 10:39:54.835097       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/66q4 567\nI0113 10:39:55.034494       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/swzx 351\nI0113 10:39:55.235111       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/fr8 314\nI0113 10:39:55.434611       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/p67 505\nI0113 10:39:55.635203       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/4jp 410\nI0113 10:39:55.834647       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/vh49 340\nI0113 10:39:56.035326       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zzm 431\nI0113 10:39:56.235101       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hz6 358\nI0113 10:39:56.436161       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/c69 502\nI0113 10:39:56.634636       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/rcff 585\n"
    STEP: limiting log lines 01/13/23 10:39:56.695
    Jan 13 10:39:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --tail=1'
    Jan 13 10:39:56.921: INFO: stderr: ""
    Jan 13 10:39:56.921: INFO: stdout: "I0113 10:39:56.838317       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/z58 550\n"
    Jan 13 10:39:56.921: INFO: got output "I0113 10:39:56.838317       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/z58 550\n"
    STEP: limiting log bytes 01/13/23 10:39:56.921
    Jan 13 10:39:56.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --limit-bytes=1'
    Jan 13 10:39:57.170: INFO: stderr: ""
    Jan 13 10:39:57.170: INFO: stdout: "I"
    Jan 13 10:39:57.170: INFO: got output "I"
    STEP: exposing timestamps 01/13/23 10:39:57.17
    Jan 13 10:39:57.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 13 10:39:57.451: INFO: stderr: ""
    Jan 13 10:39:57.451: INFO: stdout: "2023-01-13T10:39:57.438917495Z I0113 10:39:57.438168       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/zrrc 216\n"
    Jan 13 10:39:57.451: INFO: got output "2023-01-13T10:39:57.438917495Z I0113 10:39:57.438168       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/zrrc 216\n"
    STEP: restricting to a time range 01/13/23 10:39:57.451
    Jan 13 10:39:59.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --since=1s'
    Jan 13 10:40:00.150: INFO: stderr: ""
    Jan 13 10:40:00.151: INFO: stdout: "I0113 10:39:59.234568       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/df8b 251\nI0113 10:39:59.435381       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/gzr 557\nI0113 10:39:59.635005       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/ctrw 240\nI0113 10:39:59.835651       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/lbp 489\nI0113 10:40:00.035226       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/hcqv 595\n"
    Jan 13 10:40:00.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 logs logs-generator logs-generator --since=24h'
    Jan 13 10:40:00.378: INFO: stderr: ""
    Jan 13 10:40:00.378: INFO: stdout: "I0113 10:39:54.634415       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/zm7 409\nI0113 10:39:54.835097       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/66q4 567\nI0113 10:39:55.034494       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/swzx 351\nI0113 10:39:55.235111       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/fr8 314\nI0113 10:39:55.434611       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/p67 505\nI0113 10:39:55.635203       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/4jp 410\nI0113 10:39:55.834647       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/vh49 340\nI0113 10:39:56.035326       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zzm 431\nI0113 10:39:56.235101       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hz6 358\nI0113 10:39:56.436161       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/c69 502\nI0113 10:39:56.634636       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/rcff 585\nI0113 10:39:56.838317       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/z58 550\nI0113 10:39:57.035060       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/czt 375\nI0113 10:39:57.235670       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/w29m 225\nI0113 10:39:57.438168       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/zrrc 216\nI0113 10:39:57.634939       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/7jxx 309\nI0113 10:39:57.834822       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/stbd 296\nI0113 10:39:58.035354       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bfxs 518\nI0113 10:39:58.235277       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/m26j 456\nI0113 10:39:58.435840       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/mdr 490\nI0113 10:39:58.635447       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/qn7 492\nI0113 10:39:58.835437       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/m7x 598\nI0113 10:39:59.035153       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/cff 538\nI0113 10:39:59.234568       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/df8b 251\nI0113 10:39:59.435381       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/gzr 557\nI0113 10:39:59.635005       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/ctrw 240\nI0113 10:39:59.835651       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/lbp 489\nI0113 10:40:00.035226       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/hcqv 595\nI0113 10:40:00.234666       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/2mpq 348\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 13 10:40:00.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-8557 delete pod logs-generator'
    Jan 13 10:40:02.598: INFO: stderr: ""
    Jan 13 10:40:02.599: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:40:02.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8557" for this suite. 01/13/23 10:40:02.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:40:02.647
Jan 13 10:40:02.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:40:02.65
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:02.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:02.706
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:40:02.718
Jan 13 10:40:02.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3" in namespace "projected-1960" to be "Succeeded or Failed"
Jan 13 10:40:02.749: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311264ms
Jan 13 10:40:04.765: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030481983s
Jan 13 10:40:06.770: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035984906s
Jan 13 10:40:08.759: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024245977s
STEP: Saw pod success 01/13/23 10:40:08.759
Jan 13 10:40:08.759: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3" satisfied condition "Succeeded or Failed"
Jan 13 10:40:08.767: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3 container client-container: <nil>
STEP: delete the pod 01/13/23 10:40:08.791
Jan 13 10:40:08.806: INFO: Waiting for pod downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3 to disappear
Jan 13 10:40:08.811: INFO: Pod downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 13 10:40:08.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1960" for this suite. 01/13/23 10:40:08.818
------------------------------
• [SLOW TEST] [6.180 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:40:02.647
    Jan 13 10:40:02.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:40:02.65
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:02.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:02.706
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:40:02.718
    Jan 13 10:40:02.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3" in namespace "projected-1960" to be "Succeeded or Failed"
    Jan 13 10:40:02.749: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.311264ms
    Jan 13 10:40:04.765: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030481983s
    Jan 13 10:40:06.770: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035984906s
    Jan 13 10:40:08.759: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024245977s
    STEP: Saw pod success 01/13/23 10:40:08.759
    Jan 13 10:40:08.759: INFO: Pod "downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3" satisfied condition "Succeeded or Failed"
    Jan 13 10:40:08.767: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:40:08.791
    Jan 13 10:40:08.806: INFO: Waiting for pod downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3 to disappear
    Jan 13 10:40:08.811: INFO: Pod downwardapi-volume-6baff92d-2391-41d3-87e2-ecc2210562b3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:40:08.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1960" for this suite. 01/13/23 10:40:08.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:40:08.828
Jan 13 10:40:08.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:40:08.83
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:08.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:08.863
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-1f820ace-bba1-40a5-99ba-9c1abc5597c4 01/13/23 10:40:08.87
STEP: Creating a pod to test consume configMaps 01/13/23 10:40:08.878
Jan 13 10:40:08.890: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800" in namespace "projected-9439" to be "Succeeded or Failed"
Jan 13 10:40:08.896: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 5.90808ms
Jan 13 10:40:10.916: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02593523s
Jan 13 10:40:12.905: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014870113s
Jan 13 10:40:14.907: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016646452s
Jan 13 10:40:16.904: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014233733s
STEP: Saw pod success 01/13/23 10:40:16.904
Jan 13 10:40:16.905: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800" satisfied condition "Succeeded or Failed"
Jan 13 10:40:16.913: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:40:16.933
Jan 13 10:40:16.953: INFO: Waiting for pod pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800 to disappear
Jan 13 10:40:16.960: INFO: Pod pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:40:16.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9439" for this suite. 01/13/23 10:40:16.969
------------------------------
• [SLOW TEST] [8.156 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:40:08.828
    Jan 13 10:40:08.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:40:08.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:08.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:08.863
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-1f820ace-bba1-40a5-99ba-9c1abc5597c4 01/13/23 10:40:08.87
    STEP: Creating a pod to test consume configMaps 01/13/23 10:40:08.878
    Jan 13 10:40:08.890: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800" in namespace "projected-9439" to be "Succeeded or Failed"
    Jan 13 10:40:08.896: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 5.90808ms
    Jan 13 10:40:10.916: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02593523s
    Jan 13 10:40:12.905: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014870113s
    Jan 13 10:40:14.907: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016646452s
    Jan 13 10:40:16.904: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014233733s
    STEP: Saw pod success 01/13/23 10:40:16.904
    Jan 13 10:40:16.905: INFO: Pod "pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800" satisfied condition "Succeeded or Failed"
    Jan 13 10:40:16.913: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:40:16.933
    Jan 13 10:40:16.953: INFO: Waiting for pod pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800 to disappear
    Jan 13 10:40:16.960: INFO: Pod pod-projected-configmaps-6c490bf1-5b0a-451f-8974-226c03892800 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:40:16.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9439" for this suite. 01/13/23 10:40:16.969
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:40:16.985
Jan 13 10:40:16.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:40:16.987
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:17.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:17.048
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5505 01/13/23 10:40:17.053
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/13/23 10:40:17.081
STEP: creating service externalsvc in namespace services-5505 01/13/23 10:40:17.081
STEP: creating replication controller externalsvc in namespace services-5505 01/13/23 10:40:17.114
I0113 10:40:17.126047      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5505, replica count: 2
I0113 10:40:20.176608      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/13/23 10:40:20.193
Jan 13 10:40:20.261: INFO: Creating new exec pod
Jan 13 10:40:20.288: INFO: Waiting up to 5m0s for pod "execpodflcsj" in namespace "services-5505" to be "running"
Jan 13 10:40:20.297: INFO: Pod "execpodflcsj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.612909ms
Jan 13 10:40:22.312: INFO: Pod "execpodflcsj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02353839s
Jan 13 10:40:24.304: INFO: Pod "execpodflcsj": Phase="Running", Reason="", readiness=true. Elapsed: 4.016026583s
Jan 13 10:40:24.305: INFO: Pod "execpodflcsj" satisfied condition "running"
Jan 13 10:40:24.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-5505 exec execpodflcsj -- /bin/sh -x -c nslookup clusterip-service.services-5505.svc.cluster.local'
Jan 13 10:40:24.798: INFO: stderr: "+ nslookup clusterip-service.services-5505.svc.cluster.local\n"
Jan 13 10:40:24.798: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5505.svc.cluster.local\tcanonical name = externalsvc.services-5505.svc.cluster.local.\nName:\texternalsvc.services-5505.svc.cluster.local\nAddress: 10.98.47.96\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5505, will wait for the garbage collector to delete the pods 01/13/23 10:40:24.798
Jan 13 10:40:24.872: INFO: Deleting ReplicationController externalsvc took: 14.0964ms
Jan 13 10:40:24.972: INFO: Terminating ReplicationController externalsvc pods took: 100.575743ms
Jan 13 10:40:28.432: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:40:28.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5505" for this suite. 01/13/23 10:40:28.469
------------------------------
• [SLOW TEST] [11.494 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:40:16.985
    Jan 13 10:40:16.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:40:16.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:17.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:17.048
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5505 01/13/23 10:40:17.053
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/13/23 10:40:17.081
    STEP: creating service externalsvc in namespace services-5505 01/13/23 10:40:17.081
    STEP: creating replication controller externalsvc in namespace services-5505 01/13/23 10:40:17.114
    I0113 10:40:17.126047      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5505, replica count: 2
    I0113 10:40:20.176608      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/13/23 10:40:20.193
    Jan 13 10:40:20.261: INFO: Creating new exec pod
    Jan 13 10:40:20.288: INFO: Waiting up to 5m0s for pod "execpodflcsj" in namespace "services-5505" to be "running"
    Jan 13 10:40:20.297: INFO: Pod "execpodflcsj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.612909ms
    Jan 13 10:40:22.312: INFO: Pod "execpodflcsj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02353839s
    Jan 13 10:40:24.304: INFO: Pod "execpodflcsj": Phase="Running", Reason="", readiness=true. Elapsed: 4.016026583s
    Jan 13 10:40:24.305: INFO: Pod "execpodflcsj" satisfied condition "running"
    Jan 13 10:40:24.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=services-5505 exec execpodflcsj -- /bin/sh -x -c nslookup clusterip-service.services-5505.svc.cluster.local'
    Jan 13 10:40:24.798: INFO: stderr: "+ nslookup clusterip-service.services-5505.svc.cluster.local\n"
    Jan 13 10:40:24.798: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5505.svc.cluster.local\tcanonical name = externalsvc.services-5505.svc.cluster.local.\nName:\texternalsvc.services-5505.svc.cluster.local\nAddress: 10.98.47.96\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5505, will wait for the garbage collector to delete the pods 01/13/23 10:40:24.798
    Jan 13 10:40:24.872: INFO: Deleting ReplicationController externalsvc took: 14.0964ms
    Jan 13 10:40:24.972: INFO: Terminating ReplicationController externalsvc pods took: 100.575743ms
    Jan 13 10:40:28.432: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:40:28.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5505" for this suite. 01/13/23 10:40:28.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:40:28.48
Jan 13 10:40:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-watch 01/13/23 10:40:28.483
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:28.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:28.577
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 13 10:40:28.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Creating first CR  01/13/23 10:40:31.259
Jan 13 10:40:31.283: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:31Z]] name:name1 resourceVersion:598399 uid:381462b9-2abc-4788-8ee1-fc95c82de733] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/13/23 10:40:41.287
Jan 13 10:40:41.303: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:41Z]] name:name2 resourceVersion:598442 uid:88f244c5-8c42-4963-96f2-aeeda60c29c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/13/23 10:40:51.306
Jan 13 10:40:51.319: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:51Z]] name:name1 resourceVersion:598458 uid:381462b9-2abc-4788-8ee1-fc95c82de733] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/13/23 10:41:01.319
Jan 13 10:41:01.336: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:41:01Z]] name:name2 resourceVersion:598473 uid:88f244c5-8c42-4963-96f2-aeeda60c29c0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/13/23 10:41:11.336
Jan 13 10:41:11.349: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:51Z]] name:name1 resourceVersion:598489 uid:381462b9-2abc-4788-8ee1-fc95c82de733] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/13/23 10:41:21.352
Jan 13 10:41:21.364: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:41:01Z]] name:name2 resourceVersion:598505 uid:88f244c5-8c42-4963-96f2-aeeda60c29c0] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:41:31.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1601" for this suite. 01/13/23 10:41:31.92
------------------------------
• [SLOW TEST] [63.456 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:40:28.48
    Jan 13 10:40:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-watch 01/13/23 10:40:28.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:40:28.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:40:28.577
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 13 10:40:28.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Creating first CR  01/13/23 10:40:31.259
    Jan 13 10:40:31.283: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:31Z]] name:name1 resourceVersion:598399 uid:381462b9-2abc-4788-8ee1-fc95c82de733] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/13/23 10:40:41.287
    Jan 13 10:40:41.303: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:41Z]] name:name2 resourceVersion:598442 uid:88f244c5-8c42-4963-96f2-aeeda60c29c0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/13/23 10:40:51.306
    Jan 13 10:40:51.319: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:51Z]] name:name1 resourceVersion:598458 uid:381462b9-2abc-4788-8ee1-fc95c82de733] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/13/23 10:41:01.319
    Jan 13 10:41:01.336: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:41:01Z]] name:name2 resourceVersion:598473 uid:88f244c5-8c42-4963-96f2-aeeda60c29c0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/13/23 10:41:11.336
    Jan 13 10:41:11.349: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:40:51Z]] name:name1 resourceVersion:598489 uid:381462b9-2abc-4788-8ee1-fc95c82de733] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/13/23 10:41:21.352
    Jan 13 10:41:21.364: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-13T10:40:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-13T10:41:01Z]] name:name2 resourceVersion:598505 uid:88f244c5-8c42-4963-96f2-aeeda60c29c0] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:41:31.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1601" for this suite. 01/13/23 10:41:31.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:41:31.941
Jan 13 10:41:31.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename configmap 01/13/23 10:41:31.945
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:32.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:32.024
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-b56787af-fb72-4e33-b476-61cff36d1213 01/13/23 10:41:32.045
STEP: Creating the pod 01/13/23 10:41:32.054
Jan 13 10:41:32.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d" in namespace "configmap-253" to be "running"
Jan 13 10:41:32.104: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.794633ms
Jan 13 10:41:34.122: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029982613s
Jan 13 10:41:36.113: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d": Phase="Running", Reason="", readiness=false. Elapsed: 4.020308269s
Jan 13 10:41:36.113: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d" satisfied condition "running"
STEP: Waiting for pod with text data 01/13/23 10:41:36.113
STEP: Waiting for pod with binary data 01/13/23 10:41:36.127
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:41:36.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-253" for this suite. 01/13/23 10:41:36.15
------------------------------
• [4.217 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:41:31.941
    Jan 13 10:41:31.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename configmap 01/13/23 10:41:31.945
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:32.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:32.024
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-b56787af-fb72-4e33-b476-61cff36d1213 01/13/23 10:41:32.045
    STEP: Creating the pod 01/13/23 10:41:32.054
    Jan 13 10:41:32.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d" in namespace "configmap-253" to be "running"
    Jan 13 10:41:32.104: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.794633ms
    Jan 13 10:41:34.122: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029982613s
    Jan 13 10:41:36.113: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d": Phase="Running", Reason="", readiness=false. Elapsed: 4.020308269s
    Jan 13 10:41:36.113: INFO: Pod "pod-configmaps-75db8aca-c98d-489f-8b05-060d3f0f323d" satisfied condition "running"
    STEP: Waiting for pod with text data 01/13/23 10:41:36.113
    STEP: Waiting for pod with binary data 01/13/23 10:41:36.127
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:41:36.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-253" for this suite. 01/13/23 10:41:36.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:41:36.165
Jan 13 10:41:36.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename gc 01/13/23 10:41:36.168
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:36.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:36.206
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/13/23 10:41:36.215
STEP: Wait for the Deployment to create new ReplicaSet 01/13/23 10:41:36.226
STEP: delete the deployment 01/13/23 10:41:36.742
STEP: wait for all rs to be garbage collected 01/13/23 10:41:36.753
STEP: expected 0 rs, got 1 rs 01/13/23 10:41:36.779
STEP: expected 0 pods, got 2 pods 01/13/23 10:41:36.791
STEP: Gathering metrics 01/13/23 10:41:37.31
Jan 13 10:41:37.366: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
Jan 13 10:41:37.372: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 5.693616ms
Jan 13 10:41:37.372: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
Jan 13 10:41:37.372: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
Jan 13 10:41:37.593: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 13 10:41:37.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9416" for this suite. 01/13/23 10:41:37.601
------------------------------
• [1.445 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:41:36.165
    Jan 13 10:41:36.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename gc 01/13/23 10:41:36.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:36.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:36.206
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/13/23 10:41:36.215
    STEP: Wait for the Deployment to create new ReplicaSet 01/13/23 10:41:36.226
    STEP: delete the deployment 01/13/23 10:41:36.742
    STEP: wait for all rs to be garbage collected 01/13/23 10:41:36.753
    STEP: expected 0 rs, got 1 rs 01/13/23 10:41:36.779
    STEP: expected 0 pods, got 2 pods 01/13/23 10:41:36.791
    STEP: Gathering metrics 01/13/23 10:41:37.31
    Jan 13 10:41:37.366: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
    Jan 13 10:41:37.372: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 5.693616ms
    Jan 13 10:41:37.372: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
    Jan 13 10:41:37.372: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
    Jan 13 10:41:37.593: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:41:37.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9416" for this suite. 01/13/23 10:41:37.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:41:37.611
Jan 13 10:41:37.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename runtimeclass 01/13/23 10:41:37.613
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:37.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:37.642
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/13/23 10:41:37.647
STEP: getting /apis/node.k8s.io 01/13/23 10:41:37.651
STEP: getting /apis/node.k8s.io/v1 01/13/23 10:41:37.653
STEP: creating 01/13/23 10:41:37.656
STEP: watching 01/13/23 10:41:37.685
Jan 13 10:41:37.685: INFO: starting watch
STEP: getting 01/13/23 10:41:37.695
STEP: listing 01/13/23 10:41:37.7
STEP: patching 01/13/23 10:41:37.706
STEP: updating 01/13/23 10:41:37.716
Jan 13 10:41:37.726: INFO: waiting for watch events with expected annotations
STEP: deleting 01/13/23 10:41:37.726
STEP: deleting a collection 01/13/23 10:41:37.748
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 13 10:41:37.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5819" for this suite. 01/13/23 10:41:37.767
------------------------------
• [0.164 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:41:37.611
    Jan 13 10:41:37.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename runtimeclass 01/13/23 10:41:37.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:37.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:37.642
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/13/23 10:41:37.647
    STEP: getting /apis/node.k8s.io 01/13/23 10:41:37.651
    STEP: getting /apis/node.k8s.io/v1 01/13/23 10:41:37.653
    STEP: creating 01/13/23 10:41:37.656
    STEP: watching 01/13/23 10:41:37.685
    Jan 13 10:41:37.685: INFO: starting watch
    STEP: getting 01/13/23 10:41:37.695
    STEP: listing 01/13/23 10:41:37.7
    STEP: patching 01/13/23 10:41:37.706
    STEP: updating 01/13/23 10:41:37.716
    Jan 13 10:41:37.726: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/13/23 10:41:37.726
    STEP: deleting a collection 01/13/23 10:41:37.748
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:41:37.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5819" for this suite. 01/13/23 10:41:37.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:41:37.778
Jan 13 10:41:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 10:41:37.78
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:37.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:37.805
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 13 10:41:37.824: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 13 10:41:42.832: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/13/23 10:41:42.833
Jan 13 10:41:42.833: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 13 10:41:44.843: INFO: Creating deployment "test-rollover-deployment"
Jan 13 10:41:44.863: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 13 10:41:46.883: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 13 10:41:46.900: INFO: Ensure that both replica sets have 1 created replica
Jan 13 10:41:46.916: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 13 10:41:46.945: INFO: Updating deployment test-rollover-deployment
Jan 13 10:41:46.945: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 13 10:41:48.972: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 13 10:41:48.992: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 13 10:41:49.012: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 10:41:49.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:41:51.037: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 10:41:51.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:41:53.041: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 10:41:53.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:41:55.037: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 10:41:55.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:41:57.039: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 10:41:57.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:41:59.029: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 10:41:59.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 10:42:01.100: INFO: 
Jan 13 10:42:01.100: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 10:42:01.145: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8837  45961ab8-45ef-47d5-9325-f7b42d2a4506 598747 2 2023-01-13 10:41:44 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-13 10:41:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a1898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 10:41:44 +0000 UTC,LastTransitionTime:2023-01-13 10:41:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-13 10:41:59 +0000 UTC,LastTransitionTime:2023-01-13 10:41:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 13 10:42:01.160: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8837  a4c82c38-7e1e-46b2-bf19-2817b2f20a7e 598737 2 2023-01-13 10:41:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 45961ab8-45ef-47d5-9325-f7b42d2a4506 0xc00460b817 0xc00460b818}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45961ab8-45ef-47d5-9325-f7b42d2a4506\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00460b8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:42:01.160: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 13 10:42:01.160: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8837  8151fbf5-b463-491f-8336-ad127465e8d8 598746 2 2023-01-13 10:41:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 45961ab8-45ef-47d5-9325-f7b42d2a4506 0xc00460b6e7 0xc00460b6e8}] [] [{e2e.test Update apps/v1 2023-01-13 10:41:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45961ab8-45ef-47d5-9325-f7b42d2a4506\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00460b7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:42:01.161: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8837  117553dc-1338-4521-91b1-c1c78aa22fe8 598706 2 2023-01-13 10:41:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 45961ab8-45ef-47d5-9325-f7b42d2a4506 0xc00460b947 0xc00460b948}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:41:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45961ab8-45ef-47d5-9325-f7b42d2a4506\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00460b9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 10:42:01.173: INFO: Pod "test-rollover-deployment-6c6df9974f-s84t7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-s84t7 test-rollover-deployment-6c6df9974f- deployment-8837  78751f41-fd81-428a-b3cc-e9fd0a35b783 598719 0 2023-01-13 10:41:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:80051ef2da515139f77dce5d0a544cd094ec4d7982c1b1e12b490ec1427c15b8 cni.projectcalico.org/podIP:10.244.27.218/32 cni.projectcalico.org/podIPs:10.244.27.218/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f a4c82c38-7e1e-46b2-bf19-2817b2f20a7e 0xc00460bf97 0xc00460bf98}] [] [{kube-controller-manager Update v1 2023-01-13 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4c82c38-7e1e-46b2-bf19-2817b2f20a7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:41:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:41:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqllj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqllj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.218,StartTime:2023-01-13 10:41:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:41:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker://sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:docker://53372771e72ded1072362e3714f455284fd12bd5f7ff1927c2b994ca9dfea5cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 10:42:01.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8837" for this suite. 01/13/23 10:42:01.19
------------------------------
• [SLOW TEST] [23.451 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:41:37.778
    Jan 13 10:41:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 10:41:37.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:41:37.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:41:37.805
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 13 10:41:37.824: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 13 10:41:42.832: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/13/23 10:41:42.833
    Jan 13 10:41:42.833: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 13 10:41:44.843: INFO: Creating deployment "test-rollover-deployment"
    Jan 13 10:41:44.863: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 13 10:41:46.883: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 13 10:41:46.900: INFO: Ensure that both replica sets have 1 created replica
    Jan 13 10:41:46.916: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 13 10:41:46.945: INFO: Updating deployment test-rollover-deployment
    Jan 13 10:41:46.945: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 13 10:41:48.972: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 13 10:41:48.992: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 13 10:41:49.012: INFO: all replica sets need to contain the pod-template-hash label
    Jan 13 10:41:49.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:41:51.037: INFO: all replica sets need to contain the pod-template-hash label
    Jan 13 10:41:51.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:41:53.041: INFO: all replica sets need to contain the pod-template-hash label
    Jan 13 10:41:53.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:41:55.037: INFO: all replica sets need to contain the pod-template-hash label
    Jan 13 10:41:55.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:41:57.039: INFO: all replica sets need to contain the pod-template-hash label
    Jan 13 10:41:57.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:41:59.029: INFO: all replica sets need to contain the pod-template-hash label
    Jan 13 10:41:59.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 41, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 41, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 13 10:42:01.100: INFO: 
    Jan 13 10:42:01.100: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 10:42:01.145: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8837  45961ab8-45ef-47d5-9325-f7b42d2a4506 598747 2 2023-01-13 10:41:44 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-13 10:41:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a1898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-13 10:41:44 +0000 UTC,LastTransitionTime:2023-01-13 10:41:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-13 10:41:59 +0000 UTC,LastTransitionTime:2023-01-13 10:41:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 13 10:42:01.160: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8837  a4c82c38-7e1e-46b2-bf19-2817b2f20a7e 598737 2 2023-01-13 10:41:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 45961ab8-45ef-47d5-9325-f7b42d2a4506 0xc00460b817 0xc00460b818}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45961ab8-45ef-47d5-9325-f7b42d2a4506\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00460b8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:42:01.160: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 13 10:42:01.160: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8837  8151fbf5-b463-491f-8336-ad127465e8d8 598746 2 2023-01-13 10:41:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 45961ab8-45ef-47d5-9325-f7b42d2a4506 0xc00460b6e7 0xc00460b6e8}] [] [{e2e.test Update apps/v1 2023-01-13 10:41:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45961ab8-45ef-47d5-9325-f7b42d2a4506\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00460b7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:42:01.161: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8837  117553dc-1338-4521-91b1-c1c78aa22fe8 598706 2 2023-01-13 10:41:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 45961ab8-45ef-47d5-9325-f7b42d2a4506 0xc00460b947 0xc00460b948}] [] [{kube-controller-manager Update apps/v1 2023-01-13 10:41:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45961ab8-45ef-47d5-9325-f7b42d2a4506\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 10:41:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00460b9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 13 10:42:01.173: INFO: Pod "test-rollover-deployment-6c6df9974f-s84t7" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-s84t7 test-rollover-deployment-6c6df9974f- deployment-8837  78751f41-fd81-428a-b3cc-e9fd0a35b783 598719 0 2023-01-13 10:41:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:80051ef2da515139f77dce5d0a544cd094ec4d7982c1b1e12b490ec1427c15b8 cni.projectcalico.org/podIP:10.244.27.218/32 cni.projectcalico.org/podIPs:10.244.27.218/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f a4c82c38-7e1e-46b2-bf19-2817b2f20a7e 0xc00460bf97 0xc00460bf98}] [] [{kube-controller-manager Update v1 2023-01-13 10:41:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4c82c38-7e1e-46b2-bf19-2817b2f20a7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 10:41:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 10:41:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqllj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqllj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 10:41:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.218,StartTime:2023-01-13 10:41:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 10:41:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker://sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:docker://53372771e72ded1072362e3714f455284fd12bd5f7ff1927c2b994ca9dfea5cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:42:01.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8837" for this suite. 01/13/23 10:42:01.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:42:01.235
Jan 13 10:42:01.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:42:01.238
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:42:01.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:42:01.303
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/13/23 10:42:01.313
Jan 13 10:42:01.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/13/23 10:42:13.58
Jan 13 10:42:13.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:42:16.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:42:31.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3225" for this suite. 01/13/23 10:42:31.119
------------------------------
• [SLOW TEST] [29.902 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:42:01.235
    Jan 13 10:42:01.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:42:01.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:42:01.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:42:01.303
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/13/23 10:42:01.313
    Jan 13 10:42:01.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/13/23 10:42:13.58
    Jan 13 10:42:13.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:42:16.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:42:31.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3225" for this suite. 01/13/23 10:42:31.119
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:42:31.139
Jan 13 10:42:31.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 10:42:31.151
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:42:31.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:42:31.238
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80 in namespace container-probe-6259 01/13/23 10:42:31.257
Jan 13 10:42:31.303: INFO: Waiting up to 5m0s for pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80" in namespace "container-probe-6259" to be "not pending"
Jan 13 10:42:31.310: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506899ms
Jan 13 10:42:33.327: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024137143s
Jan 13 10:42:35.321: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80": Phase="Running", Reason="", readiness=true. Elapsed: 4.017594642s
Jan 13 10:42:35.321: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80" satisfied condition "not pending"
Jan 13 10:42:35.321: INFO: Started pod busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80 in namespace container-probe-6259
STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:42:35.321
Jan 13 10:42:35.328: INFO: Initial restart count of pod busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80 is 0
STEP: deleting the pod 01/13/23 10:46:36.536
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 10:46:36.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6259" for this suite. 01/13/23 10:46:36.603
------------------------------
• [SLOW TEST] [245.479 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:42:31.139
    Jan 13 10:42:31.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 10:42:31.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:42:31.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:42:31.238
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80 in namespace container-probe-6259 01/13/23 10:42:31.257
    Jan 13 10:42:31.303: INFO: Waiting up to 5m0s for pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80" in namespace "container-probe-6259" to be "not pending"
    Jan 13 10:42:31.310: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506899ms
    Jan 13 10:42:33.327: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024137143s
    Jan 13 10:42:35.321: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80": Phase="Running", Reason="", readiness=true. Elapsed: 4.017594642s
    Jan 13 10:42:35.321: INFO: Pod "busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80" satisfied condition "not pending"
    Jan 13 10:42:35.321: INFO: Started pod busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80 in namespace container-probe-6259
    STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:42:35.321
    Jan 13 10:42:35.328: INFO: Initial restart count of pod busybox-c9c772ec-ce8b-4be5-aeec-0d48375b2a80 is 0
    STEP: deleting the pod 01/13/23 10:46:36.536
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:46:36.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6259" for this suite. 01/13/23 10:46:36.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:46:36.623
Jan 13 10:46:36.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:46:36.627
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:46:36.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:46:36.672
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/13/23 10:46:36.688
Jan 13 10:46:36.689: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 13 10:46:36.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
Jan 13 10:46:38.899: INFO: stderr: ""
Jan 13 10:46:38.900: INFO: stdout: "service/agnhost-replica created\n"
Jan 13 10:46:38.900: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 13 10:46:38.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
Jan 13 10:46:39.733: INFO: stderr: ""
Jan 13 10:46:39.733: INFO: stdout: "service/agnhost-primary created\n"
Jan 13 10:46:39.733: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 13 10:46:39.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
Jan 13 10:46:40.599: INFO: stderr: ""
Jan 13 10:46:40.599: INFO: stdout: "service/frontend created\n"
Jan 13 10:46:40.599: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 13 10:46:40.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
Jan 13 10:46:41.450: INFO: stderr: ""
Jan 13 10:46:41.450: INFO: stdout: "deployment.apps/frontend created\n"
Jan 13 10:46:41.450: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 13 10:46:41.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
Jan 13 10:46:42.350: INFO: stderr: ""
Jan 13 10:46:42.351: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 13 10:46:42.351: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 13 10:46:42.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
Jan 13 10:46:43.239: INFO: stderr: ""
Jan 13 10:46:43.239: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/13/23 10:46:43.239
Jan 13 10:46:43.239: INFO: Waiting for all frontend pods to be Running.
Jan 13 10:46:48.297: INFO: Waiting for frontend to serve content.
Jan 13 10:46:48.339: INFO: Trying to add a new entry to the guestbook.
Jan 13 10:46:48.456: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/13/23 10:46:48.602
Jan 13 10:46:48.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
Jan 13 10:46:49.017: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:46:49.017: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/13/23 10:46:49.018
Jan 13 10:46:49.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
Jan 13 10:46:49.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:46:49.364: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/13/23 10:46:49.364
Jan 13 10:46:49.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
Jan 13 10:46:49.772: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:46:49.772: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/13/23 10:46:49.772
Jan 13 10:46:49.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
Jan 13 10:46:50.135: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:46:50.135: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/13/23 10:46:50.136
Jan 13 10:46:50.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
Jan 13 10:46:50.496: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:46:50.496: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/13/23 10:46:50.497
Jan 13 10:46:50.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
Jan 13 10:46:50.921: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 10:46:50.921: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:46:50.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7862" for this suite. 01/13/23 10:46:50.953
------------------------------
• [SLOW TEST] [14.388 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:46:36.623
    Jan 13 10:46:36.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:46:36.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:46:36.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:46:36.672
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/13/23 10:46:36.688
    Jan 13 10:46:36.689: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 13 10:46:36.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
    Jan 13 10:46:38.899: INFO: stderr: ""
    Jan 13 10:46:38.900: INFO: stdout: "service/agnhost-replica created\n"
    Jan 13 10:46:38.900: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 13 10:46:38.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
    Jan 13 10:46:39.733: INFO: stderr: ""
    Jan 13 10:46:39.733: INFO: stdout: "service/agnhost-primary created\n"
    Jan 13 10:46:39.733: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 13 10:46:39.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
    Jan 13 10:46:40.599: INFO: stderr: ""
    Jan 13 10:46:40.599: INFO: stdout: "service/frontend created\n"
    Jan 13 10:46:40.599: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 13 10:46:40.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
    Jan 13 10:46:41.450: INFO: stderr: ""
    Jan 13 10:46:41.450: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 13 10:46:41.450: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 13 10:46:41.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
    Jan 13 10:46:42.350: INFO: stderr: ""
    Jan 13 10:46:42.351: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 13 10:46:42.351: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 13 10:46:42.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 create -f -'
    Jan 13 10:46:43.239: INFO: stderr: ""
    Jan 13 10:46:43.239: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/13/23 10:46:43.239
    Jan 13 10:46:43.239: INFO: Waiting for all frontend pods to be Running.
    Jan 13 10:46:48.297: INFO: Waiting for frontend to serve content.
    Jan 13 10:46:48.339: INFO: Trying to add a new entry to the guestbook.
    Jan 13 10:46:48.456: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/13/23 10:46:48.602
    Jan 13 10:46:48.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
    Jan 13 10:46:49.017: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:46:49.017: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/13/23 10:46:49.018
    Jan 13 10:46:49.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
    Jan 13 10:46:49.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:46:49.364: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/13/23 10:46:49.364
    Jan 13 10:46:49.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
    Jan 13 10:46:49.772: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:46:49.772: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/13/23 10:46:49.772
    Jan 13 10:46:49.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
    Jan 13 10:46:50.135: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:46:50.135: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/13/23 10:46:50.136
    Jan 13 10:46:50.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
    Jan 13 10:46:50.496: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:46:50.496: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/13/23 10:46:50.497
    Jan 13 10:46:50.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-7862 delete --grace-period=0 --force -f -'
    Jan 13 10:46:50.921: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 13 10:46:50.921: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:46:50.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7862" for this suite. 01/13/23 10:46:50.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:46:51.03
Jan 13 10:46:51.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svc-latency 01/13/23 10:46:51.033
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:46:51.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:46:51.134
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 13 10:46:51.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9866 01/13/23 10:46:51.165
I0113 10:46:51.206519      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9866, replica count: 1
I0113 10:46:52.259090      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:46:53.259374      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:46:54.260726      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 10:46:55.261135      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 10:46:55.406: INFO: Created: latency-svc-dfb6j
Jan 13 10:46:55.421: INFO: Got endpoints: latency-svc-dfb6j [58.663042ms]
Jan 13 10:46:55.530: INFO: Created: latency-svc-5ls7h
Jan 13 10:46:55.571: INFO: Created: latency-svc-s8lcm
Jan 13 10:46:55.603: INFO: Got endpoints: latency-svc-s8lcm [180.322939ms]
Jan 13 10:46:55.604: INFO: Got endpoints: latency-svc-5ls7h [181.958732ms]
Jan 13 10:46:55.643: INFO: Created: latency-svc-hfj4b
Jan 13 10:46:55.663: INFO: Got endpoints: latency-svc-hfj4b [236.778091ms]
Jan 13 10:46:55.739: INFO: Created: latency-svc-fm8vs
Jan 13 10:46:55.790: INFO: Got endpoints: latency-svc-fm8vs [360.773368ms]
Jan 13 10:46:55.826: INFO: Created: latency-svc-cbjq5
Jan 13 10:46:55.827: INFO: Got endpoints: latency-svc-cbjq5 [401.236703ms]
Jan 13 10:46:55.871: INFO: Created: latency-svc-rm2jk
Jan 13 10:46:55.901: INFO: Got endpoints: latency-svc-rm2jk [471.463641ms]
Jan 13 10:46:55.924: INFO: Created: latency-svc-hxjh6
Jan 13 10:46:55.940: INFO: Got endpoints: latency-svc-hxjh6 [510.399879ms]
Jan 13 10:46:55.955: INFO: Created: latency-svc-89sjl
Jan 13 10:46:55.982: INFO: Got endpoints: latency-svc-89sjl [551.645378ms]
Jan 13 10:46:55.987: INFO: Created: latency-svc-mb568
Jan 13 10:46:56.026: INFO: Got endpoints: latency-svc-mb568 [602.16098ms]
Jan 13 10:46:56.048: INFO: Created: latency-svc-6ptxw
Jan 13 10:46:56.059: INFO: Got endpoints: latency-svc-6ptxw [633.828933ms]
Jan 13 10:46:56.075: INFO: Created: latency-svc-dxgz6
Jan 13 10:46:56.109: INFO: Got endpoints: latency-svc-dxgz6 [682.101829ms]
Jan 13 10:46:56.135: INFO: Created: latency-svc-6mk4k
Jan 13 10:46:56.156: INFO: Got endpoints: latency-svc-6mk4k [728.483122ms]
Jan 13 10:46:56.186: INFO: Created: latency-svc-c9rzb
Jan 13 10:46:56.212: INFO: Got endpoints: latency-svc-c9rzb [781.429512ms]
Jan 13 10:46:56.269: INFO: Created: latency-svc-pnq5f
Jan 13 10:46:56.292: INFO: Got endpoints: latency-svc-pnq5f [864.579123ms]
Jan 13 10:46:56.318: INFO: Created: latency-svc-gjs86
Jan 13 10:46:56.351: INFO: Got endpoints: latency-svc-gjs86 [920.886139ms]
Jan 13 10:46:56.386: INFO: Created: latency-svc-2vdv4
Jan 13 10:46:56.437: INFO: Got endpoints: latency-svc-2vdv4 [833.759547ms]
Jan 13 10:46:56.450: INFO: Created: latency-svc-d8gw4
Jan 13 10:46:56.466: INFO: Got endpoints: latency-svc-d8gw4 [862.094349ms]
Jan 13 10:46:56.598: INFO: Created: latency-svc-6frt2
Jan 13 10:46:56.662: INFO: Got endpoints: latency-svc-6frt2 [999.570454ms]
Jan 13 10:46:56.674: INFO: Created: latency-svc-5k5lw
Jan 13 10:46:56.736: INFO: Got endpoints: latency-svc-5k5lw [945.946063ms]
Jan 13 10:46:56.759: INFO: Created: latency-svc-lntpt
Jan 13 10:46:56.788: INFO: Got endpoints: latency-svc-lntpt [961.591145ms]
Jan 13 10:46:56.820: INFO: Created: latency-svc-8q8pp
Jan 13 10:46:56.829: INFO: Got endpoints: latency-svc-8q8pp [928.729989ms]
Jan 13 10:46:56.853: INFO: Created: latency-svc-l7lkr
Jan 13 10:46:56.861: INFO: Got endpoints: latency-svc-l7lkr [920.83283ms]
Jan 13 10:46:56.869: INFO: Created: latency-svc-mrdh5
Jan 13 10:46:56.905: INFO: Got endpoints: latency-svc-mrdh5 [922.352057ms]
Jan 13 10:46:56.951: INFO: Created: latency-svc-t8rsv
Jan 13 10:46:56.978: INFO: Got endpoints: latency-svc-t8rsv [951.419251ms]
Jan 13 10:46:57.068: INFO: Created: latency-svc-g56bk
Jan 13 10:46:57.122: INFO: Got endpoints: latency-svc-g56bk [1.062826923s]
Jan 13 10:46:57.174: INFO: Created: latency-svc-sngzr
Jan 13 10:46:57.221: INFO: Got endpoints: latency-svc-sngzr [1.111329092s]
Jan 13 10:46:57.250: INFO: Created: latency-svc-lqmxn
Jan 13 10:46:57.261: INFO: Got endpoints: latency-svc-lqmxn [1.105292247s]
Jan 13 10:46:57.279: INFO: Created: latency-svc-7k2fd
Jan 13 10:46:57.302: INFO: Got endpoints: latency-svc-7k2fd [1.090305152s]
Jan 13 10:46:57.783: INFO: Created: latency-svc-5jhl4
Jan 13 10:46:57.783: INFO: Created: latency-svc-86vn5
Jan 13 10:46:57.783: INFO: Created: latency-svc-xvf94
Jan 13 10:46:57.783: INFO: Created: latency-svc-cjc6x
Jan 13 10:46:57.784: INFO: Created: latency-svc-jxhn6
Jan 13 10:46:57.784: INFO: Created: latency-svc-mmcxx
Jan 13 10:46:57.784: INFO: Created: latency-svc-h8zn6
Jan 13 10:46:57.784: INFO: Created: latency-svc-kvjb5
Jan 13 10:46:57.784: INFO: Created: latency-svc-lrskb
Jan 13 10:46:57.786: INFO: Created: latency-svc-pjbh2
Jan 13 10:46:57.786: INFO: Created: latency-svc-66hlz
Jan 13 10:46:57.807: INFO: Created: latency-svc-6k2qb
Jan 13 10:46:57.841: INFO: Got endpoints: latency-svc-pjbh2 [1.548970833s]
Jan 13 10:46:57.842: INFO: Got endpoints: latency-svc-kvjb5 [980.742704ms]
Jan 13 10:46:57.845: INFO: Created: latency-svc-nhshd
Jan 13 10:46:57.847: INFO: Created: latency-svc-fwlgp
Jan 13 10:46:57.848: INFO: Created: latency-svc-sdc9b
Jan 13 10:46:57.855: INFO: Got endpoints: latency-svc-cjc6x [1.503599153s]
Jan 13 10:46:57.855: INFO: Got endpoints: latency-svc-jxhn6 [1.025892247s]
Jan 13 10:46:57.856: INFO: Got endpoints: latency-svc-66hlz [877.844981ms]
Jan 13 10:46:57.916: INFO: Got endpoints: latency-svc-5jhl4 [1.253397845s]
Jan 13 10:46:57.918: INFO: Got endpoints: latency-svc-mmcxx [796.348751ms]
Jan 13 10:46:57.926: INFO: Got endpoints: latency-svc-86vn5 [1.137890895s]
Jan 13 10:46:57.927: INFO: Got endpoints: latency-svc-xvf94 [623.463469ms]
Jan 13 10:46:57.928: INFO: Got endpoints: latency-svc-h8zn6 [1.490537465s]
Jan 13 10:46:57.980: INFO: Got endpoints: latency-svc-nhshd [1.513128377s]
Jan 13 10:46:57.981: INFO: Got endpoints: latency-svc-lrskb [719.885804ms]
Jan 13 10:46:57.984: INFO: Got endpoints: latency-svc-6k2qb [1.247351878s]
Jan 13 10:46:57.984: INFO: Got endpoints: latency-svc-fwlgp [763.233941ms]
Jan 13 10:46:57.984: INFO: Got endpoints: latency-svc-sdc9b [1.078595913s]
Jan 13 10:46:58.007: INFO: Created: latency-svc-7dm2s
Jan 13 10:46:58.017: INFO: Got endpoints: latency-svc-7dm2s [175.466263ms]
Jan 13 10:46:58.039: INFO: Created: latency-svc-r4k9z
Jan 13 10:46:58.053: INFO: Got endpoints: latency-svc-r4k9z [211.270256ms]
Jan 13 10:46:58.106: INFO: Created: latency-svc-rgmdv
Jan 13 10:46:58.114: INFO: Got endpoints: latency-svc-rgmdv [258.371354ms]
Jan 13 10:46:58.126: INFO: Created: latency-svc-5mgqk
Jan 13 10:46:58.132: INFO: Got endpoints: latency-svc-5mgqk [277.363085ms]
Jan 13 10:46:58.142: INFO: Created: latency-svc-tj7gf
Jan 13 10:46:58.152: INFO: Got endpoints: latency-svc-tj7gf [296.711001ms]
Jan 13 10:46:58.156: INFO: Created: latency-svc-5flf6
Jan 13 10:46:58.165: INFO: Got endpoints: latency-svc-5flf6 [249.529333ms]
Jan 13 10:46:58.175: INFO: Created: latency-svc-l8glf
Jan 13 10:46:58.210: INFO: Created: latency-svc-d6bl8
Jan 13 10:46:58.211: INFO: Got endpoints: latency-svc-l8glf [292.189997ms]
Jan 13 10:46:58.223: INFO: Got endpoints: latency-svc-d6bl8 [297.069258ms]
Jan 13 10:46:58.243: INFO: Created: latency-svc-mtnp2
Jan 13 10:46:58.246: INFO: Got endpoints: latency-svc-mtnp2 [318.316185ms]
Jan 13 10:46:58.265: INFO: Created: latency-svc-7swmz
Jan 13 10:46:58.287: INFO: Created: latency-svc-kljzs
Jan 13 10:46:58.287: INFO: Got endpoints: latency-svc-7swmz [359.619348ms]
Jan 13 10:46:58.298: INFO: Got endpoints: latency-svc-kljzs [318.56862ms]
Jan 13 10:46:58.317: INFO: Created: latency-svc-l7jbg
Jan 13 10:46:58.351: INFO: Got endpoints: latency-svc-l7jbg [370.05963ms]
Jan 13 10:46:58.358: INFO: Created: latency-svc-pbzpb
Jan 13 10:46:58.365: INFO: Got endpoints: latency-svc-pbzpb [381.149394ms]
Jan 13 10:46:58.386: INFO: Created: latency-svc-p22jx
Jan 13 10:46:58.399: INFO: Created: latency-svc-k5667
Jan 13 10:46:58.426: INFO: Got endpoints: latency-svc-p22jx [442.293338ms]
Jan 13 10:46:58.428: INFO: Created: latency-svc-zsn2p
Jan 13 10:46:58.433: INFO: Got endpoints: latency-svc-k5667 [448.718785ms]
Jan 13 10:46:58.439: INFO: Got endpoints: latency-svc-zsn2p [421.219676ms]
Jan 13 10:46:58.451: INFO: Created: latency-svc-g6cxr
Jan 13 10:46:58.469: INFO: Got endpoints: latency-svc-g6cxr [415.121781ms]
Jan 13 10:46:58.471: INFO: Created: latency-svc-2qz7t
Jan 13 10:46:58.481: INFO: Got endpoints: latency-svc-2qz7t [366.719563ms]
Jan 13 10:46:58.499: INFO: Created: latency-svc-8vzdp
Jan 13 10:46:58.506: INFO: Got endpoints: latency-svc-8vzdp [373.380987ms]
Jan 13 10:46:58.523: INFO: Created: latency-svc-b5wpf
Jan 13 10:46:58.543: INFO: Got endpoints: latency-svc-b5wpf [390.986936ms]
Jan 13 10:46:58.557: INFO: Created: latency-svc-bll9c
Jan 13 10:46:58.570: INFO: Got endpoints: latency-svc-bll9c [404.174355ms]
Jan 13 10:46:58.582: INFO: Created: latency-svc-mft6f
Jan 13 10:46:58.606: INFO: Got endpoints: latency-svc-mft6f [395.45477ms]
Jan 13 10:46:58.795: INFO: Created: latency-svc-fgrms
Jan 13 10:46:58.815: INFO: Got endpoints: latency-svc-fgrms [591.188201ms]
Jan 13 10:46:58.842: INFO: Created: latency-svc-dlrwj
Jan 13 10:46:58.847: INFO: Got endpoints: latency-svc-dlrwj [600.904583ms]
Jan 13 10:46:58.885: INFO: Created: latency-svc-t4sbt
Jan 13 10:46:58.907: INFO: Got endpoints: latency-svc-t4sbt [619.620797ms]
Jan 13 10:46:58.940: INFO: Created: latency-svc-c2qr6
Jan 13 10:46:58.972: INFO: Got endpoints: latency-svc-c2qr6 [674.203092ms]
Jan 13 10:46:59.004: INFO: Created: latency-svc-6f5rc
Jan 13 10:46:59.025: INFO: Got endpoints: latency-svc-6f5rc [674.031297ms]
Jan 13 10:46:59.033: INFO: Created: latency-svc-ssgjm
Jan 13 10:46:59.046: INFO: Got endpoints: latency-svc-ssgjm [681.104587ms]
Jan 13 10:46:59.059: INFO: Created: latency-svc-96cp7
Jan 13 10:46:59.082: INFO: Got endpoints: latency-svc-96cp7 [655.702191ms]
Jan 13 10:46:59.116: INFO: Created: latency-svc-gh7zr
Jan 13 10:46:59.124: INFO: Got endpoints: latency-svc-gh7zr [690.6915ms]
Jan 13 10:46:59.141: INFO: Created: latency-svc-h2n66
Jan 13 10:46:59.181: INFO: Got endpoints: latency-svc-h2n66 [742.590788ms]
Jan 13 10:46:59.188: INFO: Created: latency-svc-rblrm
Jan 13 10:46:59.223: INFO: Got endpoints: latency-svc-rblrm [754.023477ms]
Jan 13 10:46:59.249: INFO: Created: latency-svc-58wsj
Jan 13 10:46:59.276: INFO: Got endpoints: latency-svc-58wsj [794.442751ms]
Jan 13 10:46:59.288: INFO: Created: latency-svc-cmgjs
Jan 13 10:46:59.288: INFO: Got endpoints: latency-svc-cmgjs [782.713605ms]
Jan 13 10:46:59.301: INFO: Created: latency-svc-nsztd
Jan 13 10:46:59.312: INFO: Got endpoints: latency-svc-nsztd [769.100124ms]
Jan 13 10:46:59.317: INFO: Created: latency-svc-g65bf
Jan 13 10:46:59.327: INFO: Got endpoints: latency-svc-g65bf [757.590555ms]
Jan 13 10:46:59.336: INFO: Created: latency-svc-vfcml
Jan 13 10:46:59.348: INFO: Got endpoints: latency-svc-vfcml [741.120776ms]
Jan 13 10:46:59.360: INFO: Created: latency-svc-r595f
Jan 13 10:46:59.363: INFO: Got endpoints: latency-svc-r595f [548.672762ms]
Jan 13 10:46:59.380: INFO: Created: latency-svc-w5rgh
Jan 13 10:46:59.393: INFO: Got endpoints: latency-svc-w5rgh [546.164255ms]
Jan 13 10:46:59.411: INFO: Created: latency-svc-gs2gs
Jan 13 10:46:59.451: INFO: Got endpoints: latency-svc-gs2gs [543.420008ms]
Jan 13 10:46:59.468: INFO: Created: latency-svc-rrr2h
Jan 13 10:46:59.482: INFO: Got endpoints: latency-svc-rrr2h [509.234454ms]
Jan 13 10:46:59.490: INFO: Created: latency-svc-8lfwg
Jan 13 10:46:59.507: INFO: Got endpoints: latency-svc-8lfwg [481.099324ms]
Jan 13 10:46:59.515: INFO: Created: latency-svc-97bz2
Jan 13 10:46:59.528: INFO: Got endpoints: latency-svc-97bz2 [481.407148ms]
Jan 13 10:46:59.542: INFO: Created: latency-svc-cvhmf
Jan 13 10:46:59.559: INFO: Got endpoints: latency-svc-cvhmf [476.692528ms]
Jan 13 10:46:59.566: INFO: Created: latency-svc-dslfp
Jan 13 10:46:59.577: INFO: Got endpoints: latency-svc-dslfp [453.090017ms]
Jan 13 10:46:59.598: INFO: Created: latency-svc-4kknt
Jan 13 10:46:59.605: INFO: Got endpoints: latency-svc-4kknt [423.803727ms]
Jan 13 10:46:59.624: INFO: Created: latency-svc-4ndj2
Jan 13 10:46:59.630: INFO: Got endpoints: latency-svc-4ndj2 [407.481102ms]
Jan 13 10:46:59.661: INFO: Created: latency-svc-sbksg
Jan 13 10:46:59.690: INFO: Got endpoints: latency-svc-sbksg [414.360822ms]
Jan 13 10:46:59.711: INFO: Created: latency-svc-sz2xb
Jan 13 10:46:59.775: INFO: Got endpoints: latency-svc-sz2xb [486.619399ms]
Jan 13 10:46:59.796: INFO: Created: latency-svc-v6ttx
Jan 13 10:46:59.817: INFO: Got endpoints: latency-svc-v6ttx [503.979632ms]
Jan 13 10:46:59.835: INFO: Created: latency-svc-klt2m
Jan 13 10:46:59.850: INFO: Got endpoints: latency-svc-klt2m [522.945551ms]
Jan 13 10:46:59.861: INFO: Created: latency-svc-k7mdk
Jan 13 10:46:59.868: INFO: Got endpoints: latency-svc-k7mdk [520.283528ms]
Jan 13 10:46:59.887: INFO: Created: latency-svc-7qxh5
Jan 13 10:46:59.896: INFO: Got endpoints: latency-svc-7qxh5 [532.327434ms]
Jan 13 10:46:59.904: INFO: Created: latency-svc-26ccf
Jan 13 10:46:59.914: INFO: Got endpoints: latency-svc-26ccf [521.349085ms]
Jan 13 10:46:59.928: INFO: Created: latency-svc-flmzw
Jan 13 10:46:59.948: INFO: Got endpoints: latency-svc-flmzw [496.835128ms]
Jan 13 10:46:59.953: INFO: Created: latency-svc-nr2n2
Jan 13 10:46:59.965: INFO: Got endpoints: latency-svc-nr2n2 [482.889211ms]
Jan 13 10:46:59.972: INFO: Created: latency-svc-lmplg
Jan 13 10:46:59.988: INFO: Got endpoints: latency-svc-lmplg [481.038036ms]
Jan 13 10:46:59.999: INFO: Created: latency-svc-r4zpv
Jan 13 10:47:00.007: INFO: Got endpoints: latency-svc-r4zpv [478.994016ms]
Jan 13 10:47:00.028: INFO: Created: latency-svc-nnzbq
Jan 13 10:47:00.034: INFO: Got endpoints: latency-svc-nnzbq [474.911298ms]
Jan 13 10:47:00.037: INFO: Created: latency-svc-wrtzg
Jan 13 10:47:00.053: INFO: Got endpoints: latency-svc-wrtzg [475.896122ms]
Jan 13 10:47:00.100: INFO: Created: latency-svc-s76l9
Jan 13 10:47:00.109: INFO: Got endpoints: latency-svc-s76l9 [503.533745ms]
Jan 13 10:47:00.125: INFO: Created: latency-svc-csj5b
Jan 13 10:47:00.135: INFO: Got endpoints: latency-svc-csj5b [504.693737ms]
Jan 13 10:47:00.164: INFO: Created: latency-svc-67pfb
Jan 13 10:47:00.166: INFO: Created: latency-svc-8fbf8
Jan 13 10:47:00.185: INFO: Got endpoints: latency-svc-67pfb [494.429483ms]
Jan 13 10:47:00.185: INFO: Created: latency-svc-gxvzp
Jan 13 10:47:00.240: INFO: Got endpoints: latency-svc-8fbf8 [465.008914ms]
Jan 13 10:47:00.241: INFO: Created: latency-svc-t28nc
Jan 13 10:47:00.265: INFO: Created: latency-svc-2srxw
Jan 13 10:47:00.283: INFO: Got endpoints: latency-svc-gxvzp [466.847305ms]
Jan 13 10:47:00.295: INFO: Created: latency-svc-rqfxz
Jan 13 10:47:00.321: INFO: Created: latency-svc-hztpj
Jan 13 10:47:00.350: INFO: Got endpoints: latency-svc-t28nc [499.348509ms]
Jan 13 10:47:00.359: INFO: Created: latency-svc-gtmdv
Jan 13 10:47:00.411: INFO: Got endpoints: latency-svc-2srxw [543.037144ms]
Jan 13 10:47:00.413: INFO: Created: latency-svc-zdr5g
Jan 13 10:47:00.441: INFO: Created: latency-svc-4j2vm
Jan 13 10:47:00.462: INFO: Got endpoints: latency-svc-rqfxz [566.317813ms]
Jan 13 10:47:00.484: INFO: Created: latency-svc-r7fsv
Jan 13 10:47:00.520: INFO: Got endpoints: latency-svc-hztpj [605.233788ms]
Jan 13 10:47:00.522: INFO: Created: latency-svc-gqmk6
Jan 13 10:47:00.586: INFO: Got endpoints: latency-svc-gtmdv [638.724666ms]
Jan 13 10:47:00.610: INFO: Created: latency-svc-vw82s
Jan 13 10:47:00.635: INFO: Got endpoints: latency-svc-zdr5g [669.796752ms]
Jan 13 10:47:00.643: INFO: Got endpoints: latency-svc-4j2vm [654.51301ms]
Jan 13 10:47:00.668: INFO: Created: latency-svc-s4brk
Jan 13 10:47:00.671: INFO: Created: latency-svc-8ct77
Jan 13 10:47:00.693: INFO: Got endpoints: latency-svc-r7fsv [685.855734ms]
Jan 13 10:47:00.753: INFO: Created: latency-svc-jlrvm
Jan 13 10:47:00.783: INFO: Got endpoints: latency-svc-gqmk6 [748.949515ms]
Jan 13 10:47:00.800: INFO: Got endpoints: latency-svc-vw82s [746.599615ms]
Jan 13 10:47:00.802: INFO: Created: latency-svc-jh2hb
Jan 13 10:47:00.834: INFO: Created: latency-svc-5gc9q
Jan 13 10:47:00.839: INFO: Got endpoints: latency-svc-s4brk [729.368049ms]
Jan 13 10:47:00.885: INFO: Created: latency-svc-ctn4m
Jan 13 10:47:00.900: INFO: Got endpoints: latency-svc-8ct77 [763.939189ms]
Jan 13 10:47:00.930: INFO: Created: latency-svc-hz6nd
Jan 13 10:47:00.979: INFO: Created: latency-svc-cgdw8
Jan 13 10:47:00.997: INFO: Got endpoints: latency-svc-jlrvm [811.936867ms]
Jan 13 10:47:01.066: INFO: Got endpoints: latency-svc-jh2hb [826.052365ms]
Jan 13 10:47:01.098: INFO: Created: latency-svc-csfdz
Jan 13 10:47:01.099: INFO: Got endpoints: latency-svc-5gc9q [815.130672ms]
Jan 13 10:47:01.114: INFO: Got endpoints: latency-svc-ctn4m [763.959682ms]
Jan 13 10:47:01.153: INFO: Created: latency-svc-2k57m
Jan 13 10:47:01.164: INFO: Got endpoints: latency-svc-hz6nd [752.335352ms]
Jan 13 10:47:01.172: INFO: Created: latency-svc-gtzcx
Jan 13 10:47:01.206: INFO: Got endpoints: latency-svc-cgdw8 [743.34583ms]
Jan 13 10:47:01.207: INFO: Created: latency-svc-bgj5f
Jan 13 10:47:01.243: INFO: Got endpoints: latency-svc-csfdz [722.646387ms]
Jan 13 10:47:01.262: INFO: Created: latency-svc-msd7n
Jan 13 10:47:01.363: INFO: Got endpoints: latency-svc-2k57m [776.163702ms]
Jan 13 10:47:01.364: INFO: Got endpoints: latency-svc-gtzcx [728.987457ms]
Jan 13 10:47:01.368: INFO: Created: latency-svc-hfnft
Jan 13 10:47:01.382: INFO: Created: latency-svc-kgjq8
Jan 13 10:47:01.391: INFO: Got endpoints: latency-svc-bgj5f [748.303972ms]
Jan 13 10:47:01.395: INFO: Created: latency-svc-bmgrw
Jan 13 10:47:01.443: INFO: Created: latency-svc-ftfkp
Jan 13 10:47:01.450: INFO: Got endpoints: latency-svc-msd7n [757.040205ms]
Jan 13 10:47:01.469: INFO: Created: latency-svc-9zfdd
Jan 13 10:47:01.567: INFO: Created: latency-svc-lh5dg
Jan 13 10:47:01.568: INFO: Got endpoints: latency-svc-kgjq8 [767.362885ms]
Jan 13 10:47:01.568: INFO: Got endpoints: latency-svc-hfnft [785.199215ms]
Jan 13 10:47:01.592: INFO: Got endpoints: latency-svc-bmgrw [753.195985ms]
Jan 13 10:47:01.593: INFO: Created: latency-svc-8ffwm
Jan 13 10:47:01.639: INFO: Created: latency-svc-kchcw
Jan 13 10:47:01.646: INFO: Got endpoints: latency-svc-ftfkp [746.13789ms]
Jan 13 10:47:01.656: INFO: Created: latency-svc-98znr
Jan 13 10:47:01.676: INFO: Created: latency-svc-r2gt6
Jan 13 10:47:01.685: INFO: Got endpoints: latency-svc-9zfdd [688.056914ms]
Jan 13 10:47:01.701: INFO: Created: latency-svc-x8jtj
Jan 13 10:47:01.736: INFO: Created: latency-svc-s6wjx
Jan 13 10:47:01.742: INFO: Got endpoints: latency-svc-lh5dg [675.785773ms]
Jan 13 10:47:01.762: INFO: Created: latency-svc-8jljt
Jan 13 10:47:01.780: INFO: Created: latency-svc-nkmtp
Jan 13 10:47:01.791: INFO: Got endpoints: latency-svc-8ffwm [691.505489ms]
Jan 13 10:47:01.827: INFO: Created: latency-svc-6qtk5
Jan 13 10:47:01.849: INFO: Got endpoints: latency-svc-kchcw [734.727973ms]
Jan 13 10:47:01.863: INFO: Created: latency-svc-87sxp
Jan 13 10:47:01.915: INFO: Got endpoints: latency-svc-98znr [751.038208ms]
Jan 13 10:47:01.944: INFO: Created: latency-svc-9vqdj
Jan 13 10:47:02.019: INFO: Got endpoints: latency-svc-r2gt6 [812.97444ms]
Jan 13 10:47:02.021: INFO: Created: latency-svc-xt5sm
Jan 13 10:47:02.025: INFO: Got endpoints: latency-svc-x8jtj [781.932811ms]
Jan 13 10:47:02.064: INFO: Got endpoints: latency-svc-s6wjx [699.92818ms]
Jan 13 10:47:02.092: INFO: Got endpoints: latency-svc-8jljt [729.615423ms]
Jan 13 10:47:02.104: INFO: Created: latency-svc-csthh
Jan 13 10:47:02.141: INFO: Got endpoints: latency-svc-nkmtp [750.138204ms]
Jan 13 10:47:02.146: INFO: Created: latency-svc-7p4cz
Jan 13 10:47:02.163: INFO: Created: latency-svc-s5l5l
Jan 13 10:47:02.176: INFO: Created: latency-svc-rh5kp
Jan 13 10:47:02.185: INFO: Got endpoints: latency-svc-6qtk5 [735.639688ms]
Jan 13 10:47:02.193: INFO: Created: latency-svc-8cv2k
Jan 13 10:47:02.225: INFO: Created: latency-svc-mk87b
Jan 13 10:47:02.254: INFO: Got endpoints: latency-svc-87sxp [685.861751ms]
Jan 13 10:47:02.260: INFO: Created: latency-svc-wkp2b
Jan 13 10:47:02.281: INFO: Created: latency-svc-b8vgh
Jan 13 10:47:02.292: INFO: Got endpoints: latency-svc-9vqdj [724.337769ms]
Jan 13 10:47:02.294: INFO: Created: latency-svc-tzw4f
Jan 13 10:47:02.330: INFO: Created: latency-svc-qzt72
Jan 13 10:47:02.366: INFO: Created: latency-svc-4h85x
Jan 13 10:47:02.370: INFO: Got endpoints: latency-svc-xt5sm [777.765187ms]
Jan 13 10:47:02.394: INFO: Got endpoints: latency-svc-csthh [747.333427ms]
Jan 13 10:47:02.416: INFO: Created: latency-svc-56wd4
Jan 13 10:47:02.450: INFO: Got endpoints: latency-svc-7p4cz [765.330043ms]
Jan 13 10:47:02.473: INFO: Created: latency-svc-xjxdv
Jan 13 10:47:02.487: INFO: Got endpoints: latency-svc-s5l5l [744.929661ms]
Jan 13 10:47:02.496: INFO: Created: latency-svc-nrldl
Jan 13 10:47:02.538: INFO: Created: latency-svc-rtkt5
Jan 13 10:47:02.538: INFO: Got endpoints: latency-svc-rh5kp [747.690931ms]
Jan 13 10:47:02.565: INFO: Created: latency-svc-8w6dl
Jan 13 10:47:02.601: INFO: Created: latency-svc-6swzh
Jan 13 10:47:02.602: INFO: Got endpoints: latency-svc-8cv2k [753.147028ms]
Jan 13 10:47:02.608: INFO: Created: latency-svc-5tlbg
Jan 13 10:47:02.622: INFO: Created: latency-svc-wdn59
Jan 13 10:47:02.658: INFO: Got endpoints: latency-svc-mk87b [739.814625ms]
Jan 13 10:47:02.672: INFO: Created: latency-svc-ksr79
Jan 13 10:47:02.714: INFO: Created: latency-svc-6gmvd
Jan 13 10:47:02.714: INFO: Got endpoints: latency-svc-wkp2b [695.655242ms]
Jan 13 10:47:02.736: INFO: Got endpoints: latency-svc-b8vgh [711.097619ms]
Jan 13 10:47:02.775: INFO: Created: latency-svc-ll8vw
Jan 13 10:47:02.776: INFO: Created: latency-svc-7lsgt
Jan 13 10:47:02.796: INFO: Got endpoints: latency-svc-tzw4f [731.916348ms]
Jan 13 10:47:02.826: INFO: Created: latency-svc-2f8n9
Jan 13 10:47:02.844: INFO: Got endpoints: latency-svc-qzt72 [751.274578ms]
Jan 13 10:47:02.907: INFO: Got endpoints: latency-svc-4h85x [765.044155ms]
Jan 13 10:47:02.914: INFO: Created: latency-svc-s7hc4
Jan 13 10:47:02.968: INFO: Got endpoints: latency-svc-56wd4 [782.00118ms]
Jan 13 10:47:02.969: INFO: Created: latency-svc-zsf85
Jan 13 10:47:03.012: INFO: Got endpoints: latency-svc-xjxdv [757.740427ms]
Jan 13 10:47:03.120: INFO: Got endpoints: latency-svc-nrldl [827.622801ms]
Jan 13 10:47:03.123: INFO: Got endpoints: latency-svc-rtkt5 [752.113539ms]
Jan 13 10:47:03.162: INFO: Got endpoints: latency-svc-8w6dl [768.593695ms]
Jan 13 10:47:03.249: INFO: Created: latency-svc-tljwj
Jan 13 10:47:03.249: INFO: Created: latency-svc-nvhgm
Jan 13 10:47:03.250: INFO: Got endpoints: latency-svc-6swzh [799.973601ms]
Jan 13 10:47:03.268: INFO: Got endpoints: latency-svc-5tlbg [780.191375ms]
Jan 13 10:47:03.268: INFO: Created: latency-svc-cwjn8
Jan 13 10:47:03.277: INFO: Created: latency-svc-cxcp4
Jan 13 10:47:03.299: INFO: Created: latency-svc-vxcll
Jan 13 10:47:03.300: INFO: Got endpoints: latency-svc-wdn59 [761.360941ms]
Jan 13 10:47:03.322: INFO: Created: latency-svc-2gth7
Jan 13 10:47:03.340: INFO: Got endpoints: latency-svc-ksr79 [737.948802ms]
Jan 13 10:47:03.357: INFO: Created: latency-svc-qkwjv
Jan 13 10:47:03.389: INFO: Created: latency-svc-zkl8j
Jan 13 10:47:03.414: INFO: Got endpoints: latency-svc-6gmvd [755.040073ms]
Jan 13 10:47:03.448: INFO: Created: latency-svc-bs6k8
Jan 13 10:47:03.452: INFO: Got endpoints: latency-svc-ll8vw [737.875ms]
Jan 13 10:47:03.498: INFO: Created: latency-svc-sgfg5
Jan 13 10:47:03.509: INFO: Got endpoints: latency-svc-7lsgt [772.73429ms]
Jan 13 10:47:03.530: INFO: Created: latency-svc-46dr8
Jan 13 10:47:03.596: INFO: Got endpoints: latency-svc-s7hc4 [752.108836ms]
Jan 13 10:47:03.597: INFO: Got endpoints: latency-svc-2f8n9 [800.961416ms]
Jan 13 10:47:03.613: INFO: Created: latency-svc-w2mb7
Jan 13 10:47:03.674: INFO: Got endpoints: latency-svc-zsf85 [767.579312ms]
Jan 13 10:47:03.723: INFO: Created: latency-svc-z7m28
Jan 13 10:47:03.774: INFO: Got endpoints: latency-svc-nvhgm [806.683221ms]
Jan 13 10:47:03.777: INFO: Got endpoints: latency-svc-tljwj [764.26921ms]
Jan 13 10:47:03.787: INFO: Got endpoints: latency-svc-cwjn8 [664.198405ms]
Jan 13 10:47:03.791: INFO: Created: latency-svc-cfvtr
Jan 13 10:47:03.872: INFO: Created: latency-svc-zxs5k
Jan 13 10:47:03.876: INFO: Got endpoints: latency-svc-cxcp4 [755.928774ms]
Jan 13 10:47:03.896: INFO: Got endpoints: latency-svc-vxcll [733.479321ms]
Jan 13 10:47:03.926: INFO: Created: latency-svc-qcd59
Jan 13 10:47:03.950: INFO: Got endpoints: latency-svc-2gth7 [699.777664ms]
Jan 13 10:47:03.983: INFO: Created: latency-svc-s526n
Jan 13 10:47:03.996: INFO: Created: latency-svc-cmk5c
Jan 13 10:47:03.996: INFO: Got endpoints: latency-svc-qkwjv [728.443449ms]
Jan 13 10:47:04.043: INFO: Got endpoints: latency-svc-zkl8j [743.357621ms]
Jan 13 10:47:04.045: INFO: Created: latency-svc-rj2jq
Jan 13 10:47:04.061: INFO: Created: latency-svc-jkrpb
Jan 13 10:47:04.259: INFO: Got endpoints: latency-svc-sgfg5 [845.734717ms]
Jan 13 10:47:04.260: INFO: Got endpoints: latency-svc-46dr8 [806.780748ms]
Jan 13 10:47:04.260: INFO: Got endpoints: latency-svc-bs6k8 [919.55239ms]
Jan 13 10:47:04.307: INFO: Created: latency-svc-hfzg9
Jan 13 10:47:04.307: INFO: Got endpoints: latency-svc-z7m28 [711.488447ms]
Jan 13 10:47:04.308: INFO: Got endpoints: latency-svc-w2mb7 [798.711403ms]
Jan 13 10:47:04.342: INFO: Got endpoints: latency-svc-cfvtr [744.820459ms]
Jan 13 10:47:04.342: INFO: Created: latency-svc-94s2l
Jan 13 10:47:04.375: INFO: Created: latency-svc-gnhtl
Jan 13 10:47:04.395: INFO: Got endpoints: latency-svc-zxs5k [720.457581ms]
Jan 13 10:47:04.405: INFO: Created: latency-svc-c5ll4
Jan 13 10:47:04.437: INFO: Created: latency-svc-25h48
Jan 13 10:47:04.440: INFO: Got endpoints: latency-svc-qcd59 [665.192243ms]
Jan 13 10:47:04.501: INFO: Got endpoints: latency-svc-s526n [723.980944ms]
Jan 13 10:47:04.535: INFO: Got endpoints: latency-svc-cmk5c [747.914634ms]
Jan 13 10:47:04.600: INFO: Got endpoints: latency-svc-rj2jq [723.753128ms]
Jan 13 10:47:04.640: INFO: Got endpoints: latency-svc-jkrpb [743.77561ms]
Jan 13 10:47:04.685: INFO: Got endpoints: latency-svc-hfzg9 [734.225973ms]
Jan 13 10:47:04.739: INFO: Got endpoints: latency-svc-94s2l [742.675257ms]
Jan 13 10:47:04.799: INFO: Got endpoints: latency-svc-gnhtl [755.497941ms]
Jan 13 10:47:04.839: INFO: Got endpoints: latency-svc-c5ll4 [579.218222ms]
Jan 13 10:47:04.888: INFO: Got endpoints: latency-svc-25h48 [628.875527ms]
Jan 13 10:47:04.888: INFO: Latencies: [175.466263ms 180.322939ms 181.958732ms 211.270256ms 236.778091ms 249.529333ms 258.371354ms 277.363085ms 292.189997ms 296.711001ms 297.069258ms 318.316185ms 318.56862ms 359.619348ms 360.773368ms 366.719563ms 370.05963ms 373.380987ms 381.149394ms 390.986936ms 395.45477ms 401.236703ms 404.174355ms 407.481102ms 414.360822ms 415.121781ms 421.219676ms 423.803727ms 442.293338ms 448.718785ms 453.090017ms 465.008914ms 466.847305ms 471.463641ms 474.911298ms 475.896122ms 476.692528ms 478.994016ms 481.038036ms 481.099324ms 481.407148ms 482.889211ms 486.619399ms 494.429483ms 496.835128ms 499.348509ms 503.533745ms 503.979632ms 504.693737ms 509.234454ms 510.399879ms 520.283528ms 521.349085ms 522.945551ms 532.327434ms 543.037144ms 543.420008ms 546.164255ms 548.672762ms 551.645378ms 566.317813ms 579.218222ms 591.188201ms 600.904583ms 602.16098ms 605.233788ms 619.620797ms 623.463469ms 628.875527ms 633.828933ms 638.724666ms 654.51301ms 655.702191ms 664.198405ms 665.192243ms 669.796752ms 674.031297ms 674.203092ms 675.785773ms 681.104587ms 682.101829ms 685.855734ms 685.861751ms 688.056914ms 690.6915ms 691.505489ms 695.655242ms 699.777664ms 699.92818ms 711.097619ms 711.488447ms 719.885804ms 720.457581ms 722.646387ms 723.753128ms 723.980944ms 724.337769ms 728.443449ms 728.483122ms 728.987457ms 729.368049ms 729.615423ms 731.916348ms 733.479321ms 734.225973ms 734.727973ms 735.639688ms 737.875ms 737.948802ms 739.814625ms 741.120776ms 742.590788ms 742.675257ms 743.34583ms 743.357621ms 743.77561ms 744.820459ms 744.929661ms 746.13789ms 746.599615ms 747.333427ms 747.690931ms 747.914634ms 748.303972ms 748.949515ms 750.138204ms 751.038208ms 751.274578ms 752.108836ms 752.113539ms 752.335352ms 753.147028ms 753.195985ms 754.023477ms 755.040073ms 755.497941ms 755.928774ms 757.040205ms 757.590555ms 757.740427ms 761.360941ms 763.233941ms 763.939189ms 763.959682ms 764.26921ms 765.044155ms 765.330043ms 767.362885ms 767.579312ms 768.593695ms 769.100124ms 772.73429ms 776.163702ms 777.765187ms 780.191375ms 781.429512ms 781.932811ms 782.00118ms 782.713605ms 785.199215ms 794.442751ms 796.348751ms 798.711403ms 799.973601ms 800.961416ms 806.683221ms 806.780748ms 811.936867ms 812.97444ms 815.130672ms 826.052365ms 827.622801ms 833.759547ms 845.734717ms 862.094349ms 864.579123ms 877.844981ms 919.55239ms 920.83283ms 920.886139ms 922.352057ms 928.729989ms 945.946063ms 951.419251ms 961.591145ms 980.742704ms 999.570454ms 1.025892247s 1.062826923s 1.078595913s 1.090305152s 1.105292247s 1.111329092s 1.137890895s 1.247351878s 1.253397845s 1.490537465s 1.503599153s 1.513128377s 1.548970833s]
Jan 13 10:47:04.889: INFO: 50 %ile: 729.368049ms
Jan 13 10:47:04.889: INFO: 90 %ile: 922.352057ms
Jan 13 10:47:04.889: INFO: 99 %ile: 1.513128377s
Jan 13 10:47:04.889: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 13 10:47:04.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-9866" for this suite. 01/13/23 10:47:04.944
------------------------------
• [SLOW TEST] [13.946 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:46:51.03
    Jan 13 10:46:51.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svc-latency 01/13/23 10:46:51.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:46:51.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:46:51.134
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 13 10:46:51.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-9866 01/13/23 10:46:51.165
    I0113 10:46:51.206519      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9866, replica count: 1
    I0113 10:46:52.259090      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:46:53.259374      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:46:54.260726      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0113 10:46:55.261135      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 13 10:46:55.406: INFO: Created: latency-svc-dfb6j
    Jan 13 10:46:55.421: INFO: Got endpoints: latency-svc-dfb6j [58.663042ms]
    Jan 13 10:46:55.530: INFO: Created: latency-svc-5ls7h
    Jan 13 10:46:55.571: INFO: Created: latency-svc-s8lcm
    Jan 13 10:46:55.603: INFO: Got endpoints: latency-svc-s8lcm [180.322939ms]
    Jan 13 10:46:55.604: INFO: Got endpoints: latency-svc-5ls7h [181.958732ms]
    Jan 13 10:46:55.643: INFO: Created: latency-svc-hfj4b
    Jan 13 10:46:55.663: INFO: Got endpoints: latency-svc-hfj4b [236.778091ms]
    Jan 13 10:46:55.739: INFO: Created: latency-svc-fm8vs
    Jan 13 10:46:55.790: INFO: Got endpoints: latency-svc-fm8vs [360.773368ms]
    Jan 13 10:46:55.826: INFO: Created: latency-svc-cbjq5
    Jan 13 10:46:55.827: INFO: Got endpoints: latency-svc-cbjq5 [401.236703ms]
    Jan 13 10:46:55.871: INFO: Created: latency-svc-rm2jk
    Jan 13 10:46:55.901: INFO: Got endpoints: latency-svc-rm2jk [471.463641ms]
    Jan 13 10:46:55.924: INFO: Created: latency-svc-hxjh6
    Jan 13 10:46:55.940: INFO: Got endpoints: latency-svc-hxjh6 [510.399879ms]
    Jan 13 10:46:55.955: INFO: Created: latency-svc-89sjl
    Jan 13 10:46:55.982: INFO: Got endpoints: latency-svc-89sjl [551.645378ms]
    Jan 13 10:46:55.987: INFO: Created: latency-svc-mb568
    Jan 13 10:46:56.026: INFO: Got endpoints: latency-svc-mb568 [602.16098ms]
    Jan 13 10:46:56.048: INFO: Created: latency-svc-6ptxw
    Jan 13 10:46:56.059: INFO: Got endpoints: latency-svc-6ptxw [633.828933ms]
    Jan 13 10:46:56.075: INFO: Created: latency-svc-dxgz6
    Jan 13 10:46:56.109: INFO: Got endpoints: latency-svc-dxgz6 [682.101829ms]
    Jan 13 10:46:56.135: INFO: Created: latency-svc-6mk4k
    Jan 13 10:46:56.156: INFO: Got endpoints: latency-svc-6mk4k [728.483122ms]
    Jan 13 10:46:56.186: INFO: Created: latency-svc-c9rzb
    Jan 13 10:46:56.212: INFO: Got endpoints: latency-svc-c9rzb [781.429512ms]
    Jan 13 10:46:56.269: INFO: Created: latency-svc-pnq5f
    Jan 13 10:46:56.292: INFO: Got endpoints: latency-svc-pnq5f [864.579123ms]
    Jan 13 10:46:56.318: INFO: Created: latency-svc-gjs86
    Jan 13 10:46:56.351: INFO: Got endpoints: latency-svc-gjs86 [920.886139ms]
    Jan 13 10:46:56.386: INFO: Created: latency-svc-2vdv4
    Jan 13 10:46:56.437: INFO: Got endpoints: latency-svc-2vdv4 [833.759547ms]
    Jan 13 10:46:56.450: INFO: Created: latency-svc-d8gw4
    Jan 13 10:46:56.466: INFO: Got endpoints: latency-svc-d8gw4 [862.094349ms]
    Jan 13 10:46:56.598: INFO: Created: latency-svc-6frt2
    Jan 13 10:46:56.662: INFO: Got endpoints: latency-svc-6frt2 [999.570454ms]
    Jan 13 10:46:56.674: INFO: Created: latency-svc-5k5lw
    Jan 13 10:46:56.736: INFO: Got endpoints: latency-svc-5k5lw [945.946063ms]
    Jan 13 10:46:56.759: INFO: Created: latency-svc-lntpt
    Jan 13 10:46:56.788: INFO: Got endpoints: latency-svc-lntpt [961.591145ms]
    Jan 13 10:46:56.820: INFO: Created: latency-svc-8q8pp
    Jan 13 10:46:56.829: INFO: Got endpoints: latency-svc-8q8pp [928.729989ms]
    Jan 13 10:46:56.853: INFO: Created: latency-svc-l7lkr
    Jan 13 10:46:56.861: INFO: Got endpoints: latency-svc-l7lkr [920.83283ms]
    Jan 13 10:46:56.869: INFO: Created: latency-svc-mrdh5
    Jan 13 10:46:56.905: INFO: Got endpoints: latency-svc-mrdh5 [922.352057ms]
    Jan 13 10:46:56.951: INFO: Created: latency-svc-t8rsv
    Jan 13 10:46:56.978: INFO: Got endpoints: latency-svc-t8rsv [951.419251ms]
    Jan 13 10:46:57.068: INFO: Created: latency-svc-g56bk
    Jan 13 10:46:57.122: INFO: Got endpoints: latency-svc-g56bk [1.062826923s]
    Jan 13 10:46:57.174: INFO: Created: latency-svc-sngzr
    Jan 13 10:46:57.221: INFO: Got endpoints: latency-svc-sngzr [1.111329092s]
    Jan 13 10:46:57.250: INFO: Created: latency-svc-lqmxn
    Jan 13 10:46:57.261: INFO: Got endpoints: latency-svc-lqmxn [1.105292247s]
    Jan 13 10:46:57.279: INFO: Created: latency-svc-7k2fd
    Jan 13 10:46:57.302: INFO: Got endpoints: latency-svc-7k2fd [1.090305152s]
    Jan 13 10:46:57.783: INFO: Created: latency-svc-5jhl4
    Jan 13 10:46:57.783: INFO: Created: latency-svc-86vn5
    Jan 13 10:46:57.783: INFO: Created: latency-svc-xvf94
    Jan 13 10:46:57.783: INFO: Created: latency-svc-cjc6x
    Jan 13 10:46:57.784: INFO: Created: latency-svc-jxhn6
    Jan 13 10:46:57.784: INFO: Created: latency-svc-mmcxx
    Jan 13 10:46:57.784: INFO: Created: latency-svc-h8zn6
    Jan 13 10:46:57.784: INFO: Created: latency-svc-kvjb5
    Jan 13 10:46:57.784: INFO: Created: latency-svc-lrskb
    Jan 13 10:46:57.786: INFO: Created: latency-svc-pjbh2
    Jan 13 10:46:57.786: INFO: Created: latency-svc-66hlz
    Jan 13 10:46:57.807: INFO: Created: latency-svc-6k2qb
    Jan 13 10:46:57.841: INFO: Got endpoints: latency-svc-pjbh2 [1.548970833s]
    Jan 13 10:46:57.842: INFO: Got endpoints: latency-svc-kvjb5 [980.742704ms]
    Jan 13 10:46:57.845: INFO: Created: latency-svc-nhshd
    Jan 13 10:46:57.847: INFO: Created: latency-svc-fwlgp
    Jan 13 10:46:57.848: INFO: Created: latency-svc-sdc9b
    Jan 13 10:46:57.855: INFO: Got endpoints: latency-svc-cjc6x [1.503599153s]
    Jan 13 10:46:57.855: INFO: Got endpoints: latency-svc-jxhn6 [1.025892247s]
    Jan 13 10:46:57.856: INFO: Got endpoints: latency-svc-66hlz [877.844981ms]
    Jan 13 10:46:57.916: INFO: Got endpoints: latency-svc-5jhl4 [1.253397845s]
    Jan 13 10:46:57.918: INFO: Got endpoints: latency-svc-mmcxx [796.348751ms]
    Jan 13 10:46:57.926: INFO: Got endpoints: latency-svc-86vn5 [1.137890895s]
    Jan 13 10:46:57.927: INFO: Got endpoints: latency-svc-xvf94 [623.463469ms]
    Jan 13 10:46:57.928: INFO: Got endpoints: latency-svc-h8zn6 [1.490537465s]
    Jan 13 10:46:57.980: INFO: Got endpoints: latency-svc-nhshd [1.513128377s]
    Jan 13 10:46:57.981: INFO: Got endpoints: latency-svc-lrskb [719.885804ms]
    Jan 13 10:46:57.984: INFO: Got endpoints: latency-svc-6k2qb [1.247351878s]
    Jan 13 10:46:57.984: INFO: Got endpoints: latency-svc-fwlgp [763.233941ms]
    Jan 13 10:46:57.984: INFO: Got endpoints: latency-svc-sdc9b [1.078595913s]
    Jan 13 10:46:58.007: INFO: Created: latency-svc-7dm2s
    Jan 13 10:46:58.017: INFO: Got endpoints: latency-svc-7dm2s [175.466263ms]
    Jan 13 10:46:58.039: INFO: Created: latency-svc-r4k9z
    Jan 13 10:46:58.053: INFO: Got endpoints: latency-svc-r4k9z [211.270256ms]
    Jan 13 10:46:58.106: INFO: Created: latency-svc-rgmdv
    Jan 13 10:46:58.114: INFO: Got endpoints: latency-svc-rgmdv [258.371354ms]
    Jan 13 10:46:58.126: INFO: Created: latency-svc-5mgqk
    Jan 13 10:46:58.132: INFO: Got endpoints: latency-svc-5mgqk [277.363085ms]
    Jan 13 10:46:58.142: INFO: Created: latency-svc-tj7gf
    Jan 13 10:46:58.152: INFO: Got endpoints: latency-svc-tj7gf [296.711001ms]
    Jan 13 10:46:58.156: INFO: Created: latency-svc-5flf6
    Jan 13 10:46:58.165: INFO: Got endpoints: latency-svc-5flf6 [249.529333ms]
    Jan 13 10:46:58.175: INFO: Created: latency-svc-l8glf
    Jan 13 10:46:58.210: INFO: Created: latency-svc-d6bl8
    Jan 13 10:46:58.211: INFO: Got endpoints: latency-svc-l8glf [292.189997ms]
    Jan 13 10:46:58.223: INFO: Got endpoints: latency-svc-d6bl8 [297.069258ms]
    Jan 13 10:46:58.243: INFO: Created: latency-svc-mtnp2
    Jan 13 10:46:58.246: INFO: Got endpoints: latency-svc-mtnp2 [318.316185ms]
    Jan 13 10:46:58.265: INFO: Created: latency-svc-7swmz
    Jan 13 10:46:58.287: INFO: Created: latency-svc-kljzs
    Jan 13 10:46:58.287: INFO: Got endpoints: latency-svc-7swmz [359.619348ms]
    Jan 13 10:46:58.298: INFO: Got endpoints: latency-svc-kljzs [318.56862ms]
    Jan 13 10:46:58.317: INFO: Created: latency-svc-l7jbg
    Jan 13 10:46:58.351: INFO: Got endpoints: latency-svc-l7jbg [370.05963ms]
    Jan 13 10:46:58.358: INFO: Created: latency-svc-pbzpb
    Jan 13 10:46:58.365: INFO: Got endpoints: latency-svc-pbzpb [381.149394ms]
    Jan 13 10:46:58.386: INFO: Created: latency-svc-p22jx
    Jan 13 10:46:58.399: INFO: Created: latency-svc-k5667
    Jan 13 10:46:58.426: INFO: Got endpoints: latency-svc-p22jx [442.293338ms]
    Jan 13 10:46:58.428: INFO: Created: latency-svc-zsn2p
    Jan 13 10:46:58.433: INFO: Got endpoints: latency-svc-k5667 [448.718785ms]
    Jan 13 10:46:58.439: INFO: Got endpoints: latency-svc-zsn2p [421.219676ms]
    Jan 13 10:46:58.451: INFO: Created: latency-svc-g6cxr
    Jan 13 10:46:58.469: INFO: Got endpoints: latency-svc-g6cxr [415.121781ms]
    Jan 13 10:46:58.471: INFO: Created: latency-svc-2qz7t
    Jan 13 10:46:58.481: INFO: Got endpoints: latency-svc-2qz7t [366.719563ms]
    Jan 13 10:46:58.499: INFO: Created: latency-svc-8vzdp
    Jan 13 10:46:58.506: INFO: Got endpoints: latency-svc-8vzdp [373.380987ms]
    Jan 13 10:46:58.523: INFO: Created: latency-svc-b5wpf
    Jan 13 10:46:58.543: INFO: Got endpoints: latency-svc-b5wpf [390.986936ms]
    Jan 13 10:46:58.557: INFO: Created: latency-svc-bll9c
    Jan 13 10:46:58.570: INFO: Got endpoints: latency-svc-bll9c [404.174355ms]
    Jan 13 10:46:58.582: INFO: Created: latency-svc-mft6f
    Jan 13 10:46:58.606: INFO: Got endpoints: latency-svc-mft6f [395.45477ms]
    Jan 13 10:46:58.795: INFO: Created: latency-svc-fgrms
    Jan 13 10:46:58.815: INFO: Got endpoints: latency-svc-fgrms [591.188201ms]
    Jan 13 10:46:58.842: INFO: Created: latency-svc-dlrwj
    Jan 13 10:46:58.847: INFO: Got endpoints: latency-svc-dlrwj [600.904583ms]
    Jan 13 10:46:58.885: INFO: Created: latency-svc-t4sbt
    Jan 13 10:46:58.907: INFO: Got endpoints: latency-svc-t4sbt [619.620797ms]
    Jan 13 10:46:58.940: INFO: Created: latency-svc-c2qr6
    Jan 13 10:46:58.972: INFO: Got endpoints: latency-svc-c2qr6 [674.203092ms]
    Jan 13 10:46:59.004: INFO: Created: latency-svc-6f5rc
    Jan 13 10:46:59.025: INFO: Got endpoints: latency-svc-6f5rc [674.031297ms]
    Jan 13 10:46:59.033: INFO: Created: latency-svc-ssgjm
    Jan 13 10:46:59.046: INFO: Got endpoints: latency-svc-ssgjm [681.104587ms]
    Jan 13 10:46:59.059: INFO: Created: latency-svc-96cp7
    Jan 13 10:46:59.082: INFO: Got endpoints: latency-svc-96cp7 [655.702191ms]
    Jan 13 10:46:59.116: INFO: Created: latency-svc-gh7zr
    Jan 13 10:46:59.124: INFO: Got endpoints: latency-svc-gh7zr [690.6915ms]
    Jan 13 10:46:59.141: INFO: Created: latency-svc-h2n66
    Jan 13 10:46:59.181: INFO: Got endpoints: latency-svc-h2n66 [742.590788ms]
    Jan 13 10:46:59.188: INFO: Created: latency-svc-rblrm
    Jan 13 10:46:59.223: INFO: Got endpoints: latency-svc-rblrm [754.023477ms]
    Jan 13 10:46:59.249: INFO: Created: latency-svc-58wsj
    Jan 13 10:46:59.276: INFO: Got endpoints: latency-svc-58wsj [794.442751ms]
    Jan 13 10:46:59.288: INFO: Created: latency-svc-cmgjs
    Jan 13 10:46:59.288: INFO: Got endpoints: latency-svc-cmgjs [782.713605ms]
    Jan 13 10:46:59.301: INFO: Created: latency-svc-nsztd
    Jan 13 10:46:59.312: INFO: Got endpoints: latency-svc-nsztd [769.100124ms]
    Jan 13 10:46:59.317: INFO: Created: latency-svc-g65bf
    Jan 13 10:46:59.327: INFO: Got endpoints: latency-svc-g65bf [757.590555ms]
    Jan 13 10:46:59.336: INFO: Created: latency-svc-vfcml
    Jan 13 10:46:59.348: INFO: Got endpoints: latency-svc-vfcml [741.120776ms]
    Jan 13 10:46:59.360: INFO: Created: latency-svc-r595f
    Jan 13 10:46:59.363: INFO: Got endpoints: latency-svc-r595f [548.672762ms]
    Jan 13 10:46:59.380: INFO: Created: latency-svc-w5rgh
    Jan 13 10:46:59.393: INFO: Got endpoints: latency-svc-w5rgh [546.164255ms]
    Jan 13 10:46:59.411: INFO: Created: latency-svc-gs2gs
    Jan 13 10:46:59.451: INFO: Got endpoints: latency-svc-gs2gs [543.420008ms]
    Jan 13 10:46:59.468: INFO: Created: latency-svc-rrr2h
    Jan 13 10:46:59.482: INFO: Got endpoints: latency-svc-rrr2h [509.234454ms]
    Jan 13 10:46:59.490: INFO: Created: latency-svc-8lfwg
    Jan 13 10:46:59.507: INFO: Got endpoints: latency-svc-8lfwg [481.099324ms]
    Jan 13 10:46:59.515: INFO: Created: latency-svc-97bz2
    Jan 13 10:46:59.528: INFO: Got endpoints: latency-svc-97bz2 [481.407148ms]
    Jan 13 10:46:59.542: INFO: Created: latency-svc-cvhmf
    Jan 13 10:46:59.559: INFO: Got endpoints: latency-svc-cvhmf [476.692528ms]
    Jan 13 10:46:59.566: INFO: Created: latency-svc-dslfp
    Jan 13 10:46:59.577: INFO: Got endpoints: latency-svc-dslfp [453.090017ms]
    Jan 13 10:46:59.598: INFO: Created: latency-svc-4kknt
    Jan 13 10:46:59.605: INFO: Got endpoints: latency-svc-4kknt [423.803727ms]
    Jan 13 10:46:59.624: INFO: Created: latency-svc-4ndj2
    Jan 13 10:46:59.630: INFO: Got endpoints: latency-svc-4ndj2 [407.481102ms]
    Jan 13 10:46:59.661: INFO: Created: latency-svc-sbksg
    Jan 13 10:46:59.690: INFO: Got endpoints: latency-svc-sbksg [414.360822ms]
    Jan 13 10:46:59.711: INFO: Created: latency-svc-sz2xb
    Jan 13 10:46:59.775: INFO: Got endpoints: latency-svc-sz2xb [486.619399ms]
    Jan 13 10:46:59.796: INFO: Created: latency-svc-v6ttx
    Jan 13 10:46:59.817: INFO: Got endpoints: latency-svc-v6ttx [503.979632ms]
    Jan 13 10:46:59.835: INFO: Created: latency-svc-klt2m
    Jan 13 10:46:59.850: INFO: Got endpoints: latency-svc-klt2m [522.945551ms]
    Jan 13 10:46:59.861: INFO: Created: latency-svc-k7mdk
    Jan 13 10:46:59.868: INFO: Got endpoints: latency-svc-k7mdk [520.283528ms]
    Jan 13 10:46:59.887: INFO: Created: latency-svc-7qxh5
    Jan 13 10:46:59.896: INFO: Got endpoints: latency-svc-7qxh5 [532.327434ms]
    Jan 13 10:46:59.904: INFO: Created: latency-svc-26ccf
    Jan 13 10:46:59.914: INFO: Got endpoints: latency-svc-26ccf [521.349085ms]
    Jan 13 10:46:59.928: INFO: Created: latency-svc-flmzw
    Jan 13 10:46:59.948: INFO: Got endpoints: latency-svc-flmzw [496.835128ms]
    Jan 13 10:46:59.953: INFO: Created: latency-svc-nr2n2
    Jan 13 10:46:59.965: INFO: Got endpoints: latency-svc-nr2n2 [482.889211ms]
    Jan 13 10:46:59.972: INFO: Created: latency-svc-lmplg
    Jan 13 10:46:59.988: INFO: Got endpoints: latency-svc-lmplg [481.038036ms]
    Jan 13 10:46:59.999: INFO: Created: latency-svc-r4zpv
    Jan 13 10:47:00.007: INFO: Got endpoints: latency-svc-r4zpv [478.994016ms]
    Jan 13 10:47:00.028: INFO: Created: latency-svc-nnzbq
    Jan 13 10:47:00.034: INFO: Got endpoints: latency-svc-nnzbq [474.911298ms]
    Jan 13 10:47:00.037: INFO: Created: latency-svc-wrtzg
    Jan 13 10:47:00.053: INFO: Got endpoints: latency-svc-wrtzg [475.896122ms]
    Jan 13 10:47:00.100: INFO: Created: latency-svc-s76l9
    Jan 13 10:47:00.109: INFO: Got endpoints: latency-svc-s76l9 [503.533745ms]
    Jan 13 10:47:00.125: INFO: Created: latency-svc-csj5b
    Jan 13 10:47:00.135: INFO: Got endpoints: latency-svc-csj5b [504.693737ms]
    Jan 13 10:47:00.164: INFO: Created: latency-svc-67pfb
    Jan 13 10:47:00.166: INFO: Created: latency-svc-8fbf8
    Jan 13 10:47:00.185: INFO: Got endpoints: latency-svc-67pfb [494.429483ms]
    Jan 13 10:47:00.185: INFO: Created: latency-svc-gxvzp
    Jan 13 10:47:00.240: INFO: Got endpoints: latency-svc-8fbf8 [465.008914ms]
    Jan 13 10:47:00.241: INFO: Created: latency-svc-t28nc
    Jan 13 10:47:00.265: INFO: Created: latency-svc-2srxw
    Jan 13 10:47:00.283: INFO: Got endpoints: latency-svc-gxvzp [466.847305ms]
    Jan 13 10:47:00.295: INFO: Created: latency-svc-rqfxz
    Jan 13 10:47:00.321: INFO: Created: latency-svc-hztpj
    Jan 13 10:47:00.350: INFO: Got endpoints: latency-svc-t28nc [499.348509ms]
    Jan 13 10:47:00.359: INFO: Created: latency-svc-gtmdv
    Jan 13 10:47:00.411: INFO: Got endpoints: latency-svc-2srxw [543.037144ms]
    Jan 13 10:47:00.413: INFO: Created: latency-svc-zdr5g
    Jan 13 10:47:00.441: INFO: Created: latency-svc-4j2vm
    Jan 13 10:47:00.462: INFO: Got endpoints: latency-svc-rqfxz [566.317813ms]
    Jan 13 10:47:00.484: INFO: Created: latency-svc-r7fsv
    Jan 13 10:47:00.520: INFO: Got endpoints: latency-svc-hztpj [605.233788ms]
    Jan 13 10:47:00.522: INFO: Created: latency-svc-gqmk6
    Jan 13 10:47:00.586: INFO: Got endpoints: latency-svc-gtmdv [638.724666ms]
    Jan 13 10:47:00.610: INFO: Created: latency-svc-vw82s
    Jan 13 10:47:00.635: INFO: Got endpoints: latency-svc-zdr5g [669.796752ms]
    Jan 13 10:47:00.643: INFO: Got endpoints: latency-svc-4j2vm [654.51301ms]
    Jan 13 10:47:00.668: INFO: Created: latency-svc-s4brk
    Jan 13 10:47:00.671: INFO: Created: latency-svc-8ct77
    Jan 13 10:47:00.693: INFO: Got endpoints: latency-svc-r7fsv [685.855734ms]
    Jan 13 10:47:00.753: INFO: Created: latency-svc-jlrvm
    Jan 13 10:47:00.783: INFO: Got endpoints: latency-svc-gqmk6 [748.949515ms]
    Jan 13 10:47:00.800: INFO: Got endpoints: latency-svc-vw82s [746.599615ms]
    Jan 13 10:47:00.802: INFO: Created: latency-svc-jh2hb
    Jan 13 10:47:00.834: INFO: Created: latency-svc-5gc9q
    Jan 13 10:47:00.839: INFO: Got endpoints: latency-svc-s4brk [729.368049ms]
    Jan 13 10:47:00.885: INFO: Created: latency-svc-ctn4m
    Jan 13 10:47:00.900: INFO: Got endpoints: latency-svc-8ct77 [763.939189ms]
    Jan 13 10:47:00.930: INFO: Created: latency-svc-hz6nd
    Jan 13 10:47:00.979: INFO: Created: latency-svc-cgdw8
    Jan 13 10:47:00.997: INFO: Got endpoints: latency-svc-jlrvm [811.936867ms]
    Jan 13 10:47:01.066: INFO: Got endpoints: latency-svc-jh2hb [826.052365ms]
    Jan 13 10:47:01.098: INFO: Created: latency-svc-csfdz
    Jan 13 10:47:01.099: INFO: Got endpoints: latency-svc-5gc9q [815.130672ms]
    Jan 13 10:47:01.114: INFO: Got endpoints: latency-svc-ctn4m [763.959682ms]
    Jan 13 10:47:01.153: INFO: Created: latency-svc-2k57m
    Jan 13 10:47:01.164: INFO: Got endpoints: latency-svc-hz6nd [752.335352ms]
    Jan 13 10:47:01.172: INFO: Created: latency-svc-gtzcx
    Jan 13 10:47:01.206: INFO: Got endpoints: latency-svc-cgdw8 [743.34583ms]
    Jan 13 10:47:01.207: INFO: Created: latency-svc-bgj5f
    Jan 13 10:47:01.243: INFO: Got endpoints: latency-svc-csfdz [722.646387ms]
    Jan 13 10:47:01.262: INFO: Created: latency-svc-msd7n
    Jan 13 10:47:01.363: INFO: Got endpoints: latency-svc-2k57m [776.163702ms]
    Jan 13 10:47:01.364: INFO: Got endpoints: latency-svc-gtzcx [728.987457ms]
    Jan 13 10:47:01.368: INFO: Created: latency-svc-hfnft
    Jan 13 10:47:01.382: INFO: Created: latency-svc-kgjq8
    Jan 13 10:47:01.391: INFO: Got endpoints: latency-svc-bgj5f [748.303972ms]
    Jan 13 10:47:01.395: INFO: Created: latency-svc-bmgrw
    Jan 13 10:47:01.443: INFO: Created: latency-svc-ftfkp
    Jan 13 10:47:01.450: INFO: Got endpoints: latency-svc-msd7n [757.040205ms]
    Jan 13 10:47:01.469: INFO: Created: latency-svc-9zfdd
    Jan 13 10:47:01.567: INFO: Created: latency-svc-lh5dg
    Jan 13 10:47:01.568: INFO: Got endpoints: latency-svc-kgjq8 [767.362885ms]
    Jan 13 10:47:01.568: INFO: Got endpoints: latency-svc-hfnft [785.199215ms]
    Jan 13 10:47:01.592: INFO: Got endpoints: latency-svc-bmgrw [753.195985ms]
    Jan 13 10:47:01.593: INFO: Created: latency-svc-8ffwm
    Jan 13 10:47:01.639: INFO: Created: latency-svc-kchcw
    Jan 13 10:47:01.646: INFO: Got endpoints: latency-svc-ftfkp [746.13789ms]
    Jan 13 10:47:01.656: INFO: Created: latency-svc-98znr
    Jan 13 10:47:01.676: INFO: Created: latency-svc-r2gt6
    Jan 13 10:47:01.685: INFO: Got endpoints: latency-svc-9zfdd [688.056914ms]
    Jan 13 10:47:01.701: INFO: Created: latency-svc-x8jtj
    Jan 13 10:47:01.736: INFO: Created: latency-svc-s6wjx
    Jan 13 10:47:01.742: INFO: Got endpoints: latency-svc-lh5dg [675.785773ms]
    Jan 13 10:47:01.762: INFO: Created: latency-svc-8jljt
    Jan 13 10:47:01.780: INFO: Created: latency-svc-nkmtp
    Jan 13 10:47:01.791: INFO: Got endpoints: latency-svc-8ffwm [691.505489ms]
    Jan 13 10:47:01.827: INFO: Created: latency-svc-6qtk5
    Jan 13 10:47:01.849: INFO: Got endpoints: latency-svc-kchcw [734.727973ms]
    Jan 13 10:47:01.863: INFO: Created: latency-svc-87sxp
    Jan 13 10:47:01.915: INFO: Got endpoints: latency-svc-98znr [751.038208ms]
    Jan 13 10:47:01.944: INFO: Created: latency-svc-9vqdj
    Jan 13 10:47:02.019: INFO: Got endpoints: latency-svc-r2gt6 [812.97444ms]
    Jan 13 10:47:02.021: INFO: Created: latency-svc-xt5sm
    Jan 13 10:47:02.025: INFO: Got endpoints: latency-svc-x8jtj [781.932811ms]
    Jan 13 10:47:02.064: INFO: Got endpoints: latency-svc-s6wjx [699.92818ms]
    Jan 13 10:47:02.092: INFO: Got endpoints: latency-svc-8jljt [729.615423ms]
    Jan 13 10:47:02.104: INFO: Created: latency-svc-csthh
    Jan 13 10:47:02.141: INFO: Got endpoints: latency-svc-nkmtp [750.138204ms]
    Jan 13 10:47:02.146: INFO: Created: latency-svc-7p4cz
    Jan 13 10:47:02.163: INFO: Created: latency-svc-s5l5l
    Jan 13 10:47:02.176: INFO: Created: latency-svc-rh5kp
    Jan 13 10:47:02.185: INFO: Got endpoints: latency-svc-6qtk5 [735.639688ms]
    Jan 13 10:47:02.193: INFO: Created: latency-svc-8cv2k
    Jan 13 10:47:02.225: INFO: Created: latency-svc-mk87b
    Jan 13 10:47:02.254: INFO: Got endpoints: latency-svc-87sxp [685.861751ms]
    Jan 13 10:47:02.260: INFO: Created: latency-svc-wkp2b
    Jan 13 10:47:02.281: INFO: Created: latency-svc-b8vgh
    Jan 13 10:47:02.292: INFO: Got endpoints: latency-svc-9vqdj [724.337769ms]
    Jan 13 10:47:02.294: INFO: Created: latency-svc-tzw4f
    Jan 13 10:47:02.330: INFO: Created: latency-svc-qzt72
    Jan 13 10:47:02.366: INFO: Created: latency-svc-4h85x
    Jan 13 10:47:02.370: INFO: Got endpoints: latency-svc-xt5sm [777.765187ms]
    Jan 13 10:47:02.394: INFO: Got endpoints: latency-svc-csthh [747.333427ms]
    Jan 13 10:47:02.416: INFO: Created: latency-svc-56wd4
    Jan 13 10:47:02.450: INFO: Got endpoints: latency-svc-7p4cz [765.330043ms]
    Jan 13 10:47:02.473: INFO: Created: latency-svc-xjxdv
    Jan 13 10:47:02.487: INFO: Got endpoints: latency-svc-s5l5l [744.929661ms]
    Jan 13 10:47:02.496: INFO: Created: latency-svc-nrldl
    Jan 13 10:47:02.538: INFO: Created: latency-svc-rtkt5
    Jan 13 10:47:02.538: INFO: Got endpoints: latency-svc-rh5kp [747.690931ms]
    Jan 13 10:47:02.565: INFO: Created: latency-svc-8w6dl
    Jan 13 10:47:02.601: INFO: Created: latency-svc-6swzh
    Jan 13 10:47:02.602: INFO: Got endpoints: latency-svc-8cv2k [753.147028ms]
    Jan 13 10:47:02.608: INFO: Created: latency-svc-5tlbg
    Jan 13 10:47:02.622: INFO: Created: latency-svc-wdn59
    Jan 13 10:47:02.658: INFO: Got endpoints: latency-svc-mk87b [739.814625ms]
    Jan 13 10:47:02.672: INFO: Created: latency-svc-ksr79
    Jan 13 10:47:02.714: INFO: Created: latency-svc-6gmvd
    Jan 13 10:47:02.714: INFO: Got endpoints: latency-svc-wkp2b [695.655242ms]
    Jan 13 10:47:02.736: INFO: Got endpoints: latency-svc-b8vgh [711.097619ms]
    Jan 13 10:47:02.775: INFO: Created: latency-svc-ll8vw
    Jan 13 10:47:02.776: INFO: Created: latency-svc-7lsgt
    Jan 13 10:47:02.796: INFO: Got endpoints: latency-svc-tzw4f [731.916348ms]
    Jan 13 10:47:02.826: INFO: Created: latency-svc-2f8n9
    Jan 13 10:47:02.844: INFO: Got endpoints: latency-svc-qzt72 [751.274578ms]
    Jan 13 10:47:02.907: INFO: Got endpoints: latency-svc-4h85x [765.044155ms]
    Jan 13 10:47:02.914: INFO: Created: latency-svc-s7hc4
    Jan 13 10:47:02.968: INFO: Got endpoints: latency-svc-56wd4 [782.00118ms]
    Jan 13 10:47:02.969: INFO: Created: latency-svc-zsf85
    Jan 13 10:47:03.012: INFO: Got endpoints: latency-svc-xjxdv [757.740427ms]
    Jan 13 10:47:03.120: INFO: Got endpoints: latency-svc-nrldl [827.622801ms]
    Jan 13 10:47:03.123: INFO: Got endpoints: latency-svc-rtkt5 [752.113539ms]
    Jan 13 10:47:03.162: INFO: Got endpoints: latency-svc-8w6dl [768.593695ms]
    Jan 13 10:47:03.249: INFO: Created: latency-svc-tljwj
    Jan 13 10:47:03.249: INFO: Created: latency-svc-nvhgm
    Jan 13 10:47:03.250: INFO: Got endpoints: latency-svc-6swzh [799.973601ms]
    Jan 13 10:47:03.268: INFO: Got endpoints: latency-svc-5tlbg [780.191375ms]
    Jan 13 10:47:03.268: INFO: Created: latency-svc-cwjn8
    Jan 13 10:47:03.277: INFO: Created: latency-svc-cxcp4
    Jan 13 10:47:03.299: INFO: Created: latency-svc-vxcll
    Jan 13 10:47:03.300: INFO: Got endpoints: latency-svc-wdn59 [761.360941ms]
    Jan 13 10:47:03.322: INFO: Created: latency-svc-2gth7
    Jan 13 10:47:03.340: INFO: Got endpoints: latency-svc-ksr79 [737.948802ms]
    Jan 13 10:47:03.357: INFO: Created: latency-svc-qkwjv
    Jan 13 10:47:03.389: INFO: Created: latency-svc-zkl8j
    Jan 13 10:47:03.414: INFO: Got endpoints: latency-svc-6gmvd [755.040073ms]
    Jan 13 10:47:03.448: INFO: Created: latency-svc-bs6k8
    Jan 13 10:47:03.452: INFO: Got endpoints: latency-svc-ll8vw [737.875ms]
    Jan 13 10:47:03.498: INFO: Created: latency-svc-sgfg5
    Jan 13 10:47:03.509: INFO: Got endpoints: latency-svc-7lsgt [772.73429ms]
    Jan 13 10:47:03.530: INFO: Created: latency-svc-46dr8
    Jan 13 10:47:03.596: INFO: Got endpoints: latency-svc-s7hc4 [752.108836ms]
    Jan 13 10:47:03.597: INFO: Got endpoints: latency-svc-2f8n9 [800.961416ms]
    Jan 13 10:47:03.613: INFO: Created: latency-svc-w2mb7
    Jan 13 10:47:03.674: INFO: Got endpoints: latency-svc-zsf85 [767.579312ms]
    Jan 13 10:47:03.723: INFO: Created: latency-svc-z7m28
    Jan 13 10:47:03.774: INFO: Got endpoints: latency-svc-nvhgm [806.683221ms]
    Jan 13 10:47:03.777: INFO: Got endpoints: latency-svc-tljwj [764.26921ms]
    Jan 13 10:47:03.787: INFO: Got endpoints: latency-svc-cwjn8 [664.198405ms]
    Jan 13 10:47:03.791: INFO: Created: latency-svc-cfvtr
    Jan 13 10:47:03.872: INFO: Created: latency-svc-zxs5k
    Jan 13 10:47:03.876: INFO: Got endpoints: latency-svc-cxcp4 [755.928774ms]
    Jan 13 10:47:03.896: INFO: Got endpoints: latency-svc-vxcll [733.479321ms]
    Jan 13 10:47:03.926: INFO: Created: latency-svc-qcd59
    Jan 13 10:47:03.950: INFO: Got endpoints: latency-svc-2gth7 [699.777664ms]
    Jan 13 10:47:03.983: INFO: Created: latency-svc-s526n
    Jan 13 10:47:03.996: INFO: Created: latency-svc-cmk5c
    Jan 13 10:47:03.996: INFO: Got endpoints: latency-svc-qkwjv [728.443449ms]
    Jan 13 10:47:04.043: INFO: Got endpoints: latency-svc-zkl8j [743.357621ms]
    Jan 13 10:47:04.045: INFO: Created: latency-svc-rj2jq
    Jan 13 10:47:04.061: INFO: Created: latency-svc-jkrpb
    Jan 13 10:47:04.259: INFO: Got endpoints: latency-svc-sgfg5 [845.734717ms]
    Jan 13 10:47:04.260: INFO: Got endpoints: latency-svc-46dr8 [806.780748ms]
    Jan 13 10:47:04.260: INFO: Got endpoints: latency-svc-bs6k8 [919.55239ms]
    Jan 13 10:47:04.307: INFO: Created: latency-svc-hfzg9
    Jan 13 10:47:04.307: INFO: Got endpoints: latency-svc-z7m28 [711.488447ms]
    Jan 13 10:47:04.308: INFO: Got endpoints: latency-svc-w2mb7 [798.711403ms]
    Jan 13 10:47:04.342: INFO: Got endpoints: latency-svc-cfvtr [744.820459ms]
    Jan 13 10:47:04.342: INFO: Created: latency-svc-94s2l
    Jan 13 10:47:04.375: INFO: Created: latency-svc-gnhtl
    Jan 13 10:47:04.395: INFO: Got endpoints: latency-svc-zxs5k [720.457581ms]
    Jan 13 10:47:04.405: INFO: Created: latency-svc-c5ll4
    Jan 13 10:47:04.437: INFO: Created: latency-svc-25h48
    Jan 13 10:47:04.440: INFO: Got endpoints: latency-svc-qcd59 [665.192243ms]
    Jan 13 10:47:04.501: INFO: Got endpoints: latency-svc-s526n [723.980944ms]
    Jan 13 10:47:04.535: INFO: Got endpoints: latency-svc-cmk5c [747.914634ms]
    Jan 13 10:47:04.600: INFO: Got endpoints: latency-svc-rj2jq [723.753128ms]
    Jan 13 10:47:04.640: INFO: Got endpoints: latency-svc-jkrpb [743.77561ms]
    Jan 13 10:47:04.685: INFO: Got endpoints: latency-svc-hfzg9 [734.225973ms]
    Jan 13 10:47:04.739: INFO: Got endpoints: latency-svc-94s2l [742.675257ms]
    Jan 13 10:47:04.799: INFO: Got endpoints: latency-svc-gnhtl [755.497941ms]
    Jan 13 10:47:04.839: INFO: Got endpoints: latency-svc-c5ll4 [579.218222ms]
    Jan 13 10:47:04.888: INFO: Got endpoints: latency-svc-25h48 [628.875527ms]
    Jan 13 10:47:04.888: INFO: Latencies: [175.466263ms 180.322939ms 181.958732ms 211.270256ms 236.778091ms 249.529333ms 258.371354ms 277.363085ms 292.189997ms 296.711001ms 297.069258ms 318.316185ms 318.56862ms 359.619348ms 360.773368ms 366.719563ms 370.05963ms 373.380987ms 381.149394ms 390.986936ms 395.45477ms 401.236703ms 404.174355ms 407.481102ms 414.360822ms 415.121781ms 421.219676ms 423.803727ms 442.293338ms 448.718785ms 453.090017ms 465.008914ms 466.847305ms 471.463641ms 474.911298ms 475.896122ms 476.692528ms 478.994016ms 481.038036ms 481.099324ms 481.407148ms 482.889211ms 486.619399ms 494.429483ms 496.835128ms 499.348509ms 503.533745ms 503.979632ms 504.693737ms 509.234454ms 510.399879ms 520.283528ms 521.349085ms 522.945551ms 532.327434ms 543.037144ms 543.420008ms 546.164255ms 548.672762ms 551.645378ms 566.317813ms 579.218222ms 591.188201ms 600.904583ms 602.16098ms 605.233788ms 619.620797ms 623.463469ms 628.875527ms 633.828933ms 638.724666ms 654.51301ms 655.702191ms 664.198405ms 665.192243ms 669.796752ms 674.031297ms 674.203092ms 675.785773ms 681.104587ms 682.101829ms 685.855734ms 685.861751ms 688.056914ms 690.6915ms 691.505489ms 695.655242ms 699.777664ms 699.92818ms 711.097619ms 711.488447ms 719.885804ms 720.457581ms 722.646387ms 723.753128ms 723.980944ms 724.337769ms 728.443449ms 728.483122ms 728.987457ms 729.368049ms 729.615423ms 731.916348ms 733.479321ms 734.225973ms 734.727973ms 735.639688ms 737.875ms 737.948802ms 739.814625ms 741.120776ms 742.590788ms 742.675257ms 743.34583ms 743.357621ms 743.77561ms 744.820459ms 744.929661ms 746.13789ms 746.599615ms 747.333427ms 747.690931ms 747.914634ms 748.303972ms 748.949515ms 750.138204ms 751.038208ms 751.274578ms 752.108836ms 752.113539ms 752.335352ms 753.147028ms 753.195985ms 754.023477ms 755.040073ms 755.497941ms 755.928774ms 757.040205ms 757.590555ms 757.740427ms 761.360941ms 763.233941ms 763.939189ms 763.959682ms 764.26921ms 765.044155ms 765.330043ms 767.362885ms 767.579312ms 768.593695ms 769.100124ms 772.73429ms 776.163702ms 777.765187ms 780.191375ms 781.429512ms 781.932811ms 782.00118ms 782.713605ms 785.199215ms 794.442751ms 796.348751ms 798.711403ms 799.973601ms 800.961416ms 806.683221ms 806.780748ms 811.936867ms 812.97444ms 815.130672ms 826.052365ms 827.622801ms 833.759547ms 845.734717ms 862.094349ms 864.579123ms 877.844981ms 919.55239ms 920.83283ms 920.886139ms 922.352057ms 928.729989ms 945.946063ms 951.419251ms 961.591145ms 980.742704ms 999.570454ms 1.025892247s 1.062826923s 1.078595913s 1.090305152s 1.105292247s 1.111329092s 1.137890895s 1.247351878s 1.253397845s 1.490537465s 1.503599153s 1.513128377s 1.548970833s]
    Jan 13 10:47:04.889: INFO: 50 %ile: 729.368049ms
    Jan 13 10:47:04.889: INFO: 90 %ile: 922.352057ms
    Jan 13 10:47:04.889: INFO: 99 %ile: 1.513128377s
    Jan 13 10:47:04.889: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:47:04.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-9866" for this suite. 01/13/23 10:47:04.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:47:04.993
Jan 13 10:47:04.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename namespaces 01/13/23 10:47:05.004
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:47:05.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:47:05.107
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/13/23 10:47:05.153
STEP: patching the Namespace 01/13/23 10:47:05.219
STEP: get the Namespace and ensuring it has the label 01/13/23 10:47:05.256
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:47:05.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6810" for this suite. 01/13/23 10:47:05.304
STEP: Destroying namespace "nspatchtest-aab34d41-febb-4d62-8c5c-6f794e7ae07b-4846" for this suite. 01/13/23 10:47:05.349
------------------------------
• [0.397 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:47:04.993
    Jan 13 10:47:04.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename namespaces 01/13/23 10:47:05.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:47:05.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:47:05.107
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/13/23 10:47:05.153
    STEP: patching the Namespace 01/13/23 10:47:05.219
    STEP: get the Namespace and ensuring it has the label 01/13/23 10:47:05.256
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:47:05.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6810" for this suite. 01/13/23 10:47:05.304
    STEP: Destroying namespace "nspatchtest-aab34d41-febb-4d62-8c5c-6f794e7ae07b-4846" for this suite. 01/13/23 10:47:05.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:47:05.396
Jan 13 10:47:05.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename container-probe 01/13/23 10:47:05.401
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:47:05.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:47:05.515
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-2e440552-0158-413b-8bbc-eba95696941d in namespace container-probe-469 01/13/23 10:47:05.54
Jan 13 10:47:05.572: INFO: Waiting up to 5m0s for pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d" in namespace "container-probe-469" to be "not pending"
Jan 13 10:47:05.635: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d": Phase="Pending", Reason="", readiness=false. Elapsed: 62.722058ms
Jan 13 10:47:07.645: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072882742s
Jan 13 10:47:09.684: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d": Phase="Running", Reason="", readiness=true. Elapsed: 4.11228229s
Jan 13 10:47:09.685: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d" satisfied condition "not pending"
Jan 13 10:47:09.685: INFO: Started pod test-webserver-2e440552-0158-413b-8bbc-eba95696941d in namespace container-probe-469
STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:47:09.685
Jan 13 10:47:09.732: INFO: Initial restart count of pod test-webserver-2e440552-0158-413b-8bbc-eba95696941d is 0
STEP: deleting the pod 01/13/23 10:51:11.127
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 13 10:51:11.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-469" for this suite. 01/13/23 10:51:11.156
------------------------------
• [SLOW TEST] [245.773 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:47:05.396
    Jan 13 10:47:05.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename container-probe 01/13/23 10:47:05.401
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:47:05.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:47:05.515
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-2e440552-0158-413b-8bbc-eba95696941d in namespace container-probe-469 01/13/23 10:47:05.54
    Jan 13 10:47:05.572: INFO: Waiting up to 5m0s for pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d" in namespace "container-probe-469" to be "not pending"
    Jan 13 10:47:05.635: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d": Phase="Pending", Reason="", readiness=false. Elapsed: 62.722058ms
    Jan 13 10:47:07.645: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072882742s
    Jan 13 10:47:09.684: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d": Phase="Running", Reason="", readiness=true. Elapsed: 4.11228229s
    Jan 13 10:47:09.685: INFO: Pod "test-webserver-2e440552-0158-413b-8bbc-eba95696941d" satisfied condition "not pending"
    Jan 13 10:47:09.685: INFO: Started pod test-webserver-2e440552-0158-413b-8bbc-eba95696941d in namespace container-probe-469
    STEP: checking the pod's current state and verifying that restartCount is present 01/13/23 10:47:09.685
    Jan 13 10:47:09.732: INFO: Initial restart count of pod test-webserver-2e440552-0158-413b-8bbc-eba95696941d is 0
    STEP: deleting the pod 01/13/23 10:51:11.127
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:51:11.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-469" for this suite. 01/13/23 10:51:11.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:51:11.178
Jan 13 10:51:11.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename job 01/13/23 10:51:11.182
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:11.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:11.219
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/13/23 10:51:11.232
STEP: Patching the Job 01/13/23 10:51:11.245
STEP: Watching for Job to be patched 01/13/23 10:51:11.259
Jan 13 10:51:11.263: INFO: Event ADDED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 13 10:51:11.264: INFO: Event MODIFIED found for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/13/23 10:51:11.264
STEP: Watching for Job to be updated 01/13/23 10:51:11.318
Jan 13 10:51:11.323: INFO: Event MODIFIED found for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:11.324: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/13/23 10:51:11.324
Jan 13 10:51:11.330: INFO: Job: e2e-k78hb as labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched]
STEP: Waiting for job to complete 01/13/23 10:51:11.33
STEP: Delete a job collection with a labelselector 01/13/23 10:51:23.341
STEP: Watching for Job to be deleted 01/13/23 10:51:23.363
Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:23.373: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:23.373: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:23.373: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 13 10:51:23.374: INFO: Event DELETED found for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/13/23 10:51:23.374
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 13 10:51:23.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5676" for this suite. 01/13/23 10:51:23.422
------------------------------
• [SLOW TEST] [12.262 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:51:11.178
    Jan 13 10:51:11.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename job 01/13/23 10:51:11.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:11.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:11.219
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/13/23 10:51:11.232
    STEP: Patching the Job 01/13/23 10:51:11.245
    STEP: Watching for Job to be patched 01/13/23 10:51:11.259
    Jan 13 10:51:11.263: INFO: Event ADDED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 13 10:51:11.264: INFO: Event MODIFIED found for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/13/23 10:51:11.264
    STEP: Watching for Job to be updated 01/13/23 10:51:11.318
    Jan 13 10:51:11.323: INFO: Event MODIFIED found for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:11.324: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/13/23 10:51:11.324
    Jan 13 10:51:11.330: INFO: Job: e2e-k78hb as labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched]
    STEP: Waiting for job to complete 01/13/23 10:51:11.33
    STEP: Delete a job collection with a labelselector 01/13/23 10:51:23.341
    STEP: Watching for Job to be deleted 01/13/23 10:51:23.363
    Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:23.372: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:23.373: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:23.373: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:23.373: INFO: Event MODIFIED observed for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 13 10:51:23.374: INFO: Event DELETED found for Job e2e-k78hb in namespace job-5676 with labels: map[e2e-job-label:e2e-k78hb e2e-k78hb:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/13/23 10:51:23.374
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:51:23.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5676" for this suite. 01/13/23 10:51:23.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:51:23.445
Jan 13 10:51:23.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 10:51:23.448
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:23.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:23.518
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6565.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6565.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/13/23 10:51:23.542
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6565.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6565.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/13/23 10:51:23.542
STEP: creating a pod to probe /etc/hosts 01/13/23 10:51:23.542
STEP: submitting the pod to kubernetes 01/13/23 10:51:23.543
Jan 13 10:51:23.562: INFO: Waiting up to 15m0s for pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3" in namespace "dns-6565" to be "running"
Jan 13 10:51:23.576: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.949367ms
Jan 13 10:51:25.599: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036129739s
Jan 13 10:51:27.584: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.021391293s
Jan 13 10:51:27.584: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3" satisfied condition "running"
STEP: retrieving the pod 01/13/23 10:51:27.584
STEP: looking for the results for each expected name from probers 01/13/23 10:51:27.591
Jan 13 10:51:27.633: INFO: DNS probes using dns-6565/dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3 succeeded

STEP: deleting the pod 01/13/23 10:51:27.633
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 10:51:27.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6565" for this suite. 01/13/23 10:51:27.661
------------------------------
• [4.227 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:51:23.445
    Jan 13 10:51:23.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 10:51:23.448
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:23.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:23.518
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6565.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6565.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/13/23 10:51:23.542
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6565.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6565.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/13/23 10:51:23.542
    STEP: creating a pod to probe /etc/hosts 01/13/23 10:51:23.542
    STEP: submitting the pod to kubernetes 01/13/23 10:51:23.543
    Jan 13 10:51:23.562: INFO: Waiting up to 15m0s for pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3" in namespace "dns-6565" to be "running"
    Jan 13 10:51:23.576: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.949367ms
    Jan 13 10:51:25.599: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036129739s
    Jan 13 10:51:27.584: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.021391293s
    Jan 13 10:51:27.584: INFO: Pod "dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 10:51:27.584
    STEP: looking for the results for each expected name from probers 01/13/23 10:51:27.591
    Jan 13 10:51:27.633: INFO: DNS probes using dns-6565/dns-test-96fbd91d-ab26-46c4-965b-520bed0584d3 succeeded

    STEP: deleting the pod 01/13/23 10:51:27.633
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:51:27.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6565" for this suite. 01/13/23 10:51:27.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:51:27.676
Jan 13 10:51:27.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:51:27.678
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:27.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:27.724
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 13 10:51:27.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/13/23 10:51:30.771
Jan 13 10:51:30.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 create -f -'
Jan 13 10:51:33.523: INFO: stderr: ""
Jan 13 10:51:33.523: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 13 10:51:33.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 delete e2e-test-crd-publish-openapi-8915-crds test-cr'
Jan 13 10:51:33.821: INFO: stderr: ""
Jan 13 10:51:33.821: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 13 10:51:33.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 apply -f -'
Jan 13 10:51:34.623: INFO: stderr: ""
Jan 13 10:51:34.624: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 13 10:51:34.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 delete e2e-test-crd-publish-openapi-8915-crds test-cr'
Jan 13 10:51:35.037: INFO: stderr: ""
Jan 13 10:51:35.037: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/13/23 10:51:35.037
Jan 13 10:51:35.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 explain e2e-test-crd-publish-openapi-8915-crds'
Jan 13 10:51:35.853: INFO: stderr: ""
Jan 13 10:51:35.853: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8915-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:51:38.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2831" for this suite. 01/13/23 10:51:38.518
------------------------------
• [SLOW TEST] [10.916 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:51:27.676
    Jan 13 10:51:27.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:51:27.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:27.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:27.724
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 13 10:51:27.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/13/23 10:51:30.771
    Jan 13 10:51:30.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 create -f -'
    Jan 13 10:51:33.523: INFO: stderr: ""
    Jan 13 10:51:33.523: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 13 10:51:33.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 delete e2e-test-crd-publish-openapi-8915-crds test-cr'
    Jan 13 10:51:33.821: INFO: stderr: ""
    Jan 13 10:51:33.821: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 13 10:51:33.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 apply -f -'
    Jan 13 10:51:34.623: INFO: stderr: ""
    Jan 13 10:51:34.624: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 13 10:51:34.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 --namespace=crd-publish-openapi-2831 delete e2e-test-crd-publish-openapi-8915-crds test-cr'
    Jan 13 10:51:35.037: INFO: stderr: ""
    Jan 13 10:51:35.037: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/13/23 10:51:35.037
    Jan 13 10:51:35.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-2831 explain e2e-test-crd-publish-openapi-8915-crds'
    Jan 13 10:51:35.853: INFO: stderr: ""
    Jan 13 10:51:35.853: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8915-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:51:38.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2831" for this suite. 01/13/23 10:51:38.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:51:38.595
Jan 13 10:51:38.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename security-context-test 01/13/23 10:51:38.598
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:38.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:38.635
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 13 10:51:38.671: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735" in namespace "security-context-test-9602" to be "Succeeded or Failed"
Jan 13 10:51:38.680: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Pending", Reason="", readiness=false. Elapsed: 8.576001ms
Jan 13 10:51:40.744: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073111252s
Jan 13 10:51:42.693: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021922829s
Jan 13 10:51:44.690: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019093775s
Jan 13 10:51:44.690: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 13 10:51:44.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9602" for this suite. 01/13/23 10:51:44.739
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:51:38.595
    Jan 13 10:51:38.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename security-context-test 01/13/23 10:51:38.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:38.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:38.635
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 13 10:51:38.671: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735" in namespace "security-context-test-9602" to be "Succeeded or Failed"
    Jan 13 10:51:38.680: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Pending", Reason="", readiness=false. Elapsed: 8.576001ms
    Jan 13 10:51:40.744: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073111252s
    Jan 13 10:51:42.693: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021922829s
    Jan 13 10:51:44.690: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019093775s
    Jan 13 10:51:44.690: INFO: Pod "alpine-nnp-false-b61f4698-dba8-47bb-95da-79f907886735" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:51:44.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9602" for this suite. 01/13/23 10:51:44.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:51:44.754
Jan 13 10:51:44.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replication-controller 01/13/23 10:51:44.757
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:44.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:44.799
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 13 10:51:44.805: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/13/23 10:51:45.833
STEP: Checking rc "condition-test" has the desired failure condition set 01/13/23 10:51:45.845
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/13/23 10:51:46.874
Jan 13 10:51:46.901: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/13/23 10:51:46.901
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 13 10:51:47.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6608" for this suite. 01/13/23 10:51:47.94
------------------------------
• [3.202 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:51:44.754
    Jan 13 10:51:44.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replication-controller 01/13/23 10:51:44.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:44.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:44.799
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 13 10:51:44.805: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/13/23 10:51:45.833
    STEP: Checking rc "condition-test" has the desired failure condition set 01/13/23 10:51:45.845
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/13/23 10:51:46.874
    Jan 13 10:51:46.901: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/13/23 10:51:46.901
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:51:47.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6608" for this suite. 01/13/23 10:51:47.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:51:47.958
Jan 13 10:51:47.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 10:51:47.964
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:48.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:48.019
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:51:48.038
Jan 13 10:51:48.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863" in namespace "downward-api-8099" to be "Succeeded or Failed"
Jan 13 10:51:48.067: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Pending", Reason="", readiness=false. Elapsed: 10.64862ms
Jan 13 10:51:50.084: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027166417s
Jan 13 10:51:52.087: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030253063s
Jan 13 10:51:54.088: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031188539s
STEP: Saw pod success 01/13/23 10:51:54.088
Jan 13 10:51:54.088: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863" satisfied condition "Succeeded or Failed"
Jan 13 10:51:54.107: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863 container client-container: <nil>
STEP: delete the pod 01/13/23 10:51:54.153
Jan 13 10:51:54.172: INFO: Waiting for pod downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863 to disappear
Jan 13 10:51:54.182: INFO: Pod downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 10:51:54.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8099" for this suite. 01/13/23 10:51:54.194
------------------------------
• [SLOW TEST] [6.250 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:51:47.958
    Jan 13 10:51:47.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 10:51:47.964
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:48.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:48.019
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:51:48.038
    Jan 13 10:51:48.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863" in namespace "downward-api-8099" to be "Succeeded or Failed"
    Jan 13 10:51:48.067: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Pending", Reason="", readiness=false. Elapsed: 10.64862ms
    Jan 13 10:51:50.084: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027166417s
    Jan 13 10:51:52.087: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030253063s
    Jan 13 10:51:54.088: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031188539s
    STEP: Saw pod success 01/13/23 10:51:54.088
    Jan 13 10:51:54.088: INFO: Pod "downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863" satisfied condition "Succeeded or Failed"
    Jan 13 10:51:54.107: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:51:54.153
    Jan 13 10:51:54.172: INFO: Waiting for pod downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863 to disappear
    Jan 13 10:51:54.182: INFO: Pod downwardapi-volume-7bc4939d-1622-4677-aece-dd3c90464863 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:51:54.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8099" for this suite. 01/13/23 10:51:54.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:51:54.21
Jan 13 10:51:54.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:51:54.212
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:54.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:54.245
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/13/23 10:51:54.272
STEP: watching for Pod to be ready 01/13/23 10:51:54.288
Jan 13 10:51:54.294: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 13 10:51:54.297: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
Jan 13 10:51:54.325: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
Jan 13 10:51:56.241: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
Jan 13 10:51:58.100: INFO: Found Pod pod-test in namespace pods-4021 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/13/23 10:51:58.108
STEP: getting the Pod and ensuring that it's patched 01/13/23 10:51:58.165
STEP: replacing the Pod's status Ready condition to False 01/13/23 10:51:58.187
STEP: check the Pod again to ensure its Ready conditions are False 01/13/23 10:51:58.227
STEP: deleting the Pod via a Collection with a LabelSelector 01/13/23 10:51:58.227
STEP: watching for the Pod to be deleted 01/13/23 10:51:58.254
Jan 13 10:51:58.261: INFO: observed event type MODIFIED
Jan 13 10:52:00.086: INFO: observed event type MODIFIED
Jan 13 10:52:00.626: INFO: observed event type MODIFIED
Jan 13 10:52:02.516: INFO: observed event type MODIFIED
Jan 13 10:52:02.530: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:02.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4021" for this suite. 01/13/23 10:52:02.578
------------------------------
• [SLOW TEST] [8.388 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:51:54.21
    Jan 13 10:51:54.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:51:54.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:51:54.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:51:54.245
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/13/23 10:51:54.272
    STEP: watching for Pod to be ready 01/13/23 10:51:54.288
    Jan 13 10:51:54.294: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 13 10:51:54.297: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
    Jan 13 10:51:54.325: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
    Jan 13 10:51:56.241: INFO: observed Pod pod-test in namespace pods-4021 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
    Jan 13 10:51:58.100: INFO: Found Pod pod-test in namespace pods-4021 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:51:54 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/13/23 10:51:58.108
    STEP: getting the Pod and ensuring that it's patched 01/13/23 10:51:58.165
    STEP: replacing the Pod's status Ready condition to False 01/13/23 10:51:58.187
    STEP: check the Pod again to ensure its Ready conditions are False 01/13/23 10:51:58.227
    STEP: deleting the Pod via a Collection with a LabelSelector 01/13/23 10:51:58.227
    STEP: watching for the Pod to be deleted 01/13/23 10:51:58.254
    Jan 13 10:51:58.261: INFO: observed event type MODIFIED
    Jan 13 10:52:00.086: INFO: observed event type MODIFIED
    Jan 13 10:52:00.626: INFO: observed event type MODIFIED
    Jan 13 10:52:02.516: INFO: observed event type MODIFIED
    Jan 13 10:52:02.530: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:02.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4021" for this suite. 01/13/23 10:52:02.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:02.599
Jan 13 10:52:02.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-webhook 01/13/23 10:52:02.602
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:02.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:02.648
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/13/23 10:52:02.661
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/13/23 10:52:04.281
STEP: Deploying the custom resource conversion webhook pod 01/13/23 10:52:04.302
STEP: Wait for the deployment to be ready 01/13/23 10:52:04.323
Jan 13 10:52:04.337: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jan 13 10:52:06.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:52:08.37
STEP: Verifying the service has paired with the endpoint 01/13/23 10:52:08.414
Jan 13 10:52:09.415: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 13 10:52:09.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Creating a v1 custom resource 01/13/23 10:52:12.466
STEP: Create a v2 custom resource 01/13/23 10:52:12.556
STEP: List CRs in v1 01/13/23 10:52:12.914
STEP: List CRs in v2 01/13/23 10:52:12.941
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:13.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-5181" for this suite. 01/13/23 10:52:13.727
------------------------------
• [SLOW TEST] [11.184 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:02.599
    Jan 13 10:52:02.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-webhook 01/13/23 10:52:02.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:02.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:02.648
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/13/23 10:52:02.661
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/13/23 10:52:04.281
    STEP: Deploying the custom resource conversion webhook pod 01/13/23 10:52:04.302
    STEP: Wait for the deployment to be ready 01/13/23 10:52:04.323
    Jan 13 10:52:04.337: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Jan 13 10:52:06.363: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:52:08.37
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:52:08.414
    Jan 13 10:52:09.415: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 13 10:52:09.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Creating a v1 custom resource 01/13/23 10:52:12.466
    STEP: Create a v2 custom resource 01/13/23 10:52:12.556
    STEP: List CRs in v1 01/13/23 10:52:12.914
    STEP: List CRs in v2 01/13/23 10:52:12.941
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:13.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-5181" for this suite. 01/13/23 10:52:13.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:13.797
Jan 13 10:52:13.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename disruption 01/13/23 10:52:13.802
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:13.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:13.895
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/13/23 10:52:13.917
STEP: Updating PodDisruptionBudget status 01/13/23 10:52:15.945
STEP: Waiting for all pods to be running 01/13/23 10:52:15.962
Jan 13 10:52:15.978: INFO: running pods: 0 < 1
Jan 13 10:52:17.985: INFO: running pods: 0 < 1
STEP: locating a running pod 01/13/23 10:52:19.991
STEP: Waiting for the pdb to be processed 01/13/23 10:52:20.022
STEP: Patching PodDisruptionBudget status 01/13/23 10:52:20.042
STEP: Waiting for the pdb to be processed 01/13/23 10:52:20.086
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:20.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8974" for this suite. 01/13/23 10:52:20.112
------------------------------
• [SLOW TEST] [6.344 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:13.797
    Jan 13 10:52:13.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename disruption 01/13/23 10:52:13.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:13.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:13.895
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/13/23 10:52:13.917
    STEP: Updating PodDisruptionBudget status 01/13/23 10:52:15.945
    STEP: Waiting for all pods to be running 01/13/23 10:52:15.962
    Jan 13 10:52:15.978: INFO: running pods: 0 < 1
    Jan 13 10:52:17.985: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/13/23 10:52:19.991
    STEP: Waiting for the pdb to be processed 01/13/23 10:52:20.022
    STEP: Patching PodDisruptionBudget status 01/13/23 10:52:20.042
    STEP: Waiting for the pdb to be processed 01/13/23 10:52:20.086
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:20.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8974" for this suite. 01/13/23 10:52:20.112
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:20.142
Jan 13 10:52:20.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:52:20.146
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:20.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:20.195
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/13/23 10:52:20.204
Jan 13 10:52:20.224: INFO: Waiting up to 5m0s for pod "pod-dnn7p" in namespace "pods-2314" to be "running"
Jan 13 10:52:20.231: INFO: Pod "pod-dnn7p": Phase="Pending", Reason="", readiness=false. Elapsed: 7.66932ms
Jan 13 10:52:22.241: INFO: Pod "pod-dnn7p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017695739s
Jan 13 10:52:24.257: INFO: Pod "pod-dnn7p": Phase="Running", Reason="", readiness=true. Elapsed: 4.033102577s
Jan 13 10:52:24.257: INFO: Pod "pod-dnn7p" satisfied condition "running"
STEP: patching /status 01/13/23 10:52:24.257
Jan 13 10:52:24.299: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:24.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2314" for this suite. 01/13/23 10:52:24.33
------------------------------
• [4.208 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:20.142
    Jan 13 10:52:20.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:52:20.146
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:20.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:20.195
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/13/23 10:52:20.204
    Jan 13 10:52:20.224: INFO: Waiting up to 5m0s for pod "pod-dnn7p" in namespace "pods-2314" to be "running"
    Jan 13 10:52:20.231: INFO: Pod "pod-dnn7p": Phase="Pending", Reason="", readiness=false. Elapsed: 7.66932ms
    Jan 13 10:52:22.241: INFO: Pod "pod-dnn7p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017695739s
    Jan 13 10:52:24.257: INFO: Pod "pod-dnn7p": Phase="Running", Reason="", readiness=true. Elapsed: 4.033102577s
    Jan 13 10:52:24.257: INFO: Pod "pod-dnn7p" satisfied condition "running"
    STEP: patching /status 01/13/23 10:52:24.257
    Jan 13 10:52:24.299: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:24.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2314" for this suite. 01/13/23 10:52:24.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:24.353
Jan 13 10:52:24.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:52:24.356
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:24.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:24.423
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 10:52:24.439
Jan 13 10:52:24.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6874 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan 13 10:52:24.687: INFO: stderr: ""
Jan 13 10:52:24.687: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/13/23 10:52:24.687
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 13 10:52:24.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6874 delete pods e2e-test-httpd-pod'
Jan 13 10:52:29.457: INFO: stderr: ""
Jan 13 10:52:29.457: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:29.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6874" for this suite. 01/13/23 10:52:29.467
------------------------------
• [SLOW TEST] [5.138 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:24.353
    Jan 13 10:52:24.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:52:24.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:24.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:24.423
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/13/23 10:52:24.439
    Jan 13 10:52:24.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6874 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan 13 10:52:24.687: INFO: stderr: ""
    Jan 13 10:52:24.687: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/13/23 10:52:24.687
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 13 10:52:24.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6874 delete pods e2e-test-httpd-pod'
    Jan 13 10:52:29.457: INFO: stderr: ""
    Jan 13 10:52:29.457: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:29.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6874" for this suite. 01/13/23 10:52:29.467
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:29.491
Jan 13 10:52:29.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:52:29.496
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:29.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:29.549
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-8591/secret-test-40b505cc-9979-4a97-ab60-ca7f2cbaed15 01/13/23 10:52:29.577
STEP: Creating a pod to test consume secrets 01/13/23 10:52:29.588
Jan 13 10:52:29.618: INFO: Waiting up to 5m0s for pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11" in namespace "secrets-8591" to be "Succeeded or Failed"
Jan 13 10:52:29.628: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 10.211483ms
Jan 13 10:52:31.636: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01791163s
Jan 13 10:52:33.640: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022049045s
Jan 13 10:52:35.637: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019141219s
Jan 13 10:52:37.637: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019140813s
STEP: Saw pod success 01/13/23 10:52:37.637
Jan 13 10:52:37.637: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11" satisfied condition "Succeeded or Failed"
Jan 13 10:52:37.645: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11 container env-test: <nil>
STEP: delete the pod 01/13/23 10:52:37.667
Jan 13 10:52:37.686: INFO: Waiting for pod pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11 to disappear
Jan 13 10:52:37.694: INFO: Pod pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:37.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8591" for this suite. 01/13/23 10:52:37.703
------------------------------
• [SLOW TEST] [8.225 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:29.491
    Jan 13 10:52:29.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:52:29.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:29.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:29.549
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-8591/secret-test-40b505cc-9979-4a97-ab60-ca7f2cbaed15 01/13/23 10:52:29.577
    STEP: Creating a pod to test consume secrets 01/13/23 10:52:29.588
    Jan 13 10:52:29.618: INFO: Waiting up to 5m0s for pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11" in namespace "secrets-8591" to be "Succeeded or Failed"
    Jan 13 10:52:29.628: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 10.211483ms
    Jan 13 10:52:31.636: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01791163s
    Jan 13 10:52:33.640: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022049045s
    Jan 13 10:52:35.637: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019141219s
    Jan 13 10:52:37.637: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019140813s
    STEP: Saw pod success 01/13/23 10:52:37.637
    Jan 13 10:52:37.637: INFO: Pod "pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11" satisfied condition "Succeeded or Failed"
    Jan 13 10:52:37.645: INFO: Trying to get logs from node 10.10.102.31-node pod pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11 container env-test: <nil>
    STEP: delete the pod 01/13/23 10:52:37.667
    Jan 13 10:52:37.686: INFO: Waiting for pod pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11 to disappear
    Jan 13 10:52:37.694: INFO: Pod pod-configmaps-be95b317-085e-472a-b470-517f8ab14b11 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:37.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8591" for this suite. 01/13/23 10:52:37.703
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:37.717
Jan 13 10:52:37.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:52:37.719
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:37.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:37.762
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:52:37.794
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:52:40.462
STEP: Deploying the webhook pod 01/13/23 10:52:40.494
STEP: Wait for the deployment to be ready 01/13/23 10:52:40.582
Jan 13 10:52:40.626: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:52:42.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:52:44.679
STEP: Verifying the service has paired with the endpoint 01/13/23 10:52:44.706
Jan 13 10:52:45.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/13/23 10:52:45.718
STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:52:45.852
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/13/23 10:52:45.885
STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:52:46.002
STEP: Patching a validating webhook configuration's rules to include the create operation 01/13/23 10:52:46.047
STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:52:46.07
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:46.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1098" for this suite. 01/13/23 10:52:46.198
STEP: Destroying namespace "webhook-1098-markers" for this suite. 01/13/23 10:52:46.213
------------------------------
• [SLOW TEST] [8.512 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:37.717
    Jan 13 10:52:37.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:52:37.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:37.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:37.762
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:52:37.794
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:52:40.462
    STEP: Deploying the webhook pod 01/13/23 10:52:40.494
    STEP: Wait for the deployment to be ready 01/13/23 10:52:40.582
    Jan 13 10:52:40.626: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:52:42.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 52, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:52:44.679
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:52:44.706
    Jan 13 10:52:45.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/13/23 10:52:45.718
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:52:45.852
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/13/23 10:52:45.885
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:52:46.002
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/13/23 10:52:46.047
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/13/23 10:52:46.07
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:46.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1098" for this suite. 01/13/23 10:52:46.198
    STEP: Destroying namespace "webhook-1098-markers" for this suite. 01/13/23 10:52:46.213
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:46.23
Jan 13 10:52:46.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename namespaces 01/13/23 10:52:46.233
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:46.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:46.297
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-vwgkz" 01/13/23 10:52:46.31
Jan 13 10:52:46.361: INFO: Namespace "e2e-ns-vwgkz-5386" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-vwgkz-5386" 01/13/23 10:52:46.361
Jan 13 10:52:46.388: INFO: Namespace "e2e-ns-vwgkz-5386" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-vwgkz-5386" 01/13/23 10:52:46.388
Jan 13 10:52:46.415: INFO: Namespace "e2e-ns-vwgkz-5386" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:52:46.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9404" for this suite. 01/13/23 10:52:46.423
STEP: Destroying namespace "e2e-ns-vwgkz-5386" for this suite. 01/13/23 10:52:46.47
------------------------------
• [0.256 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:46.23
    Jan 13 10:52:46.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename namespaces 01/13/23 10:52:46.233
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:46.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:46.297
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-vwgkz" 01/13/23 10:52:46.31
    Jan 13 10:52:46.361: INFO: Namespace "e2e-ns-vwgkz-5386" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-vwgkz-5386" 01/13/23 10:52:46.361
    Jan 13 10:52:46.388: INFO: Namespace "e2e-ns-vwgkz-5386" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-vwgkz-5386" 01/13/23 10:52:46.388
    Jan 13 10:52:46.415: INFO: Namespace "e2e-ns-vwgkz-5386" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:52:46.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9404" for this suite. 01/13/23 10:52:46.423
    STEP: Destroying namespace "e2e-ns-vwgkz-5386" for this suite. 01/13/23 10:52:46.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:52:46.489
Jan 13 10:52:46.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename gc 01/13/23 10:52:46.493
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:46.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:46.553
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/13/23 10:52:46.591
STEP: delete the rc 01/13/23 10:52:51.667
STEP: wait for the rc to be deleted 01/13/23 10:52:51.74
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/13/23 10:52:56.755
STEP: Gathering metrics 01/13/23 10:53:26.818
Jan 13 10:53:26.896: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
Jan 13 10:53:26.909: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 13.037008ms
Jan 13 10:53:26.909: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
Jan 13 10:53:26.909: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
Jan 13 10:53:27.265: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 13 10:53:27.265: INFO: Deleting pod "simpletest.rc-256b7" in namespace "gc-7213"
Jan 13 10:53:27.295: INFO: Deleting pod "simpletest.rc-25k75" in namespace "gc-7213"
Jan 13 10:53:27.330: INFO: Deleting pod "simpletest.rc-28pcj" in namespace "gc-7213"
Jan 13 10:53:27.403: INFO: Deleting pod "simpletest.rc-2mspj" in namespace "gc-7213"
Jan 13 10:53:27.455: INFO: Deleting pod "simpletest.rc-2zjvd" in namespace "gc-7213"
Jan 13 10:53:27.490: INFO: Deleting pod "simpletest.rc-42sjw" in namespace "gc-7213"
Jan 13 10:53:27.568: INFO: Deleting pod "simpletest.rc-47xv7" in namespace "gc-7213"
Jan 13 10:53:27.614: INFO: Deleting pod "simpletest.rc-48k4b" in namespace "gc-7213"
Jan 13 10:53:27.685: INFO: Deleting pod "simpletest.rc-4cdc4" in namespace "gc-7213"
Jan 13 10:53:27.790: INFO: Deleting pod "simpletest.rc-4g5bh" in namespace "gc-7213"
Jan 13 10:53:27.855: INFO: Deleting pod "simpletest.rc-4hvpc" in namespace "gc-7213"
Jan 13 10:53:27.909: INFO: Deleting pod "simpletest.rc-4jwg6" in namespace "gc-7213"
Jan 13 10:53:27.976: INFO: Deleting pod "simpletest.rc-52b5h" in namespace "gc-7213"
Jan 13 10:53:28.007: INFO: Deleting pod "simpletest.rc-54jr4" in namespace "gc-7213"
Jan 13 10:53:28.042: INFO: Deleting pod "simpletest.rc-5f2tk" in namespace "gc-7213"
Jan 13 10:53:28.090: INFO: Deleting pod "simpletest.rc-5mdlf" in namespace "gc-7213"
Jan 13 10:53:28.138: INFO: Deleting pod "simpletest.rc-6q5cw" in namespace "gc-7213"
Jan 13 10:53:28.167: INFO: Deleting pod "simpletest.rc-6twr5" in namespace "gc-7213"
Jan 13 10:53:28.206: INFO: Deleting pod "simpletest.rc-7fq9j" in namespace "gc-7213"
Jan 13 10:53:28.225: INFO: Deleting pod "simpletest.rc-7kx6k" in namespace "gc-7213"
Jan 13 10:53:28.252: INFO: Deleting pod "simpletest.rc-86rtv" in namespace "gc-7213"
Jan 13 10:53:28.274: INFO: Deleting pod "simpletest.rc-8dl2w" in namespace "gc-7213"
Jan 13 10:53:28.324: INFO: Deleting pod "simpletest.rc-8fp88" in namespace "gc-7213"
Jan 13 10:53:28.351: INFO: Deleting pod "simpletest.rc-8hx6q" in namespace "gc-7213"
Jan 13 10:53:28.378: INFO: Deleting pod "simpletest.rc-8r9rs" in namespace "gc-7213"
Jan 13 10:53:28.430: INFO: Deleting pod "simpletest.rc-8s5pq" in namespace "gc-7213"
Jan 13 10:53:28.456: INFO: Deleting pod "simpletest.rc-8xfws" in namespace "gc-7213"
Jan 13 10:53:28.474: INFO: Deleting pod "simpletest.rc-9bf5f" in namespace "gc-7213"
Jan 13 10:53:28.501: INFO: Deleting pod "simpletest.rc-9cx66" in namespace "gc-7213"
Jan 13 10:53:28.521: INFO: Deleting pod "simpletest.rc-9p4vc" in namespace "gc-7213"
Jan 13 10:53:28.554: INFO: Deleting pod "simpletest.rc-9sdhv" in namespace "gc-7213"
Jan 13 10:53:28.599: INFO: Deleting pod "simpletest.rc-9tf82" in namespace "gc-7213"
Jan 13 10:53:28.619: INFO: Deleting pod "simpletest.rc-b4k22" in namespace "gc-7213"
Jan 13 10:53:28.641: INFO: Deleting pod "simpletest.rc-b6m6j" in namespace "gc-7213"
Jan 13 10:53:28.774: INFO: Deleting pod "simpletest.rc-bx2sj" in namespace "gc-7213"
Jan 13 10:53:28.804: INFO: Deleting pod "simpletest.rc-cczds" in namespace "gc-7213"
Jan 13 10:53:28.826: INFO: Deleting pod "simpletest.rc-cmdfd" in namespace "gc-7213"
Jan 13 10:53:28.842: INFO: Deleting pod "simpletest.rc-dgx8d" in namespace "gc-7213"
Jan 13 10:53:28.866: INFO: Deleting pod "simpletest.rc-dtqtz" in namespace "gc-7213"
Jan 13 10:53:28.890: INFO: Deleting pod "simpletest.rc-fpfnh" in namespace "gc-7213"
Jan 13 10:53:28.939: INFO: Deleting pod "simpletest.rc-gmb2l" in namespace "gc-7213"
Jan 13 10:53:29.012: INFO: Deleting pod "simpletest.rc-gpp57" in namespace "gc-7213"
Jan 13 10:53:29.161: INFO: Deleting pod "simpletest.rc-gwwvp" in namespace "gc-7213"
Jan 13 10:53:29.273: INFO: Deleting pod "simpletest.rc-hkbjs" in namespace "gc-7213"
Jan 13 10:53:29.408: INFO: Deleting pod "simpletest.rc-hrct8" in namespace "gc-7213"
Jan 13 10:53:29.486: INFO: Deleting pod "simpletest.rc-j2tw4" in namespace "gc-7213"
Jan 13 10:53:29.549: INFO: Deleting pod "simpletest.rc-jnsxg" in namespace "gc-7213"
Jan 13 10:53:29.594: INFO: Deleting pod "simpletest.rc-jx6sn" in namespace "gc-7213"
Jan 13 10:53:29.636: INFO: Deleting pod "simpletest.rc-kjl6d" in namespace "gc-7213"
Jan 13 10:53:29.666: INFO: Deleting pod "simpletest.rc-kvhmd" in namespace "gc-7213"
Jan 13 10:53:29.739: INFO: Deleting pod "simpletest.rc-l9d2n" in namespace "gc-7213"
Jan 13 10:53:29.756: INFO: Deleting pod "simpletest.rc-ldm62" in namespace "gc-7213"
Jan 13 10:53:29.793: INFO: Deleting pod "simpletest.rc-lj5tt" in namespace "gc-7213"
Jan 13 10:53:29.923: INFO: Deleting pod "simpletest.rc-ljvzd" in namespace "gc-7213"
Jan 13 10:53:29.997: INFO: Deleting pod "simpletest.rc-lw5kg" in namespace "gc-7213"
Jan 13 10:53:30.047: INFO: Deleting pod "simpletest.rc-m7h9p" in namespace "gc-7213"
Jan 13 10:53:30.081: INFO: Deleting pod "simpletest.rc-mlncf" in namespace "gc-7213"
Jan 13 10:53:30.105: INFO: Deleting pod "simpletest.rc-mrnn6" in namespace "gc-7213"
Jan 13 10:53:30.142: INFO: Deleting pod "simpletest.rc-mzxgp" in namespace "gc-7213"
Jan 13 10:53:30.165: INFO: Deleting pod "simpletest.rc-n6ltk" in namespace "gc-7213"
Jan 13 10:53:30.196: INFO: Deleting pod "simpletest.rc-nfwkc" in namespace "gc-7213"
Jan 13 10:53:30.239: INFO: Deleting pod "simpletest.rc-pb57q" in namespace "gc-7213"
Jan 13 10:53:30.269: INFO: Deleting pod "simpletest.rc-pwrkr" in namespace "gc-7213"
Jan 13 10:53:30.326: INFO: Deleting pod "simpletest.rc-q7b6f" in namespace "gc-7213"
Jan 13 10:53:30.361: INFO: Deleting pod "simpletest.rc-qcg8j" in namespace "gc-7213"
Jan 13 10:53:30.388: INFO: Deleting pod "simpletest.rc-qcrdl" in namespace "gc-7213"
Jan 13 10:53:30.436: INFO: Deleting pod "simpletest.rc-qdbh5" in namespace "gc-7213"
Jan 13 10:53:30.498: INFO: Deleting pod "simpletest.rc-qm569" in namespace "gc-7213"
Jan 13 10:53:30.657: INFO: Deleting pod "simpletest.rc-qnc4r" in namespace "gc-7213"
Jan 13 10:53:30.768: INFO: Deleting pod "simpletest.rc-qtpw4" in namespace "gc-7213"
Jan 13 10:53:30.787: INFO: Deleting pod "simpletest.rc-rf7w4" in namespace "gc-7213"
Jan 13 10:53:30.826: INFO: Deleting pod "simpletest.rc-sfdqr" in namespace "gc-7213"
Jan 13 10:53:30.853: INFO: Deleting pod "simpletest.rc-sjbbs" in namespace "gc-7213"
Jan 13 10:53:30.959: INFO: Deleting pod "simpletest.rc-sqhwm" in namespace "gc-7213"
Jan 13 10:53:31.028: INFO: Deleting pod "simpletest.rc-t297t" in namespace "gc-7213"
Jan 13 10:53:31.066: INFO: Deleting pod "simpletest.rc-tb95s" in namespace "gc-7213"
Jan 13 10:53:31.132: INFO: Deleting pod "simpletest.rc-tdgsk" in namespace "gc-7213"
Jan 13 10:53:31.181: INFO: Deleting pod "simpletest.rc-tfrxx" in namespace "gc-7213"
Jan 13 10:53:31.299: INFO: Deleting pod "simpletest.rc-tmgkq" in namespace "gc-7213"
Jan 13 10:53:31.425: INFO: Deleting pod "simpletest.rc-v5ckv" in namespace "gc-7213"
Jan 13 10:53:31.554: INFO: Deleting pod "simpletest.rc-v6rbt" in namespace "gc-7213"
Jan 13 10:53:31.616: INFO: Deleting pod "simpletest.rc-vllwl" in namespace "gc-7213"
Jan 13 10:53:31.681: INFO: Deleting pod "simpletest.rc-vmtsl" in namespace "gc-7213"
Jan 13 10:53:31.710: INFO: Deleting pod "simpletest.rc-vmxqb" in namespace "gc-7213"
Jan 13 10:53:31.794: INFO: Deleting pod "simpletest.rc-vsr8s" in namespace "gc-7213"
Jan 13 10:53:31.867: INFO: Deleting pod "simpletest.rc-vvj8x" in namespace "gc-7213"
Jan 13 10:53:31.969: INFO: Deleting pod "simpletest.rc-vzpmq" in namespace "gc-7213"
Jan 13 10:53:32.019: INFO: Deleting pod "simpletest.rc-wf7k4" in namespace "gc-7213"
Jan 13 10:53:32.099: INFO: Deleting pod "simpletest.rc-wq59r" in namespace "gc-7213"
Jan 13 10:53:32.148: INFO: Deleting pod "simpletest.rc-xgxlr" in namespace "gc-7213"
Jan 13 10:53:32.203: INFO: Deleting pod "simpletest.rc-xk9fm" in namespace "gc-7213"
Jan 13 10:53:32.320: INFO: Deleting pod "simpletest.rc-xpm9j" in namespace "gc-7213"
Jan 13 10:53:32.341: INFO: Deleting pod "simpletest.rc-z27sp" in namespace "gc-7213"
Jan 13 10:53:32.385: INFO: Deleting pod "simpletest.rc-z9zn2" in namespace "gc-7213"
Jan 13 10:53:32.409: INFO: Deleting pod "simpletest.rc-zfz4c" in namespace "gc-7213"
Jan 13 10:53:32.466: INFO: Deleting pod "simpletest.rc-zhhl4" in namespace "gc-7213"
Jan 13 10:53:32.496: INFO: Deleting pod "simpletest.rc-zj4jg" in namespace "gc-7213"
Jan 13 10:53:32.545: INFO: Deleting pod "simpletest.rc-zjh5n" in namespace "gc-7213"
Jan 13 10:53:32.564: INFO: Deleting pod "simpletest.rc-zk7zt" in namespace "gc-7213"
Jan 13 10:53:32.586: INFO: Deleting pod "simpletest.rc-zl5dc" in namespace "gc-7213"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 13 10:53:32.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7213" for this suite. 01/13/23 10:53:32.777
------------------------------
• [SLOW TEST] [46.355 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:52:46.489
    Jan 13 10:52:46.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename gc 01/13/23 10:52:46.493
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:52:46.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:52:46.553
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/13/23 10:52:46.591
    STEP: delete the rc 01/13/23 10:52:51.667
    STEP: wait for the rc to be deleted 01/13/23 10:52:51.74
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/13/23 10:52:56.755
    STEP: Gathering metrics 01/13/23 10:53:26.818
    Jan 13 10:53:26.896: INFO: Waiting up to 5m0s for pod "kube-controller-manager-10.10.102.30-master" in namespace "kube-system" to be "running and ready"
    Jan 13 10:53:26.909: INFO: Pod "kube-controller-manager-10.10.102.30-master": Phase="Running", Reason="", readiness=true. Elapsed: 13.037008ms
    Jan 13 10:53:26.909: INFO: The phase of Pod kube-controller-manager-10.10.102.30-master is Running (Ready = true)
    Jan 13 10:53:26.909: INFO: Pod "kube-controller-manager-10.10.102.30-master" satisfied condition "running and ready"
    Jan 13 10:53:27.265: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 13 10:53:27.265: INFO: Deleting pod "simpletest.rc-256b7" in namespace "gc-7213"
    Jan 13 10:53:27.295: INFO: Deleting pod "simpletest.rc-25k75" in namespace "gc-7213"
    Jan 13 10:53:27.330: INFO: Deleting pod "simpletest.rc-28pcj" in namespace "gc-7213"
    Jan 13 10:53:27.403: INFO: Deleting pod "simpletest.rc-2mspj" in namespace "gc-7213"
    Jan 13 10:53:27.455: INFO: Deleting pod "simpletest.rc-2zjvd" in namespace "gc-7213"
    Jan 13 10:53:27.490: INFO: Deleting pod "simpletest.rc-42sjw" in namespace "gc-7213"
    Jan 13 10:53:27.568: INFO: Deleting pod "simpletest.rc-47xv7" in namespace "gc-7213"
    Jan 13 10:53:27.614: INFO: Deleting pod "simpletest.rc-48k4b" in namespace "gc-7213"
    Jan 13 10:53:27.685: INFO: Deleting pod "simpletest.rc-4cdc4" in namespace "gc-7213"
    Jan 13 10:53:27.790: INFO: Deleting pod "simpletest.rc-4g5bh" in namespace "gc-7213"
    Jan 13 10:53:27.855: INFO: Deleting pod "simpletest.rc-4hvpc" in namespace "gc-7213"
    Jan 13 10:53:27.909: INFO: Deleting pod "simpletest.rc-4jwg6" in namespace "gc-7213"
    Jan 13 10:53:27.976: INFO: Deleting pod "simpletest.rc-52b5h" in namespace "gc-7213"
    Jan 13 10:53:28.007: INFO: Deleting pod "simpletest.rc-54jr4" in namespace "gc-7213"
    Jan 13 10:53:28.042: INFO: Deleting pod "simpletest.rc-5f2tk" in namespace "gc-7213"
    Jan 13 10:53:28.090: INFO: Deleting pod "simpletest.rc-5mdlf" in namespace "gc-7213"
    Jan 13 10:53:28.138: INFO: Deleting pod "simpletest.rc-6q5cw" in namespace "gc-7213"
    Jan 13 10:53:28.167: INFO: Deleting pod "simpletest.rc-6twr5" in namespace "gc-7213"
    Jan 13 10:53:28.206: INFO: Deleting pod "simpletest.rc-7fq9j" in namespace "gc-7213"
    Jan 13 10:53:28.225: INFO: Deleting pod "simpletest.rc-7kx6k" in namespace "gc-7213"
    Jan 13 10:53:28.252: INFO: Deleting pod "simpletest.rc-86rtv" in namespace "gc-7213"
    Jan 13 10:53:28.274: INFO: Deleting pod "simpletest.rc-8dl2w" in namespace "gc-7213"
    Jan 13 10:53:28.324: INFO: Deleting pod "simpletest.rc-8fp88" in namespace "gc-7213"
    Jan 13 10:53:28.351: INFO: Deleting pod "simpletest.rc-8hx6q" in namespace "gc-7213"
    Jan 13 10:53:28.378: INFO: Deleting pod "simpletest.rc-8r9rs" in namespace "gc-7213"
    Jan 13 10:53:28.430: INFO: Deleting pod "simpletest.rc-8s5pq" in namespace "gc-7213"
    Jan 13 10:53:28.456: INFO: Deleting pod "simpletest.rc-8xfws" in namespace "gc-7213"
    Jan 13 10:53:28.474: INFO: Deleting pod "simpletest.rc-9bf5f" in namespace "gc-7213"
    Jan 13 10:53:28.501: INFO: Deleting pod "simpletest.rc-9cx66" in namespace "gc-7213"
    Jan 13 10:53:28.521: INFO: Deleting pod "simpletest.rc-9p4vc" in namespace "gc-7213"
    Jan 13 10:53:28.554: INFO: Deleting pod "simpletest.rc-9sdhv" in namespace "gc-7213"
    Jan 13 10:53:28.599: INFO: Deleting pod "simpletest.rc-9tf82" in namespace "gc-7213"
    Jan 13 10:53:28.619: INFO: Deleting pod "simpletest.rc-b4k22" in namespace "gc-7213"
    Jan 13 10:53:28.641: INFO: Deleting pod "simpletest.rc-b6m6j" in namespace "gc-7213"
    Jan 13 10:53:28.774: INFO: Deleting pod "simpletest.rc-bx2sj" in namespace "gc-7213"
    Jan 13 10:53:28.804: INFO: Deleting pod "simpletest.rc-cczds" in namespace "gc-7213"
    Jan 13 10:53:28.826: INFO: Deleting pod "simpletest.rc-cmdfd" in namespace "gc-7213"
    Jan 13 10:53:28.842: INFO: Deleting pod "simpletest.rc-dgx8d" in namespace "gc-7213"
    Jan 13 10:53:28.866: INFO: Deleting pod "simpletest.rc-dtqtz" in namespace "gc-7213"
    Jan 13 10:53:28.890: INFO: Deleting pod "simpletest.rc-fpfnh" in namespace "gc-7213"
    Jan 13 10:53:28.939: INFO: Deleting pod "simpletest.rc-gmb2l" in namespace "gc-7213"
    Jan 13 10:53:29.012: INFO: Deleting pod "simpletest.rc-gpp57" in namespace "gc-7213"
    Jan 13 10:53:29.161: INFO: Deleting pod "simpletest.rc-gwwvp" in namespace "gc-7213"
    Jan 13 10:53:29.273: INFO: Deleting pod "simpletest.rc-hkbjs" in namespace "gc-7213"
    Jan 13 10:53:29.408: INFO: Deleting pod "simpletest.rc-hrct8" in namespace "gc-7213"
    Jan 13 10:53:29.486: INFO: Deleting pod "simpletest.rc-j2tw4" in namespace "gc-7213"
    Jan 13 10:53:29.549: INFO: Deleting pod "simpletest.rc-jnsxg" in namespace "gc-7213"
    Jan 13 10:53:29.594: INFO: Deleting pod "simpletest.rc-jx6sn" in namespace "gc-7213"
    Jan 13 10:53:29.636: INFO: Deleting pod "simpletest.rc-kjl6d" in namespace "gc-7213"
    Jan 13 10:53:29.666: INFO: Deleting pod "simpletest.rc-kvhmd" in namespace "gc-7213"
    Jan 13 10:53:29.739: INFO: Deleting pod "simpletest.rc-l9d2n" in namespace "gc-7213"
    Jan 13 10:53:29.756: INFO: Deleting pod "simpletest.rc-ldm62" in namespace "gc-7213"
    Jan 13 10:53:29.793: INFO: Deleting pod "simpletest.rc-lj5tt" in namespace "gc-7213"
    Jan 13 10:53:29.923: INFO: Deleting pod "simpletest.rc-ljvzd" in namespace "gc-7213"
    Jan 13 10:53:29.997: INFO: Deleting pod "simpletest.rc-lw5kg" in namespace "gc-7213"
    Jan 13 10:53:30.047: INFO: Deleting pod "simpletest.rc-m7h9p" in namespace "gc-7213"
    Jan 13 10:53:30.081: INFO: Deleting pod "simpletest.rc-mlncf" in namespace "gc-7213"
    Jan 13 10:53:30.105: INFO: Deleting pod "simpletest.rc-mrnn6" in namespace "gc-7213"
    Jan 13 10:53:30.142: INFO: Deleting pod "simpletest.rc-mzxgp" in namespace "gc-7213"
    Jan 13 10:53:30.165: INFO: Deleting pod "simpletest.rc-n6ltk" in namespace "gc-7213"
    Jan 13 10:53:30.196: INFO: Deleting pod "simpletest.rc-nfwkc" in namespace "gc-7213"
    Jan 13 10:53:30.239: INFO: Deleting pod "simpletest.rc-pb57q" in namespace "gc-7213"
    Jan 13 10:53:30.269: INFO: Deleting pod "simpletest.rc-pwrkr" in namespace "gc-7213"
    Jan 13 10:53:30.326: INFO: Deleting pod "simpletest.rc-q7b6f" in namespace "gc-7213"
    Jan 13 10:53:30.361: INFO: Deleting pod "simpletest.rc-qcg8j" in namespace "gc-7213"
    Jan 13 10:53:30.388: INFO: Deleting pod "simpletest.rc-qcrdl" in namespace "gc-7213"
    Jan 13 10:53:30.436: INFO: Deleting pod "simpletest.rc-qdbh5" in namespace "gc-7213"
    Jan 13 10:53:30.498: INFO: Deleting pod "simpletest.rc-qm569" in namespace "gc-7213"
    Jan 13 10:53:30.657: INFO: Deleting pod "simpletest.rc-qnc4r" in namespace "gc-7213"
    Jan 13 10:53:30.768: INFO: Deleting pod "simpletest.rc-qtpw4" in namespace "gc-7213"
    Jan 13 10:53:30.787: INFO: Deleting pod "simpletest.rc-rf7w4" in namespace "gc-7213"
    Jan 13 10:53:30.826: INFO: Deleting pod "simpletest.rc-sfdqr" in namespace "gc-7213"
    Jan 13 10:53:30.853: INFO: Deleting pod "simpletest.rc-sjbbs" in namespace "gc-7213"
    Jan 13 10:53:30.959: INFO: Deleting pod "simpletest.rc-sqhwm" in namespace "gc-7213"
    Jan 13 10:53:31.028: INFO: Deleting pod "simpletest.rc-t297t" in namespace "gc-7213"
    Jan 13 10:53:31.066: INFO: Deleting pod "simpletest.rc-tb95s" in namespace "gc-7213"
    Jan 13 10:53:31.132: INFO: Deleting pod "simpletest.rc-tdgsk" in namespace "gc-7213"
    Jan 13 10:53:31.181: INFO: Deleting pod "simpletest.rc-tfrxx" in namespace "gc-7213"
    Jan 13 10:53:31.299: INFO: Deleting pod "simpletest.rc-tmgkq" in namespace "gc-7213"
    Jan 13 10:53:31.425: INFO: Deleting pod "simpletest.rc-v5ckv" in namespace "gc-7213"
    Jan 13 10:53:31.554: INFO: Deleting pod "simpletest.rc-v6rbt" in namespace "gc-7213"
    Jan 13 10:53:31.616: INFO: Deleting pod "simpletest.rc-vllwl" in namespace "gc-7213"
    Jan 13 10:53:31.681: INFO: Deleting pod "simpletest.rc-vmtsl" in namespace "gc-7213"
    Jan 13 10:53:31.710: INFO: Deleting pod "simpletest.rc-vmxqb" in namespace "gc-7213"
    Jan 13 10:53:31.794: INFO: Deleting pod "simpletest.rc-vsr8s" in namespace "gc-7213"
    Jan 13 10:53:31.867: INFO: Deleting pod "simpletest.rc-vvj8x" in namespace "gc-7213"
    Jan 13 10:53:31.969: INFO: Deleting pod "simpletest.rc-vzpmq" in namespace "gc-7213"
    Jan 13 10:53:32.019: INFO: Deleting pod "simpletest.rc-wf7k4" in namespace "gc-7213"
    Jan 13 10:53:32.099: INFO: Deleting pod "simpletest.rc-wq59r" in namespace "gc-7213"
    Jan 13 10:53:32.148: INFO: Deleting pod "simpletest.rc-xgxlr" in namespace "gc-7213"
    Jan 13 10:53:32.203: INFO: Deleting pod "simpletest.rc-xk9fm" in namespace "gc-7213"
    Jan 13 10:53:32.320: INFO: Deleting pod "simpletest.rc-xpm9j" in namespace "gc-7213"
    Jan 13 10:53:32.341: INFO: Deleting pod "simpletest.rc-z27sp" in namespace "gc-7213"
    Jan 13 10:53:32.385: INFO: Deleting pod "simpletest.rc-z9zn2" in namespace "gc-7213"
    Jan 13 10:53:32.409: INFO: Deleting pod "simpletest.rc-zfz4c" in namespace "gc-7213"
    Jan 13 10:53:32.466: INFO: Deleting pod "simpletest.rc-zhhl4" in namespace "gc-7213"
    Jan 13 10:53:32.496: INFO: Deleting pod "simpletest.rc-zj4jg" in namespace "gc-7213"
    Jan 13 10:53:32.545: INFO: Deleting pod "simpletest.rc-zjh5n" in namespace "gc-7213"
    Jan 13 10:53:32.564: INFO: Deleting pod "simpletest.rc-zk7zt" in namespace "gc-7213"
    Jan 13 10:53:32.586: INFO: Deleting pod "simpletest.rc-zl5dc" in namespace "gc-7213"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:53:32.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7213" for this suite. 01/13/23 10:53:32.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:53:32.892
Jan 13 10:53:32.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename services 01/13/23 10:53:32.934
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:53:33.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:53:33.039
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/13/23 10:53:33.056
Jan 13 10:53:33.056: INFO: Creating e2e-svc-a-4xmtg
Jan 13 10:53:33.127: INFO: Creating e2e-svc-b-mznvw
Jan 13 10:53:33.181: INFO: Creating e2e-svc-c-tmtxc
STEP: deleting service collection 01/13/23 10:53:33.252
Jan 13 10:53:33.354: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 13 10:53:33.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4404" for this suite. 01/13/23 10:53:33.384
------------------------------
• [0.559 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:53:32.892
    Jan 13 10:53:32.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename services 01/13/23 10:53:32.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:53:33.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:53:33.039
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/13/23 10:53:33.056
    Jan 13 10:53:33.056: INFO: Creating e2e-svc-a-4xmtg
    Jan 13 10:53:33.127: INFO: Creating e2e-svc-b-mznvw
    Jan 13 10:53:33.181: INFO: Creating e2e-svc-c-tmtxc
    STEP: deleting service collection 01/13/23 10:53:33.252
    Jan 13 10:53:33.354: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:53:33.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4404" for this suite. 01/13/23 10:53:33.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:53:33.465
Jan 13 10:53:33.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename init-container 01/13/23 10:53:33.505
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:53:33.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:53:33.671
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/13/23 10:53:33.686
Jan 13 10:53:33.686: INFO: PodSpec: initContainers in spec.initContainers
Jan 13 10:54:32.687: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-480435cc-e2ba-4098-a0c0-48a92eddeb05", GenerateName:"", Namespace:"init-container-956", SelfLink:"", UID:"2db22cad-a5bd-4672-bbb9-b04e1deda5ed", ResourceVersion:"605248", Generation:0, CreationTimestamp:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"686710983"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"49c45ac5af4347e0ad89e59be88634be1ed438fc3b1d00413fec42b77c4c638a", "cni.projectcalico.org/podIP":"10.244.27.245/32", "cni.projectcalico.org/podIPs":"10.244.27.245/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000da6510), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 13, 10, 53, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000da65b8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 13, 10, 54, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000da65e8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-t9ljq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0044077c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t9ljq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t9ljq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t9ljq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002e7a210), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.10.102.31-node", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004038070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e7a290)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e7a2b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002e7a2b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002e7a2bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001081c90), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.102.31", PodIP:"10.244.27.245", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.27.245"}}, StartTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004038150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0040381c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"docker://sha256:d59c675982d8692814ec9e1486d4c645cd86ad825ef33975a5db196cf2801592", ContainerID:"docker://7747672ab3cb1cbb45e7b0f92d7c67766f72adf3eef68ee75f23af5d48336c95", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004407840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004407820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002e7a33f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:54:32.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-956" for this suite. 01/13/23 10:54:32.703
------------------------------
• [SLOW TEST] [59.252 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:53:33.465
    Jan 13 10:53:33.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename init-container 01/13/23 10:53:33.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:53:33.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:53:33.671
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/13/23 10:53:33.686
    Jan 13 10:53:33.686: INFO: PodSpec: initContainers in spec.initContainers
    Jan 13 10:54:32.687: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-480435cc-e2ba-4098-a0c0-48a92eddeb05", GenerateName:"", Namespace:"init-container-956", SelfLink:"", UID:"2db22cad-a5bd-4672-bbb9-b04e1deda5ed", ResourceVersion:"605248", Generation:0, CreationTimestamp:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"686710983"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"49c45ac5af4347e0ad89e59be88634be1ed438fc3b1d00413fec42b77c4c638a", "cni.projectcalico.org/podIP":"10.244.27.245/32", "cni.projectcalico.org/podIPs":"10.244.27.245/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000da6510), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 13, 10, 53, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000da65b8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 13, 10, 54, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000da65e8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-t9ljq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0044077c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t9ljq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t9ljq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t9ljq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002e7a210), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.10.102.31-node", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004038070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e7a290)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e7a2b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002e7a2b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002e7a2bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001081c90), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.102.31", PodIP:"10.244.27.245", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.27.245"}}, StartTime:time.Date(2023, time.January, 13, 10, 53, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004038150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0040381c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"docker://sha256:d59c675982d8692814ec9e1486d4c645cd86ad825ef33975a5db196cf2801592", ContainerID:"docker://7747672ab3cb1cbb45e7b0f92d7c67766f72adf3eef68ee75f23af5d48336c95", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004407840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004407820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002e7a33f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:54:32.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-956" for this suite. 01/13/23 10:54:32.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:54:32.72
Jan 13 10:54:32.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:54:32.737
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:32.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:32.789
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/13/23 10:54:32.798
Jan 13 10:54:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6846 cluster-info'
Jan 13 10:54:33.115: INFO: stderr: ""
Jan 13 10:54:33.115: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:54:33.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6846" for this suite. 01/13/23 10:54:33.125
------------------------------
• [0.417 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:54:32.72
    Jan 13 10:54:32.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:54:32.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:32.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:32.789
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/13/23 10:54:32.798
    Jan 13 10:54:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-6846 cluster-info'
    Jan 13 10:54:33.115: INFO: stderr: ""
    Jan 13 10:54:33.115: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:54:33.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6846" for this suite. 01/13/23 10:54:33.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:54:33.141
Jan 13 10:54:33.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename statefulset 01/13/23 10:54:33.143
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:33.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:33.183
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-480 01/13/23 10:54:33.189
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-480 01/13/23 10:54:33.199
Jan 13 10:54:33.230: INFO: Found 0 stateful pods, waiting for 1
Jan 13 10:54:43.239: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/13/23 10:54:43.271
STEP: updating a scale subresource 01/13/23 10:54:43.278
STEP: verifying the statefulset Spec.Replicas was modified 01/13/23 10:54:43.294
STEP: Patch a scale subresource 01/13/23 10:54:43.302
STEP: verifying the statefulset Spec.Replicas was modified 01/13/23 10:54:43.317
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 13 10:54:43.324: INFO: Deleting all statefulset in ns statefulset-480
Jan 13 10:54:43.333: INFO: Scaling statefulset ss to 0
Jan 13 10:54:53.401: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 10:54:53.406: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:54:53.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-480" for this suite. 01/13/23 10:54:53.439
------------------------------
• [SLOW TEST] [20.316 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:54:33.141
    Jan 13 10:54:33.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename statefulset 01/13/23 10:54:33.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:33.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:33.183
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-480 01/13/23 10:54:33.189
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-480 01/13/23 10:54:33.199
    Jan 13 10:54:33.230: INFO: Found 0 stateful pods, waiting for 1
    Jan 13 10:54:43.239: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/13/23 10:54:43.271
    STEP: updating a scale subresource 01/13/23 10:54:43.278
    STEP: verifying the statefulset Spec.Replicas was modified 01/13/23 10:54:43.294
    STEP: Patch a scale subresource 01/13/23 10:54:43.302
    STEP: verifying the statefulset Spec.Replicas was modified 01/13/23 10:54:43.317
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 13 10:54:43.324: INFO: Deleting all statefulset in ns statefulset-480
    Jan 13 10:54:43.333: INFO: Scaling statefulset ss to 0
    Jan 13 10:54:53.401: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 13 10:54:53.406: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:54:53.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-480" for this suite. 01/13/23 10:54:53.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:54:53.461
Jan 13 10:54:53.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename events 01/13/23 10:54:53.463
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:53.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:53.489
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/13/23 10:54:53.493
STEP: get a list of Events with a label in the current namespace 01/13/23 10:54:53.511
STEP: delete a list of events 01/13/23 10:54:53.518
Jan 13 10:54:53.518: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/13/23 10:54:53.541
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 13 10:54:53.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1837" for this suite. 01/13/23 10:54:53.556
------------------------------
• [0.106 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:54:53.461
    Jan 13 10:54:53.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename events 01/13/23 10:54:53.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:53.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:53.489
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/13/23 10:54:53.493
    STEP: get a list of Events with a label in the current namespace 01/13/23 10:54:53.511
    STEP: delete a list of events 01/13/23 10:54:53.518
    Jan 13 10:54:53.518: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/13/23 10:54:53.541
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:54:53.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1837" for this suite. 01/13/23 10:54:53.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:54:53.572
Jan 13 10:54:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:54:53.574
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:53.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:53.604
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 13 10:54:53.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1331 version'
Jan 13 10:54:53.855: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 13 10:54:53.855: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:54:53.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1331" for this suite. 01/13/23 10:54:53.866
------------------------------
• [0.315 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:54:53.572
    Jan 13 10:54:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:54:53.574
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:53.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:53.604
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 13 10:54:53.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1331 version'
    Jan 13 10:54:53.855: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 13 10:54:53.855: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:54:53.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1331" for this suite. 01/13/23 10:54:53.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:54:53.889
Jan 13 10:54:53.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename replicaset 01/13/23 10:54:53.892
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:53.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:53.943
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/13/23 10:54:53.96
Jan 13 10:54:53.986: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 13 10:54:59.021: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/13/23 10:54:59.021
STEP: getting scale subresource 01/13/23 10:54:59.021
STEP: updating a scale subresource 01/13/23 10:54:59.029
STEP: verifying the replicaset Spec.Replicas was modified 01/13/23 10:54:59.047
STEP: Patch a scale subresource 01/13/23 10:54:59.076
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 13 10:54:59.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7978" for this suite. 01/13/23 10:54:59.158
------------------------------
• [SLOW TEST] [5.302 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:54:53.889
    Jan 13 10:54:53.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename replicaset 01/13/23 10:54:53.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:53.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:53.943
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/13/23 10:54:53.96
    Jan 13 10:54:53.986: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 13 10:54:59.021: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/13/23 10:54:59.021
    STEP: getting scale subresource 01/13/23 10:54:59.021
    STEP: updating a scale subresource 01/13/23 10:54:59.029
    STEP: verifying the replicaset Spec.Replicas was modified 01/13/23 10:54:59.047
    STEP: Patch a scale subresource 01/13/23 10:54:59.076
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:54:59.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7978" for this suite. 01/13/23 10:54:59.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:54:59.198
Jan 13 10:54:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:54:59.202
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:59.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:59.319
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 13 10:54:59.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/13/23 10:55:01.925
Jan 13 10:55:01.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
Jan 13 10:55:04.240: INFO: stderr: ""
Jan 13 10:55:04.240: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 13 10:55:04.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 delete e2e-test-crd-publish-openapi-4155-crds test-foo'
Jan 13 10:55:04.520: INFO: stderr: ""
Jan 13 10:55:04.520: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 13 10:55:04.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 apply -f -'
Jan 13 10:55:05.476: INFO: stderr: ""
Jan 13 10:55:05.476: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 13 10:55:05.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 delete e2e-test-crd-publish-openapi-4155-crds test-foo'
Jan 13 10:55:05.769: INFO: stderr: ""
Jan 13 10:55:05.769: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/13/23 10:55:05.769
Jan 13 10:55:05.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
Jan 13 10:55:06.359: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/13/23 10:55:06.359
Jan 13 10:55:06.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
Jan 13 10:55:06.962: INFO: rc: 1
Jan 13 10:55:06.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 apply -f -'
Jan 13 10:55:07.514: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/13/23 10:55:07.514
Jan 13 10:55:07.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
Jan 13 10:55:08.094: INFO: rc: 1
Jan 13 10:55:08.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 apply -f -'
Jan 13 10:55:08.781: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/13/23 10:55:08.781
Jan 13 10:55:08.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds'
Jan 13 10:55:09.594: INFO: stderr: ""
Jan 13 10:55:09.594: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/13/23 10:55:09.595
Jan 13 10:55:09.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.metadata'
Jan 13 10:55:10.367: INFO: stderr: ""
Jan 13 10:55:10.368: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 13 10:55:10.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.spec'
Jan 13 10:55:11.372: INFO: stderr: ""
Jan 13 10:55:11.372: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 13 10:55:11.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.spec.bars'
Jan 13 10:55:12.210: INFO: stderr: ""
Jan 13 10:55:12.210: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/13/23 10:55:12.21
Jan 13 10:55:12.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.spec.bars2'
Jan 13 10:55:13.233: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:16.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9362" for this suite. 01/13/23 10:55:16.158
------------------------------
• [SLOW TEST] [16.972 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:54:59.198
    Jan 13 10:54:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:54:59.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:54:59.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:54:59.319
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 13 10:54:59.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/13/23 10:55:01.925
    Jan 13 10:55:01.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
    Jan 13 10:55:04.240: INFO: stderr: ""
    Jan 13 10:55:04.240: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 13 10:55:04.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 delete e2e-test-crd-publish-openapi-4155-crds test-foo'
    Jan 13 10:55:04.520: INFO: stderr: ""
    Jan 13 10:55:04.520: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 13 10:55:04.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 apply -f -'
    Jan 13 10:55:05.476: INFO: stderr: ""
    Jan 13 10:55:05.476: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 13 10:55:05.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 delete e2e-test-crd-publish-openapi-4155-crds test-foo'
    Jan 13 10:55:05.769: INFO: stderr: ""
    Jan 13 10:55:05.769: INFO: stdout: "e2e-test-crd-publish-openapi-4155-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/13/23 10:55:05.769
    Jan 13 10:55:05.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
    Jan 13 10:55:06.359: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/13/23 10:55:06.359
    Jan 13 10:55:06.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
    Jan 13 10:55:06.962: INFO: rc: 1
    Jan 13 10:55:06.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 apply -f -'
    Jan 13 10:55:07.514: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/13/23 10:55:07.514
    Jan 13 10:55:07.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 create -f -'
    Jan 13 10:55:08.094: INFO: rc: 1
    Jan 13 10:55:08.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 --namespace=crd-publish-openapi-9362 apply -f -'
    Jan 13 10:55:08.781: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/13/23 10:55:08.781
    Jan 13 10:55:08.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds'
    Jan 13 10:55:09.594: INFO: stderr: ""
    Jan 13 10:55:09.594: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/13/23 10:55:09.595
    Jan 13 10:55:09.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.metadata'
    Jan 13 10:55:10.367: INFO: stderr: ""
    Jan 13 10:55:10.368: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 13 10:55:10.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.spec'
    Jan 13 10:55:11.372: INFO: stderr: ""
    Jan 13 10:55:11.372: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 13 10:55:11.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.spec.bars'
    Jan 13 10:55:12.210: INFO: stderr: ""
    Jan 13 10:55:12.210: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4155-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/13/23 10:55:12.21
    Jan 13 10:55:12.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-9362 explain e2e-test-crd-publish-openapi-4155-crds.spec.bars2'
    Jan 13 10:55:13.233: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:16.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9362" for this suite. 01/13/23 10:55:16.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:16.172
Jan 13 10:55:16.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pods 01/13/23 10:55:16.176
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:16.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:16.205
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/13/23 10:55:16.218
Jan 13 10:55:16.239: INFO: created test-pod-1
Jan 13 10:55:16.254: INFO: created test-pod-2
Jan 13 10:55:16.268: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/13/23 10:55:16.268
Jan 13 10:55:16.268: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8031' to be running and ready
Jan 13 10:55:16.294: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:16.294: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:16.294: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:16.294: INFO: 0 / 3 pods in namespace 'pods-8031' are running and ready (0 seconds elapsed)
Jan 13 10:55:16.294: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
Jan 13 10:55:16.294: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
Jan 13 10:55:16.294: INFO: test-pod-1  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:16.295: INFO: test-pod-2  10.10.102.31-node  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:16.295: INFO: test-pod-3  10.10.102.31-node  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:16.295: INFO: 
Jan 13 10:55:18.334: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:18.335: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:18.335: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:18.335: INFO: 0 / 3 pods in namespace 'pods-8031' are running and ready (2 seconds elapsed)
Jan 13 10:55:18.335: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
Jan 13 10:55:18.335: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
Jan 13 10:55:18.335: INFO: test-pod-1  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:18.335: INFO: test-pod-2  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:18.335: INFO: test-pod-3  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:18.335: INFO: 
Jan 13 10:55:20.321: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:20.321: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 13 10:55:20.321: INFO: 1 / 3 pods in namespace 'pods-8031' are running and ready (4 seconds elapsed)
Jan 13 10:55:20.321: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
Jan 13 10:55:20.321: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
Jan 13 10:55:20.321: INFO: test-pod-1  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:20.321: INFO: test-pod-3  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
Jan 13 10:55:20.321: INFO: 
Jan 13 10:55:22.318: INFO: 3 / 3 pods in namespace 'pods-8031' are running and ready (6 seconds elapsed)
Jan 13 10:55:22.319: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/13/23 10:55:22.397
Jan 13 10:55:22.427: INFO: Pod quantity 3 is different from expected quantity 0
Jan 13 10:55:23.449: INFO: Pod quantity 3 is different from expected quantity 0
Jan 13 10:55:24.436: INFO: Pod quantity 2 is different from expected quantity 0
Jan 13 10:55:25.440: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:26.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8031" for this suite. 01/13/23 10:55:26.472
------------------------------
• [SLOW TEST] [10.341 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:16.172
    Jan 13 10:55:16.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pods 01/13/23 10:55:16.176
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:16.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:16.205
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/13/23 10:55:16.218
    Jan 13 10:55:16.239: INFO: created test-pod-1
    Jan 13 10:55:16.254: INFO: created test-pod-2
    Jan 13 10:55:16.268: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/13/23 10:55:16.268
    Jan 13 10:55:16.268: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8031' to be running and ready
    Jan 13 10:55:16.294: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:16.294: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:16.294: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:16.294: INFO: 0 / 3 pods in namespace 'pods-8031' are running and ready (0 seconds elapsed)
    Jan 13 10:55:16.294: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
    Jan 13 10:55:16.294: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
    Jan 13 10:55:16.294: INFO: test-pod-1  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:16.295: INFO: test-pod-2  10.10.102.31-node  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:16.295: INFO: test-pod-3  10.10.102.31-node  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:16.295: INFO: 
    Jan 13 10:55:18.334: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:18.335: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:18.335: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:18.335: INFO: 0 / 3 pods in namespace 'pods-8031' are running and ready (2 seconds elapsed)
    Jan 13 10:55:18.335: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
    Jan 13 10:55:18.335: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
    Jan 13 10:55:18.335: INFO: test-pod-1  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:18.335: INFO: test-pod-2  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:18.335: INFO: test-pod-3  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:18.335: INFO: 
    Jan 13 10:55:20.321: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:20.321: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 13 10:55:20.321: INFO: 1 / 3 pods in namespace 'pods-8031' are running and ready (4 seconds elapsed)
    Jan 13 10:55:20.321: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
    Jan 13 10:55:20.321: INFO: POD         NODE               PHASE    GRACE  CONDITIONS
    Jan 13 10:55:20.321: INFO: test-pod-1  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:20.321: INFO: test-pod-3  10.10.102.31-node  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-13 10:55:16 +0000 UTC  }]
    Jan 13 10:55:20.321: INFO: 
    Jan 13 10:55:22.318: INFO: 3 / 3 pods in namespace 'pods-8031' are running and ready (6 seconds elapsed)
    Jan 13 10:55:22.319: INFO: expected 0 pod replicas in namespace 'pods-8031', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/13/23 10:55:22.397
    Jan 13 10:55:22.427: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 13 10:55:23.449: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 13 10:55:24.436: INFO: Pod quantity 2 is different from expected quantity 0
    Jan 13 10:55:25.440: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:26.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8031" for this suite. 01/13/23 10:55:26.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:26.517
Jan 13 10:55:26.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename certificates 01/13/23 10:55:26.519
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:26.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:26.645
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/13/23 10:55:30.544
STEP: getting /apis/certificates.k8s.io 01/13/23 10:55:30.575
STEP: getting /apis/certificates.k8s.io/v1 01/13/23 10:55:30.595
STEP: creating 01/13/23 10:55:30.613
STEP: getting 01/13/23 10:55:30.674
STEP: listing 01/13/23 10:55:30.684
STEP: watching 01/13/23 10:55:30.706
Jan 13 10:55:30.707: INFO: starting watch
STEP: patching 01/13/23 10:55:30.711
STEP: updating 01/13/23 10:55:30.726
Jan 13 10:55:30.750: INFO: waiting for watch events with expected annotations
Jan 13 10:55:30.750: INFO: saw patched and updated annotations
STEP: getting /approval 01/13/23 10:55:30.751
STEP: patching /approval 01/13/23 10:55:30.767
STEP: updating /approval 01/13/23 10:55:30.811
STEP: getting /status 01/13/23 10:55:30.83
STEP: patching /status 01/13/23 10:55:30.839
STEP: updating /status 01/13/23 10:55:30.857
STEP: deleting 01/13/23 10:55:30.891
STEP: deleting a collection 01/13/23 10:55:30.933
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:31.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-6460" for this suite. 01/13/23 10:55:31.06
------------------------------
• [4.565 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:26.517
    Jan 13 10:55:26.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename certificates 01/13/23 10:55:26.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:26.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:26.645
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/13/23 10:55:30.544
    STEP: getting /apis/certificates.k8s.io 01/13/23 10:55:30.575
    STEP: getting /apis/certificates.k8s.io/v1 01/13/23 10:55:30.595
    STEP: creating 01/13/23 10:55:30.613
    STEP: getting 01/13/23 10:55:30.674
    STEP: listing 01/13/23 10:55:30.684
    STEP: watching 01/13/23 10:55:30.706
    Jan 13 10:55:30.707: INFO: starting watch
    STEP: patching 01/13/23 10:55:30.711
    STEP: updating 01/13/23 10:55:30.726
    Jan 13 10:55:30.750: INFO: waiting for watch events with expected annotations
    Jan 13 10:55:30.750: INFO: saw patched and updated annotations
    STEP: getting /approval 01/13/23 10:55:30.751
    STEP: patching /approval 01/13/23 10:55:30.767
    STEP: updating /approval 01/13/23 10:55:30.811
    STEP: getting /status 01/13/23 10:55:30.83
    STEP: patching /status 01/13/23 10:55:30.839
    STEP: updating /status 01/13/23 10:55:30.857
    STEP: deleting 01/13/23 10:55:30.891
    STEP: deleting a collection 01/13/23 10:55:30.933
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:31.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-6460" for this suite. 01/13/23 10:55:31.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:31.085
Jan 13 10:55:31.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:55:31.088
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:31.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:31.129
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-83424b50-076a-446c-9e6f-f75ff3bc3846 01/13/23 10:55:31.159
STEP: Creating secret with name s-test-opt-upd-7b616110-f075-4e23-a7e1-7aa3ffe065e2 01/13/23 10:55:31.169
STEP: Creating the pod 01/13/23 10:55:31.182
Jan 13 10:55:31.202: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70" in namespace "projected-4812" to be "running and ready"
Jan 13 10:55:31.210: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.959547ms
Jan 13 10:55:31.210: INFO: The phase of Pod pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:55:33.221: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018175825s
Jan 13 10:55:33.221: INFO: The phase of Pod pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:55:35.219: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70": Phase="Running", Reason="", readiness=true. Elapsed: 4.016753637s
Jan 13 10:55:35.219: INFO: The phase of Pod pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70 is Running (Ready = true)
Jan 13 10:55:35.219: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-83424b50-076a-446c-9e6f-f75ff3bc3846 01/13/23 10:55:35.327
STEP: Updating secret s-test-opt-upd-7b616110-f075-4e23-a7e1-7aa3ffe065e2 01/13/23 10:55:35.338
STEP: Creating secret with name s-test-opt-create-ed6ac9fe-2374-4bea-b231-193751db0ecc 01/13/23 10:55:35.359
STEP: waiting to observe update in volume 01/13/23 10:55:35.374
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:39.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4812" for this suite. 01/13/23 10:55:39.565
------------------------------
• [SLOW TEST] [8.504 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:31.085
    Jan 13 10:55:31.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:55:31.088
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:31.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:31.129
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-83424b50-076a-446c-9e6f-f75ff3bc3846 01/13/23 10:55:31.159
    STEP: Creating secret with name s-test-opt-upd-7b616110-f075-4e23-a7e1-7aa3ffe065e2 01/13/23 10:55:31.169
    STEP: Creating the pod 01/13/23 10:55:31.182
    Jan 13 10:55:31.202: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70" in namespace "projected-4812" to be "running and ready"
    Jan 13 10:55:31.210: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.959547ms
    Jan 13 10:55:31.210: INFO: The phase of Pod pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:55:33.221: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018175825s
    Jan 13 10:55:33.221: INFO: The phase of Pod pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:55:35.219: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70": Phase="Running", Reason="", readiness=true. Elapsed: 4.016753637s
    Jan 13 10:55:35.219: INFO: The phase of Pod pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70 is Running (Ready = true)
    Jan 13 10:55:35.219: INFO: Pod "pod-projected-secrets-b64dfb5d-f37c-4abc-952e-99f573f33b70" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-83424b50-076a-446c-9e6f-f75ff3bc3846 01/13/23 10:55:35.327
    STEP: Updating secret s-test-opt-upd-7b616110-f075-4e23-a7e1-7aa3ffe065e2 01/13/23 10:55:35.338
    STEP: Creating secret with name s-test-opt-create-ed6ac9fe-2374-4bea-b231-193751db0ecc 01/13/23 10:55:35.359
    STEP: waiting to observe update in volume 01/13/23 10:55:35.374
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:39.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4812" for this suite. 01/13/23 10:55:39.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:39.591
Jan 13 10:55:39.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename server-version 01/13/23 10:55:39.597
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:39.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:39.675
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/13/23 10:55:39.686
STEP: Confirm major version 01/13/23 10:55:39.69
Jan 13 10:55:39.690: INFO: Major version: 1
STEP: Confirm minor version 01/13/23 10:55:39.69
Jan 13 10:55:39.690: INFO: cleanMinorVersion: 26
Jan 13 10:55:39.690: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:39.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-5784" for this suite. 01/13/23 10:55:39.705
------------------------------
• [0.154 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:39.591
    Jan 13 10:55:39.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename server-version 01/13/23 10:55:39.597
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:39.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:39.675
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/13/23 10:55:39.686
    STEP: Confirm major version 01/13/23 10:55:39.69
    Jan 13 10:55:39.690: INFO: Major version: 1
    STEP: Confirm minor version 01/13/23 10:55:39.69
    Jan 13 10:55:39.690: INFO: cleanMinorVersion: 26
    Jan 13 10:55:39.690: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:39.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-5784" for this suite. 01/13/23 10:55:39.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:39.746
Jan 13 10:55:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename limitrange 01/13/23 10:55:39.751
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:39.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:39.812
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-slrz8" in namespace "limitrange-7895" 01/13/23 10:55:39.828
STEP: Creating another limitRange in another namespace 01/13/23 10:55:39.847
Jan 13 10:55:39.907: INFO: Namespace "e2e-limitrange-slrz8-8000" created
Jan 13 10:55:39.907: INFO: Creating LimitRange "e2e-limitrange-slrz8" in namespace "e2e-limitrange-slrz8-8000"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-slrz8" 01/13/23 10:55:39.919
Jan 13 10:55:39.932: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-slrz8" in "limitrange-7895" namespace 01/13/23 10:55:39.932
Jan 13 10:55:39.948: INFO: LimitRange "e2e-limitrange-slrz8" has been patched
STEP: Delete LimitRange "e2e-limitrange-slrz8" by Collection with labelSelector: "e2e-limitrange-slrz8=patched" 01/13/23 10:55:39.949
STEP: Confirm that the limitRange "e2e-limitrange-slrz8" has been deleted 01/13/23 10:55:39.978
Jan 13 10:55:39.979: INFO: Requesting list of LimitRange to confirm quantity
Jan 13 10:55:40.012: INFO: Found 0 LimitRange with label "e2e-limitrange-slrz8=patched"
Jan 13 10:55:40.013: INFO: LimitRange "e2e-limitrange-slrz8" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-slrz8" 01/13/23 10:55:40.013
Jan 13 10:55:40.023: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:40.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7895" for this suite. 01/13/23 10:55:40.037
STEP: Destroying namespace "e2e-limitrange-slrz8-8000" for this suite. 01/13/23 10:55:40.054
------------------------------
• [0.337 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:39.746
    Jan 13 10:55:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename limitrange 01/13/23 10:55:39.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:39.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:39.812
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-slrz8" in namespace "limitrange-7895" 01/13/23 10:55:39.828
    STEP: Creating another limitRange in another namespace 01/13/23 10:55:39.847
    Jan 13 10:55:39.907: INFO: Namespace "e2e-limitrange-slrz8-8000" created
    Jan 13 10:55:39.907: INFO: Creating LimitRange "e2e-limitrange-slrz8" in namespace "e2e-limitrange-slrz8-8000"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-slrz8" 01/13/23 10:55:39.919
    Jan 13 10:55:39.932: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-slrz8" in "limitrange-7895" namespace 01/13/23 10:55:39.932
    Jan 13 10:55:39.948: INFO: LimitRange "e2e-limitrange-slrz8" has been patched
    STEP: Delete LimitRange "e2e-limitrange-slrz8" by Collection with labelSelector: "e2e-limitrange-slrz8=patched" 01/13/23 10:55:39.949
    STEP: Confirm that the limitRange "e2e-limitrange-slrz8" has been deleted 01/13/23 10:55:39.978
    Jan 13 10:55:39.979: INFO: Requesting list of LimitRange to confirm quantity
    Jan 13 10:55:40.012: INFO: Found 0 LimitRange with label "e2e-limitrange-slrz8=patched"
    Jan 13 10:55:40.013: INFO: LimitRange "e2e-limitrange-slrz8" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-slrz8" 01/13/23 10:55:40.013
    Jan 13 10:55:40.023: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:40.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7895" for this suite. 01/13/23 10:55:40.037
    STEP: Destroying namespace "e2e-limitrange-slrz8-8000" for this suite. 01/13/23 10:55:40.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:40.086
Jan 13 10:55:40.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:55:40.089
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:40.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:40.164
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/13/23 10:55:40.18
STEP: watching for the ServiceAccount to be added 01/13/23 10:55:40.21
STEP: patching the ServiceAccount 01/13/23 10:55:40.218
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/13/23 10:55:40.241
STEP: deleting the ServiceAccount 01/13/23 10:55:40.271
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:40.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8252" for this suite. 01/13/23 10:55:40.334
------------------------------
• [0.263 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:40.086
    Jan 13 10:55:40.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:55:40.089
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:40.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:40.164
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/13/23 10:55:40.18
    STEP: watching for the ServiceAccount to be added 01/13/23 10:55:40.21
    STEP: patching the ServiceAccount 01/13/23 10:55:40.218
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/13/23 10:55:40.241
    STEP: deleting the ServiceAccount 01/13/23 10:55:40.271
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:40.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8252" for this suite. 01/13/23 10:55:40.334
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:40.351
Jan 13 10:55:40.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:55:40.354
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:40.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:40.408
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 13 10:55:40.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/13/23 10:55:43.135
Jan 13 10:55:43.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 create -f -'
Jan 13 10:55:45.187: INFO: stderr: ""
Jan 13 10:55:45.188: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 13 10:55:45.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 delete e2e-test-crd-publish-openapi-888-crds test-cr'
Jan 13 10:55:45.727: INFO: stderr: ""
Jan 13 10:55:45.727: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 13 10:55:45.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 apply -f -'
Jan 13 10:55:46.496: INFO: stderr: ""
Jan 13 10:55:46.496: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 13 10:55:46.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 delete e2e-test-crd-publish-openapi-888-crds test-cr'
Jan 13 10:55:46.768: INFO: stderr: ""
Jan 13 10:55:46.768: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/13/23 10:55:46.768
Jan 13 10:55:46.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 explain e2e-test-crd-publish-openapi-888-crds'
Jan 13 10:55:47.419: INFO: stderr: ""
Jan 13 10:55:47.419: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-888-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:55:50.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3971" for this suite. 01/13/23 10:55:50.814
------------------------------
• [SLOW TEST] [10.473 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:40.351
    Jan 13 10:55:40.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 10:55:40.354
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:40.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:40.408
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 13 10:55:40.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/13/23 10:55:43.135
    Jan 13 10:55:43.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 create -f -'
    Jan 13 10:55:45.187: INFO: stderr: ""
    Jan 13 10:55:45.188: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 13 10:55:45.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 delete e2e-test-crd-publish-openapi-888-crds test-cr'
    Jan 13 10:55:45.727: INFO: stderr: ""
    Jan 13 10:55:45.727: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 13 10:55:45.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 apply -f -'
    Jan 13 10:55:46.496: INFO: stderr: ""
    Jan 13 10:55:46.496: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 13 10:55:46.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 --namespace=crd-publish-openapi-3971 delete e2e-test-crd-publish-openapi-888-crds test-cr'
    Jan 13 10:55:46.768: INFO: stderr: ""
    Jan 13 10:55:46.768: INFO: stdout: "e2e-test-crd-publish-openapi-888-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/13/23 10:55:46.768
    Jan 13 10:55:46.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=crd-publish-openapi-3971 explain e2e-test-crd-publish-openapi-888-crds'
    Jan 13 10:55:47.419: INFO: stderr: ""
    Jan 13 10:55:47.419: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-888-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:55:50.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3971" for this suite. 01/13/23 10:55:50.814
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:55:50.825
Jan 13 10:55:50.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename resourcequota 01/13/23 10:55:50.827
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:50.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:50.856
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/13/23 10:55:50.861
STEP: Counting existing ResourceQuota 01/13/23 10:55:55.872
STEP: Creating a ResourceQuota 01/13/23 10:56:00.893
STEP: Ensuring resource quota status is calculated 01/13/23 10:56:00.922
STEP: Creating a Secret 01/13/23 10:56:02.929
STEP: Ensuring resource quota status captures secret creation 01/13/23 10:56:02.95
STEP: Deleting a secret 01/13/23 10:56:04.975
STEP: Ensuring resource quota status released usage 01/13/23 10:56:04.992
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 13 10:56:07.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5244" for this suite. 01/13/23 10:56:07.034
------------------------------
• [SLOW TEST] [16.231 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:55:50.825
    Jan 13 10:55:50.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename resourcequota 01/13/23 10:55:50.827
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:55:50.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:55:50.856
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/13/23 10:55:50.861
    STEP: Counting existing ResourceQuota 01/13/23 10:55:55.872
    STEP: Creating a ResourceQuota 01/13/23 10:56:00.893
    STEP: Ensuring resource quota status is calculated 01/13/23 10:56:00.922
    STEP: Creating a Secret 01/13/23 10:56:02.929
    STEP: Ensuring resource quota status captures secret creation 01/13/23 10:56:02.95
    STEP: Deleting a secret 01/13/23 10:56:04.975
    STEP: Ensuring resource quota status released usage 01/13/23 10:56:04.992
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:56:07.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5244" for this suite. 01/13/23 10:56:07.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:56:07.057
Jan 13 10:56:07.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 10:56:07.061
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:07.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:07.125
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/13/23 10:56:07.139
STEP: waiting for pod running 01/13/23 10:56:07.161
Jan 13 10:56:07.161: INFO: Waiting up to 2m0s for pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" in namespace "var-expansion-2531" to be "running"
Jan 13 10:56:07.170: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.421008ms
Jan 13 10:56:09.178: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016732546s
Jan 13 10:56:11.198: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Running", Reason="", readiness=true. Elapsed: 4.036432684s
Jan 13 10:56:11.198: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" satisfied condition "running"
STEP: creating a file in subpath 01/13/23 10:56:11.198
Jan 13 10:56:11.222: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2531 PodName:var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:56:11.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:56:11.224: INFO: ExecWithOptions: Clientset creation
Jan 13 10:56:11.224: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2531/pods/var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/13/23 10:56:11.438
Jan 13 10:56:11.461: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2531 PodName:var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:56:11.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:56:11.463: INFO: ExecWithOptions: Clientset creation
Jan 13 10:56:11.463: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2531/pods/var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/13/23 10:56:11.711
Jan 13 10:56:12.237: INFO: Successfully updated pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a"
STEP: waiting for annotated pod running 01/13/23 10:56:12.238
Jan 13 10:56:12.238: INFO: Waiting up to 2m0s for pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" in namespace "var-expansion-2531" to be "running"
Jan 13 10:56:12.245: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Running", Reason="", readiness=true. Elapsed: 6.750955ms
Jan 13 10:56:12.245: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" satisfied condition "running"
STEP: deleting the pod gracefully 01/13/23 10:56:12.245
Jan 13 10:56:12.245: INFO: Deleting pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" in namespace "var-expansion-2531"
Jan 13 10:56:12.253: INFO: Wait up to 5m0s for pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 10:56:46.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2531" for this suite. 01/13/23 10:56:46.277
------------------------------
• [SLOW TEST] [39.232 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:56:07.057
    Jan 13 10:56:07.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 10:56:07.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:07.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:07.125
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/13/23 10:56:07.139
    STEP: waiting for pod running 01/13/23 10:56:07.161
    Jan 13 10:56:07.161: INFO: Waiting up to 2m0s for pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" in namespace "var-expansion-2531" to be "running"
    Jan 13 10:56:07.170: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.421008ms
    Jan 13 10:56:09.178: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016732546s
    Jan 13 10:56:11.198: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Running", Reason="", readiness=true. Elapsed: 4.036432684s
    Jan 13 10:56:11.198: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" satisfied condition "running"
    STEP: creating a file in subpath 01/13/23 10:56:11.198
    Jan 13 10:56:11.222: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2531 PodName:var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:56:11.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:56:11.224: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:56:11.224: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2531/pods/var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/13/23 10:56:11.438
    Jan 13 10:56:11.461: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2531 PodName:var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:56:11.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:56:11.463: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:56:11.463: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2531/pods/var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/13/23 10:56:11.711
    Jan 13 10:56:12.237: INFO: Successfully updated pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a"
    STEP: waiting for annotated pod running 01/13/23 10:56:12.238
    Jan 13 10:56:12.238: INFO: Waiting up to 2m0s for pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" in namespace "var-expansion-2531" to be "running"
    Jan 13 10:56:12.245: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a": Phase="Running", Reason="", readiness=true. Elapsed: 6.750955ms
    Jan 13 10:56:12.245: INFO: Pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" satisfied condition "running"
    STEP: deleting the pod gracefully 01/13/23 10:56:12.245
    Jan 13 10:56:12.245: INFO: Deleting pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" in namespace "var-expansion-2531"
    Jan 13 10:56:12.253: INFO: Wait up to 5m0s for pod "var-expansion-d3b55ceb-b82c-478e-927c-bc0d8610074a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:56:46.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2531" for this suite. 01/13/23 10:56:46.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:56:46.291
Jan 13 10:56:46.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename webhook 01/13/23 10:56:46.294
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:46.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:46.328
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/13/23 10:56:46.366
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:56:47.664
STEP: Deploying the webhook pod 01/13/23 10:56:47.677
STEP: Wait for the deployment to be ready 01/13/23 10:56:47.727
Jan 13 10:56:47.746: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 10:56:49.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/13/23 10:56:51.782
STEP: Verifying the service has paired with the endpoint 01/13/23 10:56:51.807
Jan 13 10:56:52.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/13/23 10:56:52.816
STEP: create a pod that should be updated by the webhook 01/13/23 10:56:52.868
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:56:52.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1681" for this suite. 01/13/23 10:56:53.05
STEP: Destroying namespace "webhook-1681-markers" for this suite. 01/13/23 10:56:53.073
------------------------------
• [SLOW TEST] [6.838 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:56:46.291
    Jan 13 10:56:46.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename webhook 01/13/23 10:56:46.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:46.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:46.328
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/13/23 10:56:46.366
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/13/23 10:56:47.664
    STEP: Deploying the webhook pod 01/13/23 10:56:47.677
    STEP: Wait for the deployment to be ready 01/13/23 10:56:47.727
    Jan 13 10:56:47.746: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 13 10:56:49.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 13, 10, 56, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/13/23 10:56:51.782
    STEP: Verifying the service has paired with the endpoint 01/13/23 10:56:51.807
    Jan 13 10:56:52.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/13/23 10:56:52.816
    STEP: create a pod that should be updated by the webhook 01/13/23 10:56:52.868
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:56:52.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1681" for this suite. 01/13/23 10:56:53.05
    STEP: Destroying namespace "webhook-1681-markers" for this suite. 01/13/23 10:56:53.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:56:53.133
Jan 13 10:56:53.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename init-container 01/13/23 10:56:53.138
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:53.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:53.196
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/13/23 10:56:53.206
Jan 13 10:56:53.207: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 13 10:56:58.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6240" for this suite. 01/13/23 10:56:58.87
------------------------------
• [SLOW TEST] [5.749 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:56:53.133
    Jan 13 10:56:53.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename init-container 01/13/23 10:56:53.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:53.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:53.196
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/13/23 10:56:53.206
    Jan 13 10:56:53.207: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:56:58.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6240" for this suite. 01/13/23 10:56:58.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:56:58.892
Jan 13 10:56:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:56:58.895
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:58.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:58.929
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-d4233a04-2408-497c-90a4-6378cd54d8b7 01/13/23 10:56:58.939
STEP: Creating a pod to test consume secrets 01/13/23 10:56:58.949
Jan 13 10:56:58.971: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67" in namespace "projected-2631" to be "Succeeded or Failed"
Jan 13 10:56:58.980: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 9.387797ms
Jan 13 10:57:00.994: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023231169s
Jan 13 10:57:02.997: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025597124s
Jan 13 10:57:04.994: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02330528s
Jan 13 10:57:07.000: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.028910706s
STEP: Saw pod success 01/13/23 10:57:07
Jan 13 10:57:07.000: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67" satisfied condition "Succeeded or Failed"
Jan 13 10:57:07.022: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/13/23 10:57:07.109
Jan 13 10:57:07.144: INFO: Waiting for pod pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67 to disappear
Jan 13 10:57:07.157: INFO: Pod pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 13 10:57:07.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2631" for this suite. 01/13/23 10:57:07.17
------------------------------
• [SLOW TEST] [8.322 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:56:58.892
    Jan 13 10:56:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:56:58.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:56:58.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:56:58.929
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-d4233a04-2408-497c-90a4-6378cd54d8b7 01/13/23 10:56:58.939
    STEP: Creating a pod to test consume secrets 01/13/23 10:56:58.949
    Jan 13 10:56:58.971: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67" in namespace "projected-2631" to be "Succeeded or Failed"
    Jan 13 10:56:58.980: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 9.387797ms
    Jan 13 10:57:00.994: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023231169s
    Jan 13 10:57:02.997: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025597124s
    Jan 13 10:57:04.994: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02330528s
    Jan 13 10:57:07.000: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.028910706s
    STEP: Saw pod success 01/13/23 10:57:07
    Jan 13 10:57:07.000: INFO: Pod "pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67" satisfied condition "Succeeded or Failed"
    Jan 13 10:57:07.022: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 10:57:07.109
    Jan 13 10:57:07.144: INFO: Waiting for pod pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67 to disappear
    Jan 13 10:57:07.157: INFO: Pod pod-projected-secrets-0913c3b2-f2d5-4bb0-9ece-8e59dd8baa67 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:57:07.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2631" for this suite. 01/13/23 10:57:07.17
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:57:07.214
Jan 13 10:57:07.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename emptydir 01/13/23 10:57:07.217
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:57:07.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:57:07.308
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/13/23 10:57:07.321
Jan 13 10:57:07.346: INFO: Waiting up to 5m0s for pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459" in namespace "emptydir-518" to be "Succeeded or Failed"
Jan 13 10:57:07.358: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Pending", Reason="", readiness=false. Elapsed: 11.605688ms
Jan 13 10:57:09.366: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019482924s
Jan 13 10:57:11.367: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020508492s
Jan 13 10:57:13.370: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023588287s
STEP: Saw pod success 01/13/23 10:57:13.37
Jan 13 10:57:13.370: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459" satisfied condition "Succeeded or Failed"
Jan 13 10:57:13.379: INFO: Trying to get logs from node 10.10.102.31-node pod pod-c495bad3-95a2-43ff-87cd-4034c1f83459 container test-container: <nil>
STEP: delete the pod 01/13/23 10:57:13.427
Jan 13 10:57:13.465: INFO: Waiting for pod pod-c495bad3-95a2-43ff-87cd-4034c1f83459 to disappear
Jan 13 10:57:13.471: INFO: Pod pod-c495bad3-95a2-43ff-87cd-4034c1f83459 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 13 10:57:13.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-518" for this suite. 01/13/23 10:57:13.483
------------------------------
• [SLOW TEST] [6.280 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:57:07.214
    Jan 13 10:57:07.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename emptydir 01/13/23 10:57:07.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:57:07.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:57:07.308
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/13/23 10:57:07.321
    Jan 13 10:57:07.346: INFO: Waiting up to 5m0s for pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459" in namespace "emptydir-518" to be "Succeeded or Failed"
    Jan 13 10:57:07.358: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Pending", Reason="", readiness=false. Elapsed: 11.605688ms
    Jan 13 10:57:09.366: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019482924s
    Jan 13 10:57:11.367: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020508492s
    Jan 13 10:57:13.370: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023588287s
    STEP: Saw pod success 01/13/23 10:57:13.37
    Jan 13 10:57:13.370: INFO: Pod "pod-c495bad3-95a2-43ff-87cd-4034c1f83459" satisfied condition "Succeeded or Failed"
    Jan 13 10:57:13.379: INFO: Trying to get logs from node 10.10.102.31-node pod pod-c495bad3-95a2-43ff-87cd-4034c1f83459 container test-container: <nil>
    STEP: delete the pod 01/13/23 10:57:13.427
    Jan 13 10:57:13.465: INFO: Waiting for pod pod-c495bad3-95a2-43ff-87cd-4034c1f83459 to disappear
    Jan 13 10:57:13.471: INFO: Pod pod-c495bad3-95a2-43ff-87cd-4034c1f83459 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:57:13.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-518" for this suite. 01/13/23 10:57:13.483
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:57:13.495
Jan 13 10:57:13.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 10:57:13.498
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:57:13.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:57:13.541
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 13 10:57:13.568: INFO: Waiting up to 2m0s for pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" in namespace "var-expansion-6052" to be "container 0 failed with reason CreateContainerConfigError"
Jan 13 10:57:13.575: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122905ms
Jan 13 10:57:15.581: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012807724s
Jan 13 10:57:17.584: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015402674s
Jan 13 10:57:17.584: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 13 10:57:17.584: INFO: Deleting pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" in namespace "var-expansion-6052"
Jan 13 10:57:17.594: INFO: Wait up to 5m0s for pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 10:57:21.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6052" for this suite. 01/13/23 10:57:21.626
------------------------------
• [SLOW TEST] [8.144 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:57:13.495
    Jan 13 10:57:13.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 10:57:13.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:57:13.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:57:13.541
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 13 10:57:13.568: INFO: Waiting up to 2m0s for pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" in namespace "var-expansion-6052" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 13 10:57:13.575: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122905ms
    Jan 13 10:57:15.581: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012807724s
    Jan 13 10:57:17.584: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015402674s
    Jan 13 10:57:17.584: INFO: Pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 13 10:57:17.584: INFO: Deleting pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" in namespace "var-expansion-6052"
    Jan 13 10:57:17.594: INFO: Wait up to 5m0s for pod "var-expansion-b8dcbe8f-95ac-40c3-bea6-469131e7dfe9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:57:21.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6052" for this suite. 01/13/23 10:57:21.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:57:21.645
Jan 13 10:57:21.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 10:57:21.648
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:57:21.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:57:21.695
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/13/23 10:57:21.704
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_tcp@PTR;sleep 1; done
 01/13/23 10:57:21.738
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_tcp@PTR;sleep 1; done
 01/13/23 10:57:21.739
STEP: creating a pod to probe DNS 01/13/23 10:57:21.739
STEP: submitting the pod to kubernetes 01/13/23 10:57:21.739
Jan 13 10:57:21.759: INFO: Waiting up to 15m0s for pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b" in namespace "dns-6876" to be "running"
Jan 13 10:57:21.778: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.975508ms
Jan 13 10:57:23.788: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028791462s
Jan 13 10:57:25.785: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b": Phase="Running", Reason="", readiness=true. Elapsed: 4.025432311s
Jan 13 10:57:25.785: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b" satisfied condition "running"
STEP: retrieving the pod 01/13/23 10:57:25.785
STEP: looking for the results for each expected name from probers 01/13/23 10:57:25.793
Jan 13 10:57:25.801: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.815: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.824: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.883: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.898: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.909: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.925: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:25.984: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

Jan 13 10:57:31.053: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.114: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.125: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.210: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.231: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.266: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.284: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:31.353: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

Jan 13 10:57:35.993: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.000: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.005: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.012: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.055: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.060: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.065: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:36.104: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

Jan 13 10:57:41.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.052: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.107: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.162: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.170: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.184: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.203: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:41.256: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

Jan 13 10:57:46.001: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.010: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.018: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.064: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.075: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.083: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.094: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:46.127: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

Jan 13 10:57:50.997: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.005: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.031: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.107: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.115: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.129: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.139: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:51.175: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

Jan 13 10:57:55.997: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:56.002: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:56.007: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:56.046: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:56.051: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:56.057: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
Jan 13 10:57:56.077: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

Jan 13 10:58:01.285: INFO: DNS probes using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b succeeded

STEP: deleting the pod 01/13/23 10:58:01.285
STEP: deleting the test service 01/13/23 10:58:01.308
STEP: deleting the test headless service 01/13/23 10:58:01.377
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 10:58:01.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6876" for this suite. 01/13/23 10:58:01.42
------------------------------
• [SLOW TEST] [39.797 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:57:21.645
    Jan 13 10:57:21.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 10:57:21.648
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:57:21.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:57:21.695
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/13/23 10:57:21.704
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_tcp@PTR;sleep 1; done
     01/13/23 10:57:21.738
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6876.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6876.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6876.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.231.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.231.87_tcp@PTR;sleep 1; done
     01/13/23 10:57:21.739
    STEP: creating a pod to probe DNS 01/13/23 10:57:21.739
    STEP: submitting the pod to kubernetes 01/13/23 10:57:21.739
    Jan 13 10:57:21.759: INFO: Waiting up to 15m0s for pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b" in namespace "dns-6876" to be "running"
    Jan 13 10:57:21.778: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.975508ms
    Jan 13 10:57:23.788: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028791462s
    Jan 13 10:57:25.785: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b": Phase="Running", Reason="", readiness=true. Elapsed: 4.025432311s
    Jan 13 10:57:25.785: INFO: Pod "dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 10:57:25.785
    STEP: looking for the results for each expected name from probers 01/13/23 10:57:25.793
    Jan 13 10:57:25.801: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.815: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.824: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.883: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.898: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.909: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.925: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:25.984: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

    Jan 13 10:57:31.053: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.114: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.125: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.210: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.231: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.266: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.284: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:31.353: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

    Jan 13 10:57:35.993: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.000: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.005: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.012: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.055: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.060: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.065: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:36.104: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

    Jan 13 10:57:41.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.052: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.107: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.162: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.170: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.184: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.203: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:41.256: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

    Jan 13 10:57:46.001: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.010: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.018: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.064: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.075: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.083: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.094: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:46.127: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

    Jan 13 10:57:50.997: INFO: Unable to read wheezy_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.005: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.031: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.107: INFO: Unable to read jessie_udp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.115: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.129: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.139: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:51.175: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_udp@dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_udp@dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

    Jan 13 10:57:55.997: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:56.002: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:56.007: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:56.046: INFO: Unable to read jessie_tcp@dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:56.051: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:56.057: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local from pod dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b: the server could not find the requested resource (get pods dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b)
    Jan 13 10:57:56.077: INFO: Lookups using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b failed for: [wheezy_tcp@dns-test-service.dns-6876.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@dns-test-service.dns-6876.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6876.svc.cluster.local]

    Jan 13 10:58:01.285: INFO: DNS probes using dns-6876/dns-test-c553ab7a-1cc8-4edf-a9c2-3812ac0ae90b succeeded

    STEP: deleting the pod 01/13/23 10:58:01.285
    STEP: deleting the test service 01/13/23 10:58:01.308
    STEP: deleting the test headless service 01/13/23 10:58:01.377
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:58:01.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6876" for this suite. 01/13/23 10:58:01.42
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:58:01.445
Jan 13 10:58:01.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pod-network-test 01/13/23 10:58:01.448
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:58:01.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:58:01.486
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3790 01/13/23 10:58:01.492
STEP: creating a selector 01/13/23 10:58:01.493
STEP: Creating the service pods in kubernetes 01/13/23 10:58:01.493
Jan 13 10:58:01.493: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 10:58:01.538: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3790" to be "running and ready"
Jan 13 10:58:01.551: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.853249ms
Jan 13 10:58:01.551: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:58:03.569: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030693496s
Jan 13 10:58:03.569: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 10:58:05.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023739405s
Jan 13 10:58:05.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:07.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021778312s
Jan 13 10:58:07.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:09.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024999091s
Jan 13 10:58:09.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:11.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020125607s
Jan 13 10:58:11.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:13.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019939994s
Jan 13 10:58:13.558: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:15.572: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.033496993s
Jan 13 10:58:15.572: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:17.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.020367659s
Jan 13 10:58:17.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:19.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.021506605s
Jan 13 10:58:19.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:21.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02298111s
Jan 13 10:58:21.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 10:58:23.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023489394s
Jan 13 10:58:23.562: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 13 10:58:23.562: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 13 10:58:23.571: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3790" to be "running and ready"
Jan 13 10:58:23.580: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.650977ms
Jan 13 10:58:23.580: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 13 10:58:23.580: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/13/23 10:58:23.589
Jan 13 10:58:23.614: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3790" to be "running"
Jan 13 10:58:23.628: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.051077ms
Jan 13 10:58:25.634: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019763101s
Jan 13 10:58:27.640: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025204943s
Jan 13 10:58:27.640: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 13 10:58:27.652: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3790" to be "running"
Jan 13 10:58:27.660: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.082703ms
Jan 13 10:58:27.660: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 13 10:58:27.670: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 10:58:27.670: INFO: Going to poll 10.244.27.202 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 13 10:58:27.678: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.27.202 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3790 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:58:27.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:58:27.680: INFO: ExecWithOptions: Clientset creation
Jan 13 10:58:27.680: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3790/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.27.202+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 13 10:58:28.992: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 13 10:58:28.992: INFO: Going to poll 10.244.169.10 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 13 10:58:28.998: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.169.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3790 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 10:58:28.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 10:58:29.000: INFO: ExecWithOptions: Clientset creation
Jan 13 10:58:29.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3790/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.169.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 13 10:58:30.230: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 13 10:58:30.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3790" for this suite. 01/13/23 10:58:30.242
------------------------------
• [SLOW TEST] [28.816 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:58:01.445
    Jan 13 10:58:01.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pod-network-test 01/13/23 10:58:01.448
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:58:01.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:58:01.486
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3790 01/13/23 10:58:01.492
    STEP: creating a selector 01/13/23 10:58:01.493
    STEP: Creating the service pods in kubernetes 01/13/23 10:58:01.493
    Jan 13 10:58:01.493: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 13 10:58:01.538: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3790" to be "running and ready"
    Jan 13 10:58:01.551: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.853249ms
    Jan 13 10:58:01.551: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:58:03.569: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030693496s
    Jan 13 10:58:03.569: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 10:58:05.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023739405s
    Jan 13 10:58:05.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:07.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021778312s
    Jan 13 10:58:07.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:09.563: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024999091s
    Jan 13 10:58:09.563: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:11.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020125607s
    Jan 13 10:58:11.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:13.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019939994s
    Jan 13 10:58:13.558: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:15.572: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.033496993s
    Jan 13 10:58:15.572: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:17.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.020367659s
    Jan 13 10:58:17.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:19.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.021506605s
    Jan 13 10:58:19.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:21.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02298111s
    Jan 13 10:58:21.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 10:58:23.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023489394s
    Jan 13 10:58:23.562: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 13 10:58:23.562: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 13 10:58:23.571: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3790" to be "running and ready"
    Jan 13 10:58:23.580: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.650977ms
    Jan 13 10:58:23.580: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 13 10:58:23.580: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/13/23 10:58:23.589
    Jan 13 10:58:23.614: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3790" to be "running"
    Jan 13 10:58:23.628: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.051077ms
    Jan 13 10:58:25.634: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019763101s
    Jan 13 10:58:27.640: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025204943s
    Jan 13 10:58:27.640: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 13 10:58:27.652: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3790" to be "running"
    Jan 13 10:58:27.660: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.082703ms
    Jan 13 10:58:27.660: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 13 10:58:27.670: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 13 10:58:27.670: INFO: Going to poll 10.244.27.202 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 13 10:58:27.678: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.27.202 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3790 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:58:27.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:58:27.680: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:58:27.680: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3790/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.27.202+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 13 10:58:28.992: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 13 10:58:28.992: INFO: Going to poll 10.244.169.10 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 13 10:58:28.998: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.169.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3790 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 10:58:28.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 10:58:29.000: INFO: ExecWithOptions: Clientset creation
    Jan 13 10:58:29.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3790/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.169.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 13 10:58:30.230: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:58:30.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3790" for this suite. 01/13/23 10:58:30.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:58:30.266
Jan 13 10:58:30.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 10:58:30.268
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:58:30.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:58:30.312
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:58:30.323
Jan 13 10:58:30.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7" in namespace "downward-api-7792" to be "Succeeded or Failed"
Jan 13 10:58:30.350: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.794413ms
Jan 13 10:58:32.361: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020262007s
Jan 13 10:58:34.372: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031114719s
Jan 13 10:58:36.361: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019473056s
Jan 13 10:58:38.361: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020044652s
STEP: Saw pod success 01/13/23 10:58:38.361
Jan 13 10:58:38.362: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7" satisfied condition "Succeeded or Failed"
Jan 13 10:58:38.375: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7 container client-container: <nil>
STEP: delete the pod 01/13/23 10:58:38.395
Jan 13 10:58:38.432: INFO: Waiting for pod downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7 to disappear
Jan 13 10:58:38.450: INFO: Pod downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 10:58:38.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7792" for this suite. 01/13/23 10:58:38.462
------------------------------
• [SLOW TEST] [8.208 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:58:30.266
    Jan 13 10:58:30.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 10:58:30.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:58:30.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:58:30.312
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:58:30.323
    Jan 13 10:58:30.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7" in namespace "downward-api-7792" to be "Succeeded or Failed"
    Jan 13 10:58:30.350: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.794413ms
    Jan 13 10:58:32.361: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020262007s
    Jan 13 10:58:34.372: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031114719s
    Jan 13 10:58:36.361: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019473056s
    Jan 13 10:58:38.361: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020044652s
    STEP: Saw pod success 01/13/23 10:58:38.361
    Jan 13 10:58:38.362: INFO: Pod "downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7" satisfied condition "Succeeded or Failed"
    Jan 13 10:58:38.375: INFO: Trying to get logs from node 10.10.102.31-node pod downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7 container client-container: <nil>
    STEP: delete the pod 01/13/23 10:58:38.395
    Jan 13 10:58:38.432: INFO: Waiting for pod downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7 to disappear
    Jan 13 10:58:38.450: INFO: Pod downwardapi-volume-5cf6d8df-8a71-4c8e-bcb3-ac3a8e3076c7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:58:38.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7792" for this suite. 01/13/23 10:58:38.462
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:58:38.475
Jan 13 10:58:38.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename subpath 01/13/23 10:58:38.477
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:58:38.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:58:38.525
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/13/23 10:58:38.536
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-gt2l 01/13/23 10:58:38.558
STEP: Creating a pod to test atomic-volume-subpath 01/13/23 10:58:38.558
Jan 13 10:58:38.587: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-gt2l" in namespace "subpath-3750" to be "Succeeded or Failed"
Jan 13 10:58:38.598: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Pending", Reason="", readiness=false. Elapsed: 10.807115ms
Jan 13 10:58:40.618: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030951707s
Jan 13 10:58:42.617: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 4.030713512s
Jan 13 10:58:44.607: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 6.019760291s
Jan 13 10:58:46.612: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 8.025133409s
Jan 13 10:58:48.610: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 10.023438077s
Jan 13 10:58:50.608: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 12.020892696s
Jan 13 10:58:52.609: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 14.021754766s
Jan 13 10:58:54.606: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 16.019113268s
Jan 13 10:58:56.620: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 18.033622647s
Jan 13 10:58:58.608: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 20.021611303s
Jan 13 10:59:00.616: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 22.029711146s
Jan 13 10:59:02.606: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=false. Elapsed: 24.019597741s
Jan 13 10:59:04.607: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=false. Elapsed: 26.020326966s
Jan 13 10:59:06.616: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.028789652s
STEP: Saw pod success 01/13/23 10:59:06.616
Jan 13 10:59:06.616: INFO: Pod "pod-subpath-test-projected-gt2l" satisfied condition "Succeeded or Failed"
Jan 13 10:59:06.624: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-projected-gt2l container test-container-subpath-projected-gt2l: <nil>
STEP: delete the pod 01/13/23 10:59:06.654
Jan 13 10:59:06.677: INFO: Waiting for pod pod-subpath-test-projected-gt2l to disappear
Jan 13 10:59:06.692: INFO: Pod pod-subpath-test-projected-gt2l no longer exists
STEP: Deleting pod pod-subpath-test-projected-gt2l 01/13/23 10:59:06.692
Jan 13 10:59:06.693: INFO: Deleting pod "pod-subpath-test-projected-gt2l" in namespace "subpath-3750"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 13 10:59:06.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3750" for this suite. 01/13/23 10:59:06.716
------------------------------
• [SLOW TEST] [28.254 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:58:38.475
    Jan 13 10:58:38.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename subpath 01/13/23 10:58:38.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:58:38.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:58:38.525
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/13/23 10:58:38.536
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-gt2l 01/13/23 10:58:38.558
    STEP: Creating a pod to test atomic-volume-subpath 01/13/23 10:58:38.558
    Jan 13 10:58:38.587: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-gt2l" in namespace "subpath-3750" to be "Succeeded or Failed"
    Jan 13 10:58:38.598: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Pending", Reason="", readiness=false. Elapsed: 10.807115ms
    Jan 13 10:58:40.618: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030951707s
    Jan 13 10:58:42.617: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 4.030713512s
    Jan 13 10:58:44.607: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 6.019760291s
    Jan 13 10:58:46.612: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 8.025133409s
    Jan 13 10:58:48.610: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 10.023438077s
    Jan 13 10:58:50.608: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 12.020892696s
    Jan 13 10:58:52.609: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 14.021754766s
    Jan 13 10:58:54.606: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 16.019113268s
    Jan 13 10:58:56.620: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 18.033622647s
    Jan 13 10:58:58.608: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 20.021611303s
    Jan 13 10:59:00.616: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=true. Elapsed: 22.029711146s
    Jan 13 10:59:02.606: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=false. Elapsed: 24.019597741s
    Jan 13 10:59:04.607: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Running", Reason="", readiness=false. Elapsed: 26.020326966s
    Jan 13 10:59:06.616: INFO: Pod "pod-subpath-test-projected-gt2l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.028789652s
    STEP: Saw pod success 01/13/23 10:59:06.616
    Jan 13 10:59:06.616: INFO: Pod "pod-subpath-test-projected-gt2l" satisfied condition "Succeeded or Failed"
    Jan 13 10:59:06.624: INFO: Trying to get logs from node 10.10.102.31-node pod pod-subpath-test-projected-gt2l container test-container-subpath-projected-gt2l: <nil>
    STEP: delete the pod 01/13/23 10:59:06.654
    Jan 13 10:59:06.677: INFO: Waiting for pod pod-subpath-test-projected-gt2l to disappear
    Jan 13 10:59:06.692: INFO: Pod pod-subpath-test-projected-gt2l no longer exists
    STEP: Deleting pod pod-subpath-test-projected-gt2l 01/13/23 10:59:06.692
    Jan 13 10:59:06.693: INFO: Deleting pod "pod-subpath-test-projected-gt2l" in namespace "subpath-3750"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:59:06.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3750" for this suite. 01/13/23 10:59:06.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:59:06.733
Jan 13 10:59:06.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 10:59:06.736
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:06.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:06.782
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/13/23 10:59:06.791
Jan 13 10:59:06.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1587 create -f -'
Jan 13 10:59:08.738: INFO: stderr: ""
Jan 13 10:59:08.738: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/13/23 10:59:08.738
Jan 13 10:59:09.761: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:59:09.761: INFO: Found 0 / 1
Jan 13 10:59:10.750: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:59:10.750: INFO: Found 0 / 1
Jan 13 10:59:11.752: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:59:11.752: INFO: Found 0 / 1
Jan 13 10:59:12.758: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:59:12.758: INFO: Found 1 / 1
Jan 13 10:59:12.758: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/13/23 10:59:12.758
Jan 13 10:59:12.774: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:59:12.775: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 13 10:59:12.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1587 patch pod agnhost-primary-s5d2r -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 13 10:59:13.094: INFO: stderr: ""
Jan 13 10:59:13.094: INFO: stdout: "pod/agnhost-primary-s5d2r patched\n"
STEP: checking annotations 01/13/23 10:59:13.094
Jan 13 10:59:13.105: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 10:59:13.105: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 10:59:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1587" for this suite. 01/13/23 10:59:13.114
------------------------------
• [SLOW TEST] [6.398 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:59:06.733
    Jan 13 10:59:06.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 10:59:06.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:06.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:06.782
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/13/23 10:59:06.791
    Jan 13 10:59:06.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1587 create -f -'
    Jan 13 10:59:08.738: INFO: stderr: ""
    Jan 13 10:59:08.738: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/13/23 10:59:08.738
    Jan 13 10:59:09.761: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:59:09.761: INFO: Found 0 / 1
    Jan 13 10:59:10.750: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:59:10.750: INFO: Found 0 / 1
    Jan 13 10:59:11.752: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:59:11.752: INFO: Found 0 / 1
    Jan 13 10:59:12.758: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:59:12.758: INFO: Found 1 / 1
    Jan 13 10:59:12.758: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/13/23 10:59:12.758
    Jan 13 10:59:12.774: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:59:12.775: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 13 10:59:12.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-1587 patch pod agnhost-primary-s5d2r -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 13 10:59:13.094: INFO: stderr: ""
    Jan 13 10:59:13.094: INFO: stdout: "pod/agnhost-primary-s5d2r patched\n"
    STEP: checking annotations 01/13/23 10:59:13.094
    Jan 13 10:59:13.105: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 13 10:59:13.105: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:59:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1587" for this suite. 01/13/23 10:59:13.114
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:59:13.132
Jan 13 10:59:13.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 10:59:13.134
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:13.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:13.197
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-ec037da8-ab4c-4321-9f87-964d5fc6e58b 01/13/23 10:59:13.203
STEP: Creating a pod to test consume configMaps 01/13/23 10:59:13.21
Jan 13 10:59:13.224: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105" in namespace "projected-1637" to be "Succeeded or Failed"
Jan 13 10:59:13.233: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 8.350466ms
Jan 13 10:59:15.244: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019258301s
Jan 13 10:59:17.245: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019974735s
Jan 13 10:59:19.246: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020975902s
Jan 13 10:59:21.244: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019869854s
STEP: Saw pod success 01/13/23 10:59:21.244
Jan 13 10:59:21.245: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105" satisfied condition "Succeeded or Failed"
Jan 13 10:59:21.255: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105 container agnhost-container: <nil>
STEP: delete the pod 01/13/23 10:59:21.281
Jan 13 10:59:21.337: INFO: Waiting for pod pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105 to disappear
Jan 13 10:59:21.344: INFO: Pod pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 13 10:59:21.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1637" for this suite. 01/13/23 10:59:21.353
------------------------------
• [SLOW TEST] [8.236 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:59:13.132
    Jan 13 10:59:13.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 10:59:13.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:13.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:13.197
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-ec037da8-ab4c-4321-9f87-964d5fc6e58b 01/13/23 10:59:13.203
    STEP: Creating a pod to test consume configMaps 01/13/23 10:59:13.21
    Jan 13 10:59:13.224: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105" in namespace "projected-1637" to be "Succeeded or Failed"
    Jan 13 10:59:13.233: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 8.350466ms
    Jan 13 10:59:15.244: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019258301s
    Jan 13 10:59:17.245: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019974735s
    Jan 13 10:59:19.246: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020975902s
    Jan 13 10:59:21.244: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019869854s
    STEP: Saw pod success 01/13/23 10:59:21.244
    Jan 13 10:59:21.245: INFO: Pod "pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105" satisfied condition "Succeeded or Failed"
    Jan 13 10:59:21.255: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105 container agnhost-container: <nil>
    STEP: delete the pod 01/13/23 10:59:21.281
    Jan 13 10:59:21.337: INFO: Waiting for pod pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105 to disappear
    Jan 13 10:59:21.344: INFO: Pod pod-projected-configmaps-895d9b68-d1be-4525-83c4-00ee418be105 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:59:21.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1637" for this suite. 01/13/23 10:59:21.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:59:21.379
Jan 13 10:59:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:59:21.382
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:21.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:21.506
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 13 10:59:21.559: INFO: created pod pod-service-account-defaultsa
Jan 13 10:59:21.559: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 13 10:59:21.572: INFO: created pod pod-service-account-mountsa
Jan 13 10:59:21.572: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 13 10:59:21.590: INFO: created pod pod-service-account-nomountsa
Jan 13 10:59:21.590: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 13 10:59:21.610: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 13 10:59:21.610: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 13 10:59:21.623: INFO: created pod pod-service-account-mountsa-mountspec
Jan 13 10:59:21.623: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 13 10:59:21.635: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 13 10:59:21.636: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 13 10:59:21.646: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 13 10:59:21.646: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 13 10:59:21.658: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 13 10:59:21.658: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 13 10:59:21.682: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 13 10:59:21.682: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 13 10:59:21.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3819" for this suite. 01/13/23 10:59:21.697
------------------------------
• [0.334 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:59:21.379
    Jan 13 10:59:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename svcaccounts 01/13/23 10:59:21.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:21.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:21.506
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 13 10:59:21.559: INFO: created pod pod-service-account-defaultsa
    Jan 13 10:59:21.559: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 13 10:59:21.572: INFO: created pod pod-service-account-mountsa
    Jan 13 10:59:21.572: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 13 10:59:21.590: INFO: created pod pod-service-account-nomountsa
    Jan 13 10:59:21.590: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 13 10:59:21.610: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 13 10:59:21.610: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 13 10:59:21.623: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 13 10:59:21.623: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 13 10:59:21.635: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 13 10:59:21.636: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 13 10:59:21.646: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 13 10:59:21.646: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 13 10:59:21.658: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 13 10:59:21.658: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 13 10:59:21.682: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 13 10:59:21.682: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:59:21.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3819" for this suite. 01/13/23 10:59:21.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:59:21.716
Jan 13 10:59:21.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 10:59:21.719
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:21.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:21.776
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/13/23 10:59:21.793
Jan 13 10:59:21.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af" in namespace "downward-api-6869" to be "Succeeded or Failed"
Jan 13 10:59:21.825: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 7.313539ms
Jan 13 10:59:23.847: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029073594s
Jan 13 10:59:25.836: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018446818s
Jan 13 10:59:27.845: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027490739s
Jan 13 10:59:29.842: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024076153s
STEP: Saw pod success 01/13/23 10:59:29.842
Jan 13 10:59:29.842: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af" satisfied condition "Succeeded or Failed"
Jan 13 10:59:29.850: INFO: Trying to get logs from node 10.10.102.32-node pod downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af container client-container: <nil>
STEP: delete the pod 01/13/23 10:59:29.939
Jan 13 10:59:29.995: INFO: Waiting for pod downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af to disappear
Jan 13 10:59:30.005: INFO: Pod downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 10:59:30.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6869" for this suite. 01/13/23 10:59:30.02
------------------------------
• [SLOW TEST] [8.338 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:59:21.716
    Jan 13 10:59:21.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 10:59:21.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:21.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:21.776
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/13/23 10:59:21.793
    Jan 13 10:59:21.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af" in namespace "downward-api-6869" to be "Succeeded or Failed"
    Jan 13 10:59:21.825: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 7.313539ms
    Jan 13 10:59:23.847: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029073594s
    Jan 13 10:59:25.836: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018446818s
    Jan 13 10:59:27.845: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027490739s
    Jan 13 10:59:29.842: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024076153s
    STEP: Saw pod success 01/13/23 10:59:29.842
    Jan 13 10:59:29.842: INFO: Pod "downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af" satisfied condition "Succeeded or Failed"
    Jan 13 10:59:29.850: INFO: Trying to get logs from node 10.10.102.32-node pod downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af container client-container: <nil>
    STEP: delete the pod 01/13/23 10:59:29.939
    Jan 13 10:59:29.995: INFO: Waiting for pod downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af to disappear
    Jan 13 10:59:30.005: INFO: Pod downwardapi-volume-03c58b68-2358-490b-9041-7b6a0250e5af no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:59:30.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6869" for this suite. 01/13/23 10:59:30.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:59:30.058
Jan 13 10:59:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename secrets 01/13/23 10:59:30.061
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:30.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:30.161
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/13/23 10:59:30.172
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/13/23 10:59:30.181
STEP: patching the secret 01/13/23 10:59:30.187
STEP: deleting the secret using a LabelSelector 01/13/23 10:59:30.208
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/13/23 10:59:30.231
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 13 10:59:30.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-542" for this suite. 01/13/23 10:59:30.257
------------------------------
• [0.217 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:59:30.058
    Jan 13 10:59:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename secrets 01/13/23 10:59:30.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:30.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:30.161
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/13/23 10:59:30.172
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/13/23 10:59:30.181
    STEP: patching the secret 01/13/23 10:59:30.187
    STEP: deleting the secret using a LabelSelector 01/13/23 10:59:30.208
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/13/23 10:59:30.231
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 13 10:59:30.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-542" for this suite. 01/13/23 10:59:30.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 10:59:30.277
Jan 13 10:59:30.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename dns 01/13/23 10:59:30.28
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:30.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:30.318
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/13/23 10:59:30.326
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local;sleep 1; done
 01/13/23 10:59:30.339
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local;sleep 1; done
 01/13/23 10:59:30.34
STEP: creating a pod to probe DNS 01/13/23 10:59:30.34
STEP: submitting the pod to kubernetes 01/13/23 10:59:30.341
Jan 13 10:59:30.358: INFO: Waiting up to 15m0s for pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2" in namespace "dns-6990" to be "running"
Jan 13 10:59:30.368: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.610846ms
Jan 13 10:59:32.378: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020125564s
Jan 13 10:59:34.378: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2": Phase="Running", Reason="", readiness=true. Elapsed: 4.019719103s
Jan 13 10:59:34.378: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2" satisfied condition "running"
STEP: retrieving the pod 01/13/23 10:59:34.378
STEP: looking for the results for each expected name from probers 01/13/23 10:59:34.385
Jan 13 10:59:34.405: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.419: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.429: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.446: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.454: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.465: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.472: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.484: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:34.484: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

Jan 13 10:59:39.501: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.551: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.570: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.589: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.598: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.617: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.635: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.660: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:39.660: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

Jan 13 10:59:44.498: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.512: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.528: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.537: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.548: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.562: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.575: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.586: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:44.586: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

Jan 13 10:59:49.503: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.511: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.519: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.529: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.537: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.546: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.556: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.565: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:49.565: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

Jan 13 10:59:54.532: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.639: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.674: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.701: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.740: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.752: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.783: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.794: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:54.794: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

Jan 13 10:59:59.499: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.513: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.526: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.534: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.541: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.550: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.560: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.571: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 10:59:59.571: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

Jan 13 11:00:04.579: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 11:00:04.608: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 11:00:04.626: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 11:00:04.643: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 11:00:04.651: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
Jan 13 11:00:04.651: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

Jan 13 11:00:09.576: INFO: DNS probes using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 succeeded

STEP: deleting the pod 01/13/23 11:00:09.576
STEP: deleting the test headless service 01/13/23 11:00:09.632
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 13 11:00:09.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6990" for this suite. 01/13/23 11:00:09.698
------------------------------
• [SLOW TEST] [39.451 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 10:59:30.277
    Jan 13 10:59:30.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename dns 01/13/23 10:59:30.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 10:59:30.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 10:59:30.318
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/13/23 10:59:30.326
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local;sleep 1; done
     01/13/23 10:59:30.339
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6990.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local;sleep 1; done
     01/13/23 10:59:30.34
    STEP: creating a pod to probe DNS 01/13/23 10:59:30.34
    STEP: submitting the pod to kubernetes 01/13/23 10:59:30.341
    Jan 13 10:59:30.358: INFO: Waiting up to 15m0s for pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2" in namespace "dns-6990" to be "running"
    Jan 13 10:59:30.368: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.610846ms
    Jan 13 10:59:32.378: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020125564s
    Jan 13 10:59:34.378: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2": Phase="Running", Reason="", readiness=true. Elapsed: 4.019719103s
    Jan 13 10:59:34.378: INFO: Pod "dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2" satisfied condition "running"
    STEP: retrieving the pod 01/13/23 10:59:34.378
    STEP: looking for the results for each expected name from probers 01/13/23 10:59:34.385
    Jan 13 10:59:34.405: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.419: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.429: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.446: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.454: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.465: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.472: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.484: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:34.484: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

    Jan 13 10:59:39.501: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.551: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.570: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.589: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.598: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.617: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.635: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.660: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:39.660: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

    Jan 13 10:59:44.498: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.512: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.528: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.537: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.548: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.562: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.575: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.586: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:44.586: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

    Jan 13 10:59:49.503: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.511: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.519: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.529: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.537: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.546: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.556: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.565: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:49.565: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

    Jan 13 10:59:54.532: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.639: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.674: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.701: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.740: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.752: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.783: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.794: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:54.794: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

    Jan 13 10:59:59.499: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.513: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.526: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.534: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.541: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.550: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.560: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.571: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 10:59:59.571: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6990.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

    Jan 13 11:00:04.579: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 11:00:04.608: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 11:00:04.626: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 11:00:04.643: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 11:00:04.651: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local from pod dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2: the server could not find the requested resource (get pods dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2)
    Jan 13 11:00:04.651: INFO: Lookups using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 failed for: [wheezy_tcp@dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6990.svc.cluster.local jessie_udp@dns-test-service-2.dns-6990.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6990.svc.cluster.local]

    Jan 13 11:00:09.576: INFO: DNS probes using dns-6990/dns-test-e388ff7f-1c8c-4ff2-8aa3-8980e48518a2 succeeded

    STEP: deleting the pod 01/13/23 11:00:09.576
    STEP: deleting the test headless service 01/13/23 11:00:09.632
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:00:09.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6990" for this suite. 01/13/23 11:00:09.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:00:09.732
Jan 13 11:00:09.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename projected 01/13/23 11:00:09.737
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:09.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:09.819
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-2257998c-17df-4dc4-9225-1e8f633f13ae 01/13/23 11:00:09.827
STEP: Creating a pod to test consume secrets 01/13/23 11:00:09.843
Jan 13 11:00:09.900: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107" in namespace "projected-2295" to be "Succeeded or Failed"
Jan 13 11:00:09.909: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Pending", Reason="", readiness=false. Elapsed: 8.423888ms
Jan 13 11:00:11.917: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016333245s
Jan 13 11:00:13.930: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029377119s
Jan 13 11:00:15.917: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016183647s
STEP: Saw pod success 01/13/23 11:00:15.917
Jan 13 11:00:15.917: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107" satisfied condition "Succeeded or Failed"
Jan 13 11:00:15.922: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/13/23 11:00:15.939
Jan 13 11:00:15.956: INFO: Waiting for pod pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107 to disappear
Jan 13 11:00:15.961: INFO: Pod pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 13 11:00:15.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2295" for this suite. 01/13/23 11:00:15.969
------------------------------
• [SLOW TEST] [6.250 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:00:09.732
    Jan 13 11:00:09.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename projected 01/13/23 11:00:09.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:09.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:09.819
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-2257998c-17df-4dc4-9225-1e8f633f13ae 01/13/23 11:00:09.827
    STEP: Creating a pod to test consume secrets 01/13/23 11:00:09.843
    Jan 13 11:00:09.900: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107" in namespace "projected-2295" to be "Succeeded or Failed"
    Jan 13 11:00:09.909: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Pending", Reason="", readiness=false. Elapsed: 8.423888ms
    Jan 13 11:00:11.917: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016333245s
    Jan 13 11:00:13.930: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029377119s
    Jan 13 11:00:15.917: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016183647s
    STEP: Saw pod success 01/13/23 11:00:15.917
    Jan 13 11:00:15.917: INFO: Pod "pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107" satisfied condition "Succeeded or Failed"
    Jan 13 11:00:15.922: INFO: Trying to get logs from node 10.10.102.31-node pod pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/13/23 11:00:15.939
    Jan 13 11:00:15.956: INFO: Waiting for pod pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107 to disappear
    Jan 13 11:00:15.961: INFO: Pod pod-projected-secrets-dc5e76a9-0d84-4d26-8c4f-a1e7ed0be107 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:00:15.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2295" for this suite. 01/13/23 11:00:15.969
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:00:15.983
Jan 13 11:00:15.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 11:00:15.985
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:16.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:16.014
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/13/23 11:00:16.021
Jan 13 11:00:16.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 11:00:19.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 13 11:00:30.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7892" for this suite. 01/13/23 11:00:31.035
------------------------------
• [SLOW TEST] [15.077 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:00:15.983
    Jan 13 11:00:15.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename crd-publish-openapi 01/13/23 11:00:15.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:16.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:16.014
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/13/23 11:00:16.021
    Jan 13 11:00:16.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 11:00:19.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:00:30.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7892" for this suite. 01/13/23 11:00:31.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:00:31.063
Jan 13 11:00:31.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 11:00:31.069
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:31.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:31.137
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/13/23 11:00:31.146
Jan 13 11:00:31.168: INFO: Waiting up to 5m0s for pod "downward-api-016a8226-95fd-4111-930a-91da5df12579" in namespace "downward-api-8242" to be "Succeeded or Failed"
Jan 13 11:00:31.194: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Pending", Reason="", readiness=false. Elapsed: 26.565327ms
Jan 13 11:00:33.206: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037857674s
Jan 13 11:00:35.222: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054770994s
Jan 13 11:00:37.203: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035687799s
STEP: Saw pod success 01/13/23 11:00:37.204
Jan 13 11:00:37.204: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579" satisfied condition "Succeeded or Failed"
Jan 13 11:00:37.209: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-016a8226-95fd-4111-930a-91da5df12579 container dapi-container: <nil>
STEP: delete the pod 01/13/23 11:00:37.23
Jan 13 11:00:37.263: INFO: Waiting for pod downward-api-016a8226-95fd-4111-930a-91da5df12579 to disappear
Jan 13 11:00:37.268: INFO: Pod downward-api-016a8226-95fd-4111-930a-91da5df12579 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 13 11:00:37.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8242" for this suite. 01/13/23 11:00:37.276
------------------------------
• [SLOW TEST] [6.224 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:00:31.063
    Jan 13 11:00:31.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 11:00:31.069
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:31.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:31.137
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/13/23 11:00:31.146
    Jan 13 11:00:31.168: INFO: Waiting up to 5m0s for pod "downward-api-016a8226-95fd-4111-930a-91da5df12579" in namespace "downward-api-8242" to be "Succeeded or Failed"
    Jan 13 11:00:31.194: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Pending", Reason="", readiness=false. Elapsed: 26.565327ms
    Jan 13 11:00:33.206: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037857674s
    Jan 13 11:00:35.222: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054770994s
    Jan 13 11:00:37.203: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035687799s
    STEP: Saw pod success 01/13/23 11:00:37.204
    Jan 13 11:00:37.204: INFO: Pod "downward-api-016a8226-95fd-4111-930a-91da5df12579" satisfied condition "Succeeded or Failed"
    Jan 13 11:00:37.209: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-016a8226-95fd-4111-930a-91da5df12579 container dapi-container: <nil>
    STEP: delete the pod 01/13/23 11:00:37.23
    Jan 13 11:00:37.263: INFO: Waiting for pod downward-api-016a8226-95fd-4111-930a-91da5df12579 to disappear
    Jan 13 11:00:37.268: INFO: Pod downward-api-016a8226-95fd-4111-930a-91da5df12579 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:00:37.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8242" for this suite. 01/13/23 11:00:37.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:00:37.291
Jan 13 11:00:37.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 11:00:37.294
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:37.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:37.323
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/13/23 11:00:37.334
Jan 13 11:00:37.347: INFO: Waiting up to 5m0s for pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d" in namespace "downward-api-2761" to be "running and ready"
Jan 13 11:00:37.352: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177372ms
Jan 13 11:00:37.352: INFO: The phase of Pod labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d is Pending, waiting for it to be Running (with Ready = true)
Jan 13 11:00:39.362: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0146582s
Jan 13 11:00:39.362: INFO: The phase of Pod labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d is Pending, waiting for it to be Running (with Ready = true)
Jan 13 11:00:41.362: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d": Phase="Running", Reason="", readiness=true. Elapsed: 4.014857893s
Jan 13 11:00:41.362: INFO: The phase of Pod labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d is Running (Ready = true)
Jan 13 11:00:41.362: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d" satisfied condition "running and ready"
Jan 13 11:00:41.943: INFO: Successfully updated pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 13 11:00:44.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2761" for this suite. 01/13/23 11:00:44.017
------------------------------
• [SLOW TEST] [6.749 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:00:37.291
    Jan 13 11:00:37.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 11:00:37.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:37.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:37.323
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/13/23 11:00:37.334
    Jan 13 11:00:37.347: INFO: Waiting up to 5m0s for pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d" in namespace "downward-api-2761" to be "running and ready"
    Jan 13 11:00:37.352: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.177372ms
    Jan 13 11:00:37.352: INFO: The phase of Pod labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 11:00:39.362: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0146582s
    Jan 13 11:00:39.362: INFO: The phase of Pod labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 11:00:41.362: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d": Phase="Running", Reason="", readiness=true. Elapsed: 4.014857893s
    Jan 13 11:00:41.362: INFO: The phase of Pod labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d is Running (Ready = true)
    Jan 13 11:00:41.362: INFO: Pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d" satisfied condition "running and ready"
    Jan 13 11:00:41.943: INFO: Successfully updated pod "labelsupdate2f807e40-c2a0-48c9-a33b-a1871c17930d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:00:44.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2761" for this suite. 01/13/23 11:00:44.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:00:44.044
Jan 13 11:00:44.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename deployment 01/13/23 11:00:44.047
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:44.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:44.098
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/13/23 11:00:44.118
STEP: waiting for Deployment to be created 01/13/23 11:00:44.147
STEP: waiting for all Replicas to be Ready 01/13/23 11:00:44.15
Jan 13 11:00:44.157: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:44.157: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:44.173: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:44.173: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:44.232: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:44.232: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:44.305: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:44.305: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 11:00:47.726: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 13 11:00:47.726: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 13 11:00:47.810: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/13/23 11:00:47.81
W0113 11:00:47.851298      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 13 11:00:47.858: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/13/23 11:00:47.858
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:47.864: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:47.864: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:47.901: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:47.901: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:48.012: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:48.012: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:48.082: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:48.082: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:48.117: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:48.117: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:51.710: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:51.710: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:51.811: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
STEP: listing Deployments 01/13/23 11:00:51.811
Jan 13 11:00:51.853: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/13/23 11:00:51.853
Jan 13 11:00:51.888: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/13/23 11:00:51.888
Jan 13 11:00:51.921: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:51.948: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:52.022: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:52.056: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:54.403: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:54.678: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:54.894: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:54.943: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 11:00:57.113: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/13/23 11:00:57.152
STEP: fetching the DeploymentStatus 01/13/23 11:00:57.169
Jan 13 11:00:57.184: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3
Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3
STEP: deleting the Deployment 01/13/23 11:00:57.19
Jan 13 11:00:57.209: INFO: observed event type MODIFIED
Jan 13 11:00:57.211: INFO: observed event type MODIFIED
Jan 13 11:00:57.211: INFO: observed event type MODIFIED
Jan 13 11:00:57.211: INFO: observed event type MODIFIED
Jan 13 11:00:57.211: INFO: observed event type MODIFIED
Jan 13 11:00:57.212: INFO: observed event type MODIFIED
Jan 13 11:00:57.212: INFO: observed event type MODIFIED
Jan 13 11:00:57.212: INFO: observed event type MODIFIED
Jan 13 11:00:57.212: INFO: observed event type MODIFIED
Jan 13 11:00:57.212: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 13 11:00:57.237: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 13 11:00:57.247: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-276  eeabf12f-c472-49c8-b24e-ef93915d4b45 607350 2 2023-01-13 11:00:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d2c361a0-612f-4702-8f1d-e5a91f3e005f 0xc004354767 0xc004354768}] [] [{kube-controller-manager Update apps/v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2c361a0-612f-4702-8f1d-e5a91f3e005f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004354810 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 13 11:00:57.264: INFO: pod: "test-deployment-7b7876f9d6-lm2gl":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-lm2gl test-deployment-7b7876f9d6- deployment-276  15611533-4fec-47eb-9016-baaa07a2760f 607311 0 2023-01-13 11:00:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:275964a5d6363bbf283067465e388487108372315beadc6d5563d76654f077cd cni.projectcalico.org/podIP:10.244.27.207/32 cni.projectcalico.org/podIPs:10.244.27.207/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 eeabf12f-c472-49c8-b24e-ef93915d4b45 0xc002c82677 0xc002c82678}] [] [{kube-controller-manager Update v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eeabf12f-c472-49c8-b24e-ef93915d4b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 11:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8xl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8xl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.207,StartTime:2023-01-13 11:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 11:00:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://ead2655affa1f8f6377b2ab18d74247bf65aff2cfb57fd6ccb283c9b7ff5041a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 13 11:00:57.265: INFO: pod: "test-deployment-7b7876f9d6-x7j2k":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-x7j2k test-deployment-7b7876f9d6- deployment-276  b214c001-26f9-44e1-afc3-0ae74a7a7122 607348 0 2023-01-13 11:00:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:65051af4aa53ee7efaaa72e9d6e5b1fc78e4661b52518db10c2c580ffe2a0a1c cni.projectcalico.org/podIP:10.244.169.56/32 cni.projectcalico.org/podIPs:10.244.169.56/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 eeabf12f-c472-49c8-b24e-ef93915d4b45 0xc002c828a7 0xc002c828a8}] [] [{kube-controller-manager Update v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eeabf12f-c472-49c8-b24e-ef93915d4b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 11:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nchxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nchxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.56,StartTime:2023-01-13 11:00:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 11:00:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://b65f090efd229ce54fdb56bd23aa005e9e981f15a496ea85980cc22f7b599ba0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 13 11:00:57.265: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-276  c83f2094-0acd-4683-837c-feee3fea9629 607359 4 2023-01-13 11:00:47 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d2c361a0-612f-4702-8f1d-e5a91f3e005f 0xc004354877 0xc004354878}] [] [{kube-controller-manager Update apps/v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2c361a0-612f-4702-8f1d-e5a91f3e005f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004354900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 13 11:00:57.281: INFO: pod: "test-deployment-7df74c55ff-xhzdd":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-xhzdd test-deployment-7df74c55ff- deployment-276  fbd25529-3493-4a32-9274-a28d871cfbfb 607353 0 2023-01-13 11:00:51 +0000 UTC 2023-01-13 11:00:58 +0000 UTC 0xc004354c88 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:c0a43f72b61214dc71b57640efafe75efe163f61862c409b0671c647f8049939 cni.projectcalico.org/podIP:10.244.169.13/32 cni.projectcalico.org/podIPs:10.244.169.13/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c83f2094-0acd-4683-837c-feee3fea9629 0xc004354cd7 0xc004354cd8}] [] [{kube-controller-manager Update v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83f2094-0acd-4683-837c-feee3fea9629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 11:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ptdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ptdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.13,StartTime:2023-01-13 11:00:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 11:00:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.aliyuncs.com/google_containers/pause:3.9,ImageID:docker-pullable://registry.aliyuncs.com/google_containers/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://da4163f02625ada9848b0f2cd6eefdfa6301ca5683b837e2f9b3b61c55460d5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 13 11:00:57.281: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-276  3e249417-c4e8-4cca-81cd-ca29bb6af69e 607257 3 2023-01-13 11:00:44 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d2c361a0-612f-4702-8f1d-e5a91f3e005f 0xc004354967 0xc004354968}] [] [{kube-controller-manager Update apps/v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2c361a0-612f-4702-8f1d-e5a91f3e005f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043549f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 13 11:00:57.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-276" for this suite. 01/13/23 11:00:57.306
------------------------------
• [SLOW TEST] [13.326 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:00:44.044
    Jan 13 11:00:44.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename deployment 01/13/23 11:00:44.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:44.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:44.098
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/13/23 11:00:44.118
    STEP: waiting for Deployment to be created 01/13/23 11:00:44.147
    STEP: waiting for all Replicas to be Ready 01/13/23 11:00:44.15
    Jan 13 11:00:44.157: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:44.157: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:44.173: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:44.173: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:44.232: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:44.232: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:44.305: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:44.305: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 13 11:00:47.726: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 13 11:00:47.726: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 13 11:00:47.810: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/13/23 11:00:47.81
    W0113 11:00:47.851298      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 13 11:00:47.858: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/13/23 11:00:47.858
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 0
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:47.863: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:47.864: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:47.864: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:47.901: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:47.901: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:48.012: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:48.012: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:48.082: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:48.082: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:48.117: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:48.117: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:51.710: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:51.710: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:51.811: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    STEP: listing Deployments 01/13/23 11:00:51.811
    Jan 13 11:00:51.853: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/13/23 11:00:51.853
    Jan 13 11:00:51.888: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/13/23 11:00:51.888
    Jan 13 11:00:51.921: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:51.948: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:52.022: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:52.056: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:54.403: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:54.678: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:54.894: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:54.943: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 13 11:00:57.113: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/13/23 11:00:57.152
    STEP: fetching the DeploymentStatus 01/13/23 11:00:57.169
    Jan 13 11:00:57.184: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 1
    Jan 13 11:00:57.189: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3
    Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 2
    Jan 13 11:00:57.190: INFO: observed Deployment test-deployment in namespace deployment-276 with ReadyReplicas 3
    STEP: deleting the Deployment 01/13/23 11:00:57.19
    Jan 13 11:00:57.209: INFO: observed event type MODIFIED
    Jan 13 11:00:57.211: INFO: observed event type MODIFIED
    Jan 13 11:00:57.211: INFO: observed event type MODIFIED
    Jan 13 11:00:57.211: INFO: observed event type MODIFIED
    Jan 13 11:00:57.211: INFO: observed event type MODIFIED
    Jan 13 11:00:57.212: INFO: observed event type MODIFIED
    Jan 13 11:00:57.212: INFO: observed event type MODIFIED
    Jan 13 11:00:57.212: INFO: observed event type MODIFIED
    Jan 13 11:00:57.212: INFO: observed event type MODIFIED
    Jan 13 11:00:57.212: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 13 11:00:57.237: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 13 11:00:57.247: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-276  eeabf12f-c472-49c8-b24e-ef93915d4b45 607350 2 2023-01-13 11:00:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d2c361a0-612f-4702-8f1d-e5a91f3e005f 0xc004354767 0xc004354768}] [] [{kube-controller-manager Update apps/v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2c361a0-612f-4702-8f1d-e5a91f3e005f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004354810 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 13 11:00:57.264: INFO: pod: "test-deployment-7b7876f9d6-lm2gl":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-lm2gl test-deployment-7b7876f9d6- deployment-276  15611533-4fec-47eb-9016-baaa07a2760f 607311 0 2023-01-13 11:00:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:275964a5d6363bbf283067465e388487108372315beadc6d5563d76654f077cd cni.projectcalico.org/podIP:10.244.27.207/32 cni.projectcalico.org/podIPs:10.244.27.207/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 eeabf12f-c472-49c8-b24e-ef93915d4b45 0xc002c82677 0xc002c82678}] [] [{kube-controller-manager Update v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eeabf12f-c472-49c8-b24e-ef93915d4b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 11:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.27.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8xl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8xl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.31-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.31,PodIP:10.244.27.207,StartTime:2023-01-13 11:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 11:00:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://ead2655affa1f8f6377b2ab18d74247bf65aff2cfb57fd6ccb283c9b7ff5041a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.27.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 13 11:00:57.265: INFO: pod: "test-deployment-7b7876f9d6-x7j2k":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-x7j2k test-deployment-7b7876f9d6- deployment-276  b214c001-26f9-44e1-afc3-0ae74a7a7122 607348 0 2023-01-13 11:00:54 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:65051af4aa53ee7efaaa72e9d6e5b1fc78e4661b52518db10c2c580ffe2a0a1c cni.projectcalico.org/podIP:10.244.169.56/32 cni.projectcalico.org/podIPs:10.244.169.56/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 eeabf12f-c472-49c8-b24e-ef93915d4b45 0xc002c828a7 0xc002c828a8}] [] [{kube-controller-manager Update v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eeabf12f-c472-49c8-b24e-ef93915d4b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 11:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nchxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nchxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.56,StartTime:2023-01-13 11:00:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 11:00:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker://sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:docker://b65f090efd229ce54fdb56bd23aa005e9e981f15a496ea85980cc22f7b599ba0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 13 11:00:57.265: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-276  c83f2094-0acd-4683-837c-feee3fea9629 607359 4 2023-01-13 11:00:47 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d2c361a0-612f-4702-8f1d-e5a91f3e005f 0xc004354877 0xc004354878}] [] [{kube-controller-manager Update apps/v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2c361a0-612f-4702-8f1d-e5a91f3e005f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 11:00:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004354900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 13 11:00:57.281: INFO: pod: "test-deployment-7df74c55ff-xhzdd":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-xhzdd test-deployment-7df74c55ff- deployment-276  fbd25529-3493-4a32-9274-a28d871cfbfb 607353 0 2023-01-13 11:00:51 +0000 UTC 2023-01-13 11:00:58 +0000 UTC 0xc004354c88 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:c0a43f72b61214dc71b57640efafe75efe163f61862c409b0671c647f8049939 cni.projectcalico.org/podIP:10.244.169.13/32 cni.projectcalico.org/podIPs:10.244.169.13/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c83f2094-0acd-4683-837c-feee3fea9629 0xc004354cd7 0xc004354cd8}] [] [{kube-controller-manager Update v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c83f2094-0acd-4683-837c-feee3fea9629\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-13 11:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-13 11:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.169.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ptdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ptdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.102.32-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-13 11:00:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.102.32,PodIP:10.244.169.13,StartTime:2023-01-13 11:00:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-13 11:00:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.aliyuncs.com/google_containers/pause:3.9,ImageID:docker-pullable://registry.aliyuncs.com/google_containers/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://da4163f02625ada9848b0f2cd6eefdfa6301ca5683b837e2f9b3b61c55460d5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.169.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 13 11:00:57.281: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-276  3e249417-c4e8-4cca-81cd-ca29bb6af69e 607257 3 2023-01-13 11:00:44 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d2c361a0-612f-4702-8f1d-e5a91f3e005f 0xc004354967 0xc004354968}] [] [{kube-controller-manager Update apps/v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2c361a0-612f-4702-8f1d-e5a91f3e005f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-13 11:00:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043549f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:00:57.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-276" for this suite. 01/13/23 11:00:57.306
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:00:57.37
Jan 13 11:00:57.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename downward-api 01/13/23 11:00:57.372
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:57.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:57.417
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/13/23 11:00:57.441
Jan 13 11:00:57.464: INFO: Waiting up to 5m0s for pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d" in namespace "downward-api-5311" to be "Succeeded or Failed"
Jan 13 11:00:57.473: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.874328ms
Jan 13 11:00:59.487: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022954244s
Jan 13 11:01:01.489: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02493668s
Jan 13 11:01:03.545: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081447887s
Jan 13 11:01:05.481: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017167323s
STEP: Saw pod success 01/13/23 11:01:05.481
Jan 13 11:01:05.482: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d" satisfied condition "Succeeded or Failed"
Jan 13 11:01:05.503: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d container dapi-container: <nil>
STEP: delete the pod 01/13/23 11:01:05.531
Jan 13 11:01:05.564: INFO: Waiting for pod downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d to disappear
Jan 13 11:01:05.574: INFO: Pod downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 13 11:01:05.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5311" for this suite. 01/13/23 11:01:05.592
------------------------------
• [SLOW TEST] [8.245 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:00:57.37
    Jan 13 11:00:57.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename downward-api 01/13/23 11:00:57.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:00:57.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:00:57.417
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/13/23 11:00:57.441
    Jan 13 11:00:57.464: INFO: Waiting up to 5m0s for pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d" in namespace "downward-api-5311" to be "Succeeded or Failed"
    Jan 13 11:00:57.473: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.874328ms
    Jan 13 11:00:59.487: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022954244s
    Jan 13 11:01:01.489: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02493668s
    Jan 13 11:01:03.545: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081447887s
    Jan 13 11:01:05.481: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017167323s
    STEP: Saw pod success 01/13/23 11:01:05.481
    Jan 13 11:01:05.482: INFO: Pod "downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d" satisfied condition "Succeeded or Failed"
    Jan 13 11:01:05.503: INFO: Trying to get logs from node 10.10.102.31-node pod downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d container dapi-container: <nil>
    STEP: delete the pod 01/13/23 11:01:05.531
    Jan 13 11:01:05.564: INFO: Waiting for pod downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d to disappear
    Jan 13 11:01:05.574: INFO: Pod downward-api-ae5f7994-8f66-4b74-8874-5f3b362e3d8d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:01:05.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5311" for this suite. 01/13/23 11:01:05.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:01:05.618
Jan 13 11:01:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename pod-network-test 01/13/23 11:01:05.621
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:01:05.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:01:05.713
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7490 01/13/23 11:01:05.735
STEP: creating a selector 01/13/23 11:01:05.735
STEP: Creating the service pods in kubernetes 01/13/23 11:01:05.735
Jan 13 11:01:05.735: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 11:01:05.792: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7490" to be "running and ready"
Jan 13 11:01:05.820: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.326416ms
Jan 13 11:01:05.820: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 11:01:07.841: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048879807s
Jan 13 11:01:07.841: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 11:01:09.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.049379836s
Jan 13 11:01:09.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:11.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044265139s
Jan 13 11:01:11.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:13.836: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.043601647s
Jan 13 11:01:13.836: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:15.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038528698s
Jan 13 11:01:15.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:17.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.050069209s
Jan 13 11:01:17.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:19.852: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.059204669s
Jan 13 11:01:19.852: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:21.832: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.040097085s
Jan 13 11:01:21.833: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:23.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.042890116s
Jan 13 11:01:23.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:25.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.034341751s
Jan 13 11:01:25.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 13 11:01:27.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.04283818s
Jan 13 11:01:27.835: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 13 11:01:27.835: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 13 11:01:27.843: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7490" to be "running and ready"
Jan 13 11:01:27.853: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.977145ms
Jan 13 11:01:27.853: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 13 11:01:27.853: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/13/23 11:01:27.863
Jan 13 11:01:27.875: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7490" to be "running"
Jan 13 11:01:27.896: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.142656ms
Jan 13 11:01:29.933: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057975896s
Jan 13 11:01:31.905: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030101854s
Jan 13 11:01:31.905: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 13 11:01:31.912: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 11:01:31.912: INFO: Breadth first check of 10.244.27.203 on host 10.10.102.31...
Jan 13 11:01:31.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.254:9080/dial?request=hostname&protocol=http&host=10.244.27.203&port=8083&tries=1'] Namespace:pod-network-test-7490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 11:01:31.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 11:01:31.920: INFO: ExecWithOptions: Clientset creation
Jan 13 11:01:31.920: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.254%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.27.203%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 13 11:01:32.198: INFO: Waiting for responses: map[]
Jan 13 11:01:32.198: INFO: reached 10.244.27.203 after 0/1 tries
Jan 13 11:01:32.198: INFO: Breadth first check of 10.244.169.57 on host 10.10.102.32...
Jan 13 11:01:32.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.254:9080/dial?request=hostname&protocol=http&host=10.244.169.57&port=8083&tries=1'] Namespace:pod-network-test-7490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 11:01:32.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
Jan 13 11:01:32.213: INFO: ExecWithOptions: Clientset creation
Jan 13 11:01:32.213: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.254%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.169.57%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 13 11:01:32.562: INFO: Waiting for responses: map[]
Jan 13 11:01:32.562: INFO: reached 10.244.169.57 after 0/1 tries
Jan 13 11:01:32.562: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 13 11:01:32.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7490" for this suite. 01/13/23 11:01:32.571
------------------------------
• [SLOW TEST] [26.964 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:01:05.618
    Jan 13 11:01:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename pod-network-test 01/13/23 11:01:05.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:01:05.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:01:05.713
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7490 01/13/23 11:01:05.735
    STEP: creating a selector 01/13/23 11:01:05.735
    STEP: Creating the service pods in kubernetes 01/13/23 11:01:05.735
    Jan 13 11:01:05.735: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 13 11:01:05.792: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7490" to be "running and ready"
    Jan 13 11:01:05.820: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.326416ms
    Jan 13 11:01:05.820: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 11:01:07.841: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048879807s
    Jan 13 11:01:07.841: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 13 11:01:09.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.049379836s
    Jan 13 11:01:09.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:11.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044265139s
    Jan 13 11:01:11.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:13.836: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.043601647s
    Jan 13 11:01:13.836: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:15.831: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038528698s
    Jan 13 11:01:15.831: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:17.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.050069209s
    Jan 13 11:01:17.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:19.852: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.059204669s
    Jan 13 11:01:19.852: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:21.832: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.040097085s
    Jan 13 11:01:21.833: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:23.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.042890116s
    Jan 13 11:01:23.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:25.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.034341751s
    Jan 13 11:01:25.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 13 11:01:27.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.04283818s
    Jan 13 11:01:27.835: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 13 11:01:27.835: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 13 11:01:27.843: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7490" to be "running and ready"
    Jan 13 11:01:27.853: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.977145ms
    Jan 13 11:01:27.853: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 13 11:01:27.853: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/13/23 11:01:27.863
    Jan 13 11:01:27.875: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7490" to be "running"
    Jan 13 11:01:27.896: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.142656ms
    Jan 13 11:01:29.933: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057975896s
    Jan 13 11:01:31.905: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030101854s
    Jan 13 11:01:31.905: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 13 11:01:31.912: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 13 11:01:31.912: INFO: Breadth first check of 10.244.27.203 on host 10.10.102.31...
    Jan 13 11:01:31.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.254:9080/dial?request=hostname&protocol=http&host=10.244.27.203&port=8083&tries=1'] Namespace:pod-network-test-7490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 11:01:31.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 11:01:31.920: INFO: ExecWithOptions: Clientset creation
    Jan 13 11:01:31.920: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.254%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.27.203%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 13 11:01:32.198: INFO: Waiting for responses: map[]
    Jan 13 11:01:32.198: INFO: reached 10.244.27.203 after 0/1 tries
    Jan 13 11:01:32.198: INFO: Breadth first check of 10.244.169.57 on host 10.10.102.32...
    Jan 13 11:01:32.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.27.254:9080/dial?request=hostname&protocol=http&host=10.244.169.57&port=8083&tries=1'] Namespace:pod-network-test-7490 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 13 11:01:32.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    Jan 13 11:01:32.213: INFO: ExecWithOptions: Clientset creation
    Jan 13 11:01:32.213: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7490/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.27.254%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.169.57%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 13 11:01:32.562: INFO: Waiting for responses: map[]
    Jan 13 11:01:32.562: INFO: reached 10.244.169.57 after 0/1 tries
    Jan 13 11:01:32.562: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:01:32.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7490" for this suite. 01/13/23 11:01:32.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:01:32.603
Jan 13 11:01:32.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename kubectl 01/13/23 11:01:32.605
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:01:32.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:01:32.635
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/13/23 11:01:32.641
Jan 13 11:01:32.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-2338 create -f -'
Jan 13 11:01:35.287: INFO: stderr: ""
Jan 13 11:01:35.287: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/13/23 11:01:35.287
Jan 13 11:01:35.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-2338 diff -f -'
Jan 13 11:01:36.045: INFO: rc: 1
Jan 13 11:01:36.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-2338 delete -f -'
Jan 13 11:01:36.216: INFO: stderr: ""
Jan 13 11:01:36.217: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 13 11:01:36.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2338" for this suite. 01/13/23 11:01:36.225
------------------------------
• [3.635 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:01:32.603
    Jan 13 11:01:32.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename kubectl 01/13/23 11:01:32.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:01:32.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:01:32.635
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/13/23 11:01:32.641
    Jan 13 11:01:32.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-2338 create -f -'
    Jan 13 11:01:35.287: INFO: stderr: ""
    Jan 13 11:01:35.287: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/13/23 11:01:35.287
    Jan 13 11:01:35.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-2338 diff -f -'
    Jan 13 11:01:36.045: INFO: rc: 1
    Jan 13 11:01:36.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2990414835 --namespace=kubectl-2338 delete -f -'
    Jan 13 11:01:36.216: INFO: stderr: ""
    Jan 13 11:01:36.217: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:01:36.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2338" for this suite. 01/13/23 11:01:36.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:01:36.24
Jan 13 11:01:36.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename job 01/13/23 11:01:36.242
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:01:36.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:01:36.287
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/13/23 11:01:36.295
STEP: Ensuring active pods == parallelism 01/13/23 11:01:36.313
STEP: delete a job 01/13/23 11:01:42.32
STEP: deleting Job.batch foo in namespace job-4297, will wait for the garbage collector to delete the pods 01/13/23 11:01:42.321
Jan 13 11:01:42.386: INFO: Deleting Job.batch foo took: 8.480671ms
Jan 13 11:01:42.487: INFO: Terminating Job.batch foo pods took: 101.163422ms
STEP: Ensuring job was deleted 01/13/23 11:02:14.987
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 13 11:02:14.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4297" for this suite. 01/13/23 11:02:15.014
------------------------------
• [SLOW TEST] [38.787 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:01:36.24
    Jan 13 11:01:36.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename job 01/13/23 11:01:36.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:01:36.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:01:36.287
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/13/23 11:01:36.295
    STEP: Ensuring active pods == parallelism 01/13/23 11:01:36.313
    STEP: delete a job 01/13/23 11:01:42.32
    STEP: deleting Job.batch foo in namespace job-4297, will wait for the garbage collector to delete the pods 01/13/23 11:01:42.321
    Jan 13 11:01:42.386: INFO: Deleting Job.batch foo took: 8.480671ms
    Jan 13 11:01:42.487: INFO: Terminating Job.batch foo pods took: 101.163422ms
    STEP: Ensuring job was deleted 01/13/23 11:02:14.987
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:02:14.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4297" for this suite. 01/13/23 11:02:15.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/13/23 11:02:15.038
Jan 13 11:02:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
STEP: Building a namespace api object, basename var-expansion 01/13/23 11:02:15.042
STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:02:15.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:02:15.105
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/13/23 11:02:15.117
Jan 13 11:02:15.154: INFO: Waiting up to 5m0s for pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a" in namespace "var-expansion-6988" to be "Succeeded or Failed"
Jan 13 11:02:15.165: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.273645ms
Jan 13 11:02:17.173: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018324735s
Jan 13 11:02:19.176: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021124614s
Jan 13 11:02:21.176: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021296652s
Jan 13 11:02:23.214: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.059875359s
STEP: Saw pod success 01/13/23 11:02:23.214
Jan 13 11:02:23.215: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a" satisfied condition "Succeeded or Failed"
Jan 13 11:02:23.224: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a container dapi-container: <nil>
STEP: delete the pod 01/13/23 11:02:23.258
Jan 13 11:02:23.285: INFO: Waiting for pod var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a to disappear
Jan 13 11:02:23.293: INFO: Pod var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 13 11:02:23.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6988" for this suite. 01/13/23 11:02:23.31
------------------------------
• [SLOW TEST] [8.285 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/13/23 11:02:15.038
    Jan 13 11:02:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2990414835
    STEP: Building a namespace api object, basename var-expansion 01/13/23 11:02:15.042
    STEP: Waiting for a default service account to be provisioned in namespace 01/13/23 11:02:15.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/13/23 11:02:15.105
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/13/23 11:02:15.117
    Jan 13 11:02:15.154: INFO: Waiting up to 5m0s for pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a" in namespace "var-expansion-6988" to be "Succeeded or Failed"
    Jan 13 11:02:15.165: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.273645ms
    Jan 13 11:02:17.173: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018324735s
    Jan 13 11:02:19.176: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021124614s
    Jan 13 11:02:21.176: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021296652s
    Jan 13 11:02:23.214: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.059875359s
    STEP: Saw pod success 01/13/23 11:02:23.214
    Jan 13 11:02:23.215: INFO: Pod "var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a" satisfied condition "Succeeded or Failed"
    Jan 13 11:02:23.224: INFO: Trying to get logs from node 10.10.102.31-node pod var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a container dapi-container: <nil>
    STEP: delete the pod 01/13/23 11:02:23.258
    Jan 13 11:02:23.285: INFO: Waiting for pod var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a to disappear
    Jan 13 11:02:23.293: INFO: Pod var-expansion-914c63e8-9138-45df-b7f3-f1a525decf2a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 13 11:02:23.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6988" for this suite. 01/13/23 11:02:23.31
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 13 11:02:23.331: INFO: Running AfterSuite actions on node 1
Jan 13 11:02:23.331: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 13 11:02:23.331: INFO: Running AfterSuite actions on node 1
    Jan 13 11:02:23.331: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.348 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 7129.858 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h58m51.086609061s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

