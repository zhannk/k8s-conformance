I0227 15:05:14.575159      19 e2e.go:126] Starting e2e run "0e1ee2c8-2a4b-4b26-a946-79230015426d" on Ginkgo node 1
Feb 27 15:05:14.589: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677510314 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Feb 27 15:05:14.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:05:14.704: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 27 15:05:14.715: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 27 15:05:14.727: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 27 15:05:14.727: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 27 15:05:14.727: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 27 15:05:14.729: INFO: e2e test version: v1.26.1
Feb 27 15:05:14.730: INFO: kube-apiserver version: v1.26.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Feb 27 15:05:14.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:05:14.734: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.030 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Feb 27 15:05:14.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:05:14.704: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Feb 27 15:05:14.715: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb 27 15:05:14.727: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Feb 27 15:05:14.727: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 27 15:05:14.727: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb 27 15:05:14.729: INFO: e2e test version: v1.26.1
    Feb 27 15:05:14.730: INFO: kube-apiserver version: v1.26.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Feb 27 15:05:14.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:05:14.734: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:05:14.754
Feb 27 15:05:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename job 02/27/23 15:05:14.755
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:05:14.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:05:14.771
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 02/27/23 15:05:14.774
STEP: Ensuring job reaches completions 02/27/23 15:05:14.78
STEP: Ensuring pods with index for job exist 02/27/23 15:05:32.783
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 15:05:32.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2662" for this suite. 02/27/23 15:05:32.79
------------------------------
• [SLOW TEST] [18.041 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:05:14.754
    Feb 27 15:05:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename job 02/27/23 15:05:14.755
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:05:14.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:05:14.771
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 02/27/23 15:05:14.774
    STEP: Ensuring job reaches completions 02/27/23 15:05:14.78
    STEP: Ensuring pods with index for job exist 02/27/23 15:05:32.783
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:05:32.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2662" for this suite. 02/27/23 15:05:32.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:05:32.796
Feb 27 15:05:32.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-preemption 02/27/23 15:05:32.797
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:05:32.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:05:32.812
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 15:05:32.826: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 15:06:32.846: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:06:32.85
Feb 27 15:06:32.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 15:06:32.85
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:06:32.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:06:32.87
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 02/27/23 15:06:32.873
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 15:06:32.874
Feb 27 15:06:32.880: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3584" to be "running"
Feb 27 15:06:32.883: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203246ms
Feb 27 15:06:34.885: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004817553s
Feb 27 15:06:34.885: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 15:06:34.888
Feb 27 15:06:34.901: INFO: found a healthy node: ip-172-31-42-40
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Feb 27 15:06:40.963: INFO: pods created so far: [1 1 1]
Feb 27 15:06:40.963: INFO: length of pods created so far: 3
Feb 27 15:06:44.973: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Feb 27 15:06:51.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:06:52.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3584" for this suite. 02/27/23 15:06:52.036
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6690" for this suite. 02/27/23 15:06:52.041
------------------------------
• [SLOW TEST] [79.250 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:05:32.796
    Feb 27 15:05:32.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 15:05:32.797
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:05:32.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:05:32.812
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 15:05:32.826: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 15:06:32.846: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:06:32.85
    Feb 27 15:06:32.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 15:06:32.85
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:06:32.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:06:32.87
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 02/27/23 15:06:32.873
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 15:06:32.874
    Feb 27 15:06:32.880: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3584" to be "running"
    Feb 27 15:06:32.883: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203246ms
    Feb 27 15:06:34.885: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004817553s
    Feb 27 15:06:34.885: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 15:06:34.888
    Feb 27 15:06:34.901: INFO: found a healthy node: ip-172-31-42-40
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Feb 27 15:06:40.963: INFO: pods created so far: [1 1 1]
    Feb 27 15:06:40.963: INFO: length of pods created so far: 3
    Feb 27 15:06:44.973: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:06:51.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:06:52.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3584" for this suite. 02/27/23 15:06:52.036
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6690" for this suite. 02/27/23 15:06:52.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:06:52.047
Feb 27 15:06:52.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replicaset 02/27/23 15:06:52.047
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:06:52.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:06:52.061
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb 27 15:06:52.080: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 15:06:57.089: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 15:06:57.089
STEP: Scaling up "test-rs" replicaset  02/27/23 15:06:57.089
Feb 27 15:06:57.097: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/27/23 15:06:57.097
W0227 15:06:57.107317      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 15:06:57.108: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 15:06:57.120: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 15:06:57.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 15:06:57.159: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 15:07:00.806: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 2, AvailableReplicas 2
Feb 27 15:07:01.322: INFO: observed Replicaset test-rs in namespace replicaset-9272 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:07:01.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9272" for this suite. 02/27/23 15:07:01.325
------------------------------
• [SLOW TEST] [9.285 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:06:52.047
    Feb 27 15:06:52.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replicaset 02/27/23 15:06:52.047
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:06:52.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:06:52.061
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb 27 15:06:52.080: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 15:06:57.089: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 15:06:57.089
    STEP: Scaling up "test-rs" replicaset  02/27/23 15:06:57.089
    Feb 27 15:06:57.097: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/27/23 15:06:57.097
    W0227 15:06:57.107317      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 15:06:57.108: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 15:06:57.120: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 15:06:57.136: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 15:06:57.159: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 15:07:00.806: INFO: observed ReplicaSet test-rs in namespace replicaset-9272 with ReadyReplicas 2, AvailableReplicas 2
    Feb 27 15:07:01.322: INFO: observed Replicaset test-rs in namespace replicaset-9272 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:07:01.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9272" for this suite. 02/27/23 15:07:01.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:07:01.333
Feb 27 15:07:01.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 15:07:01.334
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:07:01.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:07:01.351
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 02/27/23 15:07:01.353
STEP: getting 02/27/23 15:07:01.366
STEP: listing 02/27/23 15:07:01.37
STEP: deleting 02/27/23 15:07:01.373
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:07:01.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-7895" for this suite. 02/27/23 15:07:01.391
------------------------------
• [0.064 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:07:01.333
    Feb 27 15:07:01.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 15:07:01.334
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:07:01.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:07:01.351
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 02/27/23 15:07:01.353
    STEP: getting 02/27/23 15:07:01.366
    STEP: listing 02/27/23 15:07:01.37
    STEP: deleting 02/27/23 15:07:01.373
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:07:01.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-7895" for this suite. 02/27/23 15:07:01.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:07:01.397
Feb 27 15:07:01.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename events 02/27/23 15:07:01.398
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:07:01.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:07:01.411
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/27/23 15:07:01.413
STEP: listing all events in all namespaces 02/27/23 15:07:01.417
STEP: patching the test event 02/27/23 15:07:01.423
STEP: fetching the test event 02/27/23 15:07:01.428
STEP: updating the test event 02/27/23 15:07:01.431
STEP: getting the test event 02/27/23 15:07:01.438
STEP: deleting the test event 02/27/23 15:07:01.44
STEP: listing all events in all namespaces 02/27/23 15:07:01.448
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Feb 27 15:07:01.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3623" for this suite. 02/27/23 15:07:01.457
------------------------------
• [0.065 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:07:01.397
    Feb 27 15:07:01.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename events 02/27/23 15:07:01.398
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:07:01.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:07:01.411
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/27/23 15:07:01.413
    STEP: listing all events in all namespaces 02/27/23 15:07:01.417
    STEP: patching the test event 02/27/23 15:07:01.423
    STEP: fetching the test event 02/27/23 15:07:01.428
    STEP: updating the test event 02/27/23 15:07:01.431
    STEP: getting the test event 02/27/23 15:07:01.438
    STEP: deleting the test event 02/27/23 15:07:01.44
    STEP: listing all events in all namespaces 02/27/23 15:07:01.448
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:07:01.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3623" for this suite. 02/27/23 15:07:01.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:07:01.464
Feb 27 15:07:01.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 15:07:01.464
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:07:01.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:07:01.478
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-f4499211-abe3-42dc-86bd-c56bcade9fb8 02/27/23 15:07:01.484
STEP: Creating configMap with name cm-test-opt-upd-e75b9638-b0d1-4c0e-9bd4-ab45c80797c8 02/27/23 15:07:01.487
STEP: Creating the pod 02/27/23 15:07:01.491
Feb 27 15:07:01.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840" in namespace "configmap-5944" to be "running and ready"
Feb 27 15:07:01.502: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23566ms
Feb 27 15:07:01.502: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:07:03.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322683s
Feb 27 15:07:03.506: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:07:05.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006965356s
Feb 27 15:07:05.506: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:07:07.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Running", Reason="", readiness=true. Elapsed: 6.006608408s
Feb 27 15:07:07.506: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Running (Ready = true)
Feb 27 15:07:07.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-f4499211-abe3-42dc-86bd-c56bcade9fb8 02/27/23 15:07:07.531
STEP: Updating configmap cm-test-opt-upd-e75b9638-b0d1-4c0e-9bd4-ab45c80797c8 02/27/23 15:07:07.536
STEP: Creating configMap with name cm-test-opt-create-13762099-a51b-4f85-b14d-0bba9a7409f6 02/27/23 15:07:07.541
STEP: waiting to observe update in volume 02/27/23 15:07:07.544
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:08:11.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5944" for this suite. 02/27/23 15:08:11.785
------------------------------
• [SLOW TEST] [70.329 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:07:01.464
    Feb 27 15:07:01.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 15:07:01.464
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:07:01.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:07:01.478
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-f4499211-abe3-42dc-86bd-c56bcade9fb8 02/27/23 15:07:01.484
    STEP: Creating configMap with name cm-test-opt-upd-e75b9638-b0d1-4c0e-9bd4-ab45c80797c8 02/27/23 15:07:01.487
    STEP: Creating the pod 02/27/23 15:07:01.491
    Feb 27 15:07:01.499: INFO: Waiting up to 5m0s for pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840" in namespace "configmap-5944" to be "running and ready"
    Feb 27 15:07:01.502: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23566ms
    Feb 27 15:07:01.502: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:07:03.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322683s
    Feb 27 15:07:03.506: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:07:05.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006965356s
    Feb 27 15:07:05.506: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:07:07.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840": Phase="Running", Reason="", readiness=true. Elapsed: 6.006608408s
    Feb 27 15:07:07.506: INFO: The phase of Pod pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840 is Running (Ready = true)
    Feb 27 15:07:07.506: INFO: Pod "pod-configmaps-6fd8b892-b6e9-4b5d-bb4d-b32313e53840" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-f4499211-abe3-42dc-86bd-c56bcade9fb8 02/27/23 15:07:07.531
    STEP: Updating configmap cm-test-opt-upd-e75b9638-b0d1-4c0e-9bd4-ab45c80797c8 02/27/23 15:07:07.536
    STEP: Creating configMap with name cm-test-opt-create-13762099-a51b-4f85-b14d-0bba9a7409f6 02/27/23 15:07:07.541
    STEP: waiting to observe update in volume 02/27/23 15:07:07.544
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:08:11.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5944" for this suite. 02/27/23 15:08:11.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:08:11.793
Feb 27 15:08:11.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename ephemeral-containers-test 02/27/23 15:08:11.794
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:11.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:11.811
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/27/23 15:08:11.813
Feb 27 15:08:11.821: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4475" to be "running and ready"
Feb 27 15:08:11.824: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.129659ms
Feb 27 15:08:11.824: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:08:13.828: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006700404s
Feb 27 15:08:13.828: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb 27 15:08:13.828: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/27/23 15:08:13.831
Feb 27 15:08:13.841: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4475" to be "container debugger running"
Feb 27 15:08:13.844: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.981281ms
Feb 27 15:08:15.847: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00670272s
Feb 27 15:08:17.848: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007669526s
Feb 27 15:08:17.848: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/27/23 15:08:17.848
Feb 27 15:08:17.848: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4475 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:08:17.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:08:17.849: INFO: ExecWithOptions: Clientset creation
Feb 27 15:08:17.849: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-4475/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb 27 15:08:17.931: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:08:17.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4475" for this suite. 02/27/23 15:08:17.952
------------------------------
• [SLOW TEST] [6.164 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:08:11.793
    Feb 27 15:08:11.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/27/23 15:08:11.794
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:11.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:11.811
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/27/23 15:08:11.813
    Feb 27 15:08:11.821: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4475" to be "running and ready"
    Feb 27 15:08:11.824: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.129659ms
    Feb 27 15:08:11.824: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:08:13.828: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006700404s
    Feb 27 15:08:13.828: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb 27 15:08:13.828: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/27/23 15:08:13.831
    Feb 27 15:08:13.841: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4475" to be "container debugger running"
    Feb 27 15:08:13.844: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.981281ms
    Feb 27 15:08:15.847: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00670272s
    Feb 27 15:08:17.848: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007669526s
    Feb 27 15:08:17.848: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/27/23 15:08:17.848
    Feb 27 15:08:17.848: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4475 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:08:17.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:08:17.849: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:08:17.849: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-4475/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb 27 15:08:17.931: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:08:17.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4475" for this suite. 02/27/23 15:08:17.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:08:17.959
Feb 27 15:08:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sysctl 02/27/23 15:08:17.959
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:17.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:17.974
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/27/23 15:08:17.976
STEP: Watching for error events or started pod 02/27/23 15:08:17.983
STEP: Waiting for pod completion 02/27/23 15:08:19.986
Feb 27 15:08:19.986: INFO: Waiting up to 3m0s for pod "sysctl-7731003c-1818-4493-9697-31903a636ef7" in namespace "sysctl-7527" to be "completed"
Feb 27 15:08:19.989: INFO: Pod "sysctl-7731003c-1818-4493-9697-31903a636ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253004ms
Feb 27 15:08:21.993: INFO: Pod "sysctl-7731003c-1818-4493-9697-31903a636ef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006892823s
Feb 27 15:08:21.993: INFO: Pod "sysctl-7731003c-1818-4493-9697-31903a636ef7" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/27/23 15:08:21.996
STEP: Getting logs from the pod 02/27/23 15:08:21.996
STEP: Checking that the sysctl is actually updated 02/27/23 15:08:22.002
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:08:22.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7527" for this suite. 02/27/23 15:08:22.005
------------------------------
• [4.051 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:08:17.959
    Feb 27 15:08:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sysctl 02/27/23 15:08:17.959
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:17.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:17.974
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/27/23 15:08:17.976
    STEP: Watching for error events or started pod 02/27/23 15:08:17.983
    STEP: Waiting for pod completion 02/27/23 15:08:19.986
    Feb 27 15:08:19.986: INFO: Waiting up to 3m0s for pod "sysctl-7731003c-1818-4493-9697-31903a636ef7" in namespace "sysctl-7527" to be "completed"
    Feb 27 15:08:19.989: INFO: Pod "sysctl-7731003c-1818-4493-9697-31903a636ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253004ms
    Feb 27 15:08:21.993: INFO: Pod "sysctl-7731003c-1818-4493-9697-31903a636ef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006892823s
    Feb 27 15:08:21.993: INFO: Pod "sysctl-7731003c-1818-4493-9697-31903a636ef7" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/27/23 15:08:21.996
    STEP: Getting logs from the pod 02/27/23 15:08:21.996
    STEP: Checking that the sysctl is actually updated 02/27/23 15:08:22.002
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:08:22.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7527" for this suite. 02/27/23 15:08:22.005
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:08:22.01
Feb 27 15:08:22.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 15:08:22.011
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:22.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:22.027
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-25cb22fe-6791-4918-b465-fbd4499a375e 02/27/23 15:08:22.034
STEP: Creating a pod to test consume configMaps 02/27/23 15:08:22.038
Feb 27 15:08:22.045: INFO: Waiting up to 5m0s for pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7" in namespace "configmap-6694" to be "Succeeded or Failed"
Feb 27 15:08:22.048: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.749074ms
Feb 27 15:08:24.051: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005560549s
Feb 27 15:08:26.051: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00570005s
STEP: Saw pod success 02/27/23 15:08:26.051
Feb 27 15:08:26.051: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7" satisfied condition "Succeeded or Failed"
Feb 27 15:08:26.054: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:08:26.06
Feb 27 15:08:26.073: INFO: Waiting for pod pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7 to disappear
Feb 27 15:08:26.076: INFO: Pod pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:08:26.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6694" for this suite. 02/27/23 15:08:26.078
------------------------------
• [4.078 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:08:22.01
    Feb 27 15:08:22.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 15:08:22.011
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:22.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:22.027
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-25cb22fe-6791-4918-b465-fbd4499a375e 02/27/23 15:08:22.034
    STEP: Creating a pod to test consume configMaps 02/27/23 15:08:22.038
    Feb 27 15:08:22.045: INFO: Waiting up to 5m0s for pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7" in namespace "configmap-6694" to be "Succeeded or Failed"
    Feb 27 15:08:22.048: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.749074ms
    Feb 27 15:08:24.051: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005560549s
    Feb 27 15:08:26.051: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00570005s
    STEP: Saw pod success 02/27/23 15:08:26.051
    Feb 27 15:08:26.051: INFO: Pod "pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7" satisfied condition "Succeeded or Failed"
    Feb 27 15:08:26.054: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:08:26.06
    Feb 27 15:08:26.073: INFO: Waiting for pod pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7 to disappear
    Feb 27 15:08:26.076: INFO: Pod pod-configmaps-739742ab-4bb1-494a-a9ba-fec1fe5f87d7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:08:26.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6694" for this suite. 02/27/23 15:08:26.078
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:08:26.089
Feb 27 15:08:26.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:08:26.089
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:26.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:26.103
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:08:26.121
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:08:26.342
STEP: Deploying the webhook pod 02/27/23 15:08:26.349
STEP: Wait for the deployment to be ready 02/27/23 15:08:26.359
Feb 27 15:08:26.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:08:28.377
STEP: Verifying the service has paired with the endpoint 02/27/23 15:08:28.386
Feb 27 15:08:29.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 02/27/23 15:08:29.389
STEP: Creating a custom resource definition that should be denied by the webhook 02/27/23 15:08:29.407
Feb 27 15:08:29.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:08:29.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4076" for this suite. 02/27/23 15:08:29.475
STEP: Destroying namespace "webhook-4076-markers" for this suite. 02/27/23 15:08:29.482
------------------------------
• [3.401 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:08:26.089
    Feb 27 15:08:26.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:08:26.089
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:26.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:26.103
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:08:26.121
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:08:26.342
    STEP: Deploying the webhook pod 02/27/23 15:08:26.349
    STEP: Wait for the deployment to be ready 02/27/23 15:08:26.359
    Feb 27 15:08:26.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:08:28.377
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:08:28.386
    Feb 27 15:08:29.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/27/23 15:08:29.389
    STEP: Creating a custom resource definition that should be denied by the webhook 02/27/23 15:08:29.407
    Feb 27 15:08:29.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:08:29.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4076" for this suite. 02/27/23 15:08:29.475
    STEP: Destroying namespace "webhook-4076-markers" for this suite. 02/27/23 15:08:29.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:08:29.49
Feb 27 15:08:29.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename watch 02/27/23 15:08:29.491
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:29.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:29.506
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/27/23 15:08:29.508
STEP: creating a watch on configmaps with label B 02/27/23 15:08:29.51
STEP: creating a watch on configmaps with label A or B 02/27/23 15:08:29.511
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/27/23 15:08:29.512
Feb 27 15:08:29.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3152 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:08:29.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3152 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/27/23 15:08:29.516
Feb 27 15:08:29.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3153 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:08:29.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3153 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/27/23 15:08:29.526
Feb 27 15:08:29.535: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3154 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:08:29.535: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3154 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/27/23 15:08:29.535
Feb 27 15:08:29.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3155 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:08:29.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3155 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/27/23 15:08:29.54
Feb 27 15:08:29.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3156 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:08:29.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3156 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/27/23 15:08:39.545
Feb 27 15:08:39.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3209 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:08:39.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3209 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 15:08:49.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5713" for this suite. 02/27/23 15:08:49.556
------------------------------
• [SLOW TEST] [20.075 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:08:29.49
    Feb 27 15:08:29.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename watch 02/27/23 15:08:29.491
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:29.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:29.506
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/27/23 15:08:29.508
    STEP: creating a watch on configmaps with label B 02/27/23 15:08:29.51
    STEP: creating a watch on configmaps with label A or B 02/27/23 15:08:29.511
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/27/23 15:08:29.512
    Feb 27 15:08:29.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3152 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:08:29.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3152 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/27/23 15:08:29.516
    Feb 27 15:08:29.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3153 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:08:29.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3153 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/27/23 15:08:29.526
    Feb 27 15:08:29.535: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3154 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:08:29.535: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3154 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/27/23 15:08:29.535
    Feb 27 15:08:29.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3155 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:08:29.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5713  b2732ff8-72cc-44fa-ba0c-7d98fca5faa0 3155 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/27/23 15:08:29.54
    Feb 27 15:08:29.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3156 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:08:29.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3156 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/27/23 15:08:39.545
    Feb 27 15:08:39.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3209 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:08:39.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5713  e340c451-e80d-4d72-83d8-12d3ba49189d 3209 0 2023-02-27 15:08:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 15:08:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:08:49.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5713" for this suite. 02/27/23 15:08:49.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:08:49.565
Feb 27 15:08:49.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svcaccounts 02/27/23 15:08:49.566
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:49.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:49.577
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Feb 27 15:08:49.590: INFO: Waiting up to 5m0s for pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e" in namespace "svcaccounts-2029" to be "running"
Feb 27 15:08:49.594: INFO: Pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.242938ms
Feb 27 15:08:51.597: INFO: Pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006244938s
Feb 27 15:08:51.597: INFO: Pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e" satisfied condition "running"
STEP: reading a file in the container 02/27/23 15:08:51.597
Feb 27 15:08:51.597: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2029 pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/27/23 15:08:51.741
Feb 27 15:08:51.741: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2029 pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/27/23 15:08:51.861
Feb 27 15:08:51.861: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2029 pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb 27 15:08:51.955: INFO: Got root ca configmap in namespace "svcaccounts-2029"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 15:08:51.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2029" for this suite. 02/27/23 15:08:51.96
------------------------------
• [2.403 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:08:49.565
    Feb 27 15:08:49.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 15:08:49.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:49.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:49.577
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Feb 27 15:08:49.590: INFO: Waiting up to 5m0s for pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e" in namespace "svcaccounts-2029" to be "running"
    Feb 27 15:08:49.594: INFO: Pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.242938ms
    Feb 27 15:08:51.597: INFO: Pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006244938s
    Feb 27 15:08:51.597: INFO: Pod "pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e" satisfied condition "running"
    STEP: reading a file in the container 02/27/23 15:08:51.597
    Feb 27 15:08:51.597: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2029 pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/27/23 15:08:51.741
    Feb 27 15:08:51.741: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2029 pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/27/23 15:08:51.861
    Feb 27 15:08:51.861: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2029 pod-service-account-65508b68-f427-4cec-99ab-19145ab6978e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb 27 15:08:51.955: INFO: Got root ca configmap in namespace "svcaccounts-2029"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:08:51.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2029" for this suite. 02/27/23 15:08:51.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:08:51.968
Feb 27 15:08:51.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename prestop 02/27/23 15:08:51.969
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:51.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:51.982
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4878 02/27/23 15:08:51.985
STEP: Waiting for pods to come up. 02/27/23 15:08:51.992
Feb 27 15:08:51.992: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4878" to be "running"
Feb 27 15:08:51.997: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539332ms
Feb 27 15:08:54.001: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008886375s
Feb 27 15:08:54.001: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4878 02/27/23 15:08:54.004
Feb 27 15:08:54.009: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4878" to be "running"
Feb 27 15:08:54.012: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846827ms
Feb 27 15:08:56.015: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006006465s
Feb 27 15:08:56.016: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/27/23 15:08:56.016
Feb 27 15:09:01.028: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/27/23 15:09:01.028
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Feb 27 15:09:01.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-4878" for this suite. 02/27/23 15:09:01.045
------------------------------
• [SLOW TEST] [9.082 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:08:51.968
    Feb 27 15:08:51.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename prestop 02/27/23 15:08:51.969
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:08:51.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:08:51.982
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4878 02/27/23 15:08:51.985
    STEP: Waiting for pods to come up. 02/27/23 15:08:51.992
    Feb 27 15:08:51.992: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4878" to be "running"
    Feb 27 15:08:51.997: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539332ms
    Feb 27 15:08:54.001: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008886375s
    Feb 27 15:08:54.001: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4878 02/27/23 15:08:54.004
    Feb 27 15:08:54.009: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4878" to be "running"
    Feb 27 15:08:54.012: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.846827ms
    Feb 27 15:08:56.015: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006006465s
    Feb 27 15:08:56.016: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/27/23 15:08:56.016
    Feb 27 15:09:01.028: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/27/23 15:09:01.028
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:09:01.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-4878" for this suite. 02/27/23 15:09:01.045
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:09:01.051
Feb 27 15:09:01.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename namespaces 02/27/23 15:09:01.051
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:01.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:09:01.064
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 02/27/23 15:09:01.067
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:01.078
STEP: Creating a pod in the namespace 02/27/23 15:09:01.08
STEP: Waiting for the pod to have running status 02/27/23 15:09:01.093
Feb 27 15:09:01.093: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8209" to be "running"
Feb 27 15:09:01.099: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509951ms
Feb 27 15:09:03.102: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008709725s
Feb 27 15:09:03.102: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/27/23 15:09:03.102
STEP: Waiting for the namespace to be removed. 02/27/23 15:09:03.108
STEP: Recreating the namespace 02/27/23 15:09:14.111
STEP: Verifying there are no pods in the namespace 02/27/23 15:09:14.123
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:09:14.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2678" for this suite. 02/27/23 15:09:14.13
STEP: Destroying namespace "nsdeletetest-8209" for this suite. 02/27/23 15:09:14.134
Feb 27 15:09:14.138: INFO: Namespace nsdeletetest-8209 was already deleted
STEP: Destroying namespace "nsdeletetest-4888" for this suite. 02/27/23 15:09:14.138
------------------------------
• [SLOW TEST] [13.092 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:09:01.051
    Feb 27 15:09:01.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename namespaces 02/27/23 15:09:01.051
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:01.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:09:01.064
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 02/27/23 15:09:01.067
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:01.078
    STEP: Creating a pod in the namespace 02/27/23 15:09:01.08
    STEP: Waiting for the pod to have running status 02/27/23 15:09:01.093
    Feb 27 15:09:01.093: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8209" to be "running"
    Feb 27 15:09:01.099: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509951ms
    Feb 27 15:09:03.102: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008709725s
    Feb 27 15:09:03.102: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/27/23 15:09:03.102
    STEP: Waiting for the namespace to be removed. 02/27/23 15:09:03.108
    STEP: Recreating the namespace 02/27/23 15:09:14.111
    STEP: Verifying there are no pods in the namespace 02/27/23 15:09:14.123
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:09:14.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2678" for this suite. 02/27/23 15:09:14.13
    STEP: Destroying namespace "nsdeletetest-8209" for this suite. 02/27/23 15:09:14.134
    Feb 27 15:09:14.138: INFO: Namespace nsdeletetest-8209 was already deleted
    STEP: Destroying namespace "nsdeletetest-4888" for this suite. 02/27/23 15:09:14.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:09:14.143
Feb 27 15:09:14.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename gc 02/27/23 15:09:14.143
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:14.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:09:14.16
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/27/23 15:09:14.166
STEP: delete the rc 02/27/23 15:09:19.281
STEP: wait for the rc to be deleted 02/27/23 15:09:19.294
Feb 27 15:09:20.310: INFO: 80 pods remaining
Feb 27 15:09:20.310: INFO: 80 pods has nil DeletionTimestamp
Feb 27 15:09:20.310: INFO: 
Feb 27 15:09:21.329: INFO: 72 pods remaining
Feb 27 15:09:21.329: INFO: 70 pods has nil DeletionTimestamp
Feb 27 15:09:21.329: INFO: 
Feb 27 15:09:22.305: INFO: 60 pods remaining
Feb 27 15:09:22.305: INFO: 60 pods has nil DeletionTimestamp
Feb 27 15:09:22.305: INFO: 
Feb 27 15:09:23.302: INFO: 40 pods remaining
Feb 27 15:09:23.302: INFO: 40 pods has nil DeletionTimestamp
Feb 27 15:09:23.302: INFO: 
Feb 27 15:09:24.304: INFO: 31 pods remaining
Feb 27 15:09:24.304: INFO: 31 pods has nil DeletionTimestamp
Feb 27 15:09:24.304: INFO: 
Feb 27 15:09:25.306: INFO: 20 pods remaining
Feb 27 15:09:25.306: INFO: 20 pods has nil DeletionTimestamp
Feb 27 15:09:25.306: INFO: 
STEP: Gathering metrics 02/27/23 15:09:26.299
W0227 15:09:26.303617      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 15:09:26.303: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 15:09:26.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6301" for this suite. 02/27/23 15:09:26.307
------------------------------
• [SLOW TEST] [12.169 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:09:14.143
    Feb 27 15:09:14.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename gc 02/27/23 15:09:14.143
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:14.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:09:14.16
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/27/23 15:09:14.166
    STEP: delete the rc 02/27/23 15:09:19.281
    STEP: wait for the rc to be deleted 02/27/23 15:09:19.294
    Feb 27 15:09:20.310: INFO: 80 pods remaining
    Feb 27 15:09:20.310: INFO: 80 pods has nil DeletionTimestamp
    Feb 27 15:09:20.310: INFO: 
    Feb 27 15:09:21.329: INFO: 72 pods remaining
    Feb 27 15:09:21.329: INFO: 70 pods has nil DeletionTimestamp
    Feb 27 15:09:21.329: INFO: 
    Feb 27 15:09:22.305: INFO: 60 pods remaining
    Feb 27 15:09:22.305: INFO: 60 pods has nil DeletionTimestamp
    Feb 27 15:09:22.305: INFO: 
    Feb 27 15:09:23.302: INFO: 40 pods remaining
    Feb 27 15:09:23.302: INFO: 40 pods has nil DeletionTimestamp
    Feb 27 15:09:23.302: INFO: 
    Feb 27 15:09:24.304: INFO: 31 pods remaining
    Feb 27 15:09:24.304: INFO: 31 pods has nil DeletionTimestamp
    Feb 27 15:09:24.304: INFO: 
    Feb 27 15:09:25.306: INFO: 20 pods remaining
    Feb 27 15:09:25.306: INFO: 20 pods has nil DeletionTimestamp
    Feb 27 15:09:25.306: INFO: 
    STEP: Gathering metrics 02/27/23 15:09:26.299
    W0227 15:09:26.303617      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 15:09:26.303: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:09:26.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6301" for this suite. 02/27/23 15:09:26.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:09:26.312
Feb 27 15:09:26.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename init-container 02/27/23 15:09:26.313
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:26.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:09:26.429
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 02/27/23 15:09:26.435
Feb 27 15:09:26.435: INFO: PodSpec: initContainers in spec.initContainers
Feb 27 15:10:11.345: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-85c1c1da-9f32-406e-b2d3-34ee29e168f9", GenerateName:"", Namespace:"init-container-910", SelfLink:"", UID:"22f96466-910a-4b4a-ac9c-1221ca00397b", ResourceVersion:"5250", Generation:0, CreationTimestamp:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"435778968"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e71350), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 15, 10, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e71380), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tdkrd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001006b20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tdkrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tdkrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tdkrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00424d020), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-42-40", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009bf340), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00424d0b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00424d0d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00424d0d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00424d0dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0014cbe10), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.42.40", PodIP:"192.168.192.179", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.192.179"}}, StartTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc000e713c8), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009bf420)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://4deb28729799796052a5da55dc497881f63c113bc44057de45a3c91035dc3a35", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001006ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001006b80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00424d15f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-910" for this suite. 02/27/23 15:10:11.35
------------------------------
• [SLOW TEST] [45.046 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:09:26.312
    Feb 27 15:09:26.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename init-container 02/27/23 15:09:26.313
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:09:26.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:09:26.429
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 02/27/23 15:09:26.435
    Feb 27 15:09:26.435: INFO: PodSpec: initContainers in spec.initContainers
    Feb 27 15:10:11.345: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-85c1c1da-9f32-406e-b2d3-34ee29e168f9", GenerateName:"", Namespace:"init-container-910", SelfLink:"", UID:"22f96466-910a-4b4a-ac9c-1221ca00397b", ResourceVersion:"5250", Generation:0, CreationTimestamp:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"435778968"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e71350), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 15, 10, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e71380), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tdkrd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001006b20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tdkrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tdkrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tdkrd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00424d020), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-42-40", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009bf340), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00424d0b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00424d0d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00424d0d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00424d0dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0014cbe10), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.42.40", PodIP:"192.168.192.179", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.192.179"}}, StartTime:time.Date(2023, time.February, 27, 15, 9, 26, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc000e713c8), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009bf420)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://4deb28729799796052a5da55dc497881f63c113bc44057de45a3c91035dc3a35", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001006ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001006b80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00424d15f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-910" for this suite. 02/27/23 15:10:11.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:11.359
Feb 27 15:10:11.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename csistoragecapacity 02/27/23 15:10:11.36
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:11.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:11.376
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/27/23 15:10:11.378
STEP: getting /apis/storage.k8s.io 02/27/23 15:10:11.38
STEP: getting /apis/storage.k8s.io/v1 02/27/23 15:10:11.381
STEP: creating 02/27/23 15:10:11.382
STEP: watching 02/27/23 15:10:11.398
Feb 27 15:10:11.398: INFO: starting watch
STEP: getting 02/27/23 15:10:11.404
STEP: listing in namespace 02/27/23 15:10:11.407
STEP: listing across namespaces 02/27/23 15:10:11.409
STEP: patching 02/27/23 15:10:11.411
STEP: updating 02/27/23 15:10:11.418
Feb 27 15:10:11.421: INFO: waiting for watch events with expected annotations in namespace
Feb 27 15:10:11.421: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/27/23 15:10:11.422
STEP: deleting a collection 02/27/23 15:10:11.432
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:11.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-2106" for this suite. 02/27/23 15:10:11.448
------------------------------
• [0.095 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:11.359
    Feb 27 15:10:11.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename csistoragecapacity 02/27/23 15:10:11.36
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:11.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:11.376
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/27/23 15:10:11.378
    STEP: getting /apis/storage.k8s.io 02/27/23 15:10:11.38
    STEP: getting /apis/storage.k8s.io/v1 02/27/23 15:10:11.381
    STEP: creating 02/27/23 15:10:11.382
    STEP: watching 02/27/23 15:10:11.398
    Feb 27 15:10:11.398: INFO: starting watch
    STEP: getting 02/27/23 15:10:11.404
    STEP: listing in namespace 02/27/23 15:10:11.407
    STEP: listing across namespaces 02/27/23 15:10:11.409
    STEP: patching 02/27/23 15:10:11.411
    STEP: updating 02/27/23 15:10:11.418
    Feb 27 15:10:11.421: INFO: waiting for watch events with expected annotations in namespace
    Feb 27 15:10:11.421: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/27/23 15:10:11.422
    STEP: deleting a collection 02/27/23 15:10:11.432
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:11.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-2106" for this suite. 02/27/23 15:10:11.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:11.455
Feb 27 15:10:11.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 15:10:11.456
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:11.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:11.473
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/27/23 15:10:11.475
Feb 27 15:10:11.484: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7833  9a7e12af-7e56-499c-810c-09d568a324f2 5271 0 2023-02-27 15:10:11 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-27 15:10:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9d2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9d2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 15:10:11.485: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7833" to be "running and ready"
Feb 27 15:10:11.495: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 9.857157ms
Feb 27 15:10:11.495: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:10:13.498: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.013458158s
Feb 27 15:10:13.498: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb 27 15:10:13.498: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/27/23 15:10:13.498
Feb 27 15:10:13.498: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7833 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:10:13.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:10:13.499: INFO: ExecWithOptions: Clientset creation
Feb 27 15:10:13.499: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7833/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/27/23 15:10:13.624
Feb 27 15:10:13.624: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7833 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:10:13.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:10:13.625: INFO: ExecWithOptions: Clientset creation
Feb 27 15:10:13.625: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7833/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 15:10:13.709: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:13.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7833" for this suite. 02/27/23 15:10:13.723
------------------------------
• [2.274 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:11.455
    Feb 27 15:10:11.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 15:10:11.456
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:11.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:11.473
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/27/23 15:10:11.475
    Feb 27 15:10:11.484: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7833  9a7e12af-7e56-499c-810c-09d568a324f2 5271 0 2023-02-27 15:10:11 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-27 15:10:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9d2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9d2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 15:10:11.485: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7833" to be "running and ready"
    Feb 27 15:10:11.495: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 9.857157ms
    Feb 27 15:10:11.495: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:10:13.498: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.013458158s
    Feb 27 15:10:13.498: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb 27 15:10:13.498: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/27/23 15:10:13.498
    Feb 27 15:10:13.498: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7833 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:10:13.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:10:13.499: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:10:13.499: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7833/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/27/23 15:10:13.624
    Feb 27 15:10:13.624: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7833 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:10:13.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:10:13.625: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:10:13.625: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7833/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 15:10:13.709: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:13.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7833" for this suite. 02/27/23 15:10:13.723
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:13.729
Feb 27 15:10:13.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:10:13.729
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:13.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:13.744
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:10:13.76
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:10:14.004
STEP: Deploying the webhook pod 02/27/23 15:10:14.011
STEP: Wait for the deployment to be ready 02/27/23 15:10:14.024
Feb 27 15:10:14.035: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:10:16.043
STEP: Verifying the service has paired with the endpoint 02/27/23 15:10:16.052
Feb 27 15:10:17.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/27/23 15:10:17.057
Feb 27 15:10:17.074: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be updated by the webhook 02/27/23 15:10:17.182
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:17.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6743" for this suite. 02/27/23 15:10:17.255
STEP: Destroying namespace "webhook-6743-markers" for this suite. 02/27/23 15:10:17.264
------------------------------
• [3.540 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:13.729
    Feb 27 15:10:13.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:10:13.729
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:13.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:13.744
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:10:13.76
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:10:14.004
    STEP: Deploying the webhook pod 02/27/23 15:10:14.011
    STEP: Wait for the deployment to be ready 02/27/23 15:10:14.024
    Feb 27 15:10:14.035: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:10:16.043
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:10:16.052
    Feb 27 15:10:17.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/27/23 15:10:17.057
    Feb 27 15:10:17.074: INFO: Waiting for webhook configuration to be ready...
    STEP: create a pod that should be updated by the webhook 02/27/23 15:10:17.182
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:17.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6743" for this suite. 02/27/23 15:10:17.255
    STEP: Destroying namespace "webhook-6743-markers" for this suite. 02/27/23 15:10:17.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:17.27
Feb 27 15:10:17.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename gc 02/27/23 15:10:17.271
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:17.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:17.294
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/27/23 15:10:17.296
STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 15:10:17.302
STEP: delete the deployment 02/27/23 15:10:17.414
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/27/23 15:10:17.423
STEP: Gathering metrics 02/27/23 15:10:17.943
W0227 15:10:17.946513      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 15:10:17.946: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:17.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4819" for this suite. 02/27/23 15:10:17.949
------------------------------
• [0.686 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:17.27
    Feb 27 15:10:17.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename gc 02/27/23 15:10:17.271
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:17.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:17.294
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/27/23 15:10:17.296
    STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 15:10:17.302
    STEP: delete the deployment 02/27/23 15:10:17.414
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/27/23 15:10:17.423
    STEP: Gathering metrics 02/27/23 15:10:17.943
    W0227 15:10:17.946513      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 15:10:17.946: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:17.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4819" for this suite. 02/27/23 15:10:17.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:17.957
Feb 27 15:10:17.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 15:10:17.957
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:17.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:17.971
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Feb 27 15:10:17.984: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 27 15:10:22.990: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 15:10:22.99
Feb 27 15:10:22.990: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 27 15:10:24.994: INFO: Creating deployment "test-rollover-deployment"
Feb 27 15:10:25.002: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 27 15:10:27.009: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 27 15:10:27.014: INFO: Ensure that both replica sets have 1 created replica
Feb 27 15:10:27.019: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 27 15:10:27.027: INFO: Updating deployment test-rollover-deployment
Feb 27 15:10:27.027: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 27 15:10:29.034: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 27 15:10:29.040: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 27 15:10:29.044: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 15:10:29.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:10:31.052: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 15:10:31.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:10:33.051: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 15:10:33.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:10:35.053: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 15:10:35.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:10:37.051: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 15:10:37.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:10:39.050: INFO: 
Feb 27 15:10:39.050: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 15:10:39.058: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3718  4ca539e8-156a-45fe-a7b3-53251e701430 5619 2 2023-02-27 15:10:25 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047c2a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 15:10:25 +0000 UTC,LastTransitionTime:2023-02-27 15:10:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-02-27 15:10:38 +0000 UTC,LastTransitionTime:2023-02-27 15:10:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 15:10:39.060: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3718  e9c10987-24c4-437f-8908-40433af5eec2 5609 2 2023-02-27 15:10:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4ca539e8-156a-45fe-a7b3-53251e701430 0xc00483ec17 0xc00483ec18}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca539e8-156a-45fe-a7b3-53251e701430\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00483ecc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 15:10:39.060: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 27 15:10:39.060: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3718  1ea40713-4582-4231-9195-2a2c9643fa9e 5618 2 2023-02-27 15:10:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4ca539e8-156a-45fe-a7b3-53251e701430 0xc00483eaef 0xc00483eb00}] [] [{e2e.test Update apps/v1 2023-02-27 15:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca539e8-156a-45fe-a7b3-53251e701430\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00483ebb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 15:10:39.060: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3718  a7596fcd-590e-451b-b723-c13f6083d341 5564 2 2023-02-27 15:10:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4ca539e8-156a-45fe-a7b3-53251e701430 0xc00483ed27 0xc00483ed28}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca539e8-156a-45fe-a7b3-53251e701430\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00483edd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 15:10:39.063: INFO: Pod "test-rollover-deployment-6c6df9974f-57s5t" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-57s5t test-rollover-deployment-6c6df9974f- deployment-3718  2d6dea8c-d01b-475a-b8f4-50292108cc32 5586 0 2023-02-27 15:10:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f e9c10987-24c4-437f-8908-40433af5eec2 0xc00483fc87 0xc00483fc88}] [] [{kube-controller-manager Update v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9c10987-24c4-437f-8908-40433af5eec2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:10:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvqv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvqv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.185,StartTime:2023-02-27 15:10:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:10:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://d466ec5a476f07721f686f6f3815fcc95bd36bfac0421ca65db0d4bd9077a6df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:39.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3718" for this suite. 02/27/23 15:10:39.066
------------------------------
• [SLOW TEST] [21.114 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:17.957
    Feb 27 15:10:17.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 15:10:17.957
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:17.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:17.971
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Feb 27 15:10:17.984: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb 27 15:10:22.990: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 15:10:22.99
    Feb 27 15:10:22.990: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb 27 15:10:24.994: INFO: Creating deployment "test-rollover-deployment"
    Feb 27 15:10:25.002: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb 27 15:10:27.009: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb 27 15:10:27.014: INFO: Ensure that both replica sets have 1 created replica
    Feb 27 15:10:27.019: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb 27 15:10:27.027: INFO: Updating deployment test-rollover-deployment
    Feb 27 15:10:27.027: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb 27 15:10:29.034: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb 27 15:10:29.040: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb 27 15:10:29.044: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 15:10:29.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:10:31.052: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 15:10:31.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:10:33.051: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 15:10:33.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:10:35.053: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 15:10:35.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:10:37.051: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 15:10:37.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 10, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 10, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:10:39.050: INFO: 
    Feb 27 15:10:39.050: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 15:10:39.058: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3718  4ca539e8-156a-45fe-a7b3-53251e701430 5619 2 2023-02-27 15:10:25 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047c2a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 15:10:25 +0000 UTC,LastTransitionTime:2023-02-27 15:10:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-02-27 15:10:38 +0000 UTC,LastTransitionTime:2023-02-27 15:10:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 15:10:39.060: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3718  e9c10987-24c4-437f-8908-40433af5eec2 5609 2 2023-02-27 15:10:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4ca539e8-156a-45fe-a7b3-53251e701430 0xc00483ec17 0xc00483ec18}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca539e8-156a-45fe-a7b3-53251e701430\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00483ecc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 15:10:39.060: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb 27 15:10:39.060: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3718  1ea40713-4582-4231-9195-2a2c9643fa9e 5618 2 2023-02-27 15:10:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4ca539e8-156a-45fe-a7b3-53251e701430 0xc00483eaef 0xc00483eb00}] [] [{e2e.test Update apps/v1 2023-02-27 15:10:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca539e8-156a-45fe-a7b3-53251e701430\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00483ebb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 15:10:39.060: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3718  a7596fcd-590e-451b-b723-c13f6083d341 5564 2 2023-02-27 15:10:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4ca539e8-156a-45fe-a7b3-53251e701430 0xc00483ed27 0xc00483ed28}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca539e8-156a-45fe-a7b3-53251e701430\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00483edd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 15:10:39.063: INFO: Pod "test-rollover-deployment-6c6df9974f-57s5t" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-57s5t test-rollover-deployment-6c6df9974f- deployment-3718  2d6dea8c-d01b-475a-b8f4-50292108cc32 5586 0 2023-02-27 15:10:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f e9c10987-24c4-437f-8908-40433af5eec2 0xc00483fc87 0xc00483fc88}] [] [{kube-controller-manager Update v1 2023-02-27 15:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9c10987-24c4-437f-8908-40433af5eec2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:10:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvqv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvqv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.185,StartTime:2023-02-27 15:10:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:10:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://d466ec5a476f07721f686f6f3815fcc95bd36bfac0421ca65db0d4bd9077a6df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:39.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3718" for this suite. 02/27/23 15:10:39.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:39.071
Feb 27 15:10:39.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:10:39.072
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:39.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:39.085
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/27/23 15:10:39.089
Feb 27 15:10:39.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/27/23 15:10:45.247
Feb 27 15:10:45.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:10:46.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4869" for this suite. 02/27/23 15:10:52.718
------------------------------
• [SLOW TEST] [13.653 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:39.071
    Feb 27 15:10:39.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:10:39.072
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:39.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:39.085
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/27/23 15:10:39.089
    Feb 27 15:10:39.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/27/23 15:10:45.247
    Feb 27 15:10:45.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:10:46.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4869" for this suite. 02/27/23 15:10:52.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:52.725
Feb 27 15:10:52.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:10:52.726
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:52.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:52.74
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Feb 27 15:10:52.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 15:10:54.147
Feb 27 15:10:54.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 create -f -'
Feb 27 15:10:54.530: INFO: stderr: ""
Feb 27 15:10:54.530: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 27 15:10:54.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 delete e2e-test-crd-publish-openapi-6752-crds test-cr'
Feb 27 15:10:54.599: INFO: stderr: ""
Feb 27 15:10:54.599: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 27 15:10:54.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 apply -f -'
Feb 27 15:10:54.839: INFO: stderr: ""
Feb 27 15:10:54.839: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 27 15:10:54.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 delete e2e-test-crd-publish-openapi-6752-crds test-cr'
Feb 27 15:10:54.893: INFO: stderr: ""
Feb 27 15:10:54.893: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/27/23 15:10:54.893
Feb 27 15:10:54.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 explain e2e-test-crd-publish-openapi-6752-crds'
Feb 27 15:10:55.158: INFO: stderr: ""
Feb 27 15:10:55.158: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6752-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:56.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3925" for this suite. 02/27/23 15:10:56.557
------------------------------
• [3.838 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:52.725
    Feb 27 15:10:52.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:10:52.726
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:52.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:52.74
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Feb 27 15:10:52.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 15:10:54.147
    Feb 27 15:10:54.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 create -f -'
    Feb 27 15:10:54.530: INFO: stderr: ""
    Feb 27 15:10:54.530: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 27 15:10:54.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 delete e2e-test-crd-publish-openapi-6752-crds test-cr'
    Feb 27 15:10:54.599: INFO: stderr: ""
    Feb 27 15:10:54.599: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb 27 15:10:54.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 apply -f -'
    Feb 27 15:10:54.839: INFO: stderr: ""
    Feb 27 15:10:54.839: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 27 15:10:54.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 --namespace=crd-publish-openapi-3925 delete e2e-test-crd-publish-openapi-6752-crds test-cr'
    Feb 27 15:10:54.893: INFO: stderr: ""
    Feb 27 15:10:54.893: INFO: stdout: "e2e-test-crd-publish-openapi-6752-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/27/23 15:10:54.893
    Feb 27 15:10:54.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-3925 explain e2e-test-crd-publish-openapi-6752-crds'
    Feb 27 15:10:55.158: INFO: stderr: ""
    Feb 27 15:10:55.158: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6752-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:56.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3925" for this suite. 02/27/23 15:10:56.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:56.565
Feb 27 15:10:56.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 15:10:56.565
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:56.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:56.584
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb 27 15:10:56.588: INFO: Creating deployment "test-recreate-deployment"
Feb 27 15:10:56.596: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 27 15:10:56.602: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 27 15:10:58.610: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 27 15:10:58.614: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 27 15:10:58.623: INFO: Updating deployment test-recreate-deployment
Feb 27 15:10:58.623: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 15:10:58.706: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9120  afc58ae9-7107-41f9-a81c-31c56bcbed8b 5808 2 2023-02-27 15:10:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003133168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 15:10:58 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-02-27 15:10:58 +0000 UTC,LastTransitionTime:2023-02-27 15:10:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 27 15:10:58.709: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9120  381cbd7e-bec9-4ce5-9696-d562968f0189 5804 1 2023-02-27 15:10:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment afc58ae9-7107-41f9-a81c-31c56bcbed8b 0xc001e1e190 0xc001e1e191}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afc58ae9-7107-41f9-a81c-31c56bcbed8b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e1e228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 15:10:58.709: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 27 15:10:58.710: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9120  1ec9c312-d1e3-4181-b21c-0f4abc185751 5796 2 2023-02-27 15:10:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment afc58ae9-7107-41f9-a81c-31c56bcbed8b 0xc001e1e087 0xc001e1e088}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afc58ae9-7107-41f9-a81c-31c56bcbed8b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e1e138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 15:10:58.713: INFO: Pod "test-recreate-deployment-cff6dc657-kd9zx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kd9zx test-recreate-deployment-cff6dc657- deployment-9120  4f6f5d4a-db52-418f-8b86-faeda268f95a 5807 0 2023-02-27 15:10:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 381cbd7e-bec9-4ce5-9696-d562968f0189 0xc003133e40 0xc003133e41}] [] [{kube-controller-manager Update v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381cbd7e-bec9-4ce5-9696-d562968f0189\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nw8r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nw8r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:,StartTime:2023-02-27 15:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 15:10:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9120" for this suite. 02/27/23 15:10:58.717
------------------------------
• [2.164 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:56.565
    Feb 27 15:10:56.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 15:10:56.565
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:56.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:56.584
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb 27 15:10:56.588: INFO: Creating deployment "test-recreate-deployment"
    Feb 27 15:10:56.596: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb 27 15:10:56.602: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb 27 15:10:58.610: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb 27 15:10:58.614: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb 27 15:10:58.623: INFO: Updating deployment test-recreate-deployment
    Feb 27 15:10:58.623: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 15:10:58.706: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9120  afc58ae9-7107-41f9-a81c-31c56bcbed8b 5808 2 2023-02-27 15:10:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003133168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 15:10:58 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-02-27 15:10:58 +0000 UTC,LastTransitionTime:2023-02-27 15:10:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 27 15:10:58.709: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9120  381cbd7e-bec9-4ce5-9696-d562968f0189 5804 1 2023-02-27 15:10:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment afc58ae9-7107-41f9-a81c-31c56bcbed8b 0xc001e1e190 0xc001e1e191}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afc58ae9-7107-41f9-a81c-31c56bcbed8b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e1e228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 15:10:58.709: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb 27 15:10:58.710: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9120  1ec9c312-d1e3-4181-b21c-0f4abc185751 5796 2 2023-02-27 15:10:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment afc58ae9-7107-41f9-a81c-31c56bcbed8b 0xc001e1e087 0xc001e1e088}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afc58ae9-7107-41f9-a81c-31c56bcbed8b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e1e138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 15:10:58.713: INFO: Pod "test-recreate-deployment-cff6dc657-kd9zx" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kd9zx test-recreate-deployment-cff6dc657- deployment-9120  4f6f5d4a-db52-418f-8b86-faeda268f95a 5807 0 2023-02-27 15:10:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 381cbd7e-bec9-4ce5-9696-d562968f0189 0xc003133e40 0xc003133e41}] [] [{kube-controller-manager Update v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"381cbd7e-bec9-4ce5-9696-d562968f0189\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:10:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nw8r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nw8r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:10:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:,StartTime:2023-02-27 15:10:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:10:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9120" for this suite. 02/27/23 15:10:58.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:10:58.729
Feb 27 15:10:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-pred 02/27/23 15:10:58.73
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:58.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:58.75
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 15:10:58.753: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 15:10:58.761: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 15:10:58.764: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
Feb 27 15:10:58.768: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.768: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:10:58.768: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.768: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 15:10:58.768: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:10:58.768: INFO: 	Container e2e ready: true, restart count 0
Feb 27 15:10:58.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:10:58.768: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:10:58.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:10:58.768: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:10:58.768: INFO: webhook-to-be-mutated from webhook-6743 started at 2023-02-27 15:10:17 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.769: INFO: 	Container example ready: false, restart count 0
Feb 27 15:10:58.769: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
Feb 27 15:10:58.778: INFO: test-recreate-deployment-cff6dc657-kd9zx from deployment-9120 started at 2023-02-27 15:10:58 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.779: INFO: 	Container httpd ready: false, restart count 0
Feb 27 15:10:58.779: INFO: default-http-backend-kubernetes-worker-9b9488b5c-sjcpm from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.779: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Feb 27 15:10:58.779: INFO: nginx-ingress-controller-kubernetes-worker-6x6fc from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.779: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:10:58.779: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.779: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 15:10:58.779: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:10:58.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:10:58.779: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:10:58.779: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
Feb 27 15:10:58.784: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.784: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:10:58.784: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.784: INFO: 	Container coredns ready: true, restart count 0
Feb 27 15:10:58.784: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.784: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 27 15:10:58.784: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
Feb 27 15:10:58.784: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 15:10:58.784: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 27 15:10:58.784: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.784: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 15:10:58.784: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:10:58.784: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 27 15:10:58.784: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:10:58.784: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:10:58.784: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 15:10:58.784
Feb 27 15:10:58.794: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1095" to be "running"
Feb 27 15:10:58.800: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098298ms
Feb 27 15:11:00.804: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010060169s
Feb 27 15:11:00.804: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 15:11:00.808
STEP: Trying to apply a random label on the found node. 02/27/23 15:11:00.825
STEP: verifying the node has the label kubernetes.io/e2e-b82956ac-5ee5-4c1f-bb74-ec08c176305e 95 02/27/23 15:11:00.834
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/27/23 15:11:00.838
Feb 27 15:11:00.844: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1095" to be "not pending"
Feb 27 15:11:00.849: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538199ms
Feb 27 15:11:02.855: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010293688s
Feb 27 15:11:02.855: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.42.40 on the node which pod4 resides and expect not scheduled 02/27/23 15:11:02.855
Feb 27 15:11:02.861: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1095" to be "not pending"
Feb 27 15:11:02.864: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737535ms
Feb 27 15:11:04.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006310054s
Feb 27 15:11:06.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00823295s
Feb 27 15:11:08.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007294947s
Feb 27 15:11:10.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007735959s
Feb 27 15:11:12.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008632763s
Feb 27 15:11:14.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007928714s
Feb 27 15:11:16.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.00727263s
Feb 27 15:11:18.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007687127s
Feb 27 15:11:20.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007602854s
Feb 27 15:11:22.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007322447s
Feb 27 15:11:24.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007160006s
Feb 27 15:11:26.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00750358s
Feb 27 15:11:28.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007014122s
Feb 27 15:11:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007249288s
Feb 27 15:11:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007381159s
Feb 27 15:11:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007304725s
Feb 27 15:11:36.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008336969s
Feb 27 15:11:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007305212s
Feb 27 15:11:40.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007341934s
Feb 27 15:11:42.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007277452s
Feb 27 15:11:44.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.007338875s
Feb 27 15:11:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007312426s
Feb 27 15:11:48.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007223219s
Feb 27 15:11:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007632279s
Feb 27 15:11:52.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007864397s
Feb 27 15:11:54.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00661611s
Feb 27 15:11:56.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007889253s
Feb 27 15:11:58.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007856935s
Feb 27 15:12:00.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007102094s
Feb 27 15:12:02.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007843044s
Feb 27 15:12:04.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006351667s
Feb 27 15:12:06.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00804473s
Feb 27 15:12:08.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007562794s
Feb 27 15:12:10.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006223243s
Feb 27 15:12:12.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007986837s
Feb 27 15:12:14.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007100189s
Feb 27 15:12:16.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007075809s
Feb 27 15:12:18.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008001078s
Feb 27 15:12:20.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007037091s
Feb 27 15:12:22.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007214191s
Feb 27 15:12:24.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007050416s
Feb 27 15:12:26.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.0068793s
Feb 27 15:12:28.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007801416s
Feb 27 15:12:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006977295s
Feb 27 15:12:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007010869s
Feb 27 15:12:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007201047s
Feb 27 15:12:36.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007339054s
Feb 27 15:12:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006894611s
Feb 27 15:12:40.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006678852s
Feb 27 15:12:42.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008412562s
Feb 27 15:12:44.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007475542s
Feb 27 15:12:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007115477s
Feb 27 15:12:48.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008072595s
Feb 27 15:12:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007008991s
Feb 27 15:12:52.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006973564s
Feb 27 15:12:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007290919s
Feb 27 15:12:56.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007980849s
Feb 27 15:12:58.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008375393s
Feb 27 15:13:00.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007719131s
Feb 27 15:13:02.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008434357s
Feb 27 15:13:04.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006961054s
Feb 27 15:13:06.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008127099s
Feb 27 15:13:08.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007564998s
Feb 27 15:13:10.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007426047s
Feb 27 15:13:12.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007500362s
Feb 27 15:13:14.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007730886s
Feb 27 15:13:16.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008112774s
Feb 27 15:13:18.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.00743493s
Feb 27 15:13:20.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008535684s
Feb 27 15:13:22.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007981069s
Feb 27 15:13:24.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006513477s
Feb 27 15:13:26.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008615403s
Feb 27 15:13:28.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009118229s
Feb 27 15:13:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007391741s
Feb 27 15:13:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007227947s
Feb 27 15:13:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006889829s
Feb 27 15:13:36.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007341454s
Feb 27 15:13:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00717417s
Feb 27 15:13:40.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006158006s
Feb 27 15:13:42.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007853193s
Feb 27 15:13:44.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006884083s
Feb 27 15:13:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007195524s
Feb 27 15:13:48.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007871392s
Feb 27 15:13:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007180819s
Feb 27 15:13:52.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006750725s
Feb 27 15:13:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006787761s
Feb 27 15:13:56.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007616577s
Feb 27 15:13:58.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007792102s
Feb 27 15:14:00.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006913688s
Feb 27 15:14:02.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006828433s
Feb 27 15:14:04.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006521493s
Feb 27 15:14:06.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009537005s
Feb 27 15:14:08.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007851043s
Feb 27 15:14:10.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007518801s
Feb 27 15:14:12.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007344664s
Feb 27 15:14:14.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007043123s
Feb 27 15:14:16.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008659591s
Feb 27 15:14:18.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008947312s
Feb 27 15:14:20.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007928085s
Feb 27 15:14:22.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008283491s
Feb 27 15:14:24.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006471952s
Feb 27 15:14:26.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007830228s
Feb 27 15:14:28.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.00752148s
Feb 27 15:14:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007304353s
Feb 27 15:14:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007638113s
Feb 27 15:14:34.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006460578s
Feb 27 15:14:36.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.00781321s
Feb 27 15:14:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007068745s
Feb 27 15:14:40.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006949976s
Feb 27 15:14:42.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00753426s
Feb 27 15:14:44.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008017352s
Feb 27 15:14:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007302142s
Feb 27 15:14:48.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00692634s
Feb 27 15:14:50.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00787581s
Feb 27 15:14:52.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00771321s
Feb 27 15:14:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006746677s
Feb 27 15:14:56.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007930777s
Feb 27 15:14:58.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007382886s
Feb 27 15:15:00.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006035274s
Feb 27 15:15:02.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007771875s
Feb 27 15:15:04.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006649006s
Feb 27 15:15:06.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009358419s
Feb 27 15:15:08.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008013963s
Feb 27 15:15:10.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007467016s
Feb 27 15:15:12.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008881888s
Feb 27 15:15:14.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006603456s
Feb 27 15:15:16.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009144356s
Feb 27 15:15:18.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008183674s
Feb 27 15:15:20.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00672125s
Feb 27 15:15:22.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007898372s
Feb 27 15:15:24.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006496991s
Feb 27 15:15:26.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007036874s
Feb 27 15:15:28.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007963703s
Feb 27 15:15:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007402975s
Feb 27 15:15:32.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007777873s
Feb 27 15:15:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006861929s
Feb 27 15:15:36.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.00742057s
Feb 27 15:15:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007246038s
Feb 27 15:15:40.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006158008s
Feb 27 15:15:42.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008169749s
Feb 27 15:15:44.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006451542s
Feb 27 15:15:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006802062s
Feb 27 15:15:48.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008140536s
Feb 27 15:15:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006909181s
Feb 27 15:15:52.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006893412s
Feb 27 15:15:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007173755s
Feb 27 15:15:56.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007248306s
Feb 27 15:15:58.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007462868s
Feb 27 15:16:00.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009268104s
Feb 27 15:16:02.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007223071s
Feb 27 15:16:02.871: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010246611s
STEP: removing the label kubernetes.io/e2e-b82956ac-5ee5-4c1f-bb74-ec08c176305e off the node ip-172-31-42-40 02/27/23 15:16:02.871
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b82956ac-5ee5-4c1f-bb74-ec08c176305e 02/27/23 15:16:02.883
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:16:02.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1095" for this suite. 02/27/23 15:16:02.89
------------------------------
• [SLOW TEST] [304.169 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:10:58.729
    Feb 27 15:10:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-pred 02/27/23 15:10:58.73
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:10:58.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:10:58.75
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 15:10:58.753: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 15:10:58.761: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 15:10:58.764: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
    Feb 27 15:10:58.768: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.768: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:10:58.768: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.768: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 15:10:58.768: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:10:58.768: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 15:10:58.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:10:58.768: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:10:58.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:10:58.768: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:10:58.768: INFO: webhook-to-be-mutated from webhook-6743 started at 2023-02-27 15:10:17 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.769: INFO: 	Container example ready: false, restart count 0
    Feb 27 15:10:58.769: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
    Feb 27 15:10:58.778: INFO: test-recreate-deployment-cff6dc657-kd9zx from deployment-9120 started at 2023-02-27 15:10:58 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.779: INFO: 	Container httpd ready: false, restart count 0
    Feb 27 15:10:58.779: INFO: default-http-backend-kubernetes-worker-9b9488b5c-sjcpm from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.779: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
    Feb 27 15:10:58.779: INFO: nginx-ingress-controller-kubernetes-worker-6x6fc from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.779: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:10:58.779: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.779: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 15:10:58.779: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:10:58.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:10:58.779: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:10:58.779: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
    Feb 27 15:10:58.784: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.784: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.784: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.784: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
    Feb 27 15:10:58.784: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.784: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:10:58.784: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:10:58.784: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:10:58.784: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 15:10:58.784
    Feb 27 15:10:58.794: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1095" to be "running"
    Feb 27 15:10:58.800: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.098298ms
    Feb 27 15:11:00.804: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010060169s
    Feb 27 15:11:00.804: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 15:11:00.808
    STEP: Trying to apply a random label on the found node. 02/27/23 15:11:00.825
    STEP: verifying the node has the label kubernetes.io/e2e-b82956ac-5ee5-4c1f-bb74-ec08c176305e 95 02/27/23 15:11:00.834
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/27/23 15:11:00.838
    Feb 27 15:11:00.844: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1095" to be "not pending"
    Feb 27 15:11:00.849: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538199ms
    Feb 27 15:11:02.855: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010293688s
    Feb 27 15:11:02.855: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.42.40 on the node which pod4 resides and expect not scheduled 02/27/23 15:11:02.855
    Feb 27 15:11:02.861: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1095" to be "not pending"
    Feb 27 15:11:02.864: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737535ms
    Feb 27 15:11:04.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006310054s
    Feb 27 15:11:06.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00823295s
    Feb 27 15:11:08.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007294947s
    Feb 27 15:11:10.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007735959s
    Feb 27 15:11:12.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008632763s
    Feb 27 15:11:14.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007928714s
    Feb 27 15:11:16.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.00727263s
    Feb 27 15:11:18.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007687127s
    Feb 27 15:11:20.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007602854s
    Feb 27 15:11:22.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007322447s
    Feb 27 15:11:24.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007160006s
    Feb 27 15:11:26.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00750358s
    Feb 27 15:11:28.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007014122s
    Feb 27 15:11:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007249288s
    Feb 27 15:11:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007381159s
    Feb 27 15:11:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007304725s
    Feb 27 15:11:36.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008336969s
    Feb 27 15:11:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007305212s
    Feb 27 15:11:40.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007341934s
    Feb 27 15:11:42.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007277452s
    Feb 27 15:11:44.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.007338875s
    Feb 27 15:11:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007312426s
    Feb 27 15:11:48.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007223219s
    Feb 27 15:11:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007632279s
    Feb 27 15:11:52.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007864397s
    Feb 27 15:11:54.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00661611s
    Feb 27 15:11:56.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007889253s
    Feb 27 15:11:58.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007856935s
    Feb 27 15:12:00.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007102094s
    Feb 27 15:12:02.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007843044s
    Feb 27 15:12:04.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006351667s
    Feb 27 15:12:06.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00804473s
    Feb 27 15:12:08.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007562794s
    Feb 27 15:12:10.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006223243s
    Feb 27 15:12:12.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007986837s
    Feb 27 15:12:14.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007100189s
    Feb 27 15:12:16.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007075809s
    Feb 27 15:12:18.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008001078s
    Feb 27 15:12:20.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007037091s
    Feb 27 15:12:22.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007214191s
    Feb 27 15:12:24.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007050416s
    Feb 27 15:12:26.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.0068793s
    Feb 27 15:12:28.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007801416s
    Feb 27 15:12:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006977295s
    Feb 27 15:12:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007010869s
    Feb 27 15:12:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007201047s
    Feb 27 15:12:36.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007339054s
    Feb 27 15:12:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006894611s
    Feb 27 15:12:40.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006678852s
    Feb 27 15:12:42.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008412562s
    Feb 27 15:12:44.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007475542s
    Feb 27 15:12:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007115477s
    Feb 27 15:12:48.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008072595s
    Feb 27 15:12:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007008991s
    Feb 27 15:12:52.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006973564s
    Feb 27 15:12:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007290919s
    Feb 27 15:12:56.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007980849s
    Feb 27 15:12:58.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008375393s
    Feb 27 15:13:00.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007719131s
    Feb 27 15:13:02.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008434357s
    Feb 27 15:13:04.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006961054s
    Feb 27 15:13:06.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008127099s
    Feb 27 15:13:08.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007564998s
    Feb 27 15:13:10.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007426047s
    Feb 27 15:13:12.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007500362s
    Feb 27 15:13:14.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007730886s
    Feb 27 15:13:16.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008112774s
    Feb 27 15:13:18.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.00743493s
    Feb 27 15:13:20.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008535684s
    Feb 27 15:13:22.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007981069s
    Feb 27 15:13:24.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006513477s
    Feb 27 15:13:26.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008615403s
    Feb 27 15:13:28.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009118229s
    Feb 27 15:13:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007391741s
    Feb 27 15:13:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007227947s
    Feb 27 15:13:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006889829s
    Feb 27 15:13:36.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007341454s
    Feb 27 15:13:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00717417s
    Feb 27 15:13:40.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006158006s
    Feb 27 15:13:42.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007853193s
    Feb 27 15:13:44.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006884083s
    Feb 27 15:13:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007195524s
    Feb 27 15:13:48.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007871392s
    Feb 27 15:13:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007180819s
    Feb 27 15:13:52.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006750725s
    Feb 27 15:13:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006787761s
    Feb 27 15:13:56.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007616577s
    Feb 27 15:13:58.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007792102s
    Feb 27 15:14:00.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006913688s
    Feb 27 15:14:02.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006828433s
    Feb 27 15:14:04.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006521493s
    Feb 27 15:14:06.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.009537005s
    Feb 27 15:14:08.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007851043s
    Feb 27 15:14:10.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007518801s
    Feb 27 15:14:12.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007344664s
    Feb 27 15:14:14.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007043123s
    Feb 27 15:14:16.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008659591s
    Feb 27 15:14:18.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008947312s
    Feb 27 15:14:20.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007928085s
    Feb 27 15:14:22.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008283491s
    Feb 27 15:14:24.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006471952s
    Feb 27 15:14:26.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007830228s
    Feb 27 15:14:28.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.00752148s
    Feb 27 15:14:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007304353s
    Feb 27 15:14:32.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007638113s
    Feb 27 15:14:34.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006460578s
    Feb 27 15:14:36.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.00781321s
    Feb 27 15:14:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007068745s
    Feb 27 15:14:40.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006949976s
    Feb 27 15:14:42.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00753426s
    Feb 27 15:14:44.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008017352s
    Feb 27 15:14:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007302142s
    Feb 27 15:14:48.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00692634s
    Feb 27 15:14:50.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00787581s
    Feb 27 15:14:52.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00771321s
    Feb 27 15:14:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006746677s
    Feb 27 15:14:56.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007930777s
    Feb 27 15:14:58.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007382886s
    Feb 27 15:15:00.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006035274s
    Feb 27 15:15:02.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007771875s
    Feb 27 15:15:04.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006649006s
    Feb 27 15:15:06.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009358419s
    Feb 27 15:15:08.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008013963s
    Feb 27 15:15:10.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007467016s
    Feb 27 15:15:12.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008881888s
    Feb 27 15:15:14.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006603456s
    Feb 27 15:15:16.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009144356s
    Feb 27 15:15:18.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008183674s
    Feb 27 15:15:20.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00672125s
    Feb 27 15:15:22.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007898372s
    Feb 27 15:15:24.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006496991s
    Feb 27 15:15:26.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007036874s
    Feb 27 15:15:28.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007963703s
    Feb 27 15:15:30.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007402975s
    Feb 27 15:15:32.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007777873s
    Feb 27 15:15:34.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006861929s
    Feb 27 15:15:36.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.00742057s
    Feb 27 15:15:38.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007246038s
    Feb 27 15:15:40.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006158008s
    Feb 27 15:15:42.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008169749s
    Feb 27 15:15:44.867: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006451542s
    Feb 27 15:15:46.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006802062s
    Feb 27 15:15:48.869: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008140536s
    Feb 27 15:15:50.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006909181s
    Feb 27 15:15:52.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006893412s
    Feb 27 15:15:54.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007173755s
    Feb 27 15:15:56.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007248306s
    Feb 27 15:15:58.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007462868s
    Feb 27 15:16:00.870: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009268104s
    Feb 27 15:16:02.868: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007223071s
    Feb 27 15:16:02.871: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010246611s
    STEP: removing the label kubernetes.io/e2e-b82956ac-5ee5-4c1f-bb74-ec08c176305e off the node ip-172-31-42-40 02/27/23 15:16:02.871
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b82956ac-5ee5-4c1f-bb74-ec08c176305e 02/27/23 15:16:02.883
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:16:02.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1095" for this suite. 02/27/23 15:16:02.89
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:16:02.899
Feb 27 15:16:02.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 15:16:02.899
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:02.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:02.922
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-b9b5e574-35c4-4f74-b059-524f09983df0 02/27/23 15:16:02.929
STEP: Creating the pod 02/27/23 15:16:02.94
Feb 27 15:16:02.948: INFO: Waiting up to 5m0s for pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c" in namespace "configmap-1128" to be "running and ready"
Feb 27 15:16:02.954: INFO: Pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.91934ms
Feb 27 15:16:02.954: INFO: The phase of Pod pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:16:04.959: INFO: Pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010557695s
Feb 27 15:16:04.959: INFO: The phase of Pod pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c is Running (Ready = true)
Feb 27 15:16:04.959: INFO: Pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-b9b5e574-35c4-4f74-b059-524f09983df0 02/27/23 15:16:04.976
STEP: waiting to observe update in volume 02/27/23 15:16:04.981
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:16:06.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1128" for this suite. 02/27/23 15:16:06.998
------------------------------
• [4.107 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:16:02.899
    Feb 27 15:16:02.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 15:16:02.899
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:02.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:02.922
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-b9b5e574-35c4-4f74-b059-524f09983df0 02/27/23 15:16:02.929
    STEP: Creating the pod 02/27/23 15:16:02.94
    Feb 27 15:16:02.948: INFO: Waiting up to 5m0s for pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c" in namespace "configmap-1128" to be "running and ready"
    Feb 27 15:16:02.954: INFO: Pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.91934ms
    Feb 27 15:16:02.954: INFO: The phase of Pod pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:16:04.959: INFO: Pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010557695s
    Feb 27 15:16:04.959: INFO: The phase of Pod pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c is Running (Ready = true)
    Feb 27 15:16:04.959: INFO: Pod "pod-configmaps-0fb8f520-0a54-4df4-a556-9d984ce20b5c" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-b9b5e574-35c4-4f74-b059-524f09983df0 02/27/23 15:16:04.976
    STEP: waiting to observe update in volume 02/27/23 15:16:04.981
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:16:06.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1128" for this suite. 02/27/23 15:16:06.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:16:07.007
Feb 27 15:16:07.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename daemonsets 02/27/23 15:16:07.008
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:07.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:07.027
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Feb 27 15:16:07.049: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 15:16:07.054
Feb 27 15:16:07.057: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:07.057: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:07.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 15:16:07.061: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:16:08.065: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:08.065: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:08.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 15:16:08.069: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:16:09.065: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:09.066: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:09.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 15:16:09.069: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 02/27/23 15:16:09.084
STEP: Check that daemon pods images are updated. 02/27/23 15:16:09.102
Feb 27 15:16:09.109: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:09.109: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:09.115: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:09.115: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:10.120: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:10.120: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:10.124: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:10.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:11.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:11.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:11.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:11.123: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:12.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:12.119: INFO: Pod daemon-set-fdktj is not available
Feb 27 15:16:12.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:12.122: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:12.122: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:13.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:13.119: INFO: Pod daemon-set-fdktj is not available
Feb 27 15:16:13.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:13.124: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:13.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:14.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:14.119: INFO: Pod daemon-set-fdktj is not available
Feb 27 15:16:14.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:14.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:14.123: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:15.120: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:15.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:15.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:16.120: INFO: Pod daemon-set-74qjn is not available
Feb 27 15:16:16.120: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Feb 27 15:16:16.124: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:16.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:17.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:17.123: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:18.119: INFO: Pod daemon-set-qhtpm is not available
Feb 27 15:16:18.122: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:18.122: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 02/27/23 15:16:18.122
Feb 27 15:16:18.127: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:18.127: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:18.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 15:16:18.131: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:16:19.137: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:19.137: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:19.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 15:16:19.141: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:16:20.135: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:20.135: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:20.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 15:16:20.139: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:16:21.136: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:21.136: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:21.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 15:16:21.140: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:16:22.135: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:22.135: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:16:22.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 15:16:22.140: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 15:16:22.161
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2357, will wait for the garbage collector to delete the pods 02/27/23 15:16:22.161
Feb 27 15:16:22.224: INFO: Deleting DaemonSet.extensions daemon-set took: 8.476649ms
Feb 27 15:16:22.325: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.02253ms
Feb 27 15:16:25.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 15:16:25.431: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 15:16:25.434: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6773"},"items":null}

Feb 27 15:16:25.437: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6773"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:16:25.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2357" for this suite. 02/27/23 15:16:25.454
------------------------------
• [SLOW TEST] [18.454 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:16:07.007
    Feb 27 15:16:07.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename daemonsets 02/27/23 15:16:07.008
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:07.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:07.027
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Feb 27 15:16:07.049: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 15:16:07.054
    Feb 27 15:16:07.057: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:07.057: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:07.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 15:16:07.061: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:16:08.065: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:08.065: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:08.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 15:16:08.069: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:16:09.065: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:09.066: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:09.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 15:16:09.069: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 02/27/23 15:16:09.084
    STEP: Check that daemon pods images are updated. 02/27/23 15:16:09.102
    Feb 27 15:16:09.109: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:09.109: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:09.115: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:09.115: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:10.120: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:10.120: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:10.124: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:10.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:11.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:11.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:11.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:11.123: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:12.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:12.119: INFO: Pod daemon-set-fdktj is not available
    Feb 27 15:16:12.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:12.122: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:12.122: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:13.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:13.119: INFO: Pod daemon-set-fdktj is not available
    Feb 27 15:16:13.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:13.124: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:13.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:14.119: INFO: Wrong image for pod: daemon-set-dmt8z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:14.119: INFO: Pod daemon-set-fdktj is not available
    Feb 27 15:16:14.119: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:14.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:14.123: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:15.120: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:15.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:15.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:16.120: INFO: Pod daemon-set-74qjn is not available
    Feb 27 15:16:16.120: INFO: Wrong image for pod: daemon-set-z49gk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Feb 27 15:16:16.124: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:16.124: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:17.123: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:17.123: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:18.119: INFO: Pod daemon-set-qhtpm is not available
    Feb 27 15:16:18.122: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:18.122: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 02/27/23 15:16:18.122
    Feb 27 15:16:18.127: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:18.127: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:18.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 15:16:18.131: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:16:19.137: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:19.137: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:19.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 15:16:19.141: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:16:20.135: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:20.135: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:20.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 15:16:20.139: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:16:21.136: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:21.136: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:21.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 15:16:21.140: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:16:22.135: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:22.135: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:16:22.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 15:16:22.140: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 15:16:22.161
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2357, will wait for the garbage collector to delete the pods 02/27/23 15:16:22.161
    Feb 27 15:16:22.224: INFO: Deleting DaemonSet.extensions daemon-set took: 8.476649ms
    Feb 27 15:16:22.325: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.02253ms
    Feb 27 15:16:25.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 15:16:25.431: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 15:16:25.434: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6773"},"items":null}

    Feb 27 15:16:25.437: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6773"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:16:25.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2357" for this suite. 02/27/23 15:16:25.454
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:16:25.461
Feb 27 15:16:25.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-pred 02/27/23 15:16:25.462
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:25.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:25.482
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 15:16:25.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 15:16:25.492: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 15:16:25.495: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
Feb 27 15:16:25.499: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.499: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:16:25.499: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.499: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 15:16:25.499: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:16:25.499: INFO: 	Container e2e ready: true, restart count 0
Feb 27 15:16:25.499: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:16:25.499: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:16:25.499: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:16:25.499: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:16:25.499: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
Feb 27 15:16:25.505: INFO: default-http-backend-kubernetes-worker-9b9488b5c-sjcpm from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.505: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Feb 27 15:16:25.505: INFO: nginx-ingress-controller-kubernetes-worker-6x6fc from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.505: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:16:25.505: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.505: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 15:16:25.505: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:16:25.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:16:25.505: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:16:25.505: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
Feb 27 15:16:25.510: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.510: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:16:25.510: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.510: INFO: 	Container coredns ready: true, restart count 0
Feb 27 15:16:25.510: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.510: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 27 15:16:25.510: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
Feb 27 15:16:25.510: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 15:16:25.510: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 27 15:16:25.510: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.510: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 15:16:25.510: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:16:25.510: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 27 15:16:25.510: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:16:25.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:16:25.510: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-172-31-3-182 02/27/23 15:16:25.525
STEP: verifying the node has the label node ip-172-31-42-40 02/27/23 15:16:25.539
STEP: verifying the node has the label node ip-172-31-84-171 02/27/23 15:16:25.554
Feb 27 15:16:25.563: INFO: Pod default-http-backend-kubernetes-worker-9b9488b5c-sjcpm requesting resource cpu=10m on Node ip-172-31-42-40
Feb 27 15:16:25.563: INFO: Pod nginx-ingress-controller-kubernetes-worker-6x6fc requesting resource cpu=0m on Node ip-172-31-42-40
Feb 27 15:16:25.563: INFO: Pod nginx-ingress-controller-kubernetes-worker-mvzvj requesting resource cpu=0m on Node ip-172-31-84-171
Feb 27 15:16:25.563: INFO: Pod nginx-ingress-controller-kubernetes-worker-s5lnj requesting resource cpu=0m on Node ip-172-31-3-182
Feb 27 15:16:25.563: INFO: Pod calico-kube-controllers-6b5c59f67b-27fsw requesting resource cpu=0m on Node ip-172-31-3-182
Feb 27 15:16:25.563: INFO: Pod coredns-77c75468db-2xznd requesting resource cpu=100m on Node ip-172-31-84-171
Feb 27 15:16:25.563: INFO: Pod kube-state-metrics-58f6fddc6f-nhlk4 requesting resource cpu=0m on Node ip-172-31-84-171
Feb 27 15:16:25.563: INFO: Pod metrics-server-v0.5.2-5c84bcbb7f-gf62n requesting resource cpu=5m on Node ip-172-31-84-171
Feb 27 15:16:25.563: INFO: Pod dashboard-metrics-scraper-7c69979f6f-dnd2h requesting resource cpu=0m on Node ip-172-31-84-171
Feb 27 15:16:25.563: INFO: Pod kubernetes-dashboard-7ccc5b6c5f-tgbtf requesting resource cpu=0m on Node ip-172-31-84-171
Feb 27 15:16:25.563: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-42-40
Feb 27 15:16:25.563: INFO: Pod sonobuoy-e2e-job-239e950f4344406f requesting resource cpu=0m on Node ip-172-31-3-182
Feb 27 15:16:25.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 requesting resource cpu=0m on Node ip-172-31-84-171
Feb 27 15:16:25.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn requesting resource cpu=0m on Node ip-172-31-3-182
Feb 27 15:16:25.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 requesting resource cpu=0m on Node ip-172-31-42-40
STEP: Starting Pods to consume most of the cluster CPU. 02/27/23 15:16:25.563
Feb 27 15:16:25.564: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-3-182
Feb 27 15:16:25.573: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-42-40
Feb 27 15:16:25.579: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-84-171
Feb 27 15:16:25.587: INFO: Waiting up to 5m0s for pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520" in namespace "sched-pred-19" to be "running"
Feb 27 15:16:25.591: INFO: Pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359588ms
Feb 27 15:16:27.596: INFO: Pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520": Phase="Running", Reason="", readiness=true. Elapsed: 2.009357434s
Feb 27 15:16:27.596: INFO: Pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520" satisfied condition "running"
Feb 27 15:16:27.596: INFO: Waiting up to 5m0s for pod "filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9" in namespace "sched-pred-19" to be "running"
Feb 27 15:16:27.599: INFO: Pod "filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9": Phase="Running", Reason="", readiness=true. Elapsed: 3.347942ms
Feb 27 15:16:27.600: INFO: Pod "filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9" satisfied condition "running"
Feb 27 15:16:27.600: INFO: Waiting up to 5m0s for pod "filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35" in namespace "sched-pred-19" to be "running"
Feb 27 15:16:27.603: INFO: Pod "filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35": Phase="Running", Reason="", readiness=true. Elapsed: 3.87274ms
Feb 27 15:16:27.603: INFO: Pod "filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/27/23 15:16:27.603
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75a842fcb2c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-19/filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520 to ip-172-31-3-182] 02/27/23 15:16:27.607
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75aa3eef8b6], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 02/27/23 15:16:27.607
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75abaa9fa57], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 381.338018ms (381.34169ms including waiting)] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75abbfc53b7], Reason = [Created], Message = [Created container filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75abfd587f9], Reason = [Started], Message = [Started container filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75a8484202c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-19/filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9 to ip-172-31-42-40] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75aa81b348b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75aa93b310b], Reason = [Created], Message = [Created container filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75aace87a53], Reason = [Started], Message = [Started container filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75a84e1c1c8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-19/filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35 to ip-172-31-84-171] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75aa632a344], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75aa7a50fa7], Reason = [Created], Message = [Created container filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75aaac1e9c3], Reason = [Started], Message = [Started container filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35] 02/27/23 15:16:27.608
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1747b75afd7975ef], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 02/27/23 15:16:27.62
STEP: removing the label node off the node ip-172-31-84-171 02/27/23 15:16:28.619
STEP: verifying the node doesn't have the label node 02/27/23 15:16:28.631
STEP: removing the label node off the node ip-172-31-3-182 02/27/23 15:16:28.634
STEP: verifying the node doesn't have the label node 02/27/23 15:16:28.649
STEP: removing the label node off the node ip-172-31-42-40 02/27/23 15:16:28.657
STEP: verifying the node doesn't have the label node 02/27/23 15:16:28.669
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:16:28.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-19" for this suite. 02/27/23 15:16:28.679
------------------------------
• [3.226 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:16:25.461
    Feb 27 15:16:25.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-pred 02/27/23 15:16:25.462
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:25.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:25.482
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 15:16:25.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 15:16:25.492: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 15:16:25.495: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
    Feb 27 15:16:25.499: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.499: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:16:25.499: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.499: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 15:16:25.499: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:16:25.499: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 15:16:25.499: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:16:25.499: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:16:25.499: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:16:25.499: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:16:25.499: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
    Feb 27 15:16:25.505: INFO: default-http-backend-kubernetes-worker-9b9488b5c-sjcpm from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.505: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
    Feb 27 15:16:25.505: INFO: nginx-ingress-controller-kubernetes-worker-6x6fc from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.505: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:16:25.505: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.505: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 15:16:25.505: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:16:25.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:16:25.505: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:16:25.505: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
    Feb 27 15:16:25.510: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.510: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.510: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.510: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
    Feb 27 15:16:25.510: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.510: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:16:25.510: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:16:25.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:16:25.510: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-172-31-3-182 02/27/23 15:16:25.525
    STEP: verifying the node has the label node ip-172-31-42-40 02/27/23 15:16:25.539
    STEP: verifying the node has the label node ip-172-31-84-171 02/27/23 15:16:25.554
    Feb 27 15:16:25.563: INFO: Pod default-http-backend-kubernetes-worker-9b9488b5c-sjcpm requesting resource cpu=10m on Node ip-172-31-42-40
    Feb 27 15:16:25.563: INFO: Pod nginx-ingress-controller-kubernetes-worker-6x6fc requesting resource cpu=0m on Node ip-172-31-42-40
    Feb 27 15:16:25.563: INFO: Pod nginx-ingress-controller-kubernetes-worker-mvzvj requesting resource cpu=0m on Node ip-172-31-84-171
    Feb 27 15:16:25.563: INFO: Pod nginx-ingress-controller-kubernetes-worker-s5lnj requesting resource cpu=0m on Node ip-172-31-3-182
    Feb 27 15:16:25.563: INFO: Pod calico-kube-controllers-6b5c59f67b-27fsw requesting resource cpu=0m on Node ip-172-31-3-182
    Feb 27 15:16:25.563: INFO: Pod coredns-77c75468db-2xznd requesting resource cpu=100m on Node ip-172-31-84-171
    Feb 27 15:16:25.563: INFO: Pod kube-state-metrics-58f6fddc6f-nhlk4 requesting resource cpu=0m on Node ip-172-31-84-171
    Feb 27 15:16:25.563: INFO: Pod metrics-server-v0.5.2-5c84bcbb7f-gf62n requesting resource cpu=5m on Node ip-172-31-84-171
    Feb 27 15:16:25.563: INFO: Pod dashboard-metrics-scraper-7c69979f6f-dnd2h requesting resource cpu=0m on Node ip-172-31-84-171
    Feb 27 15:16:25.563: INFO: Pod kubernetes-dashboard-7ccc5b6c5f-tgbtf requesting resource cpu=0m on Node ip-172-31-84-171
    Feb 27 15:16:25.563: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-42-40
    Feb 27 15:16:25.563: INFO: Pod sonobuoy-e2e-job-239e950f4344406f requesting resource cpu=0m on Node ip-172-31-3-182
    Feb 27 15:16:25.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 requesting resource cpu=0m on Node ip-172-31-84-171
    Feb 27 15:16:25.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn requesting resource cpu=0m on Node ip-172-31-3-182
    Feb 27 15:16:25.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 requesting resource cpu=0m on Node ip-172-31-42-40
    STEP: Starting Pods to consume most of the cluster CPU. 02/27/23 15:16:25.563
    Feb 27 15:16:25.564: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-3-182
    Feb 27 15:16:25.573: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-42-40
    Feb 27 15:16:25.579: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-84-171
    Feb 27 15:16:25.587: INFO: Waiting up to 5m0s for pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520" in namespace "sched-pred-19" to be "running"
    Feb 27 15:16:25.591: INFO: Pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359588ms
    Feb 27 15:16:27.596: INFO: Pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520": Phase="Running", Reason="", readiness=true. Elapsed: 2.009357434s
    Feb 27 15:16:27.596: INFO: Pod "filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520" satisfied condition "running"
    Feb 27 15:16:27.596: INFO: Waiting up to 5m0s for pod "filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9" in namespace "sched-pred-19" to be "running"
    Feb 27 15:16:27.599: INFO: Pod "filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9": Phase="Running", Reason="", readiness=true. Elapsed: 3.347942ms
    Feb 27 15:16:27.600: INFO: Pod "filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9" satisfied condition "running"
    Feb 27 15:16:27.600: INFO: Waiting up to 5m0s for pod "filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35" in namespace "sched-pred-19" to be "running"
    Feb 27 15:16:27.603: INFO: Pod "filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35": Phase="Running", Reason="", readiness=true. Elapsed: 3.87274ms
    Feb 27 15:16:27.603: INFO: Pod "filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/27/23 15:16:27.603
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75a842fcb2c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-19/filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520 to ip-172-31-3-182] 02/27/23 15:16:27.607
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75aa3eef8b6], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 02/27/23 15:16:27.607
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75abaa9fa57], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 381.338018ms (381.34169ms including waiting)] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75abbfc53b7], Reason = [Created], Message = [Created container filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520.1747b75abfd587f9], Reason = [Started], Message = [Started container filler-pod-2ebd51d0-d8f7-4dbc-980a-5162da36b520] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75a8484202c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-19/filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9 to ip-172-31-42-40] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75aa81b348b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75aa93b310b], Reason = [Created], Message = [Created container filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9.1747b75aace87a53], Reason = [Started], Message = [Started container filler-pod-53377c5a-e530-4f6c-b23b-13113cab41b9] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75a84e1c1c8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-19/filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35 to ip-172-31-84-171] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75aa632a344], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75aa7a50fa7], Reason = [Created], Message = [Created container filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35.1747b75aaac1e9c3], Reason = [Started], Message = [Started container filler-pod-81a2d3f9-4aee-4d6b-ade9-bcfedb7fca35] 02/27/23 15:16:27.608
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1747b75afd7975ef], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 02/27/23 15:16:27.62
    STEP: removing the label node off the node ip-172-31-84-171 02/27/23 15:16:28.619
    STEP: verifying the node doesn't have the label node 02/27/23 15:16:28.631
    STEP: removing the label node off the node ip-172-31-3-182 02/27/23 15:16:28.634
    STEP: verifying the node doesn't have the label node 02/27/23 15:16:28.649
    STEP: removing the label node off the node ip-172-31-42-40 02/27/23 15:16:28.657
    STEP: verifying the node doesn't have the label node 02/27/23 15:16:28.669
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:16:28.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-19" for this suite. 02/27/23 15:16:28.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:16:28.689
Feb 27 15:16:28.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:16:28.69
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:28.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:28.712
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3032 02/27/23 15:16:28.715
STEP: changing the ExternalName service to type=ClusterIP 02/27/23 15:16:28.72
STEP: creating replication controller externalname-service in namespace services-3032 02/27/23 15:16:28.739
I0227 15:16:28.746944      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3032, replica count: 2
I0227 15:16:31.798165      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 15:16:31.798: INFO: Creating new exec pod
Feb 27 15:16:31.807: INFO: Waiting up to 5m0s for pod "execpodd7dqh" in namespace "services-3032" to be "running"
Feb 27 15:16:31.812: INFO: Pod "execpodd7dqh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.70152ms
Feb 27 15:16:33.816: INFO: Pod "execpodd7dqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008751604s
Feb 27 15:16:33.816: INFO: Pod "execpodd7dqh" satisfied condition "running"
Feb 27 15:16:34.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3032 exec execpodd7dqh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Feb 27 15:16:34.924: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 27 15:16:34.924: INFO: stdout: ""
Feb 27 15:16:34.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3032 exec execpodd7dqh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.58 80'
Feb 27 15:16:35.023: INFO: stderr: "+ nc -v -z -w 2 10.152.183.58 80\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n"
Feb 27 15:16:35.023: INFO: stdout: ""
Feb 27 15:16:35.023: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:16:35.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3032" for this suite. 02/27/23 15:16:35.05
------------------------------
• [SLOW TEST] [6.367 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:16:28.689
    Feb 27 15:16:28.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:16:28.69
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:28.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:28.712
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3032 02/27/23 15:16:28.715
    STEP: changing the ExternalName service to type=ClusterIP 02/27/23 15:16:28.72
    STEP: creating replication controller externalname-service in namespace services-3032 02/27/23 15:16:28.739
    I0227 15:16:28.746944      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3032, replica count: 2
    I0227 15:16:31.798165      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 15:16:31.798: INFO: Creating new exec pod
    Feb 27 15:16:31.807: INFO: Waiting up to 5m0s for pod "execpodd7dqh" in namespace "services-3032" to be "running"
    Feb 27 15:16:31.812: INFO: Pod "execpodd7dqh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.70152ms
    Feb 27 15:16:33.816: INFO: Pod "execpodd7dqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008751604s
    Feb 27 15:16:33.816: INFO: Pod "execpodd7dqh" satisfied condition "running"
    Feb 27 15:16:34.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3032 exec execpodd7dqh -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Feb 27 15:16:34.924: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 27 15:16:34.924: INFO: stdout: ""
    Feb 27 15:16:34.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3032 exec execpodd7dqh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.58 80'
    Feb 27 15:16:35.023: INFO: stderr: "+ nc -v -z -w 2 10.152.183.58 80\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n"
    Feb 27 15:16:35.023: INFO: stdout: ""
    Feb 27 15:16:35.023: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:16:35.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3032" for this suite. 02/27/23 15:16:35.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:16:35.057
Feb 27 15:16:35.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 15:16:35.057
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:35.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:35.078
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 02/27/23 15:16:35.082
Feb 27 15:16:35.089: INFO: created test-pod-1
Feb 27 15:16:35.094: INFO: created test-pod-2
Feb 27 15:16:35.104: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/27/23 15:16:35.104
Feb 27 15:16:35.105: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7625' to be running and ready
Feb 27 15:16:35.125: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 15:16:35.125: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 15:16:35.125: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 15:16:35.125: INFO: 0 / 3 pods in namespace 'pods-7625' are running and ready (0 seconds elapsed)
Feb 27 15:16:35.125: INFO: expected 0 pod replicas in namespace 'pods-7625', 0 are Running and Ready.
Feb 27 15:16:35.125: INFO: POD         NODE             PHASE    GRACE  CONDITIONS
Feb 27 15:16:35.125: INFO: test-pod-1  ip-172-31-42-40  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  }]
Feb 27 15:16:35.125: INFO: test-pod-2  ip-172-31-3-182  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  }]
Feb 27 15:16:35.125: INFO: test-pod-3  ip-172-31-42-40  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  }]
Feb 27 15:16:35.125: INFO: 
Feb 27 15:16:37.135: INFO: 3 / 3 pods in namespace 'pods-7625' are running and ready (2 seconds elapsed)
Feb 27 15:16:37.135: INFO: expected 0 pod replicas in namespace 'pods-7625', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/27/23 15:16:37.156
Feb 27 15:16:37.160: INFO: Pod quantity 3 is different from expected quantity 0
Feb 27 15:16:38.165: INFO: Pod quantity 3 is different from expected quantity 0
Feb 27 15:16:39.164: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 15:16:40.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7625" for this suite. 02/27/23 15:16:40.169
------------------------------
• [SLOW TEST] [5.121 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:16:35.057
    Feb 27 15:16:35.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 15:16:35.057
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:35.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:35.078
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 02/27/23 15:16:35.082
    Feb 27 15:16:35.089: INFO: created test-pod-1
    Feb 27 15:16:35.094: INFO: created test-pod-2
    Feb 27 15:16:35.104: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/27/23 15:16:35.104
    Feb 27 15:16:35.105: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7625' to be running and ready
    Feb 27 15:16:35.125: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 15:16:35.125: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 15:16:35.125: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 15:16:35.125: INFO: 0 / 3 pods in namespace 'pods-7625' are running and ready (0 seconds elapsed)
    Feb 27 15:16:35.125: INFO: expected 0 pod replicas in namespace 'pods-7625', 0 are Running and Ready.
    Feb 27 15:16:35.125: INFO: POD         NODE             PHASE    GRACE  CONDITIONS
    Feb 27 15:16:35.125: INFO: test-pod-1  ip-172-31-42-40  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  }]
    Feb 27 15:16:35.125: INFO: test-pod-2  ip-172-31-3-182  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  }]
    Feb 27 15:16:35.125: INFO: test-pod-3  ip-172-31-42-40  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 15:16:35 +0000 UTC  }]
    Feb 27 15:16:35.125: INFO: 
    Feb 27 15:16:37.135: INFO: 3 / 3 pods in namespace 'pods-7625' are running and ready (2 seconds elapsed)
    Feb 27 15:16:37.135: INFO: expected 0 pod replicas in namespace 'pods-7625', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/27/23 15:16:37.156
    Feb 27 15:16:37.160: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 27 15:16:38.165: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 27 15:16:39.164: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:16:40.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7625" for this suite. 02/27/23 15:16:40.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:16:40.178
Feb 27 15:16:40.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:16:40.179
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:40.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:40.206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 15:16:40.208
Feb 27 15:16:40.216: INFO: Waiting up to 5m0s for pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da" in namespace "emptydir-7554" to be "Succeeded or Failed"
Feb 27 15:16:40.219: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752743ms
Feb 27 15:16:42.223: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007059872s
Feb 27 15:16:44.225: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008153083s
STEP: Saw pod success 02/27/23 15:16:44.225
Feb 27 15:16:44.225: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da" satisfied condition "Succeeded or Failed"
Feb 27 15:16:44.228: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da container test-container: <nil>
STEP: delete the pod 02/27/23 15:16:44.234
Feb 27 15:16:44.253: INFO: Waiting for pod pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da to disappear
Feb 27 15:16:44.256: INFO: Pod pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:16:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7554" for this suite. 02/27/23 15:16:44.259
------------------------------
• [4.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:16:40.178
    Feb 27 15:16:40.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:16:40.179
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:40.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:40.206
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 15:16:40.208
    Feb 27 15:16:40.216: INFO: Waiting up to 5m0s for pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da" in namespace "emptydir-7554" to be "Succeeded or Failed"
    Feb 27 15:16:40.219: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752743ms
    Feb 27 15:16:42.223: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007059872s
    Feb 27 15:16:44.225: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008153083s
    STEP: Saw pod success 02/27/23 15:16:44.225
    Feb 27 15:16:44.225: INFO: Pod "pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da" satisfied condition "Succeeded or Failed"
    Feb 27 15:16:44.228: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da container test-container: <nil>
    STEP: delete the pod 02/27/23 15:16:44.234
    Feb 27 15:16:44.253: INFO: Waiting for pod pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da to disappear
    Feb 27 15:16:44.256: INFO: Pod pod-b5b8bc58-17c8-4732-a64c-0e2ff2c9f0da no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:16:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7554" for this suite. 02/27/23 15:16:44.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:16:44.266
Feb 27 15:16:44.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename cronjob 02/27/23 15:16:44.267
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:44.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:44.285
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/27/23 15:16:44.287
STEP: Ensuring a job is scheduled 02/27/23 15:16:44.294
STEP: Ensuring exactly one is scheduled 02/27/23 15:17:00.3
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 15:17:00.305
STEP: Ensuring no more jobs are scheduled 02/27/23 15:17:00.31
STEP: Removing cronjob 02/27/23 15:22:00.318
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 15:22:00.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4567" for this suite. 02/27/23 15:22:00.328
------------------------------
• [SLOW TEST] [316.068 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:16:44.266
    Feb 27 15:16:44.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename cronjob 02/27/23 15:16:44.267
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:16:44.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:16:44.285
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/27/23 15:16:44.287
    STEP: Ensuring a job is scheduled 02/27/23 15:16:44.294
    STEP: Ensuring exactly one is scheduled 02/27/23 15:17:00.3
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 15:17:00.305
    STEP: Ensuring no more jobs are scheduled 02/27/23 15:17:00.31
    STEP: Removing cronjob 02/27/23 15:22:00.318
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:22:00.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4567" for this suite. 02/27/23 15:22:00.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:22:00.334
Feb 27 15:22:00.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:22:00.335
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:22:00.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:22:00.357
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-0b696ab7-9c61-4523-b240-8b51b9de4a20 02/27/23 15:22:00.36
STEP: Creating a pod to test consume configMaps 02/27/23 15:22:00.369
Feb 27 15:22:00.375: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9" in namespace "projected-779" to be "Succeeded or Failed"
Feb 27 15:22:00.378: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63776ms
Feb 27 15:22:02.383: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008313371s
Feb 27 15:22:04.382: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007439856s
STEP: Saw pod success 02/27/23 15:22:04.382
Feb 27 15:22:04.382: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9" satisfied condition "Succeeded or Failed"
Feb 27 15:22:04.386: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:22:04.4
Feb 27 15:22:04.414: INFO: Waiting for pod pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9 to disappear
Feb 27 15:22:04.418: INFO: Pod pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:22:04.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-779" for this suite. 02/27/23 15:22:04.421
------------------------------
• [4.093 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:22:00.334
    Feb 27 15:22:00.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:22:00.335
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:22:00.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:22:00.357
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-0b696ab7-9c61-4523-b240-8b51b9de4a20 02/27/23 15:22:00.36
    STEP: Creating a pod to test consume configMaps 02/27/23 15:22:00.369
    Feb 27 15:22:00.375: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9" in namespace "projected-779" to be "Succeeded or Failed"
    Feb 27 15:22:00.378: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63776ms
    Feb 27 15:22:02.383: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008313371s
    Feb 27 15:22:04.382: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007439856s
    STEP: Saw pod success 02/27/23 15:22:04.382
    Feb 27 15:22:04.382: INFO: Pod "pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9" satisfied condition "Succeeded or Failed"
    Feb 27 15:22:04.386: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:22:04.4
    Feb 27 15:22:04.414: INFO: Waiting for pod pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9 to disappear
    Feb 27 15:22:04.418: INFO: Pod pod-projected-configmaps-dd1ed72d-8ca4-43f2-96d3-1acb673068d9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:22:04.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-779" for this suite. 02/27/23 15:22:04.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:22:04.428
Feb 27 15:22:04.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 15:22:04.429
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:22:04.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:22:04.447
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 02/27/23 15:22:04.45
Feb 27 15:22:04.459: INFO: Waiting up to 5m0s for pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183" in namespace "var-expansion-8500" to be "Succeeded or Failed"
Feb 27 15:22:04.462: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070273ms
Feb 27 15:22:06.467: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007926864s
Feb 27 15:22:08.467: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008291905s
STEP: Saw pod success 02/27/23 15:22:08.467
Feb 27 15:22:08.467: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183" satisfied condition "Succeeded or Failed"
Feb 27 15:22:08.471: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183 container dapi-container: <nil>
STEP: delete the pod 02/27/23 15:22:08.477
Feb 27 15:22:08.492: INFO: Waiting for pod var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183 to disappear
Feb 27 15:22:08.495: INFO: Pod var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 15:22:08.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8500" for this suite. 02/27/23 15:22:08.498
------------------------------
• [4.076 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:22:04.428
    Feb 27 15:22:04.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 15:22:04.429
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:22:04.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:22:04.447
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 02/27/23 15:22:04.45
    Feb 27 15:22:04.459: INFO: Waiting up to 5m0s for pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183" in namespace "var-expansion-8500" to be "Succeeded or Failed"
    Feb 27 15:22:04.462: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183": Phase="Pending", Reason="", readiness=false. Elapsed: 3.070273ms
    Feb 27 15:22:06.467: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007926864s
    Feb 27 15:22:08.467: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008291905s
    STEP: Saw pod success 02/27/23 15:22:08.467
    Feb 27 15:22:08.467: INFO: Pod "var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183" satisfied condition "Succeeded or Failed"
    Feb 27 15:22:08.471: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 15:22:08.477
    Feb 27 15:22:08.492: INFO: Waiting for pod var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183 to disappear
    Feb 27 15:22:08.495: INFO: Pod var-expansion-f2a0bfdc-3b04-4fab-8694-7c3b58a02183 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:22:08.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8500" for this suite. 02/27/23 15:22:08.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:22:08.506
Feb 27 15:22:08.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 15:22:08.507
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:22:08.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:22:08.527
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263 in namespace container-probe-2367 02/27/23 15:22:08.53
Feb 27 15:22:08.538: INFO: Waiting up to 5m0s for pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263" in namespace "container-probe-2367" to be "not pending"
Feb 27 15:22:08.543: INFO: Pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515084ms
Feb 27 15:22:10.547: INFO: Pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263": Phase="Running", Reason="", readiness=true. Elapsed: 2.00901762s
Feb 27 15:22:10.547: INFO: Pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263" satisfied condition "not pending"
Feb 27 15:22:10.547: INFO: Started pod liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263 in namespace container-probe-2367
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 15:22:10.547
Feb 27 15:22:10.551: INFO: Initial restart count of pod liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263 is 0
STEP: deleting the pod 02/27/23 15:26:11.084
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 15:26:11.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2367" for this suite. 02/27/23 15:26:11.101
------------------------------
• [SLOW TEST] [242.603 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:22:08.506
    Feb 27 15:22:08.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 15:22:08.507
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:22:08.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:22:08.527
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263 in namespace container-probe-2367 02/27/23 15:22:08.53
    Feb 27 15:22:08.538: INFO: Waiting up to 5m0s for pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263" in namespace "container-probe-2367" to be "not pending"
    Feb 27 15:22:08.543: INFO: Pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515084ms
    Feb 27 15:22:10.547: INFO: Pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263": Phase="Running", Reason="", readiness=true. Elapsed: 2.00901762s
    Feb 27 15:22:10.547: INFO: Pod "liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263" satisfied condition "not pending"
    Feb 27 15:22:10.547: INFO: Started pod liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263 in namespace container-probe-2367
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 15:22:10.547
    Feb 27 15:22:10.551: INFO: Initial restart count of pod liveness-d2b425bd-0ab5-4a95-9bb5-f1b51176d263 is 0
    STEP: deleting the pod 02/27/23 15:26:11.084
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:26:11.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2367" for this suite. 02/27/23 15:26:11.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:26:11.109
Feb 27 15:26:11.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename hostport 02/27/23 15:26:11.11
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:11.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:11.132
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/27/23 15:26:11.139
Feb 27 15:26:11.147: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1609" to be "running and ready"
Feb 27 15:26:11.151: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38949ms
Feb 27 15:26:11.151: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:26:13.154: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006941248s
Feb 27 15:26:13.154: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 15:26:13.154: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.42.40 on the node which pod1 resides and expect scheduled 02/27/23 15:26:13.154
Feb 27 15:26:13.161: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1609" to be "running and ready"
Feb 27 15:26:13.164: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330425ms
Feb 27 15:26:13.164: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:26:15.170: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008722954s
Feb 27 15:26:15.170: INFO: The phase of Pod pod2 is Running (Ready = false)
Feb 27 15:26:17.168: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007304622s
Feb 27 15:26:17.168: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 15:26:17.168: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.42.40 but use UDP protocol on the node which pod2 resides 02/27/23 15:26:17.168
Feb 27 15:26:17.174: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1609" to be "running and ready"
Feb 27 15:26:17.178: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091501ms
Feb 27 15:26:17.178: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:26:19.183: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009250959s
Feb 27 15:26:19.183: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb 27 15:26:19.183: INFO: Pod "pod3" satisfied condition "running and ready"
Feb 27 15:26:19.189: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1609" to be "running and ready"
Feb 27 15:26:19.194: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905596ms
Feb 27 15:26:19.194: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:26:21.198: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009331735s
Feb 27 15:26:21.198: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb 27 15:26:21.198: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/27/23 15:26:21.203
Feb 27 15:26:21.203: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.42.40 http://127.0.0.1:54323/hostname] Namespace:hostport-1609 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:26:21.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:26:21.203: INFO: ExecWithOptions: Clientset creation
Feb 27 15:26:21.203: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1609/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.42.40+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.42.40, port: 54323 02/27/23 15:26:21.287
Feb 27 15:26:21.287: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.42.40:54323/hostname] Namespace:hostport-1609 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:26:21.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:26:21.287: INFO: ExecWithOptions: Clientset creation
Feb 27 15:26:21.287: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1609/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.42.40%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.42.40, port: 54323 UDP 02/27/23 15:26:21.357
Feb 27 15:26:21.357: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.42.40 54323] Namespace:hostport-1609 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:26:21.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:26:21.358: INFO: ExecWithOptions: Clientset creation
Feb 27 15:26:21.358: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1609/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.42.40+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Feb 27 15:26:26.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-1609" for this suite. 02/27/23 15:26:26.425
------------------------------
• [SLOW TEST] [15.323 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:26:11.109
    Feb 27 15:26:11.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename hostport 02/27/23 15:26:11.11
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:11.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:11.132
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/27/23 15:26:11.139
    Feb 27 15:26:11.147: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1609" to be "running and ready"
    Feb 27 15:26:11.151: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38949ms
    Feb 27 15:26:11.151: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:26:13.154: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006941248s
    Feb 27 15:26:13.154: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 15:26:13.154: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.42.40 on the node which pod1 resides and expect scheduled 02/27/23 15:26:13.154
    Feb 27 15:26:13.161: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1609" to be "running and ready"
    Feb 27 15:26:13.164: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330425ms
    Feb 27 15:26:13.164: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:26:15.170: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008722954s
    Feb 27 15:26:15.170: INFO: The phase of Pod pod2 is Running (Ready = false)
    Feb 27 15:26:17.168: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007304622s
    Feb 27 15:26:17.168: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 15:26:17.168: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.42.40 but use UDP protocol on the node which pod2 resides 02/27/23 15:26:17.168
    Feb 27 15:26:17.174: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1609" to be "running and ready"
    Feb 27 15:26:17.178: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091501ms
    Feb 27 15:26:17.178: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:26:19.183: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009250959s
    Feb 27 15:26:19.183: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb 27 15:26:19.183: INFO: Pod "pod3" satisfied condition "running and ready"
    Feb 27 15:26:19.189: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1609" to be "running and ready"
    Feb 27 15:26:19.194: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905596ms
    Feb 27 15:26:19.194: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:26:21.198: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009331735s
    Feb 27 15:26:21.198: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb 27 15:26:21.198: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/27/23 15:26:21.203
    Feb 27 15:26:21.203: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.42.40 http://127.0.0.1:54323/hostname] Namespace:hostport-1609 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:26:21.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:26:21.203: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:26:21.203: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1609/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.42.40+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.42.40, port: 54323 02/27/23 15:26:21.287
    Feb 27 15:26:21.287: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.42.40:54323/hostname] Namespace:hostport-1609 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:26:21.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:26:21.287: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:26:21.287: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1609/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.42.40%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.42.40, port: 54323 UDP 02/27/23 15:26:21.357
    Feb 27 15:26:21.357: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.42.40 54323] Namespace:hostport-1609 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:26:21.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:26:21.358: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:26:21.358: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1609/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.42.40+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:26:26.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-1609" for this suite. 02/27/23 15:26:26.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:26:26.433
Feb 27 15:26:26.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:26:26.433
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:26.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:26.453
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 02/27/23 15:26:26.456
Feb 27 15:26:26.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 create -f -'
Feb 27 15:26:26.743: INFO: stderr: ""
Feb 27 15:26:26.743: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 15:26:26.743
Feb 27 15:26:26.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 15:26:26.799: INFO: stderr: ""
Feb 27 15:26:26.799: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-tkdvd "
Feb 27 15:26:26.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:26.846: INFO: stderr: ""
Feb 27 15:26:26.846: INFO: stdout: ""
Feb 27 15:26:26.846: INFO: update-demo-nautilus-gtqcq is created but not running
Feb 27 15:26:31.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 15:26:31.899: INFO: stderr: ""
Feb 27 15:26:31.899: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-tkdvd "
Feb 27 15:26:31.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:31.945: INFO: stderr: ""
Feb 27 15:26:31.945: INFO: stdout: "true"
Feb 27 15:26:31.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 15:26:31.993: INFO: stderr: ""
Feb 27 15:26:31.993: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 15:26:31.993: INFO: validating pod update-demo-nautilus-gtqcq
Feb 27 15:26:31.999: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 15:26:31.999: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 15:26:31.999: INFO: update-demo-nautilus-gtqcq is verified up and running
Feb 27 15:26:31.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-tkdvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:32.046: INFO: stderr: ""
Feb 27 15:26:32.046: INFO: stdout: "true"
Feb 27 15:26:32.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-tkdvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 15:26:32.094: INFO: stderr: ""
Feb 27 15:26:32.095: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 15:26:32.095: INFO: validating pod update-demo-nautilus-tkdvd
Feb 27 15:26:32.099: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 15:26:32.099: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 15:26:32.099: INFO: update-demo-nautilus-tkdvd is verified up and running
STEP: scaling down the replication controller 02/27/23 15:26:32.099
Feb 27 15:26:32.100: INFO: scanned /root for discovery docs: <nil>
Feb 27 15:26:32.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 27 15:26:33.158: INFO: stderr: ""
Feb 27 15:26:33.158: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 15:26:33.158
Feb 27 15:26:33.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 15:26:33.207: INFO: stderr: ""
Feb 27 15:26:33.207: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-tkdvd "
STEP: Replicas for name=update-demo: expected=1 actual=2 02/27/23 15:26:33.207
Feb 27 15:26:38.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 15:26:38.258: INFO: stderr: ""
Feb 27 15:26:38.258: INFO: stdout: "update-demo-nautilus-gtqcq "
Feb 27 15:26:38.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:38.307: INFO: stderr: ""
Feb 27 15:26:38.307: INFO: stdout: "true"
Feb 27 15:26:38.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 15:26:38.356: INFO: stderr: ""
Feb 27 15:26:38.356: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 15:26:38.356: INFO: validating pod update-demo-nautilus-gtqcq
Feb 27 15:26:38.361: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 15:26:38.361: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 15:26:38.361: INFO: update-demo-nautilus-gtqcq is verified up and running
STEP: scaling up the replication controller 02/27/23 15:26:38.361
Feb 27 15:26:38.362: INFO: scanned /root for discovery docs: <nil>
Feb 27 15:26:38.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 27 15:26:39.432: INFO: stderr: ""
Feb 27 15:26:39.432: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 15:26:39.432
Feb 27 15:26:39.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 15:26:39.481: INFO: stderr: ""
Feb 27 15:26:39.481: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-lmk79 "
Feb 27 15:26:39.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:39.529: INFO: stderr: ""
Feb 27 15:26:39.529: INFO: stdout: "true"
Feb 27 15:26:39.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 15:26:39.574: INFO: stderr: ""
Feb 27 15:26:39.574: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 15:26:39.574: INFO: validating pod update-demo-nautilus-gtqcq
Feb 27 15:26:39.579: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 15:26:39.579: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 15:26:39.579: INFO: update-demo-nautilus-gtqcq is verified up and running
Feb 27 15:26:39.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-lmk79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:39.624: INFO: stderr: ""
Feb 27 15:26:39.624: INFO: stdout: ""
Feb 27 15:26:39.624: INFO: update-demo-nautilus-lmk79 is created but not running
Feb 27 15:26:44.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 15:26:44.680: INFO: stderr: ""
Feb 27 15:26:44.680: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-lmk79 "
Feb 27 15:26:44.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:44.727: INFO: stderr: ""
Feb 27 15:26:44.727: INFO: stdout: "true"
Feb 27 15:26:44.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 15:26:44.777: INFO: stderr: ""
Feb 27 15:26:44.777: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 15:26:44.777: INFO: validating pod update-demo-nautilus-gtqcq
Feb 27 15:26:44.782: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 15:26:44.782: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 15:26:44.782: INFO: update-demo-nautilus-gtqcq is verified up and running
Feb 27 15:26:44.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-lmk79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 15:26:44.829: INFO: stderr: ""
Feb 27 15:26:44.829: INFO: stdout: "true"
Feb 27 15:26:44.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-lmk79 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 15:26:44.877: INFO: stderr: ""
Feb 27 15:26:44.877: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 15:26:44.877: INFO: validating pod update-demo-nautilus-lmk79
Feb 27 15:26:44.881: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 15:26:44.881: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 15:26:44.881: INFO: update-demo-nautilus-lmk79 is verified up and running
STEP: using delete to clean up resources 02/27/23 15:26:44.881
Feb 27 15:26:44.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 delete --grace-period=0 --force -f -'
Feb 27 15:26:44.933: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:26:44.933: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 27 15:26:44.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get rc,svc -l name=update-demo --no-headers'
Feb 27 15:26:45.003: INFO: stderr: "No resources found in kubectl-4114 namespace.\n"
Feb 27 15:26:45.003: INFO: stdout: ""
Feb 27 15:26:45.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 15:26:45.052: INFO: stderr: ""
Feb 27 15:26:45.052: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:26:45.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4114" for this suite. 02/27/23 15:26:45.058
------------------------------
• [SLOW TEST] [18.633 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:26:26.433
    Feb 27 15:26:26.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:26:26.433
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:26.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:26.453
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 02/27/23 15:26:26.456
    Feb 27 15:26:26.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 create -f -'
    Feb 27 15:26:26.743: INFO: stderr: ""
    Feb 27 15:26:26.743: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 15:26:26.743
    Feb 27 15:26:26.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 15:26:26.799: INFO: stderr: ""
    Feb 27 15:26:26.799: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-tkdvd "
    Feb 27 15:26:26.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:26.846: INFO: stderr: ""
    Feb 27 15:26:26.846: INFO: stdout: ""
    Feb 27 15:26:26.846: INFO: update-demo-nautilus-gtqcq is created but not running
    Feb 27 15:26:31.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 15:26:31.899: INFO: stderr: ""
    Feb 27 15:26:31.899: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-tkdvd "
    Feb 27 15:26:31.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:31.945: INFO: stderr: ""
    Feb 27 15:26:31.945: INFO: stdout: "true"
    Feb 27 15:26:31.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 15:26:31.993: INFO: stderr: ""
    Feb 27 15:26:31.993: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 15:26:31.993: INFO: validating pod update-demo-nautilus-gtqcq
    Feb 27 15:26:31.999: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 15:26:31.999: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 15:26:31.999: INFO: update-demo-nautilus-gtqcq is verified up and running
    Feb 27 15:26:31.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-tkdvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:32.046: INFO: stderr: ""
    Feb 27 15:26:32.046: INFO: stdout: "true"
    Feb 27 15:26:32.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-tkdvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 15:26:32.094: INFO: stderr: ""
    Feb 27 15:26:32.095: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 15:26:32.095: INFO: validating pod update-demo-nautilus-tkdvd
    Feb 27 15:26:32.099: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 15:26:32.099: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 15:26:32.099: INFO: update-demo-nautilus-tkdvd is verified up and running
    STEP: scaling down the replication controller 02/27/23 15:26:32.099
    Feb 27 15:26:32.100: INFO: scanned /root for discovery docs: <nil>
    Feb 27 15:26:32.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb 27 15:26:33.158: INFO: stderr: ""
    Feb 27 15:26:33.158: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 15:26:33.158
    Feb 27 15:26:33.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 15:26:33.207: INFO: stderr: ""
    Feb 27 15:26:33.207: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-tkdvd "
    STEP: Replicas for name=update-demo: expected=1 actual=2 02/27/23 15:26:33.207
    Feb 27 15:26:38.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 15:26:38.258: INFO: stderr: ""
    Feb 27 15:26:38.258: INFO: stdout: "update-demo-nautilus-gtqcq "
    Feb 27 15:26:38.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:38.307: INFO: stderr: ""
    Feb 27 15:26:38.307: INFO: stdout: "true"
    Feb 27 15:26:38.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 15:26:38.356: INFO: stderr: ""
    Feb 27 15:26:38.356: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 15:26:38.356: INFO: validating pod update-demo-nautilus-gtqcq
    Feb 27 15:26:38.361: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 15:26:38.361: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 15:26:38.361: INFO: update-demo-nautilus-gtqcq is verified up and running
    STEP: scaling up the replication controller 02/27/23 15:26:38.361
    Feb 27 15:26:38.362: INFO: scanned /root for discovery docs: <nil>
    Feb 27 15:26:38.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb 27 15:26:39.432: INFO: stderr: ""
    Feb 27 15:26:39.432: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 15:26:39.432
    Feb 27 15:26:39.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 15:26:39.481: INFO: stderr: ""
    Feb 27 15:26:39.481: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-lmk79 "
    Feb 27 15:26:39.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:39.529: INFO: stderr: ""
    Feb 27 15:26:39.529: INFO: stdout: "true"
    Feb 27 15:26:39.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 15:26:39.574: INFO: stderr: ""
    Feb 27 15:26:39.574: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 15:26:39.574: INFO: validating pod update-demo-nautilus-gtqcq
    Feb 27 15:26:39.579: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 15:26:39.579: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 15:26:39.579: INFO: update-demo-nautilus-gtqcq is verified up and running
    Feb 27 15:26:39.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-lmk79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:39.624: INFO: stderr: ""
    Feb 27 15:26:39.624: INFO: stdout: ""
    Feb 27 15:26:39.624: INFO: update-demo-nautilus-lmk79 is created but not running
    Feb 27 15:26:44.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 15:26:44.680: INFO: stderr: ""
    Feb 27 15:26:44.680: INFO: stdout: "update-demo-nautilus-gtqcq update-demo-nautilus-lmk79 "
    Feb 27 15:26:44.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:44.727: INFO: stderr: ""
    Feb 27 15:26:44.727: INFO: stdout: "true"
    Feb 27 15:26:44.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-gtqcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 15:26:44.777: INFO: stderr: ""
    Feb 27 15:26:44.777: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 15:26:44.777: INFO: validating pod update-demo-nautilus-gtqcq
    Feb 27 15:26:44.782: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 15:26:44.782: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 15:26:44.782: INFO: update-demo-nautilus-gtqcq is verified up and running
    Feb 27 15:26:44.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-lmk79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 15:26:44.829: INFO: stderr: ""
    Feb 27 15:26:44.829: INFO: stdout: "true"
    Feb 27 15:26:44.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods update-demo-nautilus-lmk79 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 15:26:44.877: INFO: stderr: ""
    Feb 27 15:26:44.877: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 15:26:44.877: INFO: validating pod update-demo-nautilus-lmk79
    Feb 27 15:26:44.881: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 15:26:44.881: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 15:26:44.881: INFO: update-demo-nautilus-lmk79 is verified up and running
    STEP: using delete to clean up resources 02/27/23 15:26:44.881
    Feb 27 15:26:44.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 delete --grace-period=0 --force -f -'
    Feb 27 15:26:44.933: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:26:44.933: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 27 15:26:44.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get rc,svc -l name=update-demo --no-headers'
    Feb 27 15:26:45.003: INFO: stderr: "No resources found in kubectl-4114 namespace.\n"
    Feb 27 15:26:45.003: INFO: stdout: ""
    Feb 27 15:26:45.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4114 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 15:26:45.052: INFO: stderr: ""
    Feb 27 15:26:45.052: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:26:45.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4114" for this suite. 02/27/23 15:26:45.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:26:45.066
Feb 27 15:26:45.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 15:26:45.067
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:45.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:45.089
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb 27 15:26:45.091: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 27 15:26:45.102: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 15:26:50.106: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 15:26:50.106
Feb 27 15:26:50.107: INFO: Creating deployment "test-rolling-update-deployment"
Feb 27 15:26:50.113: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 27 15:26:50.119: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 27 15:26:52.126: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 27 15:26:52.130: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 15:26:52.141: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8495  a0115b87-0e35-4e24-b1cd-9a428525e572 8823 1 2023-02-27 15:26:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00483eeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 15:26:50 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-02-27 15:26:51 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 15:26:52.144: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-8495  c4abb6e4-1697-4aec-ac94-1c02b3520217 8812 1 2023-02-27 15:26:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a0115b87-0e35-4e24-b1cd-9a428525e572 0xc002e41b97 0xc002e41b98}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0115b87-0e35-4e24-b1cd-9a428525e572\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e41c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 15:26:52.144: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 27 15:26:52.144: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8495  1766ff1d-6150-4513-88c3-fd119ffe6399 8822 2 2023-02-27 15:26:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a0115b87-0e35-4e24-b1cd-9a428525e572 0xc002e41a6f 0xc002e41a80}] [] [{e2e.test Update apps/v1 2023-02-27 15:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0115b87-0e35-4e24-b1cd-9a428525e572\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002e41b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 15:26:52.147: INFO: Pod "test-rolling-update-deployment-7549d9f46d-tlmkh" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-tlmkh test-rolling-update-deployment-7549d9f46d- deployment-8495  58d21104-ae80-430c-9b97-be3b20cd656c 8811 0 2023-02-27 15:26:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d c4abb6e4-1697-4aec-ac94-1c02b3520217 0xc00483f2b7 0xc00483f2b8}] [] [{kube-controller-manager Update v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4abb6e4-1697-4aec-ac94-1c02b3520217\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcth6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcth6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.147,StartTime:2023-02-27 15:26:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:26:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://95097aaa07ce8aaef8c436372d46c40ebb8498d01152cf93c0ba9ae72f2e17f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 15:26:52.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8495" for this suite. 02/27/23 15:26:52.151
------------------------------
• [SLOW TEST] [7.093 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:26:45.066
    Feb 27 15:26:45.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 15:26:45.067
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:45.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:45.089
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb 27 15:26:45.091: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb 27 15:26:45.102: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 15:26:50.106: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 15:26:50.106
    Feb 27 15:26:50.107: INFO: Creating deployment "test-rolling-update-deployment"
    Feb 27 15:26:50.113: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb 27 15:26:50.119: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Feb 27 15:26:52.126: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb 27 15:26:52.130: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 15:26:52.141: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8495  a0115b87-0e35-4e24-b1cd-9a428525e572 8823 1 2023-02-27 15:26:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00483eeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 15:26:50 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-02-27 15:26:51 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 15:26:52.144: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-8495  c4abb6e4-1697-4aec-ac94-1c02b3520217 8812 1 2023-02-27 15:26:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a0115b87-0e35-4e24-b1cd-9a428525e572 0xc002e41b97 0xc002e41b98}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0115b87-0e35-4e24-b1cd-9a428525e572\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e41c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 15:26:52.144: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb 27 15:26:52.144: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8495  1766ff1d-6150-4513-88c3-fd119ffe6399 8822 2 2023-02-27 15:26:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a0115b87-0e35-4e24-b1cd-9a428525e572 0xc002e41a6f 0xc002e41a80}] [] [{e2e.test Update apps/v1 2023-02-27 15:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0115b87-0e35-4e24-b1cd-9a428525e572\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:26:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002e41b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 15:26:52.147: INFO: Pod "test-rolling-update-deployment-7549d9f46d-tlmkh" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-tlmkh test-rolling-update-deployment-7549d9f46d- deployment-8495  58d21104-ae80-430c-9b97-be3b20cd656c 8811 0 2023-02-27 15:26:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d c4abb6e4-1697-4aec-ac94-1c02b3520217 0xc00483f2b7 0xc00483f2b8}] [] [{kube-controller-manager Update v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4abb6e4-1697-4aec-ac94-1c02b3520217\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:26:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcth6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcth6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:26:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.147,StartTime:2023-02-27 15:26:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:26:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://95097aaa07ce8aaef8c436372d46c40ebb8498d01152cf93c0ba9ae72f2e17f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:26:52.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8495" for this suite. 02/27/23 15:26:52.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:26:52.16
Feb 27 15:26:52.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svcaccounts 02/27/23 15:26:52.161
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:52.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:52.179
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Feb 27 15:26:52.204: INFO: created pod pod-service-account-defaultsa
Feb 27 15:26:52.204: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 27 15:26:52.210: INFO: created pod pod-service-account-mountsa
Feb 27 15:26:52.210: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 27 15:26:52.214: INFO: created pod pod-service-account-nomountsa
Feb 27 15:26:52.214: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 27 15:26:52.224: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 27 15:26:52.224: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 27 15:26:52.230: INFO: created pod pod-service-account-mountsa-mountspec
Feb 27 15:26:52.230: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 27 15:26:52.235: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 27 15:26:52.235: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 27 15:26:52.241: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 27 15:26:52.241: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 27 15:26:52.249: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 27 15:26:52.249: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 27 15:26:52.256: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 27 15:26:52.256: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 15:26:52.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2305" for this suite. 02/27/23 15:26:52.261
------------------------------
• [0.113 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:26:52.16
    Feb 27 15:26:52.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 15:26:52.161
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:52.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:52.179
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Feb 27 15:26:52.204: INFO: created pod pod-service-account-defaultsa
    Feb 27 15:26:52.204: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb 27 15:26:52.210: INFO: created pod pod-service-account-mountsa
    Feb 27 15:26:52.210: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb 27 15:26:52.214: INFO: created pod pod-service-account-nomountsa
    Feb 27 15:26:52.214: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb 27 15:26:52.224: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb 27 15:26:52.224: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb 27 15:26:52.230: INFO: created pod pod-service-account-mountsa-mountspec
    Feb 27 15:26:52.230: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb 27 15:26:52.235: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb 27 15:26:52.235: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb 27 15:26:52.241: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb 27 15:26:52.241: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb 27 15:26:52.249: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb 27 15:26:52.249: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb 27 15:26:52.256: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb 27 15:26:52.256: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:26:52.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2305" for this suite. 02/27/23 15:26:52.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:26:52.274
Feb 27 15:26:52.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename taint-single-pod 02/27/23 15:26:52.275
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:52.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:52.299
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Feb 27 15:26:52.303: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 15:27:52.317: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Feb 27 15:27:52.320: INFO: Starting informer...
STEP: Starting pod... 02/27/23 15:27:52.32
Feb 27 15:27:52.535: INFO: Pod is running on ip-172-31-42-40. Tainting Node
STEP: Trying to apply a taint on the Node 02/27/23 15:27:52.535
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:27:52.544
STEP: Waiting short time to make sure Pod is queued for deletion 02/27/23 15:27:52.549
Feb 27 15:27:52.549: INFO: Pod wasn't evicted. Proceeding
Feb 27 15:27:52.549: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:27:52.56
STEP: Waiting some time to make sure that toleration time passed. 02/27/23 15:27:52.566
Feb 27 15:29:07.566: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:07.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-1636" for this suite. 02/27/23 15:29:07.573
------------------------------
• [SLOW TEST] [135.308 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:26:52.274
    Feb 27 15:26:52.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename taint-single-pod 02/27/23 15:26:52.275
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:26:52.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:26:52.299
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Feb 27 15:26:52.303: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 15:27:52.317: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Feb 27 15:27:52.320: INFO: Starting informer...
    STEP: Starting pod... 02/27/23 15:27:52.32
    Feb 27 15:27:52.535: INFO: Pod is running on ip-172-31-42-40. Tainting Node
    STEP: Trying to apply a taint on the Node 02/27/23 15:27:52.535
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:27:52.544
    STEP: Waiting short time to make sure Pod is queued for deletion 02/27/23 15:27:52.549
    Feb 27 15:27:52.549: INFO: Pod wasn't evicted. Proceeding
    Feb 27 15:27:52.549: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:27:52.56
    STEP: Waiting some time to make sure that toleration time passed. 02/27/23 15:27:52.566
    Feb 27 15:29:07.566: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:07.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-1636" for this suite. 02/27/23 15:29:07.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:07.584
Feb 27 15:29:07.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 15:29:07.585
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:07.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:07.608
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 02/27/23 15:29:07.611
STEP: submitting the pod to kubernetes 02/27/23 15:29:07.611
Feb 27 15:29:07.621: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" in namespace "pods-8469" to be "running and ready"
Feb 27 15:29:07.627: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171612ms
Feb 27 15:29:07.627: INFO: The phase of Pod pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:29:09.633: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=true. Elapsed: 2.011396701s
Feb 27 15:29:09.633: INFO: The phase of Pod pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56 is Running (Ready = true)
Feb 27 15:29:09.633: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/27/23 15:29:09.637
STEP: updating the pod 02/27/23 15:29:09.64
Feb 27 15:29:10.152: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56"
Feb 27 15:29:10.152: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" in namespace "pods-8469" to be "terminated with reason DeadlineExceeded"
Feb 27 15:29:10.155: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=true. Elapsed: 3.230769ms
Feb 27 15:29:12.159: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=true. Elapsed: 2.007524637s
Feb 27 15:29:14.159: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=false. Elapsed: 4.006979213s
Feb 27 15:29:16.161: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.00894471s
Feb 27 15:29:16.161: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:16.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8469" for this suite. 02/27/23 15:29:16.165
------------------------------
• [SLOW TEST] [8.587 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:07.584
    Feb 27 15:29:07.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 15:29:07.585
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:07.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:07.608
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 02/27/23 15:29:07.611
    STEP: submitting the pod to kubernetes 02/27/23 15:29:07.611
    Feb 27 15:29:07.621: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" in namespace "pods-8469" to be "running and ready"
    Feb 27 15:29:07.627: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171612ms
    Feb 27 15:29:07.627: INFO: The phase of Pod pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:29:09.633: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=true. Elapsed: 2.011396701s
    Feb 27 15:29:09.633: INFO: The phase of Pod pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56 is Running (Ready = true)
    Feb 27 15:29:09.633: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/27/23 15:29:09.637
    STEP: updating the pod 02/27/23 15:29:09.64
    Feb 27 15:29:10.152: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56"
    Feb 27 15:29:10.152: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" in namespace "pods-8469" to be "terminated with reason DeadlineExceeded"
    Feb 27 15:29:10.155: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=true. Elapsed: 3.230769ms
    Feb 27 15:29:12.159: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=true. Elapsed: 2.007524637s
    Feb 27 15:29:14.159: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Running", Reason="", readiness=false. Elapsed: 4.006979213s
    Feb 27 15:29:16.161: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.00894471s
    Feb 27 15:29:16.161: INFO: Pod "pod-update-activedeadlineseconds-c8d68976-441c-41c4-8a3c-d384b0694f56" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:16.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8469" for this suite. 02/27/23 15:29:16.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:16.173
Feb 27 15:29:16.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-runtime 02/27/23 15:29:16.173
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:16.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:16.195
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 02/27/23 15:29:16.197
STEP: wait for the container to reach Succeeded 02/27/23 15:29:16.207
STEP: get the container status 02/27/23 15:29:19.223
STEP: the container should be terminated 02/27/23 15:29:19.226
STEP: the termination message should be set 02/27/23 15:29:19.226
Feb 27 15:29:19.226: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/27/23 15:29:19.226
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:19.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8869" for this suite. 02/27/23 15:29:19.244
------------------------------
• [3.077 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:16.173
    Feb 27 15:29:16.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-runtime 02/27/23 15:29:16.173
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:16.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:16.195
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 02/27/23 15:29:16.197
    STEP: wait for the container to reach Succeeded 02/27/23 15:29:16.207
    STEP: get the container status 02/27/23 15:29:19.223
    STEP: the container should be terminated 02/27/23 15:29:19.226
    STEP: the termination message should be set 02/27/23 15:29:19.226
    Feb 27 15:29:19.226: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/27/23 15:29:19.226
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:19.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8869" for this suite. 02/27/23 15:29:19.244
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:19.25
Feb 27 15:29:19.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:29:19.25
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:19.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:19.275
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-6647 02/27/23 15:29:19.278
STEP: creating service affinity-clusterip in namespace services-6647 02/27/23 15:29:19.278
STEP: creating replication controller affinity-clusterip in namespace services-6647 02/27/23 15:29:19.289
I0227 15:29:19.297868      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6647, replica count: 3
I0227 15:29:22.348820      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 15:29:22.355: INFO: Creating new exec pod
Feb 27 15:29:22.370: INFO: Waiting up to 5m0s for pod "execpod-affinityrmcmv" in namespace "services-6647" to be "running"
Feb 27 15:29:22.374: INFO: Pod "execpod-affinityrmcmv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478716ms
Feb 27 15:29:24.381: INFO: Pod "execpod-affinityrmcmv": Phase="Running", Reason="", readiness=true. Elapsed: 2.011309287s
Feb 27 15:29:24.381: INFO: Pod "execpod-affinityrmcmv" satisfied condition "running"
Feb 27 15:29:25.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-6647 exec execpod-affinityrmcmv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Feb 27 15:29:25.509: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 27 15:29:25.509: INFO: stdout: ""
Feb 27 15:29:25.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-6647 exec execpod-affinityrmcmv -- /bin/sh -x -c nc -v -z -w 2 10.152.183.108 80'
Feb 27 15:29:25.609: INFO: stderr: "+ nc -v -z -w 2 10.152.183.108 80\nConnection to 10.152.183.108 80 port [tcp/http] succeeded!\n"
Feb 27 15:29:25.609: INFO: stdout: ""
Feb 27 15:29:25.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-6647 exec execpod-affinityrmcmv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.108:80/ ; done'
Feb 27 15:29:25.772: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n"
Feb 27 15:29:25.772: INFO: stdout: "\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm"
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
Feb 27 15:29:25.772: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6647, will wait for the garbage collector to delete the pods 02/27/23 15:29:25.786
Feb 27 15:29:25.846: INFO: Deleting ReplicationController affinity-clusterip took: 6.086803ms
Feb 27 15:29:25.947: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.202745ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:27.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6647" for this suite. 02/27/23 15:29:27.869
------------------------------
• [SLOW TEST] [8.626 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:19.25
    Feb 27 15:29:19.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:29:19.25
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:19.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:19.275
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-6647 02/27/23 15:29:19.278
    STEP: creating service affinity-clusterip in namespace services-6647 02/27/23 15:29:19.278
    STEP: creating replication controller affinity-clusterip in namespace services-6647 02/27/23 15:29:19.289
    I0227 15:29:19.297868      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6647, replica count: 3
    I0227 15:29:22.348820      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 15:29:22.355: INFO: Creating new exec pod
    Feb 27 15:29:22.370: INFO: Waiting up to 5m0s for pod "execpod-affinityrmcmv" in namespace "services-6647" to be "running"
    Feb 27 15:29:22.374: INFO: Pod "execpod-affinityrmcmv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478716ms
    Feb 27 15:29:24.381: INFO: Pod "execpod-affinityrmcmv": Phase="Running", Reason="", readiness=true. Elapsed: 2.011309287s
    Feb 27 15:29:24.381: INFO: Pod "execpod-affinityrmcmv" satisfied condition "running"
    Feb 27 15:29:25.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-6647 exec execpod-affinityrmcmv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Feb 27 15:29:25.509: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb 27 15:29:25.509: INFO: stdout: ""
    Feb 27 15:29:25.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-6647 exec execpod-affinityrmcmv -- /bin/sh -x -c nc -v -z -w 2 10.152.183.108 80'
    Feb 27 15:29:25.609: INFO: stderr: "+ nc -v -z -w 2 10.152.183.108 80\nConnection to 10.152.183.108 80 port [tcp/http] succeeded!\n"
    Feb 27 15:29:25.609: INFO: stdout: ""
    Feb 27 15:29:25.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-6647 exec execpod-affinityrmcmv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.108:80/ ; done'
    Feb 27 15:29:25.772: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.108:80/\n"
    Feb 27 15:29:25.772: INFO: stdout: "\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm\naffinity-clusterip-hgkdm"
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Received response from host: affinity-clusterip-hgkdm
    Feb 27 15:29:25.772: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6647, will wait for the garbage collector to delete the pods 02/27/23 15:29:25.786
    Feb 27 15:29:25.846: INFO: Deleting ReplicationController affinity-clusterip took: 6.086803ms
    Feb 27 15:29:25.947: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.202745ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:27.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6647" for this suite. 02/27/23 15:29:27.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:27.878
Feb 27 15:29:27.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename tables 02/27/23 15:29:27.878
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:27.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:27.895
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:27.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3540" for this suite. 02/27/23 15:29:27.904
------------------------------
• [0.032 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:27.878
    Feb 27 15:29:27.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename tables 02/27/23 15:29:27.878
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:27.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:27.895
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:27.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3540" for this suite. 02/27/23 15:29:27.904
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:27.91
Feb 27 15:29:27.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename limitrange 02/27/23 15:29:27.911
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:27.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:27.933
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-4b6f6" in namespace "limitrange-4504" 02/27/23 15:29:27.936
STEP: Creating another limitRange in another namespace 02/27/23 15:29:27.942
Feb 27 15:29:27.969: INFO: Namespace "e2e-limitrange-4b6f6-234" created
Feb 27 15:29:27.969: INFO: Creating LimitRange "e2e-limitrange-4b6f6" in namespace "e2e-limitrange-4b6f6-234"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-4b6f6" 02/27/23 15:29:27.975
Feb 27 15:29:27.978: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-4b6f6" in "limitrange-4504" namespace 02/27/23 15:29:27.978
Feb 27 15:29:27.984: INFO: LimitRange "e2e-limitrange-4b6f6" has been patched
STEP: Delete LimitRange "e2e-limitrange-4b6f6" by Collection with labelSelector: "e2e-limitrange-4b6f6=patched" 02/27/23 15:29:27.984
STEP: Confirm that the limitRange "e2e-limitrange-4b6f6" has been deleted 02/27/23 15:29:27.992
Feb 27 15:29:27.992: INFO: Requesting list of LimitRange to confirm quantity
Feb 27 15:29:27.996: INFO: Found 0 LimitRange with label "e2e-limitrange-4b6f6=patched"
Feb 27 15:29:27.996: INFO: LimitRange "e2e-limitrange-4b6f6" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-4b6f6" 02/27/23 15:29:27.996
Feb 27 15:29:27.999: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-4504" for this suite. 02/27/23 15:29:28.002
STEP: Destroying namespace "e2e-limitrange-4b6f6-234" for this suite. 02/27/23 15:29:28.009
------------------------------
• [0.105 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:27.91
    Feb 27 15:29:27.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename limitrange 02/27/23 15:29:27.911
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:27.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:27.933
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-4b6f6" in namespace "limitrange-4504" 02/27/23 15:29:27.936
    STEP: Creating another limitRange in another namespace 02/27/23 15:29:27.942
    Feb 27 15:29:27.969: INFO: Namespace "e2e-limitrange-4b6f6-234" created
    Feb 27 15:29:27.969: INFO: Creating LimitRange "e2e-limitrange-4b6f6" in namespace "e2e-limitrange-4b6f6-234"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-4b6f6" 02/27/23 15:29:27.975
    Feb 27 15:29:27.978: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-4b6f6" in "limitrange-4504" namespace 02/27/23 15:29:27.978
    Feb 27 15:29:27.984: INFO: LimitRange "e2e-limitrange-4b6f6" has been patched
    STEP: Delete LimitRange "e2e-limitrange-4b6f6" by Collection with labelSelector: "e2e-limitrange-4b6f6=patched" 02/27/23 15:29:27.984
    STEP: Confirm that the limitRange "e2e-limitrange-4b6f6" has been deleted 02/27/23 15:29:27.992
    Feb 27 15:29:27.992: INFO: Requesting list of LimitRange to confirm quantity
    Feb 27 15:29:27.996: INFO: Found 0 LimitRange with label "e2e-limitrange-4b6f6=patched"
    Feb 27 15:29:27.996: INFO: LimitRange "e2e-limitrange-4b6f6" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-4b6f6" 02/27/23 15:29:27.996
    Feb 27 15:29:27.999: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:27.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-4504" for this suite. 02/27/23 15:29:28.002
    STEP: Destroying namespace "e2e-limitrange-4b6f6-234" for this suite. 02/27/23 15:29:28.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:28.015
Feb 27 15:29:28.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 15:29:28.016
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:28.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:28.036
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1495 02/27/23 15:29:28.039
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-1495 02/27/23 15:29:28.051
Feb 27 15:29:28.061: INFO: Found 0 stateful pods, waiting for 1
Feb 27 15:29:38.066: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/27/23 15:29:38.073
STEP: Getting /status 02/27/23 15:29:38.083
Feb 27 15:29:38.087: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/27/23 15:29:38.087
Feb 27 15:29:38.096: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/27/23 15:29:38.096
Feb 27 15:29:38.097: INFO: Observed &StatefulSet event: ADDED
Feb 27 15:29:38.097: INFO: Found Statefulset ss in namespace statefulset-1495 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 15:29:38.098: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/27/23 15:29:38.098
Feb 27 15:29:38.098: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 15:29:38.105: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/27/23 15:29:38.105
Feb 27 15:29:38.107: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 15:29:38.107: INFO: Deleting all statefulset in ns statefulset-1495
Feb 27 15:29:38.110: INFO: Scaling statefulset ss to 0
Feb 27 15:29:48.128: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 15:29:48.132: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:48.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1495" for this suite. 02/27/23 15:29:48.15
------------------------------
• [SLOW TEST] [20.142 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:28.015
    Feb 27 15:29:28.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 15:29:28.016
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:28.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:28.036
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1495 02/27/23 15:29:28.039
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-1495 02/27/23 15:29:28.051
    Feb 27 15:29:28.061: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 15:29:38.066: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/27/23 15:29:38.073
    STEP: Getting /status 02/27/23 15:29:38.083
    Feb 27 15:29:38.087: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/27/23 15:29:38.087
    Feb 27 15:29:38.096: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/27/23 15:29:38.096
    Feb 27 15:29:38.097: INFO: Observed &StatefulSet event: ADDED
    Feb 27 15:29:38.097: INFO: Found Statefulset ss in namespace statefulset-1495 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 15:29:38.098: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/27/23 15:29:38.098
    Feb 27 15:29:38.098: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 15:29:38.105: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/27/23 15:29:38.105
    Feb 27 15:29:38.107: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 15:29:38.107: INFO: Deleting all statefulset in ns statefulset-1495
    Feb 27 15:29:38.110: INFO: Scaling statefulset ss to 0
    Feb 27 15:29:48.128: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 15:29:48.132: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:48.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1495" for this suite. 02/27/23 15:29:48.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:48.158
Feb 27 15:29:48.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:29:48.159
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.185
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 02/27/23 15:29:48.189
Feb 27 15:29:48.189: INFO: Creating e2e-svc-a-d6q9g
Feb 27 15:29:48.199: INFO: Creating e2e-svc-b-r8hpd
Feb 27 15:29:48.210: INFO: Creating e2e-svc-c-r5bv2
STEP: deleting service collection 02/27/23 15:29:48.226
Feb 27 15:29:48.256: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:48.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9417" for this suite. 02/27/23 15:29:48.26
------------------------------
• [0.109 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:48.158
    Feb 27 15:29:48.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:29:48.159
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.185
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 02/27/23 15:29:48.189
    Feb 27 15:29:48.189: INFO: Creating e2e-svc-a-d6q9g
    Feb 27 15:29:48.199: INFO: Creating e2e-svc-b-r8hpd
    Feb 27 15:29:48.210: INFO: Creating e2e-svc-c-r5bv2
    STEP: deleting service collection 02/27/23 15:29:48.226
    Feb 27 15:29:48.256: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:48.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9417" for this suite. 02/27/23 15:29:48.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:48.268
Feb 27 15:29:48.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:29:48.269
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.291
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:48.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-650" for this suite. 02/27/23 15:29:48.316
------------------------------
• [0.055 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:48.268
    Feb 27 15:29:48.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:29:48.269
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.291
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:48.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-650" for this suite. 02/27/23 15:29:48.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:48.324
Feb 27 15:29:48.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename endpointslice 02/27/23 15:29:48.325
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.348
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Feb 27 15:29:48.360: INFO: Endpoints addresses: [172.31.34.33 172.31.87.225] , ports: [6443]
Feb 27 15:29:48.360: INFO: EndpointSlices addresses: [172.31.34.33 172.31.87.225] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:48.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6121" for this suite. 02/27/23 15:29:48.364
------------------------------
• [0.050 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:48.324
    Feb 27 15:29:48.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename endpointslice 02/27/23 15:29:48.325
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.348
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Feb 27 15:29:48.360: INFO: Endpoints addresses: [172.31.34.33 172.31.87.225] , ports: [6443]
    Feb 27 15:29:48.360: INFO: EndpointSlices addresses: [172.31.34.33 172.31.87.225] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:48.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6121" for this suite. 02/27/23 15:29:48.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:48.375
Feb 27 15:29:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:29:48.376
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.393
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:29:48.409
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:29:48.589
STEP: Deploying the webhook pod 02/27/23 15:29:48.596
STEP: Wait for the deployment to be ready 02/27/23 15:29:48.608
Feb 27 15:29:48.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:29:50.624
STEP: Verifying the service has paired with the endpoint 02/27/23 15:29:50.635
Feb 27 15:29:51.636: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 02/27/23 15:29:51.708
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:29:51.74
STEP: Deleting the collection of validation webhooks 02/27/23 15:29:51.768
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:29:51.823
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:51.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1675" for this suite. 02/27/23 15:29:51.872
STEP: Destroying namespace "webhook-1675-markers" for this suite. 02/27/23 15:29:51.881
------------------------------
• [3.512 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:48.375
    Feb 27 15:29:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:29:48.376
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:48.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:48.393
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:29:48.409
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:29:48.589
    STEP: Deploying the webhook pod 02/27/23 15:29:48.596
    STEP: Wait for the deployment to be ready 02/27/23 15:29:48.608
    Feb 27 15:29:48.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:29:50.624
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:29:50.635
    Feb 27 15:29:51.636: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 02/27/23 15:29:51.708
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:29:51.74
    STEP: Deleting the collection of validation webhooks 02/27/23 15:29:51.768
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:29:51.823
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:51.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1675" for this suite. 02/27/23 15:29:51.872
    STEP: Destroying namespace "webhook-1675-markers" for this suite. 02/27/23 15:29:51.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:51.887
Feb 27 15:29:51.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename disruption 02/27/23 15:29:51.888
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:51.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:51.908
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 02/27/23 15:29:51.916
STEP: Updating PodDisruptionBudget status 02/27/23 15:29:53.925
STEP: Waiting for all pods to be running 02/27/23 15:29:53.931
Feb 27 15:29:53.935: INFO: running pods: 0 < 1
STEP: locating a running pod 02/27/23 15:29:55.94
STEP: Waiting for the pdb to be processed 02/27/23 15:29:55.951
STEP: Patching PodDisruptionBudget status 02/27/23 15:29:55.96
STEP: Waiting for the pdb to be processed 02/27/23 15:29:55.97
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 15:29:55.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3395" for this suite. 02/27/23 15:29:55.979
------------------------------
• [4.098 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:51.887
    Feb 27 15:29:51.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename disruption 02/27/23 15:29:51.888
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:51.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:51.908
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 02/27/23 15:29:51.916
    STEP: Updating PodDisruptionBudget status 02/27/23 15:29:53.925
    STEP: Waiting for all pods to be running 02/27/23 15:29:53.931
    Feb 27 15:29:53.935: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/27/23 15:29:55.94
    STEP: Waiting for the pdb to be processed 02/27/23 15:29:55.951
    STEP: Patching PodDisruptionBudget status 02/27/23 15:29:55.96
    STEP: Waiting for the pdb to be processed 02/27/23 15:29:55.97
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:29:55.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3395" for this suite. 02/27/23 15:29:55.979
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:29:55.986
Feb 27 15:29:55.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:29:55.986
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:56.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:56.008
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 15:29:56.011
Feb 27 15:29:56.021: INFO: Waiting up to 5m0s for pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62" in namespace "emptydir-8656" to be "Succeeded or Failed"
Feb 27 15:29:56.025: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018269ms
Feb 27 15:29:58.029: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008137871s
Feb 27 15:30:00.030: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009320937s
STEP: Saw pod success 02/27/23 15:30:00.03
Feb 27 15:30:00.030: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62" satisfied condition "Succeeded or Failed"
Feb 27 15:30:00.034: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62 container test-container: <nil>
STEP: delete the pod 02/27/23 15:30:00.048
Feb 27 15:30:00.060: INFO: Waiting for pod pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62 to disappear
Feb 27 15:30:00.064: INFO: Pod pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:30:00.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8656" for this suite. 02/27/23 15:30:00.067
------------------------------
• [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:29:55.986
    Feb 27 15:29:55.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:29:55.986
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:29:56.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:29:56.008
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 15:29:56.011
    Feb 27 15:29:56.021: INFO: Waiting up to 5m0s for pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62" in namespace "emptydir-8656" to be "Succeeded or Failed"
    Feb 27 15:29:56.025: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018269ms
    Feb 27 15:29:58.029: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008137871s
    Feb 27 15:30:00.030: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009320937s
    STEP: Saw pod success 02/27/23 15:30:00.03
    Feb 27 15:30:00.030: INFO: Pod "pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62" satisfied condition "Succeeded or Failed"
    Feb 27 15:30:00.034: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:30:00.048
    Feb 27 15:30:00.060: INFO: Waiting for pod pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62 to disappear
    Feb 27 15:30:00.064: INFO: Pod pod-df6735ee-9780-4b91-a1a7-3f0db86e9d62 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:30:00.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8656" for this suite. 02/27/23 15:30:00.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:30:00.075
Feb 27 15:30:00.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename containers 02/27/23 15:30:00.076
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:30:00.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:30:00.093
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 02/27/23 15:30:00.096
Feb 27 15:30:00.105: INFO: Waiting up to 5m0s for pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a" in namespace "containers-9010" to be "Succeeded or Failed"
Feb 27 15:30:00.107: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629958ms
Feb 27 15:30:02.112: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006856876s
Feb 27 15:30:04.112: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007283007s
STEP: Saw pod success 02/27/23 15:30:04.112
Feb 27 15:30:04.112: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a" satisfied condition "Succeeded or Failed"
Feb 27 15:30:04.115: INFO: Trying to get logs from node ip-172-31-42-40 pod client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:30:04.124
Feb 27 15:30:04.136: INFO: Waiting for pod client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a to disappear
Feb 27 15:30:04.139: INFO: Pod client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 15:30:04.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9010" for this suite. 02/27/23 15:30:04.143
------------------------------
• [4.074 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:30:00.075
    Feb 27 15:30:00.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename containers 02/27/23 15:30:00.076
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:30:00.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:30:00.093
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 02/27/23 15:30:00.096
    Feb 27 15:30:00.105: INFO: Waiting up to 5m0s for pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a" in namespace "containers-9010" to be "Succeeded or Failed"
    Feb 27 15:30:00.107: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629958ms
    Feb 27 15:30:02.112: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006856876s
    Feb 27 15:30:04.112: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007283007s
    STEP: Saw pod success 02/27/23 15:30:04.112
    Feb 27 15:30:04.112: INFO: Pod "client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a" satisfied condition "Succeeded or Failed"
    Feb 27 15:30:04.115: INFO: Trying to get logs from node ip-172-31-42-40 pod client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:30:04.124
    Feb 27 15:30:04.136: INFO: Waiting for pod client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a to disappear
    Feb 27 15:30:04.139: INFO: Pod client-containers-6cb99ea8-aaa7-40bc-a917-8ee34c03248a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:30:04.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9010" for this suite. 02/27/23 15:30:04.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:30:04.15
Feb 27 15:30:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename security-context-test 02/27/23 15:30:04.151
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:30:04.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:30:04.172
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Feb 27 15:30:04.183: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd" in namespace "security-context-test-6716" to be "Succeeded or Failed"
Feb 27 15:30:04.188: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.250981ms
Feb 27 15:30:06.193: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009946846s
Feb 27 15:30:08.192: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009039487s
Feb 27 15:30:08.192: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 15:30:08.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6716" for this suite. 02/27/23 15:30:08.196
------------------------------
• [4.052 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:30:04.15
    Feb 27 15:30:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename security-context-test 02/27/23 15:30:04.151
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:30:04.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:30:04.172
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Feb 27 15:30:04.183: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd" in namespace "security-context-test-6716" to be "Succeeded or Failed"
    Feb 27 15:30:04.188: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.250981ms
    Feb 27 15:30:06.193: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009946846s
    Feb 27 15:30:08.192: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009039487s
    Feb 27 15:30:08.192: INFO: Pod "busybox-readonly-false-a0c0fc28-0e16-40ad-ab23-e27e43fc86bd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:30:08.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6716" for this suite. 02/27/23 15:30:08.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:30:08.203
Feb 27 15:30:08.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 15:30:08.204
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:30:08.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:30:08.273
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e in namespace container-probe-2753 02/27/23 15:30:08.277
Feb 27 15:30:08.286: INFO: Waiting up to 5m0s for pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e" in namespace "container-probe-2753" to be "not pending"
Feb 27 15:30:08.291: INFO: Pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.590047ms
Feb 27 15:30:10.297: INFO: Pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011228193s
Feb 27 15:30:10.297: INFO: Pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e" satisfied condition "not pending"
Feb 27 15:30:10.297: INFO: Started pod busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e in namespace container-probe-2753
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 15:30:10.297
Feb 27 15:30:10.301: INFO: Initial restart count of pod busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e is 0
Feb 27 15:31:00.413: INFO: Restart count of pod container-probe-2753/busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e is now 1 (50.112588188s elapsed)
STEP: deleting the pod 02/27/23 15:31:00.413
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 15:31:00.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2753" for this suite. 02/27/23 15:31:00.433
------------------------------
• [SLOW TEST] [52.236 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:30:08.203
    Feb 27 15:30:08.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 15:30:08.204
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:30:08.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:30:08.273
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e in namespace container-probe-2753 02/27/23 15:30:08.277
    Feb 27 15:30:08.286: INFO: Waiting up to 5m0s for pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e" in namespace "container-probe-2753" to be "not pending"
    Feb 27 15:30:08.291: INFO: Pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.590047ms
    Feb 27 15:30:10.297: INFO: Pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011228193s
    Feb 27 15:30:10.297: INFO: Pod "busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e" satisfied condition "not pending"
    Feb 27 15:30:10.297: INFO: Started pod busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e in namespace container-probe-2753
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 15:30:10.297
    Feb 27 15:30:10.301: INFO: Initial restart count of pod busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e is 0
    Feb 27 15:31:00.413: INFO: Restart count of pod container-probe-2753/busybox-6ebe1e0d-9dbc-4fde-aced-255af19dd19e is now 1 (50.112588188s elapsed)
    STEP: deleting the pod 02/27/23 15:31:00.413
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:31:00.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2753" for this suite. 02/27/23 15:31:00.433
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:31:00.439
Feb 27 15:31:00.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:31:00.44
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:00.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:00.46
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-c0c301a9-42c8-4323-84bb-14b46ba232f2 02/27/23 15:31:00.462
STEP: Creating secret with name secret-projected-all-test-volume-77b46d31-f1fe-40ce-afa2-e2a5b79ac95c 02/27/23 15:31:00.466
STEP: Creating a pod to test Check all projections for projected volume plugin 02/27/23 15:31:00.473
Feb 27 15:31:00.480: INFO: Waiting up to 5m0s for pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4" in namespace "projected-1005" to be "Succeeded or Failed"
Feb 27 15:31:00.486: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.236108ms
Feb 27 15:31:02.489: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00891073s
Feb 27 15:31:04.489: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008920406s
STEP: Saw pod success 02/27/23 15:31:04.489
Feb 27 15:31:04.489: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4" satisfied condition "Succeeded or Failed"
Feb 27 15:31:04.493: INFO: Trying to get logs from node ip-172-31-42-40 pod projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4 container projected-all-volume-test: <nil>
STEP: delete the pod 02/27/23 15:31:04.499
Feb 27 15:31:04.509: INFO: Waiting for pod projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4 to disappear
Feb 27 15:31:04.512: INFO: Pod projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Feb 27 15:31:04.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1005" for this suite. 02/27/23 15:31:04.517
------------------------------
• [4.086 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:31:00.439
    Feb 27 15:31:00.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:31:00.44
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:00.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:00.46
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-c0c301a9-42c8-4323-84bb-14b46ba232f2 02/27/23 15:31:00.462
    STEP: Creating secret with name secret-projected-all-test-volume-77b46d31-f1fe-40ce-afa2-e2a5b79ac95c 02/27/23 15:31:00.466
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/27/23 15:31:00.473
    Feb 27 15:31:00.480: INFO: Waiting up to 5m0s for pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4" in namespace "projected-1005" to be "Succeeded or Failed"
    Feb 27 15:31:00.486: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.236108ms
    Feb 27 15:31:02.489: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00891073s
    Feb 27 15:31:04.489: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008920406s
    STEP: Saw pod success 02/27/23 15:31:04.489
    Feb 27 15:31:04.489: INFO: Pod "projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4" satisfied condition "Succeeded or Failed"
    Feb 27 15:31:04.493: INFO: Trying to get logs from node ip-172-31-42-40 pod projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4 container projected-all-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:31:04.499
    Feb 27 15:31:04.509: INFO: Waiting for pod projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4 to disappear
    Feb 27 15:31:04.512: INFO: Pod projected-volume-eec5b866-3ae9-4405-90e2-919230256cc4 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:31:04.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1005" for this suite. 02/27/23 15:31:04.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:31:04.526
Feb 27 15:31:04.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:31:04.527
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:04.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:04.551
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:31:04.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2201" for this suite. 02/27/23 15:31:04.563
------------------------------
• [0.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:31:04.526
    Feb 27 15:31:04.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:31:04.527
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:04.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:04.551
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:31:04.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2201" for this suite. 02/27/23 15:31:04.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:31:04.57
Feb 27 15:31:04.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:31:04.571
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:04.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:04.59
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 15:31:04.596
Feb 27 15:31:04.607: INFO: Waiting up to 5m0s for pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743" in namespace "emptydir-4707" to be "Succeeded or Failed"
Feb 27 15:31:04.611: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318107ms
Feb 27 15:31:06.616: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743": Phase="Running", Reason="", readiness=false. Elapsed: 2.009510956s
Feb 27 15:31:08.618: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010626347s
STEP: Saw pod success 02/27/23 15:31:08.618
Feb 27 15:31:08.618: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743" satisfied condition "Succeeded or Failed"
Feb 27 15:31:08.623: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-f83ada12-c174-4887-b39f-36eb0f9ac743 container test-container: <nil>
STEP: delete the pod 02/27/23 15:31:08.629
Feb 27 15:31:08.641: INFO: Waiting for pod pod-f83ada12-c174-4887-b39f-36eb0f9ac743 to disappear
Feb 27 15:31:08.645: INFO: Pod pod-f83ada12-c174-4887-b39f-36eb0f9ac743 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:31:08.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4707" for this suite. 02/27/23 15:31:08.649
------------------------------
• [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:31:04.57
    Feb 27 15:31:04.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:31:04.571
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:04.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:04.59
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 15:31:04.596
    Feb 27 15:31:04.607: INFO: Waiting up to 5m0s for pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743" in namespace "emptydir-4707" to be "Succeeded or Failed"
    Feb 27 15:31:04.611: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318107ms
    Feb 27 15:31:06.616: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743": Phase="Running", Reason="", readiness=false. Elapsed: 2.009510956s
    Feb 27 15:31:08.618: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010626347s
    STEP: Saw pod success 02/27/23 15:31:08.618
    Feb 27 15:31:08.618: INFO: Pod "pod-f83ada12-c174-4887-b39f-36eb0f9ac743" satisfied condition "Succeeded or Failed"
    Feb 27 15:31:08.623: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-f83ada12-c174-4887-b39f-36eb0f9ac743 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:31:08.629
    Feb 27 15:31:08.641: INFO: Waiting for pod pod-f83ada12-c174-4887-b39f-36eb0f9ac743 to disappear
    Feb 27 15:31:08.645: INFO: Pod pod-f83ada12-c174-4887-b39f-36eb0f9ac743 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:31:08.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4707" for this suite. 02/27/23 15:31:08.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:31:08.659
Feb 27 15:31:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename security-context-test 02/27/23 15:31:08.66
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:08.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:08.684
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Feb 27 15:31:08.698: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306" in namespace "security-context-test-6958" to be "Succeeded or Failed"
Feb 27 15:31:08.701: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127356ms
Feb 27 15:31:10.706: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007548513s
Feb 27 15:31:12.707: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008342133s
Feb 27 15:31:12.707: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 15:31:12.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6958" for this suite. 02/27/23 15:31:12.711
------------------------------
• [4.060 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:31:08.659
    Feb 27 15:31:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename security-context-test 02/27/23 15:31:08.66
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:08.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:08.684
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Feb 27 15:31:08.698: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306" in namespace "security-context-test-6958" to be "Succeeded or Failed"
    Feb 27 15:31:08.701: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127356ms
    Feb 27 15:31:10.706: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007548513s
    Feb 27 15:31:12.707: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008342133s
    Feb 27 15:31:12.707: INFO: Pod "busybox-user-65534-4f9a7953-0881-46f2-b092-41721b121306" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:31:12.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6958" for this suite. 02/27/23 15:31:12.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:31:12.719
Feb 27 15:31:12.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:31:12.72
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:12.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:12.738
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:31:12.755
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:31:13.112
STEP: Deploying the webhook pod 02/27/23 15:31:13.12
STEP: Wait for the deployment to be ready 02/27/23 15:31:13.133
Feb 27 15:31:13.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:31:15.154
STEP: Verifying the service has paired with the endpoint 02/27/23 15:31:15.165
Feb 27 15:31:16.165: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 02/27/23 15:31:16.17
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:31:16.184
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/27/23 15:31:16.19
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:31:16.201
STEP: Patching a validating webhook configuration's rules to include the create operation 02/27/23 15:31:16.212
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:31:16.222
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:31:16.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7716" for this suite. 02/27/23 15:31:16.274
STEP: Destroying namespace "webhook-7716-markers" for this suite. 02/27/23 15:31:16.283
------------------------------
• [3.571 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:31:12.719
    Feb 27 15:31:12.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:31:12.72
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:12.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:12.738
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:31:12.755
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:31:13.112
    STEP: Deploying the webhook pod 02/27/23 15:31:13.12
    STEP: Wait for the deployment to be ready 02/27/23 15:31:13.133
    Feb 27 15:31:13.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:31:15.154
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:31:15.165
    Feb 27 15:31:16.165: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 02/27/23 15:31:16.17
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:31:16.184
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/27/23 15:31:16.19
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:31:16.201
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/27/23 15:31:16.212
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 15:31:16.222
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:31:16.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7716" for this suite. 02/27/23 15:31:16.274
    STEP: Destroying namespace "webhook-7716-markers" for this suite. 02/27/23 15:31:16.283
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:31:16.29
Feb 27 15:31:16.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 15:31:16.291
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:16.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:16.311
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5934 02/27/23 15:31:16.315
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-5934 02/27/23 15:31:16.321
Feb 27 15:31:16.330: INFO: Found 0 stateful pods, waiting for 1
Feb 27 15:31:26.335: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/27/23 15:31:26.342
STEP: updating a scale subresource 02/27/23 15:31:26.346
STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 15:31:26.353
STEP: Patch a scale subresource 02/27/23 15:31:26.356
STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 15:31:26.365
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 15:31:26.373: INFO: Deleting all statefulset in ns statefulset-5934
Feb 27 15:31:26.376: INFO: Scaling statefulset ss to 0
Feb 27 15:31:36.398: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 15:31:36.401: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:31:36.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5934" for this suite. 02/27/23 15:31:36.419
------------------------------
• [SLOW TEST] [20.135 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:31:16.29
    Feb 27 15:31:16.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 15:31:16.291
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:16.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:16.311
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5934 02/27/23 15:31:16.315
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-5934 02/27/23 15:31:16.321
    Feb 27 15:31:16.330: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 15:31:26.335: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/27/23 15:31:26.342
    STEP: updating a scale subresource 02/27/23 15:31:26.346
    STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 15:31:26.353
    STEP: Patch a scale subresource 02/27/23 15:31:26.356
    STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 15:31:26.365
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 15:31:26.373: INFO: Deleting all statefulset in ns statefulset-5934
    Feb 27 15:31:26.376: INFO: Scaling statefulset ss to 0
    Feb 27 15:31:36.398: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 15:31:36.401: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:31:36.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5934" for this suite. 02/27/23 15:31:36.419
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:31:36.425
Feb 27 15:31:36.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename taint-multiple-pods 02/27/23 15:31:36.426
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:36.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:36.445
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Feb 27 15:31:36.448: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 15:32:36.462: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Feb 27 15:32:36.466: INFO: Starting informer...
STEP: Starting pods... 02/27/23 15:32:36.466
Feb 27 15:32:36.683: INFO: Pod1 is running on ip-172-31-42-40. Tainting Node
Feb 27 15:32:36.892: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9612" to be "running"
Feb 27 15:32:36.895: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91133ms
Feb 27 15:32:38.899: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006660214s
Feb 27 15:32:38.899: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb 27 15:32:38.899: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9612" to be "running"
Feb 27 15:32:38.903: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.891473ms
Feb 27 15:32:38.903: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb 27 15:32:38.903: INFO: Pod2 is running on ip-172-31-42-40. Tainting Node
STEP: Trying to apply a taint on the Node 02/27/23 15:32:38.903
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:32:38.913
STEP: Waiting for Pod1 and Pod2 to be deleted 02/27/23 15:32:38.916
Feb 27 15:32:44.617: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 27 15:33:04.649: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:33:04.662
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:04.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-9612" for this suite. 02/27/23 15:33:04.672
------------------------------
• [SLOW TEST] [88.258 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:31:36.425
    Feb 27 15:31:36.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename taint-multiple-pods 02/27/23 15:31:36.426
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:31:36.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:31:36.445
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Feb 27 15:31:36.448: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 15:32:36.462: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Feb 27 15:32:36.466: INFO: Starting informer...
    STEP: Starting pods... 02/27/23 15:32:36.466
    Feb 27 15:32:36.683: INFO: Pod1 is running on ip-172-31-42-40. Tainting Node
    Feb 27 15:32:36.892: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9612" to be "running"
    Feb 27 15:32:36.895: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91133ms
    Feb 27 15:32:38.899: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006660214s
    Feb 27 15:32:38.899: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb 27 15:32:38.899: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9612" to be "running"
    Feb 27 15:32:38.903: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.891473ms
    Feb 27 15:32:38.903: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb 27 15:32:38.903: INFO: Pod2 is running on ip-172-31-42-40. Tainting Node
    STEP: Trying to apply a taint on the Node 02/27/23 15:32:38.903
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:32:38.913
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/27/23 15:32:38.916
    Feb 27 15:32:44.617: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb 27 15:33:04.649: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 15:33:04.662
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:04.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-9612" for this suite. 02/27/23 15:33:04.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:04.686
Feb 27 15:33:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:33:04.687
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:04.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:04.71
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:33:04.727
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:33:05.075
STEP: Deploying the webhook pod 02/27/23 15:33:05.083
STEP: Wait for the deployment to be ready 02/27/23 15:33:05.096
Feb 27 15:33:05.106: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:33:07.116
STEP: Verifying the service has paired with the endpoint 02/27/23 15:33:07.127
Feb 27 15:33:08.127: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Feb 27 15:33:08.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1135-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 15:33:08.642
STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 15:33:08.656
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:11.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1884" for this suite. 02/27/23 15:33:11.267
STEP: Destroying namespace "webhook-1884-markers" for this suite. 02/27/23 15:33:11.274
------------------------------
• [SLOW TEST] [6.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:04.686
    Feb 27 15:33:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:33:04.687
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:04.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:04.71
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:33:04.727
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:33:05.075
    STEP: Deploying the webhook pod 02/27/23 15:33:05.083
    STEP: Wait for the deployment to be ready 02/27/23 15:33:05.096
    Feb 27 15:33:05.106: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:33:07.116
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:33:07.127
    Feb 27 15:33:08.127: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Feb 27 15:33:08.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1135-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 15:33:08.642
    STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 15:33:08.656
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:11.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1884" for this suite. 02/27/23 15:33:11.267
    STEP: Destroying namespace "webhook-1884-markers" for this suite. 02/27/23 15:33:11.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:11.285
Feb 27 15:33:11.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:33:11.286
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:11.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:11.309
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:33:11.326
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:33:11.691
STEP: Deploying the webhook pod 02/27/23 15:33:11.698
STEP: Wait for the deployment to be ready 02/27/23 15:33:11.714
Feb 27 15:33:11.830: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 27 15:33:13.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/27/23 15:33:15.846
STEP: Verifying the service has paired with the endpoint 02/27/23 15:33:15.856
Feb 27 15:33:16.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 02/27/23 15:33:16.861
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/27/23 15:33:16.862
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 15:33:16.862
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/27/23 15:33:16.863
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/27/23 15:33:16.864
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 15:33:16.864
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 15:33:16.865
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:16.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6972" for this suite. 02/27/23 15:33:16.911
STEP: Destroying namespace "webhook-6972-markers" for this suite. 02/27/23 15:33:16.922
------------------------------
• [SLOW TEST] [5.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:11.285
    Feb 27 15:33:11.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:33:11.286
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:11.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:11.309
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:33:11.326
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:33:11.691
    STEP: Deploying the webhook pod 02/27/23 15:33:11.698
    STEP: Wait for the deployment to be ready 02/27/23 15:33:11.714
    Feb 27 15:33:11.830: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 27 15:33:13.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/27/23 15:33:15.846
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:33:15.856
    Feb 27 15:33:16.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 02/27/23 15:33:16.861
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/27/23 15:33:16.862
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 15:33:16.862
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/27/23 15:33:16.863
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/27/23 15:33:16.864
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 15:33:16.864
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 15:33:16.865
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:16.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6972" for this suite. 02/27/23 15:33:16.911
    STEP: Destroying namespace "webhook-6972-markers" for this suite. 02/27/23 15:33:16.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:16.934
Feb 27 15:33:16.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replication-controller 02/27/23 15:33:16.934
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:16.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:16.959
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-8fwpv" 02/27/23 15:33:16.964
Feb 27 15:33:16.972: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
Feb 27 15:33:17.983: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
Feb 27 15:33:17.987: INFO: Found 1 replicas for "e2e-rc-8fwpv" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-8fwpv" 02/27/23 15:33:17.987
STEP: Updating a scale subresource 02/27/23 15:33:17.99
STEP: Verifying replicas where modified for replication controller "e2e-rc-8fwpv" 02/27/23 15:33:17.997
Feb 27 15:33:17.997: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
Feb 27 15:33:19.001: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
Feb 27 15:33:19.005: INFO: Found 2 replicas for "e2e-rc-8fwpv" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:19.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9006" for this suite. 02/27/23 15:33:19.009
------------------------------
• [2.083 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:16.934
    Feb 27 15:33:16.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replication-controller 02/27/23 15:33:16.934
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:16.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:16.959
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-8fwpv" 02/27/23 15:33:16.964
    Feb 27 15:33:16.972: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
    Feb 27 15:33:17.983: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
    Feb 27 15:33:17.987: INFO: Found 1 replicas for "e2e-rc-8fwpv" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-8fwpv" 02/27/23 15:33:17.987
    STEP: Updating a scale subresource 02/27/23 15:33:17.99
    STEP: Verifying replicas where modified for replication controller "e2e-rc-8fwpv" 02/27/23 15:33:17.997
    Feb 27 15:33:17.997: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
    Feb 27 15:33:19.001: INFO: Get Replication Controller "e2e-rc-8fwpv" to confirm replicas
    Feb 27 15:33:19.005: INFO: Found 2 replicas for "e2e-rc-8fwpv" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:19.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9006" for this suite. 02/27/23 15:33:19.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:19.017
Feb 27 15:33:19.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 15:33:19.018
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:19.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:19.042
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:19.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4671" for this suite. 02/27/23 15:33:19.094
------------------------------
• [0.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:19.017
    Feb 27 15:33:19.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 15:33:19.018
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:19.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:19.042
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:19.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4671" for this suite. 02/27/23 15:33:19.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:19.104
Feb 27 15:33:19.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:33:19.104
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:19.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:19.125
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Feb 27 15:33:19.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 create -f -'
Feb 27 15:33:19.805: INFO: stderr: ""
Feb 27 15:33:19.805: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 27 15:33:19.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 create -f -'
Feb 27 15:33:20.453: INFO: stderr: ""
Feb 27 15:33:20.453: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 15:33:20.453
Feb 27 15:33:21.457: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 15:33:21.457: INFO: Found 1 / 1
Feb 27 15:33:21.457: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 27 15:33:21.461: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 15:33:21.461: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 15:33:21.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe pod agnhost-primary-9pkjk'
Feb 27 15:33:21.521: INFO: stderr: ""
Feb 27 15:33:21.521: INFO: stdout: "Name:             agnhost-primary-9pkjk\nNamespace:        kubectl-8297\nPriority:         0\nService Account:  default\nNode:             ip-172-31-42-40/172.31.42.40\nStart Time:       Mon, 27 Feb 2023 15:33:19 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.192.175\nIPs:\n  IP:           192.168.192.175\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6c19aafd55bfbeeb637e15274e826a698685fe4890aa486cc440c6dc966d91a7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Feb 2023 15:33:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ldtgn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ldtgn:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8297/agnhost-primary-9pkjk to ip-172-31-42-40\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Feb 27 15:33:21.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe rc agnhost-primary'
Feb 27 15:33:21.585: INFO: stderr: ""
Feb 27 15:33:21.585: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8297\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-9pkjk\n"
Feb 27 15:33:21.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe service agnhost-primary'
Feb 27 15:33:21.638: INFO: stderr: ""
Feb 27 15:33:21.638: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8297\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.163\nIPs:               10.152.183.163\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.192.175:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 27 15:33:21.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe node ip-172-31-3-182'
Feb 27 15:33:21.732: INFO: stderr: ""
Feb 27 15:33:21.732: INFO: stdout: "Name:               ip-172-31-3-182\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-3-182\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Feb 2023 14:58:12 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-3-182\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Feb 2023 15:33:15 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:42 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.3.182\n  Hostname:    ip-172-31-3-182\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7995372Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7892972Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec2420c70beac6059dd99af6c40928d6\n  System UUID:                     ec2420c7-0bea-c605-9dd9-9af6c40928d6\n  Boot ID:                         9c6c25d2-ff6a-4655-a37b-673a1c1a134f\n  Kernel Version:                  5.15.0-1030-aws\n  OS Image:                        Ubuntu 20.04.5 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.26.1\n  Kube-Proxy Version:              v1.26.1\nNon-terminated Pods:               (6 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  default-http-backend-kubernetes-worker-9b9488b5c-nx5j4     10m (0%)      10m (0%)    20Mi (0%)        20Mi (0%)      43s\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-s5lnj           0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  kube-system                      calico-kube-controllers-6b5c59f67b-27fsw                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  replication-controller-9006      e2e-rc-8fwpv-4k9wk                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                         sonobuoy-e2e-job-239e950f4344406f                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                10m (0%)   10m (0%)\n  memory             20Mi (0%)  20Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 34m   kube-proxy       \n  Normal   Starting                 35m   kube-proxy       \n  Normal   Starting                 35m   kube-proxy       \n  Normal   Starting                 35m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      35m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  35m   kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                35m   kubelet          Node ip-172-31-3-182 status is now: NodeReady\n  Normal   NodeHasSufficientMemory  35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      35m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasNoDiskPressure    35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  35m   kubelet          Updated Node Allocatable limit across pods\n  Normal   Starting                 35m   kubelet          Starting kubelet.\n  Normal   RegisteredNode           34m   node-controller  Node ip-172-31-3-182 event: Registered Node ip-172-31-3-182 in Controller\n  Normal   Starting                 34m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      34m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  34m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    34m   kubelet          Node ip-172-31-3-182 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     34m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             34m   kubelet          Node ip-172-31-3-182 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  34m   kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                34m   kubelet          Node ip-172-31-3-182 status is now: NodeReady\n"
Feb 27 15:33:21.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe namespace kubectl-8297'
Feb 27 15:33:21.786: INFO: stderr: ""
Feb 27 15:33:21.787: INFO: stdout: "Name:         kubectl-8297\nLabels:       e2e-framework=kubectl\n              e2e-run=0e1ee2c8-2a4b-4b26-a946-79230015426d\n              kubernetes.io/metadata.name=kubectl-8297\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:21.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8297" for this suite. 02/27/23 15:33:21.79
------------------------------
• [2.694 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:19.104
    Feb 27 15:33:19.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:33:19.104
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:19.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:19.125
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Feb 27 15:33:19.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 create -f -'
    Feb 27 15:33:19.805: INFO: stderr: ""
    Feb 27 15:33:19.805: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb 27 15:33:19.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 create -f -'
    Feb 27 15:33:20.453: INFO: stderr: ""
    Feb 27 15:33:20.453: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 15:33:20.453
    Feb 27 15:33:21.457: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 15:33:21.457: INFO: Found 1 / 1
    Feb 27 15:33:21.457: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 27 15:33:21.461: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 15:33:21.461: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 15:33:21.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe pod agnhost-primary-9pkjk'
    Feb 27 15:33:21.521: INFO: stderr: ""
    Feb 27 15:33:21.521: INFO: stdout: "Name:             agnhost-primary-9pkjk\nNamespace:        kubectl-8297\nPriority:         0\nService Account:  default\nNode:             ip-172-31-42-40/172.31.42.40\nStart Time:       Mon, 27 Feb 2023 15:33:19 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.192.175\nIPs:\n  IP:           192.168.192.175\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6c19aafd55bfbeeb637e15274e826a698685fe4890aa486cc440c6dc966d91a7\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Feb 2023 15:33:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ldtgn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ldtgn:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8297/agnhost-primary-9pkjk to ip-172-31-42-40\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Feb 27 15:33:21.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe rc agnhost-primary'
    Feb 27 15:33:21.585: INFO: stderr: ""
    Feb 27 15:33:21.585: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8297\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-9pkjk\n"
    Feb 27 15:33:21.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe service agnhost-primary'
    Feb 27 15:33:21.638: INFO: stderr: ""
    Feb 27 15:33:21.638: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8297\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.163\nIPs:               10.152.183.163\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.192.175:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb 27 15:33:21.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe node ip-172-31-3-182'
    Feb 27 15:33:21.732: INFO: stderr: ""
    Feb 27 15:33:21.732: INFO: stdout: "Name:               ip-172-31-3-182\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-3-182\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Feb 2023 14:58:12 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-3-182\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Feb 2023 15:33:15 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 27 Feb 2023 15:31:50 +0000   Mon, 27 Feb 2023 14:58:42 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.3.182\n  Hostname:    ip-172-31-3-182\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7995372Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7892972Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec2420c70beac6059dd99af6c40928d6\n  System UUID:                     ec2420c7-0bea-c605-9dd9-9af6c40928d6\n  Boot ID:                         9c6c25d2-ff6a-4655-a37b-673a1c1a134f\n  Kernel Version:                  5.15.0-1030-aws\n  OS Image:                        Ubuntu 20.04.5 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.26.1\n  Kube-Proxy Version:              v1.26.1\nNon-terminated Pods:               (6 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  default-http-backend-kubernetes-worker-9b9488b5c-nx5j4     10m (0%)      10m (0%)    20Mi (0%)        20Mi (0%)      43s\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-s5lnj           0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  kube-system                      calico-kube-controllers-6b5c59f67b-27fsw                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  replication-controller-9006      e2e-rc-8fwpv-4k9wk                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                         sonobuoy-e2e-job-239e950f4344406f                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                10m (0%)   10m (0%)\n  memory             20Mi (0%)  20Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 34m   kube-proxy       \n  Normal   Starting                 35m   kube-proxy       \n  Normal   Starting                 35m   kube-proxy       \n  Normal   Starting                 35m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      35m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  35m   kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                35m   kubelet          Node ip-172-31-3-182 status is now: NodeReady\n  Normal   NodeHasSufficientMemory  35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      35m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasNoDiskPressure    35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     35m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  35m   kubelet          Updated Node Allocatable limit across pods\n  Normal   Starting                 35m   kubelet          Starting kubelet.\n  Normal   RegisteredNode           34m   node-controller  Node ip-172-31-3-182 event: Registered Node ip-172-31-3-182 in Controller\n  Normal   Starting                 34m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      34m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  34m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    34m   kubelet          Node ip-172-31-3-182 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     34m   kubelet          Node ip-172-31-3-182 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             34m   kubelet          Node ip-172-31-3-182 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  34m   kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                34m   kubelet          Node ip-172-31-3-182 status is now: NodeReady\n"
    Feb 27 15:33:21.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-8297 describe namespace kubectl-8297'
    Feb 27 15:33:21.786: INFO: stderr: ""
    Feb 27 15:33:21.787: INFO: stdout: "Name:         kubectl-8297\nLabels:       e2e-framework=kubectl\n              e2e-run=0e1ee2c8-2a4b-4b26-a946-79230015426d\n              kubernetes.io/metadata.name=kubectl-8297\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:21.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8297" for this suite. 02/27/23 15:33:21.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:21.799
Feb 27 15:33:21.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:33:21.8
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:21.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:21.823
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:33:21.827
Feb 27 15:33:21.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca" in namespace "projected-9626" to be "Succeeded or Failed"
Feb 27 15:33:21.842: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.006422ms
Feb 27 15:33:23.846: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca": Phase="Running", Reason="", readiness=false. Elapsed: 2.009347382s
Feb 27 15:33:25.847: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009891562s
STEP: Saw pod success 02/27/23 15:33:25.847
Feb 27 15:33:25.847: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca" satisfied condition "Succeeded or Failed"
Feb 27 15:33:25.850: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca container client-container: <nil>
STEP: delete the pod 02/27/23 15:33:25.861
Feb 27 15:33:25.875: INFO: Waiting for pod downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca to disappear
Feb 27 15:33:25.878: INFO: Pod downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:25.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9626" for this suite. 02/27/23 15:33:25.881
------------------------------
• [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:21.799
    Feb 27 15:33:21.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:33:21.8
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:21.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:21.823
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:33:21.827
    Feb 27 15:33:21.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca" in namespace "projected-9626" to be "Succeeded or Failed"
    Feb 27 15:33:21.842: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.006422ms
    Feb 27 15:33:23.846: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca": Phase="Running", Reason="", readiness=false. Elapsed: 2.009347382s
    Feb 27 15:33:25.847: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009891562s
    STEP: Saw pod success 02/27/23 15:33:25.847
    Feb 27 15:33:25.847: INFO: Pod "downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca" satisfied condition "Succeeded or Failed"
    Feb 27 15:33:25.850: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca container client-container: <nil>
    STEP: delete the pod 02/27/23 15:33:25.861
    Feb 27 15:33:25.875: INFO: Waiting for pod downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca to disappear
    Feb 27 15:33:25.878: INFO: Pod downwardapi-volume-bf9e8b0a-c686-4db9-83b0-4875fbdafbca no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:25.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9626" for this suite. 02/27/23 15:33:25.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:25.888
Feb 27 15:33:25.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:33:25.889
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:25.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:25.912
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Feb 27 15:33:25.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/27/23 15:33:27.897
Feb 27 15:33:27.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
Feb 27 15:33:28.410: INFO: stderr: ""
Feb 27 15:33:28.410: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 27 15:33:28.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 delete e2e-test-crd-publish-openapi-4235-crds test-foo'
Feb 27 15:33:28.461: INFO: stderr: ""
Feb 27 15:33:28.461: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 27 15:33:28.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 apply -f -'
Feb 27 15:33:28.689: INFO: stderr: ""
Feb 27 15:33:28.689: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 27 15:33:28.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 delete e2e-test-crd-publish-openapi-4235-crds test-foo'
Feb 27 15:33:28.745: INFO: stderr: ""
Feb 27 15:33:28.745: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/27/23 15:33:28.745
Feb 27 15:33:28.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
Feb 27 15:33:28.985: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/27/23 15:33:28.985
Feb 27 15:33:28.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
Feb 27 15:33:29.199: INFO: rc: 1
Feb 27 15:33:29.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 apply -f -'
Feb 27 15:33:29.581: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/27/23 15:33:29.581
Feb 27 15:33:29.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
Feb 27 15:33:29.711: INFO: rc: 1
Feb 27 15:33:29.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 apply -f -'
Feb 27 15:33:29.846: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/27/23 15:33:29.846
Feb 27 15:33:29.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds'
Feb 27 15:33:29.979: INFO: stderr: ""
Feb 27 15:33:29.979: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/27/23 15:33:29.979
Feb 27 15:33:29.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.metadata'
Feb 27 15:33:30.111: INFO: stderr: ""
Feb 27 15:33:30.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 27 15:33:30.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.spec'
Feb 27 15:33:30.347: INFO: stderr: ""
Feb 27 15:33:30.347: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 27 15:33:30.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.spec.bars'
Feb 27 15:33:30.597: INFO: stderr: ""
Feb 27 15:33:30.597: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/27/23 15:33:30.597
Feb 27 15:33:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.spec.bars2'
Feb 27 15:33:30.901: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:32.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5532" for this suite. 02/27/23 15:33:32.383
------------------------------
• [SLOW TEST] [6.502 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:25.888
    Feb 27 15:33:25.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:33:25.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:25.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:25.912
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Feb 27 15:33:25.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/27/23 15:33:27.897
    Feb 27 15:33:27.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
    Feb 27 15:33:28.410: INFO: stderr: ""
    Feb 27 15:33:28.410: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 27 15:33:28.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 delete e2e-test-crd-publish-openapi-4235-crds test-foo'
    Feb 27 15:33:28.461: INFO: stderr: ""
    Feb 27 15:33:28.461: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb 27 15:33:28.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 apply -f -'
    Feb 27 15:33:28.689: INFO: stderr: ""
    Feb 27 15:33:28.689: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 27 15:33:28.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 delete e2e-test-crd-publish-openapi-4235-crds test-foo'
    Feb 27 15:33:28.745: INFO: stderr: ""
    Feb 27 15:33:28.745: INFO: stdout: "e2e-test-crd-publish-openapi-4235-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/27/23 15:33:28.745
    Feb 27 15:33:28.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
    Feb 27 15:33:28.985: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/27/23 15:33:28.985
    Feb 27 15:33:28.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
    Feb 27 15:33:29.199: INFO: rc: 1
    Feb 27 15:33:29.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 apply -f -'
    Feb 27 15:33:29.581: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/27/23 15:33:29.581
    Feb 27 15:33:29.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 create -f -'
    Feb 27 15:33:29.711: INFO: rc: 1
    Feb 27 15:33:29.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 --namespace=crd-publish-openapi-5532 apply -f -'
    Feb 27 15:33:29.846: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/27/23 15:33:29.846
    Feb 27 15:33:29.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds'
    Feb 27 15:33:29.979: INFO: stderr: ""
    Feb 27 15:33:29.979: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/27/23 15:33:29.979
    Feb 27 15:33:29.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.metadata'
    Feb 27 15:33:30.111: INFO: stderr: ""
    Feb 27 15:33:30.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb 27 15:33:30.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.spec'
    Feb 27 15:33:30.347: INFO: stderr: ""
    Feb 27 15:33:30.347: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb 27 15:33:30.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.spec.bars'
    Feb 27 15:33:30.597: INFO: stderr: ""
    Feb 27 15:33:30.597: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4235-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/27/23 15:33:30.597
    Feb 27 15:33:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-5532 explain e2e-test-crd-publish-openapi-4235-crds.spec.bars2'
    Feb 27 15:33:30.901: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:32.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5532" for this suite. 02/27/23 15:33:32.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:32.391
Feb 27 15:33:32.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:33:32.391
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:32.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:32.409
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 15:33:32.412
Feb 27 15:33:32.424: INFO: Waiting up to 5m0s for pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0" in namespace "emptydir-9293" to be "Succeeded or Failed"
Feb 27 15:33:32.427: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.957398ms
Feb 27 15:33:34.432: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00797594s
Feb 27 15:33:36.431: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007625431s
STEP: Saw pod success 02/27/23 15:33:36.431
Feb 27 15:33:36.432: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0" satisfied condition "Succeeded or Failed"
Feb 27 15:33:36.435: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-c161312e-9f88-4176-b7eb-dde36b8a30c0 container test-container: <nil>
STEP: delete the pod 02/27/23 15:33:36.441
Feb 27 15:33:36.454: INFO: Waiting for pod pod-c161312e-9f88-4176-b7eb-dde36b8a30c0 to disappear
Feb 27 15:33:36.457: INFO: Pod pod-c161312e-9f88-4176-b7eb-dde36b8a30c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:36.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9293" for this suite. 02/27/23 15:33:36.46
------------------------------
• [4.076 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:32.391
    Feb 27 15:33:32.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:33:32.391
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:32.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:32.409
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 15:33:32.412
    Feb 27 15:33:32.424: INFO: Waiting up to 5m0s for pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0" in namespace "emptydir-9293" to be "Succeeded or Failed"
    Feb 27 15:33:32.427: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.957398ms
    Feb 27 15:33:34.432: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00797594s
    Feb 27 15:33:36.431: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007625431s
    STEP: Saw pod success 02/27/23 15:33:36.431
    Feb 27 15:33:36.432: INFO: Pod "pod-c161312e-9f88-4176-b7eb-dde36b8a30c0" satisfied condition "Succeeded or Failed"
    Feb 27 15:33:36.435: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-c161312e-9f88-4176-b7eb-dde36b8a30c0 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:33:36.441
    Feb 27 15:33:36.454: INFO: Waiting for pod pod-c161312e-9f88-4176-b7eb-dde36b8a30c0 to disappear
    Feb 27 15:33:36.457: INFO: Pod pod-c161312e-9f88-4176-b7eb-dde36b8a30c0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:36.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9293" for this suite. 02/27/23 15:33:36.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:36.467
Feb 27 15:33:36.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 15:33:36.468
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:36.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:36.489
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 02/27/23 15:33:36.491
STEP: getting 02/27/23 15:33:36.508
STEP: listing in namespace 02/27/23 15:33:36.512
STEP: patching 02/27/23 15:33:36.515
STEP: deleting 02/27/23 15:33:36.526
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:36.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2102" for this suite. 02/27/23 15:33:36.54
------------------------------
• [0.079 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:36.467
    Feb 27 15:33:36.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 15:33:36.468
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:36.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:36.489
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 02/27/23 15:33:36.491
    STEP: getting 02/27/23 15:33:36.508
    STEP: listing in namespace 02/27/23 15:33:36.512
    STEP: patching 02/27/23 15:33:36.515
    STEP: deleting 02/27/23 15:33:36.526
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:36.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2102" for this suite. 02/27/23 15:33:36.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:36.547
Feb 27 15:33:36.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename controllerrevisions 02/27/23 15:33:36.548
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:36.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:36.565
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-qbqkc-daemon-set" 02/27/23 15:33:36.585
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 15:33:36.591
Feb 27 15:33:36.594: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:33:36.595: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:33:36.598: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 0
Feb 27 15:33:36.598: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:33:37.602: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:33:37.602: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:33:37.605: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 0
Feb 27 15:33:37.605: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 15:33:38.602: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:33:38.602: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 15:33:38.606: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 3
Feb 27 15:33:38.606: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-qbqkc-daemon-set
STEP: Confirm DaemonSet "e2e-qbqkc-daemon-set" successfully created with "daemonset-name=e2e-qbqkc-daemon-set" label 02/27/23 15:33:38.609
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-qbqkc-daemon-set" 02/27/23 15:33:38.616
Feb 27 15:33:38.619: INFO: Located ControllerRevision: "e2e-qbqkc-daemon-set-5b9589b8f7"
STEP: Patching ControllerRevision "e2e-qbqkc-daemon-set-5b9589b8f7" 02/27/23 15:33:38.622
Feb 27 15:33:38.628: INFO: e2e-qbqkc-daemon-set-5b9589b8f7 has been patched
STEP: Create a new ControllerRevision 02/27/23 15:33:38.628
Feb 27 15:33:38.634: INFO: Created ControllerRevision: e2e-qbqkc-daemon-set-7f97756977
STEP: Confirm that there are two ControllerRevisions 02/27/23 15:33:38.634
Feb 27 15:33:38.634: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 15:33:38.638: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-qbqkc-daemon-set-5b9589b8f7" 02/27/23 15:33:38.638
STEP: Confirm that there is only one ControllerRevision 02/27/23 15:33:38.644
Feb 27 15:33:38.644: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 15:33:38.648: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-qbqkc-daemon-set-7f97756977" 02/27/23 15:33:38.651
Feb 27 15:33:38.660: INFO: e2e-qbqkc-daemon-set-7f97756977 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/27/23 15:33:38.66
W0227 15:33:38.666555      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/27/23 15:33:38.666
Feb 27 15:33:38.666: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 15:33:39.671: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 15:33:39.675: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-qbqkc-daemon-set-7f97756977=updated" 02/27/23 15:33:39.675
STEP: Confirm that there is only one ControllerRevision 02/27/23 15:33:39.683
Feb 27 15:33:39.683: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 15:33:39.687: INFO: Found 1 ControllerRevisions
Feb 27 15:33:39.690: INFO: ControllerRevision "e2e-qbqkc-daemon-set-854fb47664" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-qbqkc-daemon-set" 02/27/23 15:33:39.694
STEP: deleting DaemonSet.extensions e2e-qbqkc-daemon-set in namespace controllerrevisions-9386, will wait for the garbage collector to delete the pods 02/27/23 15:33:39.694
Feb 27 15:33:39.756: INFO: Deleting DaemonSet.extensions e2e-qbqkc-daemon-set took: 7.70905ms
Feb 27 15:33:39.857: INFO: Terminating DaemonSet.extensions e2e-qbqkc-daemon-set pods took: 101.118172ms
Feb 27 15:33:41.261: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 0
Feb 27 15:33:41.261: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-qbqkc-daemon-set
Feb 27 15:33:41.264: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11524"},"items":null}

Feb 27 15:33:41.267: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11524"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:41.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-9386" for this suite. 02/27/23 15:33:41.284
------------------------------
• [4.743 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:36.547
    Feb 27 15:33:36.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename controllerrevisions 02/27/23 15:33:36.548
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:36.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:36.565
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-qbqkc-daemon-set" 02/27/23 15:33:36.585
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 15:33:36.591
    Feb 27 15:33:36.594: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:33:36.595: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:33:36.598: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 0
    Feb 27 15:33:36.598: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:33:37.602: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:33:37.602: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:33:37.605: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 0
    Feb 27 15:33:37.605: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 15:33:38.602: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:33:38.602: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 15:33:38.606: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 3
    Feb 27 15:33:38.606: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-qbqkc-daemon-set
    STEP: Confirm DaemonSet "e2e-qbqkc-daemon-set" successfully created with "daemonset-name=e2e-qbqkc-daemon-set" label 02/27/23 15:33:38.609
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-qbqkc-daemon-set" 02/27/23 15:33:38.616
    Feb 27 15:33:38.619: INFO: Located ControllerRevision: "e2e-qbqkc-daemon-set-5b9589b8f7"
    STEP: Patching ControllerRevision "e2e-qbqkc-daemon-set-5b9589b8f7" 02/27/23 15:33:38.622
    Feb 27 15:33:38.628: INFO: e2e-qbqkc-daemon-set-5b9589b8f7 has been patched
    STEP: Create a new ControllerRevision 02/27/23 15:33:38.628
    Feb 27 15:33:38.634: INFO: Created ControllerRevision: e2e-qbqkc-daemon-set-7f97756977
    STEP: Confirm that there are two ControllerRevisions 02/27/23 15:33:38.634
    Feb 27 15:33:38.634: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 15:33:38.638: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-qbqkc-daemon-set-5b9589b8f7" 02/27/23 15:33:38.638
    STEP: Confirm that there is only one ControllerRevision 02/27/23 15:33:38.644
    Feb 27 15:33:38.644: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 15:33:38.648: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-qbqkc-daemon-set-7f97756977" 02/27/23 15:33:38.651
    Feb 27 15:33:38.660: INFO: e2e-qbqkc-daemon-set-7f97756977 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/27/23 15:33:38.66
    W0227 15:33:38.666555      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/27/23 15:33:38.666
    Feb 27 15:33:38.666: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 15:33:39.671: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 15:33:39.675: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-qbqkc-daemon-set-7f97756977=updated" 02/27/23 15:33:39.675
    STEP: Confirm that there is only one ControllerRevision 02/27/23 15:33:39.683
    Feb 27 15:33:39.683: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 15:33:39.687: INFO: Found 1 ControllerRevisions
    Feb 27 15:33:39.690: INFO: ControllerRevision "e2e-qbqkc-daemon-set-854fb47664" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-qbqkc-daemon-set" 02/27/23 15:33:39.694
    STEP: deleting DaemonSet.extensions e2e-qbqkc-daemon-set in namespace controllerrevisions-9386, will wait for the garbage collector to delete the pods 02/27/23 15:33:39.694
    Feb 27 15:33:39.756: INFO: Deleting DaemonSet.extensions e2e-qbqkc-daemon-set took: 7.70905ms
    Feb 27 15:33:39.857: INFO: Terminating DaemonSet.extensions e2e-qbqkc-daemon-set pods took: 101.118172ms
    Feb 27 15:33:41.261: INFO: Number of nodes with available pods controlled by daemonset e2e-qbqkc-daemon-set: 0
    Feb 27 15:33:41.261: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-qbqkc-daemon-set
    Feb 27 15:33:41.264: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11524"},"items":null}

    Feb 27 15:33:41.267: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11524"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:41.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-9386" for this suite. 02/27/23 15:33:41.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:41.291
Feb 27 15:33:41.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:33:41.291
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:41.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:41.311
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:33:41.314
Feb 27 15:33:41.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b" in namespace "projected-9586" to be "Succeeded or Failed"
Feb 27 15:33:41.327: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.973713ms
Feb 27 15:33:43.332: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009248517s
Feb 27 15:33:45.332: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009175593s
STEP: Saw pod success 02/27/23 15:33:45.332
Feb 27 15:33:45.332: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b" satisfied condition "Succeeded or Failed"
Feb 27 15:33:45.335: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b container client-container: <nil>
STEP: delete the pod 02/27/23 15:33:45.342
Feb 27 15:33:45.357: INFO: Waiting for pod downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b to disappear
Feb 27 15:33:45.360: INFO: Pod downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:45.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9586" for this suite. 02/27/23 15:33:45.364
------------------------------
• [4.079 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:41.291
    Feb 27 15:33:41.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:33:41.291
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:41.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:41.311
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:33:41.314
    Feb 27 15:33:41.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b" in namespace "projected-9586" to be "Succeeded or Failed"
    Feb 27 15:33:41.327: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.973713ms
    Feb 27 15:33:43.332: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009248517s
    Feb 27 15:33:45.332: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009175593s
    STEP: Saw pod success 02/27/23 15:33:45.332
    Feb 27 15:33:45.332: INFO: Pod "downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b" satisfied condition "Succeeded or Failed"
    Feb 27 15:33:45.335: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b container client-container: <nil>
    STEP: delete the pod 02/27/23 15:33:45.342
    Feb 27 15:33:45.357: INFO: Waiting for pod downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b to disappear
    Feb 27 15:33:45.360: INFO: Pod downwardapi-volume-e17b7294-fb9b-437e-9dc3-c05926f1b41b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:45.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9586" for this suite. 02/27/23 15:33:45.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:45.37
Feb 27 15:33:45.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 15:33:45.371
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:45.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:45.39
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 02/27/23 15:33:45.394
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/27/23 15:33:45.4
STEP: patching the secret 02/27/23 15:33:45.404
STEP: deleting the secret using a LabelSelector 02/27/23 15:33:45.413
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/27/23 15:33:45.425
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 15:33:45.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8445" for this suite. 02/27/23 15:33:45.434
------------------------------
• [0.071 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:45.37
    Feb 27 15:33:45.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 15:33:45.371
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:45.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:45.39
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 02/27/23 15:33:45.394
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/27/23 15:33:45.4
    STEP: patching the secret 02/27/23 15:33:45.404
    STEP: deleting the secret using a LabelSelector 02/27/23 15:33:45.413
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/27/23 15:33:45.425
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:33:45.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8445" for this suite. 02/27/23 15:33:45.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:33:45.443
Feb 27 15:33:45.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename aggregator 02/27/23 15:33:45.444
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:45.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:45.465
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb 27 15:33:45.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/27/23 15:33:45.469
Feb 27 15:33:45.659: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 27 15:33:47.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:33:49.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:33:51.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:33:53.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:33:55.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:33:57.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:33:59.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:34:01.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:34:03.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:34:05.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:34:07.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 15:34:09.848: INFO: Waited 117.603057ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 02/27/23 15:34:09.889
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/27/23 15:34:09.893
STEP: List APIServices 02/27/23 15:34:09.899
Feb 27 15:34:09.905: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:10.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-9288" for this suite. 02/27/23 15:34:10.047
------------------------------
• [SLOW TEST] [24.649 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:33:45.443
    Feb 27 15:33:45.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename aggregator 02/27/23 15:33:45.444
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:33:45.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:33:45.465
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb 27 15:33:45.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/27/23 15:33:45.469
    Feb 27 15:33:45.659: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Feb 27 15:33:47.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:33:49.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:33:51.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:33:53.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:33:55.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:33:57.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:33:59.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:34:01.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:34:03.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:34:05.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:34:07.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 15:34:09.848: INFO: Waited 117.603057ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 02/27/23 15:34:09.889
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/27/23 15:34:09.893
    STEP: List APIServices 02/27/23 15:34:09.899
    Feb 27 15:34:09.905: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:10.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-9288" for this suite. 02/27/23 15:34:10.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:10.093
Feb 27 15:34:10.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:34:10.094
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:10.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:10.115
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb 27 15:34:10.127: INFO: Waiting up to 5m0s for pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83" in namespace "kubelet-test-9635" to be "running and ready"
Feb 27 15:34:10.130: INFO: Pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909378ms
Feb 27 15:34:10.130: INFO: The phase of Pod busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:34:12.134: INFO: Pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83": Phase="Running", Reason="", readiness=true. Elapsed: 2.007199331s
Feb 27 15:34:12.134: INFO: The phase of Pod busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 is Running (Ready = true)
Feb 27 15:34:12.134: INFO: Pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:12.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9635" for this suite. 02/27/23 15:34:12.148
------------------------------
• [2.062 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:10.093
    Feb 27 15:34:10.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:34:10.094
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:10.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:10.115
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb 27 15:34:10.127: INFO: Waiting up to 5m0s for pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83" in namespace "kubelet-test-9635" to be "running and ready"
    Feb 27 15:34:10.130: INFO: Pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909378ms
    Feb 27 15:34:10.130: INFO: The phase of Pod busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:34:12.134: INFO: Pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83": Phase="Running", Reason="", readiness=true. Elapsed: 2.007199331s
    Feb 27 15:34:12.134: INFO: The phase of Pod busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 is Running (Ready = true)
    Feb 27 15:34:12.134: INFO: Pod "busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:12.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9635" for this suite. 02/27/23 15:34:12.148
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:12.156
Feb 27 15:34:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replicaset 02/27/23 15:34:12.156
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:12.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:12.19
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/27/23 15:34:12.197
STEP: Verify that the required pods have come up. 02/27/23 15:34:12.202
Feb 27 15:34:12.205: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 15:34:17.209: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 15:34:17.209
STEP: Getting /status 02/27/23 15:34:17.21
Feb 27 15:34:17.214: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/27/23 15:34:17.214
Feb 27 15:34:17.225: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/27/23 15:34:17.225
Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: ADDED
Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 15:34:17.227: INFO: Found replicaset test-rs in namespace replicaset-1956 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 15:34:17.227: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/27/23 15:34:17.227
Feb 27 15:34:17.227: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 15:34:17.235: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/27/23 15:34:17.235
Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: ADDED
Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 15:34:17.237: INFO: Observed replicaset test-rs in namespace replicaset-1956 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 15:34:17.237: INFO: Found replicaset test-rs in namespace replicaset-1956 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 27 15:34:17.237: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:17.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1956" for this suite. 02/27/23 15:34:17.242
------------------------------
• [SLOW TEST] [5.093 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:12.156
    Feb 27 15:34:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replicaset 02/27/23 15:34:12.156
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:12.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:12.19
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/27/23 15:34:12.197
    STEP: Verify that the required pods have come up. 02/27/23 15:34:12.202
    Feb 27 15:34:12.205: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 15:34:17.209: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 15:34:17.209
    STEP: Getting /status 02/27/23 15:34:17.21
    Feb 27 15:34:17.214: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/27/23 15:34:17.214
    Feb 27 15:34:17.225: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/27/23 15:34:17.225
    Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: ADDED
    Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 15:34:17.227: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 15:34:17.227: INFO: Found replicaset test-rs in namespace replicaset-1956 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 15:34:17.227: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/27/23 15:34:17.227
    Feb 27 15:34:17.227: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 15:34:17.235: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/27/23 15:34:17.235
    Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: ADDED
    Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 15:34:17.237: INFO: Observed replicaset test-rs in namespace replicaset-1956 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 15:34:17.237: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 15:34:17.237: INFO: Found replicaset test-rs in namespace replicaset-1956 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb 27 15:34:17.237: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:17.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1956" for this suite. 02/27/23 15:34:17.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:17.249
Feb 27 15:34:17.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:34:17.25
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:17.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:17.273
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 15:34:17.276
Feb 27 15:34:17.287: INFO: Waiting up to 5m0s for pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f" in namespace "emptydir-1115" to be "Succeeded or Failed"
Feb 27 15:34:17.292: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.011704ms
Feb 27 15:34:19.298: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010248092s
Feb 27 15:34:21.297: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009937433s
STEP: Saw pod success 02/27/23 15:34:21.297
Feb 27 15:34:21.298: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f" satisfied condition "Succeeded or Failed"
Feb 27 15:34:21.301: INFO: Trying to get logs from node ip-172-31-3-182 pod pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f container test-container: <nil>
STEP: delete the pod 02/27/23 15:34:21.317
Feb 27 15:34:21.333: INFO: Waiting for pod pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f to disappear
Feb 27 15:34:21.337: INFO: Pod pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:21.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1115" for this suite. 02/27/23 15:34:21.341
------------------------------
• [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:17.249
    Feb 27 15:34:17.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:34:17.25
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:17.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:17.273
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 15:34:17.276
    Feb 27 15:34:17.287: INFO: Waiting up to 5m0s for pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f" in namespace "emptydir-1115" to be "Succeeded or Failed"
    Feb 27 15:34:17.292: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.011704ms
    Feb 27 15:34:19.298: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010248092s
    Feb 27 15:34:21.297: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009937433s
    STEP: Saw pod success 02/27/23 15:34:21.297
    Feb 27 15:34:21.298: INFO: Pod "pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f" satisfied condition "Succeeded or Failed"
    Feb 27 15:34:21.301: INFO: Trying to get logs from node ip-172-31-3-182 pod pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f container test-container: <nil>
    STEP: delete the pod 02/27/23 15:34:21.317
    Feb 27 15:34:21.333: INFO: Waiting for pod pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f to disappear
    Feb 27 15:34:21.337: INFO: Pod pod-3775658f-0eb2-4fbf-b3be-de0e6ea6913f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:21.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1115" for this suite. 02/27/23 15:34:21.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:21.349
Feb 27 15:34:21.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:34:21.35
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:21.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:21.374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-d90a86eb-24fc-436e-81ad-78cfc067b9db 02/27/23 15:34:21.377
STEP: Creating a pod to test consume configMaps 02/27/23 15:34:21.383
Feb 27 15:34:21.390: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24" in namespace "projected-7469" to be "Succeeded or Failed"
Feb 27 15:34:21.394: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24": Phase="Pending", Reason="", readiness=false. Elapsed: 3.96579ms
Feb 27 15:34:23.398: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008151106s
Feb 27 15:34:25.398: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00802s
STEP: Saw pod success 02/27/23 15:34:25.398
Feb 27 15:34:25.398: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24" satisfied condition "Succeeded or Failed"
Feb 27 15:34:25.402: INFO: Trying to get logs from node ip-172-31-3-182 pod pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:34:25.409
Feb 27 15:34:25.422: INFO: Waiting for pod pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24 to disappear
Feb 27 15:34:25.425: INFO: Pod pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:25.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7469" for this suite. 02/27/23 15:34:25.429
------------------------------
• [4.087 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:21.349
    Feb 27 15:34:21.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:34:21.35
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:21.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:21.374
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-d90a86eb-24fc-436e-81ad-78cfc067b9db 02/27/23 15:34:21.377
    STEP: Creating a pod to test consume configMaps 02/27/23 15:34:21.383
    Feb 27 15:34:21.390: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24" in namespace "projected-7469" to be "Succeeded or Failed"
    Feb 27 15:34:21.394: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24": Phase="Pending", Reason="", readiness=false. Elapsed: 3.96579ms
    Feb 27 15:34:23.398: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008151106s
    Feb 27 15:34:25.398: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00802s
    STEP: Saw pod success 02/27/23 15:34:25.398
    Feb 27 15:34:25.398: INFO: Pod "pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24" satisfied condition "Succeeded or Failed"
    Feb 27 15:34:25.402: INFO: Trying to get logs from node ip-172-31-3-182 pod pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:34:25.409
    Feb 27 15:34:25.422: INFO: Waiting for pod pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24 to disappear
    Feb 27 15:34:25.425: INFO: Pod pod-projected-configmaps-ac851ee8-c191-439a-90ca-8afbe6548a24 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:25.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7469" for this suite. 02/27/23 15:34:25.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:25.437
Feb 27 15:34:25.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename watch 02/27/23 15:34:25.437
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:25.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:25.457
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/27/23 15:34:25.46
STEP: modifying the configmap once 02/27/23 15:34:25.464
STEP: modifying the configmap a second time 02/27/23 15:34:25.472
STEP: deleting the configmap 02/27/23 15:34:25.48
STEP: creating a watch on configmaps from the resource version returned by the first update 02/27/23 15:34:25.488
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/27/23 15:34:25.489
Feb 27 15:34:25.489: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2111  5cad45e4-ecdc-4b6a-950a-fd9386382751 11919 0 2023-02-27 15:34:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 15:34:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:34:25.490: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2111  5cad45e4-ecdc-4b6a-950a-fd9386382751 11920 0 2023-02-27 15:34:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 15:34:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:25.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2111" for this suite. 02/27/23 15:34:25.494
------------------------------
• [0.064 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:25.437
    Feb 27 15:34:25.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename watch 02/27/23 15:34:25.437
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:25.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:25.457
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/27/23 15:34:25.46
    STEP: modifying the configmap once 02/27/23 15:34:25.464
    STEP: modifying the configmap a second time 02/27/23 15:34:25.472
    STEP: deleting the configmap 02/27/23 15:34:25.48
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/27/23 15:34:25.488
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/27/23 15:34:25.489
    Feb 27 15:34:25.489: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2111  5cad45e4-ecdc-4b6a-950a-fd9386382751 11919 0 2023-02-27 15:34:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 15:34:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:34:25.490: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2111  5cad45e4-ecdc-4b6a-950a-fd9386382751 11920 0 2023-02-27 15:34:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 15:34:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:25.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2111" for this suite. 02/27/23 15:34:25.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:25.501
Feb 27 15:34:25.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename disruption 02/27/23 15:34:25.501
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:25.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:25.521
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 02/27/23 15:34:25.53
STEP: Waiting for all pods to be running 02/27/23 15:34:27.566
Feb 27 15:34:27.578: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:29.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8984" for this suite. 02/27/23 15:34:29.589
------------------------------
• [4.094 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:25.501
    Feb 27 15:34:25.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename disruption 02/27/23 15:34:25.501
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:25.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:25.521
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 02/27/23 15:34:25.53
    STEP: Waiting for all pods to be running 02/27/23 15:34:27.566
    Feb 27 15:34:27.578: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:29.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8984" for this suite. 02/27/23 15:34:29.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:29.595
Feb 27 15:34:29.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-runtime 02/27/23 15:34:29.596
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:29.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:29.618
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 02/27/23 15:34:29.62
STEP: wait for the container to reach Succeeded 02/27/23 15:34:29.629
STEP: get the container status 02/27/23 15:34:33.65
STEP: the container should be terminated 02/27/23 15:34:33.654
STEP: the termination message should be set 02/27/23 15:34:33.654
Feb 27 15:34:33.654: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/27/23 15:34:33.654
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:33.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3253" for this suite. 02/27/23 15:34:33.675
------------------------------
• [4.087 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:29.595
    Feb 27 15:34:29.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-runtime 02/27/23 15:34:29.596
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:29.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:29.618
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 02/27/23 15:34:29.62
    STEP: wait for the container to reach Succeeded 02/27/23 15:34:29.629
    STEP: get the container status 02/27/23 15:34:33.65
    STEP: the container should be terminated 02/27/23 15:34:33.654
    STEP: the termination message should be set 02/27/23 15:34:33.654
    Feb 27 15:34:33.654: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/27/23 15:34:33.654
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:33.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3253" for this suite. 02/27/23 15:34:33.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:33.682
Feb 27 15:34:33.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-pred 02/27/23 15:34:33.683
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:33.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:33.701
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 15:34:33.705: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 15:34:33.713: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 15:34:33.717: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
Feb 27 15:34:33.723: INFO: pod-1 from disruption-8984 started at 2023-02-27 15:34:27 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.723: INFO: 	Container donothing ready: true, restart count 0
Feb 27 15:34:33.723: INFO: default-http-backend-kubernetes-worker-9b9488b5c-nx5j4 from ingress-nginx-kubernetes-worker started at 2023-02-27 15:32:38 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.723: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Feb 27 15:34:33.723: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.723: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:34:33.723: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.723: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 15:34:33.723: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:34:33.723: INFO: 	Container e2e ready: true, restart count 0
Feb 27 15:34:33.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:34:33.723: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:34:33.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:34:33.723: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:34:33.723: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
Feb 27 15:34:33.728: INFO: pod-csi-inline-volumes from csiinlinevolumes-2102 started at 2023-02-27 15:33:36 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.728: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Feb 27 15:34:33.728: INFO: pod-0 from disruption-8984 started at 2023-02-27 15:34:27 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.728: INFO: 	Container donothing ready: true, restart count 0
Feb 27 15:34:33.728: INFO: pod-2 from disruption-8984 started at 2023-02-27 15:34:27 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.728: INFO: 	Container donothing ready: true, restart count 0
Feb 27 15:34:33.728: INFO: nginx-ingress-controller-kubernetes-worker-4v9jp from ingress-nginx-kubernetes-worker started at 2023-02-27 15:33:04 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.728: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:34:33.728: INFO: busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 from kubelet-test-9635 started at 2023-02-27 15:34:10 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.728: INFO: 	Container busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 ready: true, restart count 0
Feb 27 15:34:33.728: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.728: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 15:34:33.728: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:34:33.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:34:33.728: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:34:33.728: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
Feb 27 15:34:33.734: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.734: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:34:33.734: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.734: INFO: 	Container coredns ready: true, restart count 0
Feb 27 15:34:33.734: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.734: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 27 15:34:33.734: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
Feb 27 15:34:33.734: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 15:34:33.734: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 27 15:34:33.734: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.734: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 15:34:33.734: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:34:33.734: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 27 15:34:33.734: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:34:33.734: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:34:33.734: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/27/23 15:34:33.734
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1747b857e0873a26], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 02/27/23 15:34:33.762
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:34.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-333" for this suite. 02/27/23 15:34:34.762
------------------------------
• [1.086 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:33.682
    Feb 27 15:34:33.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-pred 02/27/23 15:34:33.683
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:33.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:33.701
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 15:34:33.705: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 15:34:33.713: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 15:34:33.717: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
    Feb 27 15:34:33.723: INFO: pod-1 from disruption-8984 started at 2023-02-27 15:34:27 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.723: INFO: 	Container donothing ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: default-http-backend-kubernetes-worker-9b9488b5c-nx5j4 from ingress-nginx-kubernetes-worker started at 2023-02-27 15:32:38 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.723: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.723: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.723: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:34:33.723: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:34:33.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:34:33.723: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
    Feb 27 15:34:33.728: INFO: pod-csi-inline-volumes from csiinlinevolumes-2102 started at 2023-02-27 15:33:36 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.728: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Feb 27 15:34:33.728: INFO: pod-0 from disruption-8984 started at 2023-02-27 15:34:27 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.728: INFO: 	Container donothing ready: true, restart count 0
    Feb 27 15:34:33.728: INFO: pod-2 from disruption-8984 started at 2023-02-27 15:34:27 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.728: INFO: 	Container donothing ready: true, restart count 0
    Feb 27 15:34:33.728: INFO: nginx-ingress-controller-kubernetes-worker-4v9jp from ingress-nginx-kubernetes-worker started at 2023-02-27 15:33:04 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.728: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:34:33.728: INFO: busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 from kubelet-test-9635 started at 2023-02-27 15:34:10 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.728: INFO: 	Container busybox-scheduling-bc4cc27d-f212-4f4a-b3d6-626a67004a83 ready: true, restart count 0
    Feb 27 15:34:33.728: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.728: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 15:34:33.728: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:34:33.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:34:33.728: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:34:33.728: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
    Feb 27 15:34:33.734: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.734: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.734: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.734: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
    Feb 27 15:34:33.734: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.734: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:34:33.734: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:34:33.734: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:34:33.734: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/27/23 15:34:33.734
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1747b857e0873a26], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 02/27/23 15:34:33.762
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:34.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-333" for this suite. 02/27/23 15:34:34.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:34.77
Feb 27 15:34:34.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 15:34:34.771
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:34.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:34.789
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/27/23 15:34:34.793
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/27/23 15:34:34.793
STEP: creating a pod to probe DNS 02/27/23 15:34:34.793
STEP: submitting the pod to kubernetes 02/27/23 15:34:34.793
Feb 27 15:34:34.803: INFO: Waiting up to 15m0s for pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012" in namespace "dns-3059" to be "running"
Feb 27 15:34:34.810: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493672ms
Feb 27 15:34:36.814: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010404296s
Feb 27 15:34:38.814: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011077539s
Feb 27 15:34:40.813: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009546318s
Feb 27 15:34:42.815: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Running", Reason="", readiness=true. Elapsed: 8.011546654s
Feb 27 15:34:42.815: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012" satisfied condition "running"
STEP: retrieving the pod 02/27/23 15:34:42.815
STEP: looking for the results for each expected name from probers 02/27/23 15:34:42.818
Feb 27 15:34:42.836: INFO: DNS probes using dns-3059/dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012 succeeded

STEP: deleting the pod 02/27/23 15:34:42.836
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:42.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3059" for this suite. 02/27/23 15:34:42.855
------------------------------
• [SLOW TEST] [8.097 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:34.77
    Feb 27 15:34:34.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 15:34:34.771
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:34.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:34.789
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/27/23 15:34:34.793
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/27/23 15:34:34.793
    STEP: creating a pod to probe DNS 02/27/23 15:34:34.793
    STEP: submitting the pod to kubernetes 02/27/23 15:34:34.793
    Feb 27 15:34:34.803: INFO: Waiting up to 15m0s for pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012" in namespace "dns-3059" to be "running"
    Feb 27 15:34:34.810: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493672ms
    Feb 27 15:34:36.814: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010404296s
    Feb 27 15:34:38.814: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011077539s
    Feb 27 15:34:40.813: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009546318s
    Feb 27 15:34:42.815: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012": Phase="Running", Reason="", readiness=true. Elapsed: 8.011546654s
    Feb 27 15:34:42.815: INFO: Pod "dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 15:34:42.815
    STEP: looking for the results for each expected name from probers 02/27/23 15:34:42.818
    Feb 27 15:34:42.836: INFO: DNS probes using dns-3059/dns-test-d18da2a6-4f02-46d0-84c5-ddd3df712012 succeeded

    STEP: deleting the pod 02/27/23 15:34:42.836
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:42.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3059" for this suite. 02/27/23 15:34:42.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:42.868
Feb 27 15:34:42.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:34:42.868
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:42.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:42.886
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 02/27/23 15:34:42.889
Feb 27 15:34:42.889: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2813 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/27/23 15:34:42.922
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2813" for this suite. 02/27/23 15:34:42.932
------------------------------
• [0.071 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:42.868
    Feb 27 15:34:42.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:34:42.868
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:42.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:42.886
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 02/27/23 15:34:42.889
    Feb 27 15:34:42.889: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2813 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/27/23 15:34:42.922
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2813" for this suite. 02/27/23 15:34:42.932
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:42.938
Feb 27 15:34:42.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:34:42.939
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:42.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:42.958
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/27/23 15:34:42.962
Feb 27 15:34:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:34:44.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:50.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4792" for this suite. 02/27/23 15:34:50.106
------------------------------
• [SLOW TEST] [7.173 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:42.938
    Feb 27 15:34:42.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:34:42.939
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:42.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:42.958
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/27/23 15:34:42.962
    Feb 27 15:34:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:34:44.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:50.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4792" for this suite. 02/27/23 15:34:50.106
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:50.111
Feb 27 15:34:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replicaset 02/27/23 15:34:50.112
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:50.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:50.127
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/27/23 15:34:50.13
STEP: Verify that the required pods have come up 02/27/23 15:34:50.134
Feb 27 15:34:50.137: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 27 15:34:55.141: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/27/23 15:34:55.141
Feb 27 15:34:55.143: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/27/23 15:34:55.143
STEP: DeleteCollection of the ReplicaSets 02/27/23 15:34:55.146
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/27/23 15:34:55.154
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:34:55.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6574" for this suite. 02/27/23 15:34:55.167
------------------------------
• [SLOW TEST] [5.062 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:50.111
    Feb 27 15:34:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replicaset 02/27/23 15:34:50.112
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:50.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:50.127
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/27/23 15:34:50.13
    STEP: Verify that the required pods have come up 02/27/23 15:34:50.134
    Feb 27 15:34:50.137: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb 27 15:34:55.141: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/27/23 15:34:55.141
    Feb 27 15:34:55.143: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/27/23 15:34:55.143
    STEP: DeleteCollection of the ReplicaSets 02/27/23 15:34:55.146
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/27/23 15:34:55.154
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:34:55.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6574" for this suite. 02/27/23 15:34:55.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:34:55.174
Feb 27 15:34:55.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:34:55.175
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:55.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:55.194
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 02/27/23 15:34:55.196
Feb 27 15:34:55.203: INFO: Waiting up to 5m0s for pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8" in namespace "projected-6159" to be "running and ready"
Feb 27 15:34:55.206: INFO: Pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.764359ms
Feb 27 15:34:55.206: INFO: The phase of Pod labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:34:57.210: INFO: Pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.00632061s
Feb 27 15:34:57.210: INFO: The phase of Pod labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8 is Running (Ready = true)
Feb 27 15:34:57.210: INFO: Pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8" satisfied condition "running and ready"
Feb 27 15:34:57.734: INFO: Successfully updated pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 15:35:01.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6159" for this suite. 02/27/23 15:35:01.759
------------------------------
• [SLOW TEST] [6.590 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:34:55.174
    Feb 27 15:34:55.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:34:55.175
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:34:55.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:34:55.194
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 02/27/23 15:34:55.196
    Feb 27 15:34:55.203: INFO: Waiting up to 5m0s for pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8" in namespace "projected-6159" to be "running and ready"
    Feb 27 15:34:55.206: INFO: Pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.764359ms
    Feb 27 15:34:55.206: INFO: The phase of Pod labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:34:57.210: INFO: Pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8": Phase="Running", Reason="", readiness=true. Elapsed: 2.00632061s
    Feb 27 15:34:57.210: INFO: The phase of Pod labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8 is Running (Ready = true)
    Feb 27 15:34:57.210: INFO: Pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8" satisfied condition "running and ready"
    Feb 27 15:34:57.734: INFO: Successfully updated pod "labelsupdateeda73d74-ab6c-4985-9f4e-29ca83ebbed8"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:35:01.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6159" for this suite. 02/27/23 15:35:01.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:35:01.764
Feb 27 15:35:01.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:35:01.765
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:35:01.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:35:01.779
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:35:01.782
Feb 27 15:35:01.789: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e" in namespace "downward-api-4662" to be "Succeeded or Failed"
Feb 27 15:35:01.797: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.182467ms
Feb 27 15:35:03.801: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012528019s
Feb 27 15:35:05.804: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014752298s
STEP: Saw pod success 02/27/23 15:35:05.804
Feb 27 15:35:05.804: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e" satisfied condition "Succeeded or Failed"
Feb 27 15:35:05.807: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e container client-container: <nil>
STEP: delete the pod 02/27/23 15:35:05.812
Feb 27 15:35:05.825: INFO: Waiting for pod downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e to disappear
Feb 27 15:35:05.828: INFO: Pod downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 15:35:05.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4662" for this suite. 02/27/23 15:35:05.83
------------------------------
• [4.071 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:35:01.764
    Feb 27 15:35:01.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:35:01.765
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:35:01.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:35:01.779
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:35:01.782
    Feb 27 15:35:01.789: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e" in namespace "downward-api-4662" to be "Succeeded or Failed"
    Feb 27 15:35:01.797: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.182467ms
    Feb 27 15:35:03.801: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012528019s
    Feb 27 15:35:05.804: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014752298s
    STEP: Saw pod success 02/27/23 15:35:05.804
    Feb 27 15:35:05.804: INFO: Pod "downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e" satisfied condition "Succeeded or Failed"
    Feb 27 15:35:05.807: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e container client-container: <nil>
    STEP: delete the pod 02/27/23 15:35:05.812
    Feb 27 15:35:05.825: INFO: Waiting for pod downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e to disappear
    Feb 27 15:35:05.828: INFO: Pod downwardapi-volume-d967b1c0-91c7-4422-a7b5-c60ff67e7f7e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:35:05.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4662" for this suite. 02/27/23 15:35:05.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:35:05.836
Feb 27 15:35:05.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename job 02/27/23 15:35:05.836
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:35:05.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:35:05.851
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 02/27/23 15:35:05.853
STEP: Ensuring active pods == parallelism 02/27/23 15:35:05.858
STEP: delete a job 02/27/23 15:35:07.862
STEP: deleting Job.batch foo in namespace job-1830, will wait for the garbage collector to delete the pods 02/27/23 15:35:07.862
Feb 27 15:35:07.923: INFO: Deleting Job.batch foo took: 7.669277ms
Feb 27 15:35:08.024: INFO: Terminating Job.batch foo pods took: 100.824302ms
STEP: Ensuring job was deleted 02/27/23 15:35:40.224
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 15:35:40.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1830" for this suite. 02/27/23 15:35:40.23
------------------------------
• [SLOW TEST] [34.400 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:35:05.836
    Feb 27 15:35:05.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename job 02/27/23 15:35:05.836
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:35:05.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:35:05.851
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 02/27/23 15:35:05.853
    STEP: Ensuring active pods == parallelism 02/27/23 15:35:05.858
    STEP: delete a job 02/27/23 15:35:07.862
    STEP: deleting Job.batch foo in namespace job-1830, will wait for the garbage collector to delete the pods 02/27/23 15:35:07.862
    Feb 27 15:35:07.923: INFO: Deleting Job.batch foo took: 7.669277ms
    Feb 27 15:35:08.024: INFO: Terminating Job.batch foo pods took: 100.824302ms
    STEP: Ensuring job was deleted 02/27/23 15:35:40.224
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:35:40.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1830" for this suite. 02/27/23 15:35:40.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:35:40.236
Feb 27 15:35:40.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename subpath 02/27/23 15:35:40.237
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:35:40.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:35:40.252
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 15:35:40.255
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-wr4x 02/27/23 15:35:40.263
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 15:35:40.263
Feb 27 15:35:40.273: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wr4x" in namespace "subpath-6067" to be "Succeeded or Failed"
Feb 27 15:35:40.278: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.09981ms
Feb 27 15:35:42.281: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008131457s
Feb 27 15:35:44.284: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 4.010504615s
Feb 27 15:35:46.281: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 6.00818945s
Feb 27 15:35:48.284: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 8.010660336s
Feb 27 15:35:50.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 10.008644525s
Feb 27 15:35:52.283: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 12.010225327s
Feb 27 15:35:54.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 14.008773148s
Feb 27 15:35:56.284: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 16.010426724s
Feb 27 15:35:58.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 18.008451073s
Feb 27 15:36:00.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 20.008571738s
Feb 27 15:36:02.283: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=false. Elapsed: 22.009800664s
Feb 27 15:36:04.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00886376s
STEP: Saw pod success 02/27/23 15:36:04.282
Feb 27 15:36:04.282: INFO: Pod "pod-subpath-test-secret-wr4x" satisfied condition "Succeeded or Failed"
Feb 27 15:36:04.285: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-secret-wr4x container test-container-subpath-secret-wr4x: <nil>
STEP: delete the pod 02/27/23 15:36:04.291
Feb 27 15:36:04.301: INFO: Waiting for pod pod-subpath-test-secret-wr4x to disappear
Feb 27 15:36:04.304: INFO: Pod pod-subpath-test-secret-wr4x no longer exists
STEP: Deleting pod pod-subpath-test-secret-wr4x 02/27/23 15:36:04.304
Feb 27 15:36:04.304: INFO: Deleting pod "pod-subpath-test-secret-wr4x" in namespace "subpath-6067"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 15:36:04.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6067" for this suite. 02/27/23 15:36:04.31
------------------------------
• [SLOW TEST] [24.079 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:35:40.236
    Feb 27 15:35:40.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename subpath 02/27/23 15:35:40.237
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:35:40.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:35:40.252
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 15:35:40.255
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-wr4x 02/27/23 15:35:40.263
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 15:35:40.263
    Feb 27 15:35:40.273: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wr4x" in namespace "subpath-6067" to be "Succeeded or Failed"
    Feb 27 15:35:40.278: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.09981ms
    Feb 27 15:35:42.281: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008131457s
    Feb 27 15:35:44.284: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 4.010504615s
    Feb 27 15:35:46.281: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 6.00818945s
    Feb 27 15:35:48.284: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 8.010660336s
    Feb 27 15:35:50.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 10.008644525s
    Feb 27 15:35:52.283: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 12.010225327s
    Feb 27 15:35:54.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 14.008773148s
    Feb 27 15:35:56.284: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 16.010426724s
    Feb 27 15:35:58.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 18.008451073s
    Feb 27 15:36:00.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=true. Elapsed: 20.008571738s
    Feb 27 15:36:02.283: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Running", Reason="", readiness=false. Elapsed: 22.009800664s
    Feb 27 15:36:04.282: INFO: Pod "pod-subpath-test-secret-wr4x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00886376s
    STEP: Saw pod success 02/27/23 15:36:04.282
    Feb 27 15:36:04.282: INFO: Pod "pod-subpath-test-secret-wr4x" satisfied condition "Succeeded or Failed"
    Feb 27 15:36:04.285: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-secret-wr4x container test-container-subpath-secret-wr4x: <nil>
    STEP: delete the pod 02/27/23 15:36:04.291
    Feb 27 15:36:04.301: INFO: Waiting for pod pod-subpath-test-secret-wr4x to disappear
    Feb 27 15:36:04.304: INFO: Pod pod-subpath-test-secret-wr4x no longer exists
    STEP: Deleting pod pod-subpath-test-secret-wr4x 02/27/23 15:36:04.304
    Feb 27 15:36:04.304: INFO: Deleting pod "pod-subpath-test-secret-wr4x" in namespace "subpath-6067"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:36:04.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6067" for this suite. 02/27/23 15:36:04.31
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:36:04.315
Feb 27 15:36:04.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 15:36:04.316
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:04.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:04.33
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Feb 27 15:36:04.341: INFO: Waiting up to 2m0s for pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" in namespace "var-expansion-7348" to be "container 0 failed with reason CreateContainerConfigError"
Feb 27 15:36:04.345: INFO: Pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330162ms
Feb 27 15:36:06.348: INFO: Pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00757651s
Feb 27 15:36:06.348: INFO: Pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 27 15:36:06.348: INFO: Deleting pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" in namespace "var-expansion-7348"
Feb 27 15:36:06.354: INFO: Wait up to 5m0s for pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 15:36:08.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7348" for this suite. 02/27/23 15:36:08.364
------------------------------
• [4.054 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:36:04.315
    Feb 27 15:36:04.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 15:36:04.316
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:04.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:04.33
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Feb 27 15:36:04.341: INFO: Waiting up to 2m0s for pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" in namespace "var-expansion-7348" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 27 15:36:04.345: INFO: Pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.330162ms
    Feb 27 15:36:06.348: INFO: Pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00757651s
    Feb 27 15:36:06.348: INFO: Pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 27 15:36:06.348: INFO: Deleting pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" in namespace "var-expansion-7348"
    Feb 27 15:36:06.354: INFO: Wait up to 5m0s for pod "var-expansion-4d48e686-c206-4619-a39a-536d6f8f2df1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:36:08.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7348" for this suite. 02/27/23 15:36:08.364
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:36:08.369
Feb 27 15:36:08.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 15:36:08.37
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:08.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:08.385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-c1731110-1cea-494e-a131-a65cc8f6b2e6 02/27/23 15:36:08.387
STEP: Creating a pod to test consume secrets 02/27/23 15:36:08.391
Feb 27 15:36:08.398: INFO: Waiting up to 5m0s for pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff" in namespace "secrets-8163" to be "Succeeded or Failed"
Feb 27 15:36:08.404: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433334ms
Feb 27 15:36:10.407: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008356905s
Feb 27 15:36:12.407: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008419651s
STEP: Saw pod success 02/27/23 15:36:12.407
Feb 27 15:36:12.407: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff" satisfied condition "Succeeded or Failed"
Feb 27 15:36:12.410: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:36:12.415
Feb 27 15:36:12.426: INFO: Waiting for pod pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff to disappear
Feb 27 15:36:12.428: INFO: Pod pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 15:36:12.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8163" for this suite. 02/27/23 15:36:12.431
------------------------------
• [4.067 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:36:08.369
    Feb 27 15:36:08.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 15:36:08.37
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:08.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:08.385
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-c1731110-1cea-494e-a131-a65cc8f6b2e6 02/27/23 15:36:08.387
    STEP: Creating a pod to test consume secrets 02/27/23 15:36:08.391
    Feb 27 15:36:08.398: INFO: Waiting up to 5m0s for pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff" in namespace "secrets-8163" to be "Succeeded or Failed"
    Feb 27 15:36:08.404: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433334ms
    Feb 27 15:36:10.407: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008356905s
    Feb 27 15:36:12.407: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008419651s
    STEP: Saw pod success 02/27/23 15:36:12.407
    Feb 27 15:36:12.407: INFO: Pod "pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff" satisfied condition "Succeeded or Failed"
    Feb 27 15:36:12.410: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:36:12.415
    Feb 27 15:36:12.426: INFO: Waiting for pod pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff to disappear
    Feb 27 15:36:12.428: INFO: Pod pod-secrets-e406399f-28bb-4ed1-a90e-302328a1dcff no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:36:12.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8163" for this suite. 02/27/23 15:36:12.431
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:36:12.437
Feb 27 15:36:12.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/27/23 15:36:12.437
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:12.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:12.452
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/27/23 15:36:12.454
STEP: Creating hostNetwork=false pod 02/27/23 15:36:12.454
Feb 27 15:36:12.462: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7286" to be "running and ready"
Feb 27 15:36:12.465: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20078ms
Feb 27 15:36:12.465: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:36:14.470: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007628047s
Feb 27 15:36:14.470: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb 27 15:36:14.470: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/27/23 15:36:14.472
Feb 27 15:36:14.479: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7286" to be "running and ready"
Feb 27 15:36:14.484: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100877ms
Feb 27 15:36:14.484: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:36:16.488: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008266297s
Feb 27 15:36:16.488: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb 27 15:36:16.488: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/27/23 15:36:16.49
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/27/23 15:36:16.49
Feb 27 15:36:16.491: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.491: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.491: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 15:36:16.579: INFO: Exec stderr: ""
Feb 27 15:36:16.579: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.579: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.579: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 15:36:16.654: INFO: Exec stderr: ""
Feb 27 15:36:16.655: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.655: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.655: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 15:36:16.692: INFO: Exec stderr: ""
Feb 27 15:36:16.692: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.692: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.692: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 15:36:16.743: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/27/23 15:36:16.743
Feb 27 15:36:16.743: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.743: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.743: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 27 15:36:16.791: INFO: Exec stderr: ""
Feb 27 15:36:16.791: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.791: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.791: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 27 15:36:16.838: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/27/23 15:36:16.838
Feb 27 15:36:16.838: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.838: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.838: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 15:36:16.910: INFO: Exec stderr: ""
Feb 27 15:36:16.911: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.911: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.911: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 15:36:16.964: INFO: Exec stderr: ""
Feb 27 15:36:16.964: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:16.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:16.964: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:16.964: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 15:36:17.018: INFO: Exec stderr: ""
Feb 27 15:36:17.018: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:36:17.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:36:17.019: INFO: ExecWithOptions: Clientset creation
Feb 27 15:36:17.019: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 15:36:17.086: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Feb 27 15:36:17.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7286" for this suite. 02/27/23 15:36:17.089
------------------------------
• [4.658 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:36:12.437
    Feb 27 15:36:12.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/27/23 15:36:12.437
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:12.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:12.452
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/27/23 15:36:12.454
    STEP: Creating hostNetwork=false pod 02/27/23 15:36:12.454
    Feb 27 15:36:12.462: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7286" to be "running and ready"
    Feb 27 15:36:12.465: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20078ms
    Feb 27 15:36:12.465: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:36:14.470: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007628047s
    Feb 27 15:36:14.470: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb 27 15:36:14.470: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/27/23 15:36:14.472
    Feb 27 15:36:14.479: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7286" to be "running and ready"
    Feb 27 15:36:14.484: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100877ms
    Feb 27 15:36:14.484: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:36:16.488: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008266297s
    Feb 27 15:36:16.488: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb 27 15:36:16.488: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/27/23 15:36:16.49
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/27/23 15:36:16.49
    Feb 27 15:36:16.491: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.491: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.491: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 15:36:16.579: INFO: Exec stderr: ""
    Feb 27 15:36:16.579: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.579: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.579: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 15:36:16.654: INFO: Exec stderr: ""
    Feb 27 15:36:16.655: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.655: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.655: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 15:36:16.692: INFO: Exec stderr: ""
    Feb 27 15:36:16.692: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.692: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.692: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 15:36:16.743: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/27/23 15:36:16.743
    Feb 27 15:36:16.743: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.743: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.743: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 27 15:36:16.791: INFO: Exec stderr: ""
    Feb 27 15:36:16.791: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.791: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.791: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 27 15:36:16.838: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/27/23 15:36:16.838
    Feb 27 15:36:16.838: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.838: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.838: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 15:36:16.910: INFO: Exec stderr: ""
    Feb 27 15:36:16.911: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.911: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.911: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 15:36:16.964: INFO: Exec stderr: ""
    Feb 27 15:36:16.964: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:16.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:16.964: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:16.964: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 15:36:17.018: INFO: Exec stderr: ""
    Feb 27 15:36:17.018: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7286 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:36:17.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:36:17.019: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:36:17.019: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7286/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 15:36:17.086: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:36:17.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-7286" for this suite. 02/27/23 15:36:17.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:36:17.096
Feb 27 15:36:17.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 15:36:17.096
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:17.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:17.11
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 02/27/23 15:36:17.113
Feb 27 15:36:17.120: INFO: Waiting up to 5m0s for pod "pod-c9hs6" in namespace "pods-9804" to be "running"
Feb 27 15:36:17.122: INFO: Pod "pod-c9hs6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412873ms
Feb 27 15:36:19.126: INFO: Pod "pod-c9hs6": Phase="Running", Reason="", readiness=true. Elapsed: 2.00620766s
Feb 27 15:36:19.126: INFO: Pod "pod-c9hs6" satisfied condition "running"
STEP: patching /status 02/27/23 15:36:19.126
Feb 27 15:36:19.132: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 15:36:19.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9804" for this suite. 02/27/23 15:36:19.135
------------------------------
• [2.045 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:36:17.096
    Feb 27 15:36:17.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 15:36:17.096
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:17.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:17.11
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 02/27/23 15:36:17.113
    Feb 27 15:36:17.120: INFO: Waiting up to 5m0s for pod "pod-c9hs6" in namespace "pods-9804" to be "running"
    Feb 27 15:36:17.122: INFO: Pod "pod-c9hs6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.412873ms
    Feb 27 15:36:19.126: INFO: Pod "pod-c9hs6": Phase="Running", Reason="", readiness=true. Elapsed: 2.00620766s
    Feb 27 15:36:19.126: INFO: Pod "pod-c9hs6" satisfied condition "running"
    STEP: patching /status 02/27/23 15:36:19.126
    Feb 27 15:36:19.132: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:36:19.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9804" for this suite. 02/27/23 15:36:19.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:36:19.141
Feb 27 15:36:19.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svcaccounts 02/27/23 15:36:19.141
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:19.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:19.155
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Feb 27 15:36:19.168: INFO: created pod
Feb 27 15:36:19.168: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3868" to be "Succeeded or Failed"
Feb 27 15:36:19.173: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.584944ms
Feb 27 15:36:21.177: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008622772s
Feb 27 15:36:23.176: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007573402s
STEP: Saw pod success 02/27/23 15:36:23.176
Feb 27 15:36:23.176: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 27 15:36:53.176: INFO: polling logs
Feb 27 15:36:53.181: INFO: Pod logs: 
I0227 15:36:19.872361       1 log.go:198] OK: Got token
I0227 15:36:19.872391       1 log.go:198] validating with in-cluster discovery
I0227 15:36:19.872561       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0227 15:36:19.872580       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3868:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677512779, NotBefore:1677512179, IssuedAt:1677512179, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3868", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"43959625-3a27-411b-ae99-f4cb34c23c45"}}}
I0227 15:36:19.879989       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0227 15:36:19.883808       1 log.go:198] OK: Validated signature on JWT
I0227 15:36:19.883868       1 log.go:198] OK: Got valid claims from token!
I0227 15:36:19.883887       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3868:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677512779, NotBefore:1677512179, IssuedAt:1677512179, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3868", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"43959625-3a27-411b-ae99-f4cb34c23c45"}}}

Feb 27 15:36:53.181: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 15:36:53.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3868" for this suite. 02/27/23 15:36:53.19
------------------------------
• [SLOW TEST] [34.056 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:36:19.141
    Feb 27 15:36:19.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 15:36:19.141
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:19.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:19.155
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Feb 27 15:36:19.168: INFO: created pod
    Feb 27 15:36:19.168: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3868" to be "Succeeded or Failed"
    Feb 27 15:36:19.173: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.584944ms
    Feb 27 15:36:21.177: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008622772s
    Feb 27 15:36:23.176: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007573402s
    STEP: Saw pod success 02/27/23 15:36:23.176
    Feb 27 15:36:23.176: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb 27 15:36:53.176: INFO: polling logs
    Feb 27 15:36:53.181: INFO: Pod logs: 
    I0227 15:36:19.872361       1 log.go:198] OK: Got token
    I0227 15:36:19.872391       1 log.go:198] validating with in-cluster discovery
    I0227 15:36:19.872561       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0227 15:36:19.872580       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3868:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677512779, NotBefore:1677512179, IssuedAt:1677512179, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3868", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"43959625-3a27-411b-ae99-f4cb34c23c45"}}}
    I0227 15:36:19.879989       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0227 15:36:19.883808       1 log.go:198] OK: Validated signature on JWT
    I0227 15:36:19.883868       1 log.go:198] OK: Got valid claims from token!
    I0227 15:36:19.883887       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3868:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677512779, NotBefore:1677512179, IssuedAt:1677512179, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3868", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"43959625-3a27-411b-ae99-f4cb34c23c45"}}}

    Feb 27 15:36:53.181: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:36:53.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3868" for this suite. 02/27/23 15:36:53.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:36:53.197
Feb 27 15:36:53.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 15:36:53.198
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:53.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:53.346
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 15:36:53.351
Feb 27 15:36:53.358: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5574" to be "running and ready"
Feb 27 15:36:53.360: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51901ms
Feb 27 15:36:53.360: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:36:55.364: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006227935s
Feb 27 15:36:55.364: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 15:36:55.364: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 02/27/23 15:36:55.367
Feb 27 15:36:55.371: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5574" to be "running and ready"
Feb 27 15:36:55.374: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074888ms
Feb 27 15:36:55.374: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:36:57.377: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006800358s
Feb 27 15:36:57.378: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb 27 15:36:57.378: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/27/23 15:36:57.38
Feb 27 15:36:57.386: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 15:36:57.389: INFO: Pod pod-with-prestop-http-hook still exists
Feb 27 15:36:59.390: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 15:36:59.393: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/27/23 15:36:59.393
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 15:36:59.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5574" for this suite. 02/27/23 15:36:59.406
------------------------------
• [SLOW TEST] [6.215 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:36:53.197
    Feb 27 15:36:53.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 15:36:53.198
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:53.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:53.346
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 15:36:53.351
    Feb 27 15:36:53.358: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5574" to be "running and ready"
    Feb 27 15:36:53.360: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51901ms
    Feb 27 15:36:53.360: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:36:55.364: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006227935s
    Feb 27 15:36:55.364: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 15:36:55.364: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 02/27/23 15:36:55.367
    Feb 27 15:36:55.371: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5574" to be "running and ready"
    Feb 27 15:36:55.374: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074888ms
    Feb 27 15:36:55.374: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:36:57.377: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006800358s
    Feb 27 15:36:57.378: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb 27 15:36:57.378: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/27/23 15:36:57.38
    Feb 27 15:36:57.386: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 15:36:57.389: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 27 15:36:59.390: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 15:36:59.393: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/27/23 15:36:59.393
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:36:59.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5574" for this suite. 02/27/23 15:36:59.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:36:59.413
Feb 27 15:36:59.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 15:36:59.413
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:59.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:59.427
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 02/27/23 15:37:16.433
STEP: Creating a ResourceQuota 02/27/23 15:37:21.436
STEP: Ensuring resource quota status is calculated 02/27/23 15:37:21.441
STEP: Creating a ConfigMap 02/27/23 15:37:23.444
STEP: Ensuring resource quota status captures configMap creation 02/27/23 15:37:23.453
STEP: Deleting a ConfigMap 02/27/23 15:37:25.458
STEP: Ensuring resource quota status released usage 02/27/23 15:37:25.464
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:27.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7195" for this suite. 02/27/23 15:37:27.47
------------------------------
• [SLOW TEST] [28.063 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:36:59.413
    Feb 27 15:36:59.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 15:36:59.413
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:36:59.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:36:59.427
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 02/27/23 15:37:16.433
    STEP: Creating a ResourceQuota 02/27/23 15:37:21.436
    STEP: Ensuring resource quota status is calculated 02/27/23 15:37:21.441
    STEP: Creating a ConfigMap 02/27/23 15:37:23.444
    STEP: Ensuring resource quota status captures configMap creation 02/27/23 15:37:23.453
    STEP: Deleting a ConfigMap 02/27/23 15:37:25.458
    STEP: Ensuring resource quota status released usage 02/27/23 15:37:25.464
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:27.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7195" for this suite. 02/27/23 15:37:27.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:27.477
Feb 27 15:37:27.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 15:37:27.478
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:27.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:27.495
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 02/27/23 15:37:27.497
STEP: submitting the pod to kubernetes 02/27/23 15:37:27.497
Feb 27 15:37:27.504: INFO: Waiting up to 5m0s for pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" in namespace "pods-6185" to be "running and ready"
Feb 27 15:37:27.510: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.471723ms
Feb 27 15:37:27.510: INFO: The phase of Pod pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:37:29.514: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.00977186s
Feb 27 15:37:29.514: INFO: The phase of Pod pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7 is Running (Ready = true)
Feb 27 15:37:29.514: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/27/23 15:37:29.517
STEP: updating the pod 02/27/23 15:37:29.519
Feb 27 15:37:30.031: INFO: Successfully updated pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7"
Feb 27 15:37:30.031: INFO: Waiting up to 5m0s for pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" in namespace "pods-6185" to be "running"
Feb 27 15:37:30.033: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.219064ms
Feb 27 15:37:30.033: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/27/23 15:37:30.033
Feb 27 15:37:30.037: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:30.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6185" for this suite. 02/27/23 15:37:30.04
------------------------------
• [2.569 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:27.477
    Feb 27 15:37:27.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 15:37:27.478
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:27.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:27.495
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 02/27/23 15:37:27.497
    STEP: submitting the pod to kubernetes 02/27/23 15:37:27.497
    Feb 27 15:37:27.504: INFO: Waiting up to 5m0s for pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" in namespace "pods-6185" to be "running and ready"
    Feb 27 15:37:27.510: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.471723ms
    Feb 27 15:37:27.510: INFO: The phase of Pod pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:37:29.514: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.00977186s
    Feb 27 15:37:29.514: INFO: The phase of Pod pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7 is Running (Ready = true)
    Feb 27 15:37:29.514: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/27/23 15:37:29.517
    STEP: updating the pod 02/27/23 15:37:29.519
    Feb 27 15:37:30.031: INFO: Successfully updated pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7"
    Feb 27 15:37:30.031: INFO: Waiting up to 5m0s for pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" in namespace "pods-6185" to be "running"
    Feb 27 15:37:30.033: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.219064ms
    Feb 27 15:37:30.033: INFO: Pod "pod-update-df2a2b24-2600-43b3-a322-d20f07bab5b7" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/27/23 15:37:30.033
    Feb 27 15:37:30.037: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:30.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6185" for this suite. 02/27/23 15:37:30.04
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:30.046
Feb 27 15:37:30.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:37:30.047
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:30.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:30.061
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-0cd849a2-ff49-46b6-8650-c4173892fc27 02/27/23 15:37:30.063
STEP: Creating a pod to test consume secrets 02/27/23 15:37:30.068
Feb 27 15:37:30.076: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3" in namespace "projected-9871" to be "Succeeded or Failed"
Feb 27 15:37:30.081: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.759981ms
Feb 27 15:37:32.085: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0088409s
Feb 27 15:37:34.086: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010219991s
STEP: Saw pod success 02/27/23 15:37:34.086
Feb 27 15:37:34.086: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3" satisfied condition "Succeeded or Failed"
Feb 27 15:37:34.089: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:37:34.094
Feb 27 15:37:34.107: INFO: Waiting for pod pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3 to disappear
Feb 27 15:37:34.109: INFO: Pod pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:34.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9871" for this suite. 02/27/23 15:37:34.112
------------------------------
• [4.072 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:30.046
    Feb 27 15:37:30.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:37:30.047
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:30.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:30.061
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-0cd849a2-ff49-46b6-8650-c4173892fc27 02/27/23 15:37:30.063
    STEP: Creating a pod to test consume secrets 02/27/23 15:37:30.068
    Feb 27 15:37:30.076: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3" in namespace "projected-9871" to be "Succeeded or Failed"
    Feb 27 15:37:30.081: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.759981ms
    Feb 27 15:37:32.085: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0088409s
    Feb 27 15:37:34.086: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010219991s
    STEP: Saw pod success 02/27/23 15:37:34.086
    Feb 27 15:37:34.086: INFO: Pod "pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3" satisfied condition "Succeeded or Failed"
    Feb 27 15:37:34.089: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:37:34.094
    Feb 27 15:37:34.107: INFO: Waiting for pod pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3 to disappear
    Feb 27 15:37:34.109: INFO: Pod pod-projected-secrets-0bca927f-d35a-4c60-9753-3dbf2a4e9cf3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:34.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9871" for this suite. 02/27/23 15:37:34.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:34.118
Feb 27 15:37:34.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:37:34.119
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:34.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:34.132
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 02/27/23 15:37:34.135
Feb 27 15:37:34.141: INFO: Waiting up to 5m0s for pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c" in namespace "downward-api-1861" to be "running and ready"
Feb 27 15:37:34.147: INFO: Pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.758522ms
Feb 27 15:37:34.147: INFO: The phase of Pod annotationupdate9b150ece-0432-4d8f-8650-f7425019602c is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:37:36.151: INFO: Pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00971565s
Feb 27 15:37:36.151: INFO: The phase of Pod annotationupdate9b150ece-0432-4d8f-8650-f7425019602c is Running (Ready = true)
Feb 27 15:37:36.151: INFO: Pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c" satisfied condition "running and ready"
Feb 27 15:37:36.668: INFO: Successfully updated pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:40.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1861" for this suite. 02/27/23 15:37:40.693
------------------------------
• [SLOW TEST] [6.580 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:34.118
    Feb 27 15:37:34.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:37:34.119
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:34.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:34.132
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 02/27/23 15:37:34.135
    Feb 27 15:37:34.141: INFO: Waiting up to 5m0s for pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c" in namespace "downward-api-1861" to be "running and ready"
    Feb 27 15:37:34.147: INFO: Pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.758522ms
    Feb 27 15:37:34.147: INFO: The phase of Pod annotationupdate9b150ece-0432-4d8f-8650-f7425019602c is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:37:36.151: INFO: Pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00971565s
    Feb 27 15:37:36.151: INFO: The phase of Pod annotationupdate9b150ece-0432-4d8f-8650-f7425019602c is Running (Ready = true)
    Feb 27 15:37:36.151: INFO: Pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c" satisfied condition "running and ready"
    Feb 27 15:37:36.668: INFO: Successfully updated pod "annotationupdate9b150ece-0432-4d8f-8650-f7425019602c"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:40.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1861" for this suite. 02/27/23 15:37:40.693
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:40.698
Feb 27 15:37:40.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:37:40.699
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:40.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:40.713
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 02/27/23 15:37:40.719
STEP: watching for the Service to be added 02/27/23 15:37:40.726
Feb 27 15:37:40.728: INFO: Found Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 27 15:37:40.728: INFO: Service test-service-6jcv4 created
STEP: Getting /status 02/27/23 15:37:40.728
Feb 27 15:37:40.733: INFO: Service test-service-6jcv4 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/27/23 15:37:40.733
STEP: watching for the Service to be patched 02/27/23 15:37:40.739
Feb 27 15:37:40.742: INFO: observed Service test-service-6jcv4 in namespace services-2228 with annotations: map[] & LoadBalancer: {[]}
Feb 27 15:37:40.742: INFO: Found Service test-service-6jcv4 in namespace services-2228 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 27 15:37:40.742: INFO: Service test-service-6jcv4 has service status patched
STEP: updating the ServiceStatus 02/27/23 15:37:40.742
Feb 27 15:37:40.751: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/27/23 15:37:40.751
Feb 27 15:37:40.752: INFO: Observed Service test-service-6jcv4 in namespace services-2228 with annotations: map[] & Conditions: {[]}
Feb 27 15:37:40.752: INFO: Observed event: &Service{ObjectMeta:{test-service-6jcv4  services-2228  a6576296-8794-4854-bc30-38f9f64f19f1 13237 0 2023-02-27 15:37:40 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 15:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 15:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.70,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.70],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 27 15:37:40.752: INFO: Found Service test-service-6jcv4 in namespace services-2228 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 15:37:40.752: INFO: Service test-service-6jcv4 has service status updated
STEP: patching the service 02/27/23 15:37:40.752
STEP: watching for the Service to be patched 02/27/23 15:37:40.764
Feb 27 15:37:40.766: INFO: observed Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true]
Feb 27 15:37:40.766: INFO: observed Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true]
Feb 27 15:37:40.766: INFO: observed Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true]
Feb 27 15:37:40.766: INFO: Found Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service:patched test-service-static:true]
Feb 27 15:37:40.766: INFO: Service test-service-6jcv4 patched
STEP: deleting the service 02/27/23 15:37:40.766
STEP: watching for the Service to be deleted 02/27/23 15:37:40.78
Feb 27 15:37:40.782: INFO: Observed event: ADDED
Feb 27 15:37:40.782: INFO: Observed event: MODIFIED
Feb 27 15:37:40.782: INFO: Observed event: MODIFIED
Feb 27 15:37:40.782: INFO: Observed event: MODIFIED
Feb 27 15:37:40.782: INFO: Found Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 27 15:37:40.782: INFO: Service test-service-6jcv4 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:40.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2228" for this suite. 02/27/23 15:37:40.785
------------------------------
• [0.095 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:40.698
    Feb 27 15:37:40.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:37:40.699
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:40.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:40.713
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 02/27/23 15:37:40.719
    STEP: watching for the Service to be added 02/27/23 15:37:40.726
    Feb 27 15:37:40.728: INFO: Found Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb 27 15:37:40.728: INFO: Service test-service-6jcv4 created
    STEP: Getting /status 02/27/23 15:37:40.728
    Feb 27 15:37:40.733: INFO: Service test-service-6jcv4 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/27/23 15:37:40.733
    STEP: watching for the Service to be patched 02/27/23 15:37:40.739
    Feb 27 15:37:40.742: INFO: observed Service test-service-6jcv4 in namespace services-2228 with annotations: map[] & LoadBalancer: {[]}
    Feb 27 15:37:40.742: INFO: Found Service test-service-6jcv4 in namespace services-2228 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb 27 15:37:40.742: INFO: Service test-service-6jcv4 has service status patched
    STEP: updating the ServiceStatus 02/27/23 15:37:40.742
    Feb 27 15:37:40.751: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/27/23 15:37:40.751
    Feb 27 15:37:40.752: INFO: Observed Service test-service-6jcv4 in namespace services-2228 with annotations: map[] & Conditions: {[]}
    Feb 27 15:37:40.752: INFO: Observed event: &Service{ObjectMeta:{test-service-6jcv4  services-2228  a6576296-8794-4854-bc30-38f9f64f19f1 13237 0 2023-02-27 15:37:40 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 15:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 15:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.70,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.70],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb 27 15:37:40.752: INFO: Found Service test-service-6jcv4 in namespace services-2228 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 15:37:40.752: INFO: Service test-service-6jcv4 has service status updated
    STEP: patching the service 02/27/23 15:37:40.752
    STEP: watching for the Service to be patched 02/27/23 15:37:40.764
    Feb 27 15:37:40.766: INFO: observed Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true]
    Feb 27 15:37:40.766: INFO: observed Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true]
    Feb 27 15:37:40.766: INFO: observed Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service-static:true]
    Feb 27 15:37:40.766: INFO: Found Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service:patched test-service-static:true]
    Feb 27 15:37:40.766: INFO: Service test-service-6jcv4 patched
    STEP: deleting the service 02/27/23 15:37:40.766
    STEP: watching for the Service to be deleted 02/27/23 15:37:40.78
    Feb 27 15:37:40.782: INFO: Observed event: ADDED
    Feb 27 15:37:40.782: INFO: Observed event: MODIFIED
    Feb 27 15:37:40.782: INFO: Observed event: MODIFIED
    Feb 27 15:37:40.782: INFO: Observed event: MODIFIED
    Feb 27 15:37:40.782: INFO: Found Service test-service-6jcv4 in namespace services-2228 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb 27 15:37:40.782: INFO: Service test-service-6jcv4 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:40.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2228" for this suite. 02/27/23 15:37:40.785
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:40.794
Feb 27 15:37:40.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:37:40.795
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:40.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:40.807
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:37:40.81
Feb 27 15:37:40.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41" in namespace "projected-8912" to be "Succeeded or Failed"
Feb 27 15:37:40.822: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123741ms
Feb 27 15:37:42.826: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007784503s
Feb 27 15:37:44.827: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008649105s
STEP: Saw pod success 02/27/23 15:37:44.827
Feb 27 15:37:44.827: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41" satisfied condition "Succeeded or Failed"
Feb 27 15:37:44.829: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41 container client-container: <nil>
STEP: delete the pod 02/27/23 15:37:44.834
Feb 27 15:37:44.851: INFO: Waiting for pod downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41 to disappear
Feb 27 15:37:44.853: INFO: Pod downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:44.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8912" for this suite. 02/27/23 15:37:44.856
------------------------------
• [4.069 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:40.794
    Feb 27 15:37:40.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:37:40.795
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:40.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:40.807
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:37:40.81
    Feb 27 15:37:40.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41" in namespace "projected-8912" to be "Succeeded or Failed"
    Feb 27 15:37:40.822: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123741ms
    Feb 27 15:37:42.826: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007784503s
    Feb 27 15:37:44.827: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008649105s
    STEP: Saw pod success 02/27/23 15:37:44.827
    Feb 27 15:37:44.827: INFO: Pod "downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41" satisfied condition "Succeeded or Failed"
    Feb 27 15:37:44.829: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41 container client-container: <nil>
    STEP: delete the pod 02/27/23 15:37:44.834
    Feb 27 15:37:44.851: INFO: Waiting for pod downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41 to disappear
    Feb 27 15:37:44.853: INFO: Pod downwardapi-volume-eb227297-19a4-4cda-bac2-76bd385c1a41 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:44.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8912" for this suite. 02/27/23 15:37:44.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:44.863
Feb 27 15:37:44.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:37:44.864
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:44.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:44.88
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9231 02/27/23 15:37:44.883
STEP: creating replication controller nodeport-test in namespace services-9231 02/27/23 15:37:44.897
I0227 15:37:44.906021      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9231, replica count: 2
I0227 15:37:47.956779      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 15:37:47.956: INFO: Creating new exec pod
Feb 27 15:37:47.964: INFO: Waiting up to 5m0s for pod "execpodgvcxd" in namespace "services-9231" to be "running"
Feb 27 15:37:47.969: INFO: Pod "execpodgvcxd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574956ms
Feb 27 15:37:49.972: INFO: Pod "execpodgvcxd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008772277s
Feb 27 15:37:49.972: INFO: Pod "execpodgvcxd" satisfied condition "running"
Feb 27 15:37:50.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Feb 27 15:37:51.111: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 27 15:37:51.111: INFO: stdout: ""
Feb 27 15:37:51.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 10.152.183.23 80'
Feb 27 15:37:51.223: INFO: stderr: "+ nc -v -z -w 2 10.152.183.23 80\nConnection to 10.152.183.23 80 port [tcp/http] succeeded!\n"
Feb 27 15:37:51.223: INFO: stdout: ""
Feb 27 15:37:51.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 172.31.3.182 31638'
Feb 27 15:37:51.323: INFO: stderr: "+ nc -v -z -w 2 172.31.3.182 31638\nConnection to 172.31.3.182 31638 port [tcp/*] succeeded!\n"
Feb 27 15:37:51.323: INFO: stdout: ""
Feb 27 15:37:51.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 31638'
Feb 27 15:37:51.423: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 31638\nConnection to 172.31.42.40 31638 port [tcp/*] succeeded!\n"
Feb 27 15:37:51.423: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:51.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9231" for this suite. 02/27/23 15:37:51.427
------------------------------
• [SLOW TEST] [6.570 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:44.863
    Feb 27 15:37:44.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:37:44.864
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:44.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:44.88
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9231 02/27/23 15:37:44.883
    STEP: creating replication controller nodeport-test in namespace services-9231 02/27/23 15:37:44.897
    I0227 15:37:44.906021      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9231, replica count: 2
    I0227 15:37:47.956779      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 15:37:47.956: INFO: Creating new exec pod
    Feb 27 15:37:47.964: INFO: Waiting up to 5m0s for pod "execpodgvcxd" in namespace "services-9231" to be "running"
    Feb 27 15:37:47.969: INFO: Pod "execpodgvcxd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574956ms
    Feb 27 15:37:49.972: INFO: Pod "execpodgvcxd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008772277s
    Feb 27 15:37:49.972: INFO: Pod "execpodgvcxd" satisfied condition "running"
    Feb 27 15:37:50.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Feb 27 15:37:51.111: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 27 15:37:51.111: INFO: stdout: ""
    Feb 27 15:37:51.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 10.152.183.23 80'
    Feb 27 15:37:51.223: INFO: stderr: "+ nc -v -z -w 2 10.152.183.23 80\nConnection to 10.152.183.23 80 port [tcp/http] succeeded!\n"
    Feb 27 15:37:51.223: INFO: stdout: ""
    Feb 27 15:37:51.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 172.31.3.182 31638'
    Feb 27 15:37:51.323: INFO: stderr: "+ nc -v -z -w 2 172.31.3.182 31638\nConnection to 172.31.3.182 31638 port [tcp/*] succeeded!\n"
    Feb 27 15:37:51.323: INFO: stdout: ""
    Feb 27 15:37:51.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9231 exec execpodgvcxd -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 31638'
    Feb 27 15:37:51.423: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 31638\nConnection to 172.31.42.40 31638 port [tcp/*] succeeded!\n"
    Feb 27 15:37:51.423: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:51.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9231" for this suite. 02/27/23 15:37:51.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:51.434
Feb 27 15:37:51.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:37:51.435
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:51.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:51.455
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-90b6e5fc-fd6f-43a1-9b20-1f6320bcd5fa 02/27/23 15:37:51.457
STEP: Creating a pod to test consume configMaps 02/27/23 15:37:51.46
Feb 27 15:37:51.471: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b" in namespace "projected-1511" to be "Succeeded or Failed"
Feb 27 15:37:51.474: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.956234ms
Feb 27 15:37:53.477: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b": Phase="Running", Reason="", readiness=false. Elapsed: 2.005928479s
Feb 27 15:37:55.478: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006384361s
STEP: Saw pod success 02/27/23 15:37:55.478
Feb 27 15:37:55.478: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b" satisfied condition "Succeeded or Failed"
Feb 27 15:37:55.481: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:37:55.487
Feb 27 15:37:55.499: INFO: Waiting for pod pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b to disappear
Feb 27 15:37:55.502: INFO: Pod pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:37:55.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1511" for this suite. 02/27/23 15:37:55.505
------------------------------
• [4.077 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:51.434
    Feb 27 15:37:51.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:37:51.435
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:51.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:51.455
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-90b6e5fc-fd6f-43a1-9b20-1f6320bcd5fa 02/27/23 15:37:51.457
    STEP: Creating a pod to test consume configMaps 02/27/23 15:37:51.46
    Feb 27 15:37:51.471: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b" in namespace "projected-1511" to be "Succeeded or Failed"
    Feb 27 15:37:51.474: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.956234ms
    Feb 27 15:37:53.477: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b": Phase="Running", Reason="", readiness=false. Elapsed: 2.005928479s
    Feb 27 15:37:55.478: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006384361s
    STEP: Saw pod success 02/27/23 15:37:55.478
    Feb 27 15:37:55.478: INFO: Pod "pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b" satisfied condition "Succeeded or Failed"
    Feb 27 15:37:55.481: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:37:55.487
    Feb 27 15:37:55.499: INFO: Waiting for pod pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b to disappear
    Feb 27 15:37:55.502: INFO: Pod pod-projected-configmaps-d51bd601-e25b-496c-93dd-a6de07daa08b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:37:55.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1511" for this suite. 02/27/23 15:37:55.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:37:55.512
Feb 27 15:37:55.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:37:55.513
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:55.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:55.525
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 02/27/23 15:37:55.528
Feb 27 15:37:55.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: mark a version not serverd 02/27/23 15:37:59.028
STEP: check the unserved version gets removed 02/27/23 15:37:59.044
STEP: check the other version is not changed 02/27/23 15:38:00.536
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:03.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5450" for this suite. 02/27/23 15:38:03.301
------------------------------
• [SLOW TEST] [7.794 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:37:55.512
    Feb 27 15:37:55.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:37:55.513
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:37:55.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:37:55.525
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 02/27/23 15:37:55.528
    Feb 27 15:37:55.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: mark a version not serverd 02/27/23 15:37:59.028
    STEP: check the unserved version gets removed 02/27/23 15:37:59.044
    STEP: check the other version is not changed 02/27/23 15:38:00.536
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:03.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5450" for this suite. 02/27/23 15:38:03.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:03.307
Feb 27 15:38:03.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:38:03.307
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:03.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:03.32
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-149f3560-ad42-40bd-bf7b-13f71059d0fa 02/27/23 15:38:03.328
STEP: Creating configMap with name cm-test-opt-upd-48b349bb-3761-46c7-84ec-e2f15bdbbf51 02/27/23 15:38:03.333
STEP: Creating the pod 02/27/23 15:38:03.337
Feb 27 15:38:03.345: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b" in namespace "projected-7963" to be "running and ready"
Feb 27 15:38:03.352: INFO: Pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.927619ms
Feb 27 15:38:03.352: INFO: The phase of Pod pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:38:05.356: INFO: Pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010812564s
Feb 27 15:38:05.356: INFO: The phase of Pod pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b is Running (Ready = true)
Feb 27 15:38:05.356: INFO: Pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-149f3560-ad42-40bd-bf7b-13f71059d0fa 02/27/23 15:38:05.374
STEP: Updating configmap cm-test-opt-upd-48b349bb-3761-46c7-84ec-e2f15bdbbf51 02/27/23 15:38:05.38
STEP: Creating configMap with name cm-test-opt-create-ec784468-c1ab-4813-bfad-e28dd945c394 02/27/23 15:38:05.384
STEP: waiting to observe update in volume 02/27/23 15:38:05.388
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:09.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7963" for this suite. 02/27/23 15:38:09.42
------------------------------
• [SLOW TEST] [6.119 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:03.307
    Feb 27 15:38:03.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:38:03.307
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:03.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:03.32
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-149f3560-ad42-40bd-bf7b-13f71059d0fa 02/27/23 15:38:03.328
    STEP: Creating configMap with name cm-test-opt-upd-48b349bb-3761-46c7-84ec-e2f15bdbbf51 02/27/23 15:38:03.333
    STEP: Creating the pod 02/27/23 15:38:03.337
    Feb 27 15:38:03.345: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b" in namespace "projected-7963" to be "running and ready"
    Feb 27 15:38:03.352: INFO: Pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.927619ms
    Feb 27 15:38:03.352: INFO: The phase of Pod pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:38:05.356: INFO: Pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010812564s
    Feb 27 15:38:05.356: INFO: The phase of Pod pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b is Running (Ready = true)
    Feb 27 15:38:05.356: INFO: Pod "pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-149f3560-ad42-40bd-bf7b-13f71059d0fa 02/27/23 15:38:05.374
    STEP: Updating configmap cm-test-opt-upd-48b349bb-3761-46c7-84ec-e2f15bdbbf51 02/27/23 15:38:05.38
    STEP: Creating configMap with name cm-test-opt-create-ec784468-c1ab-4813-bfad-e28dd945c394 02/27/23 15:38:05.384
    STEP: waiting to observe update in volume 02/27/23 15:38:05.388
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:09.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7963" for this suite. 02/27/23 15:38:09.42
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:09.426
Feb 27 15:38:09.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-pred 02/27/23 15:38:09.427
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:09.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:09.441
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 15:38:09.443: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 15:38:09.448: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 15:38:09.450: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
Feb 27 15:38:09.454: INFO: default-http-backend-kubernetes-worker-9b9488b5c-nx5j4 from ingress-nginx-kubernetes-worker started at 2023-02-27 15:32:38 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.454: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Feb 27 15:38:09.454: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.454: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:38:09.454: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.454: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 15:38:09.454: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:38:09.454: INFO: 	Container e2e ready: true, restart count 0
Feb 27 15:38:09.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:38:09.454: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:38:09.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:38:09.454: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:38:09.454: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
Feb 27 15:38:09.458: INFO: nginx-ingress-controller-kubernetes-worker-4v9jp from ingress-nginx-kubernetes-worker started at 2023-02-27 15:33:04 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.458: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:38:09.458: INFO: pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b from projected-7963 started at 2023-02-27 15:38:03 +0000 UTC (3 container statuses recorded)
Feb 27 15:38:09.458: INFO: 	Container createcm-volume-test ready: true, restart count 0
Feb 27 15:38:09.458: INFO: 	Container delcm-volume-test ready: true, restart count 0
Feb 27 15:38:09.458: INFO: 	Container updcm-volume-test ready: true, restart count 0
Feb 27 15:38:09.458: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.458: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 15:38:09.458: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:38:09.458: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:38:09.458: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 15:38:09.458: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
Feb 27 15:38:09.461: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.461: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Feb 27 15:38:09.461: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.461: INFO: 	Container coredns ready: true, restart count 0
Feb 27 15:38:09.461: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.461: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 27 15:38:09.461: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
Feb 27 15:38:09.461: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 15:38:09.461: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 27 15:38:09.461: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.461: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 15:38:09.461: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
Feb 27 15:38:09.461: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 27 15:38:09.461: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
Feb 27 15:38:09.462: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 15:38:09.462: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 15:38:09.462
Feb 27 15:38:09.467: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1289" to be "running"
Feb 27 15:38:09.470: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229957ms
Feb 27 15:38:11.474: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007003988s
Feb 27 15:38:11.474: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 15:38:11.477
STEP: Trying to apply a random label on the found node. 02/27/23 15:38:11.49
STEP: verifying the node has the label kubernetes.io/e2e-65824d42-aefc-4dcb-9a1f-da0860e0e25e 42 02/27/23 15:38:11.497
STEP: Trying to relaunch the pod, now with labels. 02/27/23 15:38:11.5
Feb 27 15:38:11.505: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1289" to be "not pending"
Feb 27 15:38:11.509: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.806641ms
Feb 27 15:38:13.513: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007805731s
Feb 27 15:38:13.513: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-65824d42-aefc-4dcb-9a1f-da0860e0e25e off the node ip-172-31-3-182 02/27/23 15:38:13.516
STEP: verifying the node doesn't have the label kubernetes.io/e2e-65824d42-aefc-4dcb-9a1f-da0860e0e25e 02/27/23 15:38:13.529
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:13.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1289" for this suite. 02/27/23 15:38:13.537
------------------------------
• [4.117 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:09.426
    Feb 27 15:38:09.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-pred 02/27/23 15:38:09.427
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:09.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:09.441
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 15:38:09.443: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 15:38:09.448: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 15:38:09.450: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-3-182 before test
    Feb 27 15:38:09.454: INFO: default-http-backend-kubernetes-worker-9b9488b5c-nx5j4 from ingress-nginx-kubernetes-worker started at 2023-02-27 15:32:38 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.454: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
    Feb 27 15:38:09.454: INFO: nginx-ingress-controller-kubernetes-worker-s5lnj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:22 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.454: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:38:09.454: INFO: calico-kube-controllers-6b5c59f67b-27fsw from kube-system started at 2023-02-27 14:58:23 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.454: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 15:38:09.454: INFO: sonobuoy-e2e-job-239e950f4344406f from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:38:09.454: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 15:38:09.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:38:09.454: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-f2wvn from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:38:09.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:38:09.454: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:38:09.454: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-42-40 before test
    Feb 27 15:38:09.458: INFO: nginx-ingress-controller-kubernetes-worker-4v9jp from ingress-nginx-kubernetes-worker started at 2023-02-27 15:33:04 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.458: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:38:09.458: INFO: pod-projected-configmaps-09ad029c-b187-4f98-b10d-2d0c17a0c21b from projected-7963 started at 2023-02-27 15:38:03 +0000 UTC (3 container statuses recorded)
    Feb 27 15:38:09.458: INFO: 	Container createcm-volume-test ready: true, restart count 0
    Feb 27 15:38:09.458: INFO: 	Container delcm-volume-test ready: true, restart count 0
    Feb 27 15:38:09.458: INFO: 	Container updcm-volume-test ready: true, restart count 0
    Feb 27 15:38:09.458: INFO: sonobuoy from sonobuoy started at 2023-02-27 15:05:00 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.458: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 15:38:09.458: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-ghbx9 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:38:09.458: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:38:09.458: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 15:38:09.458: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-84-171 before test
    Feb 27 15:38:09.461: INFO: nginx-ingress-controller-kubernetes-worker-mvzvj from ingress-nginx-kubernetes-worker started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.461: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
    Feb 27 15:38:09.461: INFO: coredns-77c75468db-2xznd from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.461: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 15:38:09.461: INFO: kube-state-metrics-58f6fddc6f-nhlk4 from kube-system started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.461: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 27 15:38:09.461: INFO: metrics-server-v0.5.2-5c84bcbb7f-gf62n from kube-system started at 2023-02-27 14:58:24 +0000 UTC (2 container statuses recorded)
    Feb 27 15:38:09.461: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 15:38:09.461: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Feb 27 15:38:09.461: INFO: dashboard-metrics-scraper-7c69979f6f-dnd2h from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.461: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 15:38:09.461: INFO: kubernetes-dashboard-7ccc5b6c5f-tgbtf from kubernetes-dashboard started at 2023-02-27 14:58:24 +0000 UTC (1 container statuses recorded)
    Feb 27 15:38:09.461: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Feb 27 15:38:09.461: INFO: sonobuoy-systemd-logs-daemon-set-77db518271df4703-5wvw2 from sonobuoy started at 2023-02-27 15:05:03 +0000 UTC (2 container statuses recorded)
    Feb 27 15:38:09.462: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 15:38:09.462: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 15:38:09.462
    Feb 27 15:38:09.467: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1289" to be "running"
    Feb 27 15:38:09.470: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229957ms
    Feb 27 15:38:11.474: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007003988s
    Feb 27 15:38:11.474: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 15:38:11.477
    STEP: Trying to apply a random label on the found node. 02/27/23 15:38:11.49
    STEP: verifying the node has the label kubernetes.io/e2e-65824d42-aefc-4dcb-9a1f-da0860e0e25e 42 02/27/23 15:38:11.497
    STEP: Trying to relaunch the pod, now with labels. 02/27/23 15:38:11.5
    Feb 27 15:38:11.505: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1289" to be "not pending"
    Feb 27 15:38:11.509: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.806641ms
    Feb 27 15:38:13.513: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007805731s
    Feb 27 15:38:13.513: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-65824d42-aefc-4dcb-9a1f-da0860e0e25e off the node ip-172-31-3-182 02/27/23 15:38:13.516
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-65824d42-aefc-4dcb-9a1f-da0860e0e25e 02/27/23 15:38:13.529
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:13.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1289" for this suite. 02/27/23 15:38:13.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:13.544
Feb 27 15:38:13.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 15:38:13.545
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:13.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:13.566
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Feb 27 15:38:13.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: creating the pod 02/27/23 15:38:13.569
STEP: submitting the pod to kubernetes 02/27/23 15:38:13.569
Feb 27 15:38:13.581: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac" in namespace "pods-4198" to be "running and ready"
Feb 27 15:38:13.584: INFO: Pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944061ms
Feb 27 15:38:13.584: INFO: The phase of Pod pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:38:15.587: INFO: Pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac": Phase="Running", Reason="", readiness=true. Elapsed: 2.006067958s
Feb 27 15:38:15.587: INFO: The phase of Pod pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac is Running (Ready = true)
Feb 27 15:38:15.587: INFO: Pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:15.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4198" for this suite. 02/27/23 15:38:15.692
------------------------------
• [2.154 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:13.544
    Feb 27 15:38:13.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 15:38:13.545
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:13.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:13.566
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Feb 27 15:38:13.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: creating the pod 02/27/23 15:38:13.569
    STEP: submitting the pod to kubernetes 02/27/23 15:38:13.569
    Feb 27 15:38:13.581: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac" in namespace "pods-4198" to be "running and ready"
    Feb 27 15:38:13.584: INFO: Pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944061ms
    Feb 27 15:38:13.584: INFO: The phase of Pod pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:38:15.587: INFO: Pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac": Phase="Running", Reason="", readiness=true. Elapsed: 2.006067958s
    Feb 27 15:38:15.587: INFO: The phase of Pod pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac is Running (Ready = true)
    Feb 27 15:38:15.587: INFO: Pod "pod-exec-websocket-fb404e59-3209-477d-8078-e851c794adac" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:15.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4198" for this suite. 02/27/23 15:38:15.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:15.699
Feb 27 15:38:15.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:38:15.699
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:15.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:15.713
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 02/27/23 15:38:15.724
STEP: waiting for available Endpoint 02/27/23 15:38:15.731
STEP: listing all Endpoints 02/27/23 15:38:15.732
STEP: updating the Endpoint 02/27/23 15:38:15.734
STEP: fetching the Endpoint 02/27/23 15:38:15.741
STEP: patching the Endpoint 02/27/23 15:38:15.744
STEP: fetching the Endpoint 02/27/23 15:38:15.75
STEP: deleting the Endpoint by Collection 02/27/23 15:38:15.753
STEP: waiting for Endpoint deletion 02/27/23 15:38:15.761
STEP: fetching the Endpoint 02/27/23 15:38:15.762
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:15.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4229" for this suite. 02/27/23 15:38:15.767
------------------------------
• [0.076 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:15.699
    Feb 27 15:38:15.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:38:15.699
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:15.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:15.713
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 02/27/23 15:38:15.724
    STEP: waiting for available Endpoint 02/27/23 15:38:15.731
    STEP: listing all Endpoints 02/27/23 15:38:15.732
    STEP: updating the Endpoint 02/27/23 15:38:15.734
    STEP: fetching the Endpoint 02/27/23 15:38:15.741
    STEP: patching the Endpoint 02/27/23 15:38:15.744
    STEP: fetching the Endpoint 02/27/23 15:38:15.75
    STEP: deleting the Endpoint by Collection 02/27/23 15:38:15.753
    STEP: waiting for Endpoint deletion 02/27/23 15:38:15.761
    STEP: fetching the Endpoint 02/27/23 15:38:15.762
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:15.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4229" for this suite. 02/27/23 15:38:15.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:15.775
Feb 27 15:38:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 15:38:15.776
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:15.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:15.79
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 15:38:15.795
Feb 27 15:38:15.801: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6858" to be "running and ready"
Feb 27 15:38:15.803: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341038ms
Feb 27 15:38:15.803: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:38:17.808: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006646107s
Feb 27 15:38:17.808: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 15:38:17.808: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 02/27/23 15:38:17.81
Feb 27 15:38:17.814: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6858" to be "running and ready"
Feb 27 15:38:17.819: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.688807ms
Feb 27 15:38:17.819: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:38:19.822: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008319863s
Feb 27 15:38:19.823: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb 27 15:38:19.823: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/27/23 15:38:19.825
STEP: delete the pod with lifecycle hook 02/27/23 15:38:19.839
Feb 27 15:38:19.845: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 15:38:19.848: INFO: Pod pod-with-poststart-http-hook still exists
Feb 27 15:38:21.849: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 15:38:21.853: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:21.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6858" for this suite. 02/27/23 15:38:21.856
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:15.775
    Feb 27 15:38:15.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 15:38:15.776
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:15.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:15.79
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 15:38:15.795
    Feb 27 15:38:15.801: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6858" to be "running and ready"
    Feb 27 15:38:15.803: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341038ms
    Feb 27 15:38:15.803: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:38:17.808: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006646107s
    Feb 27 15:38:17.808: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 15:38:17.808: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 02/27/23 15:38:17.81
    Feb 27 15:38:17.814: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6858" to be "running and ready"
    Feb 27 15:38:17.819: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.688807ms
    Feb 27 15:38:17.819: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:38:19.822: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008319863s
    Feb 27 15:38:19.823: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb 27 15:38:19.823: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/27/23 15:38:19.825
    STEP: delete the pod with lifecycle hook 02/27/23 15:38:19.839
    Feb 27 15:38:19.845: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 15:38:19.848: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 27 15:38:21.849: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 15:38:21.853: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:21.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6858" for this suite. 02/27/23 15:38:21.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:21.865
Feb 27 15:38:21.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:38:21.866
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:21.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:21.88
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-3234 02/27/23 15:38:21.883
STEP: creating service affinity-nodeport-transition in namespace services-3234 02/27/23 15:38:21.883
STEP: creating replication controller affinity-nodeport-transition in namespace services-3234 02/27/23 15:38:21.901
I0227 15:38:21.906956      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3234, replica count: 3
I0227 15:38:24.958381      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 15:38:24.967: INFO: Creating new exec pod
Feb 27 15:38:24.972: INFO: Waiting up to 5m0s for pod "execpod-affinityvngbp" in namespace "services-3234" to be "running"
Feb 27 15:38:24.974: INFO: Pod "execpod-affinityvngbp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612734ms
Feb 27 15:38:26.977: INFO: Pod "execpod-affinityvngbp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005652203s
Feb 27 15:38:26.977: INFO: Pod "execpod-affinityvngbp" satisfied condition "running"
Feb 27 15:38:27.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Feb 27 15:38:28.099: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 27 15:38:28.099: INFO: stdout: ""
Feb 27 15:38:28.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 10.152.183.224 80'
Feb 27 15:38:28.203: INFO: stderr: "+ nc -v -z -w 2 10.152.183.224 80\nConnection to 10.152.183.224 80 port [tcp/http] succeeded!\n"
Feb 27 15:38:28.203: INFO: stdout: ""
Feb 27 15:38:28.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 172.31.3.182 31154'
Feb 27 15:38:28.324: INFO: stderr: "+ nc -v -z -w 2 172.31.3.182 31154\nConnection to 172.31.3.182 31154 port [tcp/*] succeeded!\n"
Feb 27 15:38:28.324: INFO: stdout: ""
Feb 27 15:38:28.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 31154'
Feb 27 15:38:28.431: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 31154\nConnection to 172.31.42.40 31154 port [tcp/*] succeeded!\n"
Feb 27 15:38:28.431: INFO: stdout: ""
Feb 27 15:38:28.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.182:31154/ ; done'
Feb 27 15:38:28.633: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n"
Feb 27 15:38:28.633: INFO: stdout: "\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-lxf4s"
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.182:31154/ ; done'
Feb 27 15:38:28.808: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n"
Feb 27 15:38:28.808: INFO: stdout: "\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s"
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
Feb 27 15:38:28.808: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3234, will wait for the garbage collector to delete the pods 02/27/23 15:38:28.823
Feb 27 15:38:28.883: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.988486ms
Feb 27 15:38:28.983: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.104205ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:30.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3234" for this suite. 02/27/23 15:38:30.705
------------------------------
• [SLOW TEST] [8.846 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:21.865
    Feb 27 15:38:21.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:38:21.866
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:21.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:21.88
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-3234 02/27/23 15:38:21.883
    STEP: creating service affinity-nodeport-transition in namespace services-3234 02/27/23 15:38:21.883
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3234 02/27/23 15:38:21.901
    I0227 15:38:21.906956      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3234, replica count: 3
    I0227 15:38:24.958381      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 15:38:24.967: INFO: Creating new exec pod
    Feb 27 15:38:24.972: INFO: Waiting up to 5m0s for pod "execpod-affinityvngbp" in namespace "services-3234" to be "running"
    Feb 27 15:38:24.974: INFO: Pod "execpod-affinityvngbp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612734ms
    Feb 27 15:38:26.977: INFO: Pod "execpod-affinityvngbp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005652203s
    Feb 27 15:38:26.977: INFO: Pod "execpod-affinityvngbp" satisfied condition "running"
    Feb 27 15:38:27.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Feb 27 15:38:28.099: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb 27 15:38:28.099: INFO: stdout: ""
    Feb 27 15:38:28.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 10.152.183.224 80'
    Feb 27 15:38:28.203: INFO: stderr: "+ nc -v -z -w 2 10.152.183.224 80\nConnection to 10.152.183.224 80 port [tcp/http] succeeded!\n"
    Feb 27 15:38:28.203: INFO: stdout: ""
    Feb 27 15:38:28.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 172.31.3.182 31154'
    Feb 27 15:38:28.324: INFO: stderr: "+ nc -v -z -w 2 172.31.3.182 31154\nConnection to 172.31.3.182 31154 port [tcp/*] succeeded!\n"
    Feb 27 15:38:28.324: INFO: stdout: ""
    Feb 27 15:38:28.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 31154'
    Feb 27 15:38:28.431: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 31154\nConnection to 172.31.42.40 31154 port [tcp/*] succeeded!\n"
    Feb 27 15:38:28.431: INFO: stdout: ""
    Feb 27 15:38:28.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.182:31154/ ; done'
    Feb 27 15:38:28.633: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n"
    Feb 27 15:38:28.633: INFO: stdout: "\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-gwp98\naffinity-nodeport-transition-wl7jz\naffinity-nodeport-transition-lxf4s"
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-gwp98
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-wl7jz
    Feb 27 15:38:28.633: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3234 exec execpod-affinityvngbp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.182:31154/ ; done'
    Feb 27 15:38:28.808: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:31154/\n"
    Feb 27 15:38:28.808: INFO: stdout: "\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s\naffinity-nodeport-transition-lxf4s"
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Received response from host: affinity-nodeport-transition-lxf4s
    Feb 27 15:38:28.808: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3234, will wait for the garbage collector to delete the pods 02/27/23 15:38:28.823
    Feb 27 15:38:28.883: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.988486ms
    Feb 27 15:38:28.983: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.104205ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:30.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3234" for this suite. 02/27/23 15:38:30.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:30.712
Feb 27 15:38:30.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pod-network-test 02/27/23 15:38:30.713
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:30.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:30.729
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6523 02/27/23 15:38:30.732
STEP: creating a selector 02/27/23 15:38:30.732
STEP: Creating the service pods in kubernetes 02/27/23 15:38:30.732
Feb 27 15:38:30.732: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 15:38:30.762: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6523" to be "running and ready"
Feb 27 15:38:30.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.164734ms
Feb 27 15:38:30.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:38:32.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.0093285s
Feb 27 15:38:32.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:38:34.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009924167s
Feb 27 15:38:34.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:38:36.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009976389s
Feb 27 15:38:36.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:38:38.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009999515s
Feb 27 15:38:38.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:38:40.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010443398s
Feb 27 15:38:40.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:38:42.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010487022s
Feb 27 15:38:42.773: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 15:38:42.773: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 15:38:42.775: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6523" to be "running and ready"
Feb 27 15:38:42.779: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.219815ms
Feb 27 15:38:42.779: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 15:38:42.779: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 15:38:42.791: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6523" to be "running and ready"
Feb 27 15:38:42.795: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.993007ms
Feb 27 15:38:42.795: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 15:38:42.795: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 15:38:42.797
Feb 27 15:38:42.809: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6523" to be "running"
Feb 27 15:38:42.812: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506603ms
Feb 27 15:38:44.816: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006846506s
Feb 27 15:38:44.816: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 15:38:44.818: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 15:38:44.818: INFO: Breadth first check of 192.168.212.188 on host 172.31.3.182...
Feb 27 15:38:44.822: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.145:9080/dial?request=hostname&protocol=http&host=192.168.212.188&port=8083&tries=1'] Namespace:pod-network-test-6523 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:38:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:38:44.822: INFO: ExecWithOptions: Clientset creation
Feb 27 15:38:44.822: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6523/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.212.188%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 15:38:44.893: INFO: Waiting for responses: map[]
Feb 27 15:38:44.893: INFO: reached 192.168.212.188 after 0/1 tries
Feb 27 15:38:44.893: INFO: Breadth first check of 192.168.192.142 on host 172.31.42.40...
Feb 27 15:38:44.896: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.145:9080/dial?request=hostname&protocol=http&host=192.168.192.142&port=8083&tries=1'] Namespace:pod-network-test-6523 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:38:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:38:44.896: INFO: ExecWithOptions: Clientset creation
Feb 27 15:38:44.896: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6523/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.192.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 15:38:44.939: INFO: Waiting for responses: map[]
Feb 27 15:38:44.939: INFO: reached 192.168.192.142 after 0/1 tries
Feb 27 15:38:44.939: INFO: Breadth first check of 192.168.214.174 on host 172.31.84.171...
Feb 27 15:38:44.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.145:9080/dial?request=hostname&protocol=http&host=192.168.214.174&port=8083&tries=1'] Namespace:pod-network-test-6523 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:38:44.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:38:44.943: INFO: ExecWithOptions: Clientset creation
Feb 27 15:38:44.943: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6523/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.214.174%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 15:38:44.984: INFO: Waiting for responses: map[]
Feb 27 15:38:44.984: INFO: reached 192.168.214.174 after 0/1 tries
Feb 27 15:38:44.984: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:44.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6523" for this suite. 02/27/23 15:38:44.987
------------------------------
• [SLOW TEST] [14.280 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:30.712
    Feb 27 15:38:30.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 15:38:30.713
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:30.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:30.729
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6523 02/27/23 15:38:30.732
    STEP: creating a selector 02/27/23 15:38:30.732
    STEP: Creating the service pods in kubernetes 02/27/23 15:38:30.732
    Feb 27 15:38:30.732: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 15:38:30.762: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6523" to be "running and ready"
    Feb 27 15:38:30.768: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.164734ms
    Feb 27 15:38:30.768: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:38:32.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.0093285s
    Feb 27 15:38:32.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:38:34.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009924167s
    Feb 27 15:38:34.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:38:36.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009976389s
    Feb 27 15:38:36.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:38:38.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009999515s
    Feb 27 15:38:38.772: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:38:40.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010443398s
    Feb 27 15:38:40.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:38:42.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010487022s
    Feb 27 15:38:42.773: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 15:38:42.773: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 15:38:42.775: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6523" to be "running and ready"
    Feb 27 15:38:42.779: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.219815ms
    Feb 27 15:38:42.779: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 15:38:42.779: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 15:38:42.791: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6523" to be "running and ready"
    Feb 27 15:38:42.795: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.993007ms
    Feb 27 15:38:42.795: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 15:38:42.795: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 15:38:42.797
    Feb 27 15:38:42.809: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6523" to be "running"
    Feb 27 15:38:42.812: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506603ms
    Feb 27 15:38:44.816: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006846506s
    Feb 27 15:38:44.816: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 15:38:44.818: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 15:38:44.818: INFO: Breadth first check of 192.168.212.188 on host 172.31.3.182...
    Feb 27 15:38:44.822: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.145:9080/dial?request=hostname&protocol=http&host=192.168.212.188&port=8083&tries=1'] Namespace:pod-network-test-6523 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:38:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:38:44.822: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:38:44.822: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6523/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.212.188%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 15:38:44.893: INFO: Waiting for responses: map[]
    Feb 27 15:38:44.893: INFO: reached 192.168.212.188 after 0/1 tries
    Feb 27 15:38:44.893: INFO: Breadth first check of 192.168.192.142 on host 172.31.42.40...
    Feb 27 15:38:44.896: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.145:9080/dial?request=hostname&protocol=http&host=192.168.192.142&port=8083&tries=1'] Namespace:pod-network-test-6523 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:38:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:38:44.896: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:38:44.896: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6523/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.192.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 15:38:44.939: INFO: Waiting for responses: map[]
    Feb 27 15:38:44.939: INFO: reached 192.168.192.142 after 0/1 tries
    Feb 27 15:38:44.939: INFO: Breadth first check of 192.168.214.174 on host 172.31.84.171...
    Feb 27 15:38:44.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.145:9080/dial?request=hostname&protocol=http&host=192.168.214.174&port=8083&tries=1'] Namespace:pod-network-test-6523 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:38:44.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:38:44.943: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:38:44.943: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6523/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.214.174%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 15:38:44.984: INFO: Waiting for responses: map[]
    Feb 27 15:38:44.984: INFO: reached 192.168.214.174 after 0/1 tries
    Feb 27 15:38:44.984: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:44.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6523" for this suite. 02/27/23 15:38:44.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:44.993
Feb 27 15:38:44.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:38:44.994
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:45.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:45.009
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 15:38:45.011
Feb 27 15:38:45.018: INFO: Waiting up to 5m0s for pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f" in namespace "emptydir-1894" to be "Succeeded or Failed"
Feb 27 15:38:45.020: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395553ms
Feb 27 15:38:47.025: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006736336s
Feb 27 15:38:49.025: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007131759s
STEP: Saw pod success 02/27/23 15:38:49.025
Feb 27 15:38:49.025: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f" satisfied condition "Succeeded or Failed"
Feb 27 15:38:49.028: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-2faa51d9-ca44-4fa7-945a-91576163ad0f container test-container: <nil>
STEP: delete the pod 02/27/23 15:38:49.033
Feb 27 15:38:49.043: INFO: Waiting for pod pod-2faa51d9-ca44-4fa7-945a-91576163ad0f to disappear
Feb 27 15:38:49.045: INFO: Pod pod-2faa51d9-ca44-4fa7-945a-91576163ad0f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:49.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1894" for this suite. 02/27/23 15:38:49.048
------------------------------
• [4.060 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:44.993
    Feb 27 15:38:44.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:38:44.994
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:45.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:45.009
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 15:38:45.011
    Feb 27 15:38:45.018: INFO: Waiting up to 5m0s for pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f" in namespace "emptydir-1894" to be "Succeeded or Failed"
    Feb 27 15:38:45.020: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395553ms
    Feb 27 15:38:47.025: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006736336s
    Feb 27 15:38:49.025: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007131759s
    STEP: Saw pod success 02/27/23 15:38:49.025
    Feb 27 15:38:49.025: INFO: Pod "pod-2faa51d9-ca44-4fa7-945a-91576163ad0f" satisfied condition "Succeeded or Failed"
    Feb 27 15:38:49.028: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-2faa51d9-ca44-4fa7-945a-91576163ad0f container test-container: <nil>
    STEP: delete the pod 02/27/23 15:38:49.033
    Feb 27 15:38:49.043: INFO: Waiting for pod pod-2faa51d9-ca44-4fa7-945a-91576163ad0f to disappear
    Feb 27 15:38:49.045: INFO: Pod pod-2faa51d9-ca44-4fa7-945a-91576163ad0f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:49.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1894" for this suite. 02/27/23 15:38:49.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:49.054
Feb 27 15:38:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 15:38:49.054
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:49.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:49.071
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-731df263-abb9-4ad3-b83b-816eb95e9478 02/27/23 15:38:49.074
STEP: Creating a pod to test consume configMaps 02/27/23 15:38:49.078
Feb 27 15:38:49.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e" in namespace "configmap-5969" to be "Succeeded or Failed"
Feb 27 15:38:49.088: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.894777ms
Feb 27 15:38:51.092: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007041461s
Feb 27 15:38:53.091: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006852169s
STEP: Saw pod success 02/27/23 15:38:53.091
Feb 27 15:38:53.092: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e" satisfied condition "Succeeded or Failed"
Feb 27 15:38:53.094: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:38:53.099
Feb 27 15:38:53.112: INFO: Waiting for pod pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e to disappear
Feb 27 15:38:53.115: INFO: Pod pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:53.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5969" for this suite. 02/27/23 15:38:53.118
------------------------------
• [4.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:49.054
    Feb 27 15:38:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 15:38:49.054
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:49.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:49.071
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-731df263-abb9-4ad3-b83b-816eb95e9478 02/27/23 15:38:49.074
    STEP: Creating a pod to test consume configMaps 02/27/23 15:38:49.078
    Feb 27 15:38:49.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e" in namespace "configmap-5969" to be "Succeeded or Failed"
    Feb 27 15:38:49.088: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.894777ms
    Feb 27 15:38:51.092: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007041461s
    Feb 27 15:38:53.091: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006852169s
    STEP: Saw pod success 02/27/23 15:38:53.091
    Feb 27 15:38:53.092: INFO: Pod "pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e" satisfied condition "Succeeded or Failed"
    Feb 27 15:38:53.094: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:38:53.099
    Feb 27 15:38:53.112: INFO: Waiting for pod pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e to disappear
    Feb 27 15:38:53.115: INFO: Pod pod-configmaps-f8966d5d-001d-4e4f-b1cc-d3efe68c661e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:53.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5969" for this suite. 02/27/23 15:38:53.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:53.125
Feb 27 15:38:53.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:38:53.125
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:53.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:53.142
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:38:53.158
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:38:53.432
STEP: Deploying the webhook pod 02/27/23 15:38:53.439
STEP: Wait for the deployment to be ready 02/27/23 15:38:53.452
Feb 27 15:38:53.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:38:55.468
STEP: Verifying the service has paired with the endpoint 02/27/23 15:38:55.478
Feb 27 15:38:56.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 02/27/23 15:38:56.542
STEP: Creating a configMap that should be mutated 02/27/23 15:38:56.552
STEP: Deleting the collection of validation webhooks 02/27/23 15:38:56.577
STEP: Creating a configMap that should not be mutated 02/27/23 15:38:56.621
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:38:56.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-831" for this suite. 02/27/23 15:38:56.681
STEP: Destroying namespace "webhook-831-markers" for this suite. 02/27/23 15:38:56.686
------------------------------
• [3.567 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:53.125
    Feb 27 15:38:53.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:38:53.125
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:53.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:53.142
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:38:53.158
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:38:53.432
    STEP: Deploying the webhook pod 02/27/23 15:38:53.439
    STEP: Wait for the deployment to be ready 02/27/23 15:38:53.452
    Feb 27 15:38:53.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:38:55.468
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:38:55.478
    Feb 27 15:38:56.478: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 02/27/23 15:38:56.542
    STEP: Creating a configMap that should be mutated 02/27/23 15:38:56.552
    STEP: Deleting the collection of validation webhooks 02/27/23 15:38:56.577
    STEP: Creating a configMap that should not be mutated 02/27/23 15:38:56.621
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:38:56.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-831" for this suite. 02/27/23 15:38:56.681
    STEP: Destroying namespace "webhook-831-markers" for this suite. 02/27/23 15:38:56.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:38:56.693
Feb 27 15:38:56.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 15:38:56.693
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:56.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:56.708
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-fee36007-7d9c-49da-aeca-561260ab4ebe 02/27/23 15:38:56.714
STEP: Creating secret with name s-test-opt-upd-4d827fc8-12a7-4034-a138-01c09053d4e2 02/27/23 15:38:56.718
STEP: Creating the pod 02/27/23 15:38:56.722
Feb 27 15:38:56.730: INFO: Waiting up to 5m0s for pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3" in namespace "secrets-594" to be "running and ready"
Feb 27 15:38:56.735: INFO: Pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.658546ms
Feb 27 15:38:56.735: INFO: The phase of Pod pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:38:58.739: INFO: Pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008919303s
Feb 27 15:38:58.739: INFO: The phase of Pod pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3 is Running (Ready = true)
Feb 27 15:38:58.739: INFO: Pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-fee36007-7d9c-49da-aeca-561260ab4ebe 02/27/23 15:38:58.757
STEP: Updating secret s-test-opt-upd-4d827fc8-12a7-4034-a138-01c09053d4e2 02/27/23 15:38:58.762
STEP: Creating secret with name s-test-opt-create-decdf84d-d3fd-45e2-b47a-85384bc9f3f9 02/27/23 15:38:58.768
STEP: waiting to observe update in volume 02/27/23 15:38:58.772
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 15:39:00.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-594" for this suite. 02/27/23 15:39:00.796
------------------------------
• [4.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:38:56.693
    Feb 27 15:38:56.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 15:38:56.693
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:38:56.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:38:56.708
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-fee36007-7d9c-49da-aeca-561260ab4ebe 02/27/23 15:38:56.714
    STEP: Creating secret with name s-test-opt-upd-4d827fc8-12a7-4034-a138-01c09053d4e2 02/27/23 15:38:56.718
    STEP: Creating the pod 02/27/23 15:38:56.722
    Feb 27 15:38:56.730: INFO: Waiting up to 5m0s for pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3" in namespace "secrets-594" to be "running and ready"
    Feb 27 15:38:56.735: INFO: Pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.658546ms
    Feb 27 15:38:56.735: INFO: The phase of Pod pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:38:58.739: INFO: Pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008919303s
    Feb 27 15:38:58.739: INFO: The phase of Pod pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3 is Running (Ready = true)
    Feb 27 15:38:58.739: INFO: Pod "pod-secrets-0b465e6a-4e27-4e5f-b9f8-a9ea33787ad3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-fee36007-7d9c-49da-aeca-561260ab4ebe 02/27/23 15:38:58.757
    STEP: Updating secret s-test-opt-upd-4d827fc8-12a7-4034-a138-01c09053d4e2 02/27/23 15:38:58.762
    STEP: Creating secret with name s-test-opt-create-decdf84d-d3fd-45e2-b47a-85384bc9f3f9 02/27/23 15:38:58.768
    STEP: waiting to observe update in volume 02/27/23 15:38:58.772
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:39:00.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-594" for this suite. 02/27/23 15:39:00.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:39:00.802
Feb 27 15:39:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename security-context-test 02/27/23 15:39:00.803
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:00.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:00.816
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Feb 27 15:39:00.824: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7" in namespace "security-context-test-3352" to be "Succeeded or Failed"
Feb 27 15:39:00.826: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124253ms
Feb 27 15:39:02.829: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004740428s
Feb 27 15:39:04.830: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Running", Reason="", readiness=false. Elapsed: 4.005303231s
Feb 27 15:39:06.831: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00671964s
Feb 27 15:39:06.831: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 15:39:06.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3352" for this suite. 02/27/23 15:39:06.844
------------------------------
• [SLOW TEST] [6.048 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:39:00.802
    Feb 27 15:39:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename security-context-test 02/27/23 15:39:00.803
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:00.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:00.816
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Feb 27 15:39:00.824: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7" in namespace "security-context-test-3352" to be "Succeeded or Failed"
    Feb 27 15:39:00.826: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124253ms
    Feb 27 15:39:02.829: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.004740428s
    Feb 27 15:39:04.830: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Running", Reason="", readiness=false. Elapsed: 4.005303231s
    Feb 27 15:39:06.831: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00671964s
    Feb 27 15:39:06.831: INFO: Pod "alpine-nnp-false-556f1be7-16f9-4328-a80b-3dc9281918f7" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:39:06.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3352" for this suite. 02/27/23 15:39:06.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:39:06.85
Feb 27 15:39:06.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename containers 02/27/23 15:39:06.851
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:06.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:06.866
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Feb 27 15:39:06.876: INFO: Waiting up to 5m0s for pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9" in namespace "containers-7158" to be "running"
Feb 27 15:39:06.879: INFO: Pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.870855ms
Feb 27 15:39:08.882: INFO: Pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006556354s
Feb 27 15:39:08.882: INFO: Pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 15:39:08.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7158" for this suite. 02/27/23 15:39:08.891
------------------------------
• [2.046 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:39:06.85
    Feb 27 15:39:06.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename containers 02/27/23 15:39:06.851
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:06.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:06.866
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Feb 27 15:39:06.876: INFO: Waiting up to 5m0s for pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9" in namespace "containers-7158" to be "running"
    Feb 27 15:39:06.879: INFO: Pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.870855ms
    Feb 27 15:39:08.882: INFO: Pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006556354s
    Feb 27 15:39:08.882: INFO: Pod "client-containers-6eae4d7d-62fb-4c01-8169-104ee5410cf9" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:39:08.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7158" for this suite. 02/27/23 15:39:08.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:39:08.897
Feb 27 15:39:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename endpointslicemirroring 02/27/23 15:39:08.898
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:08.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:08.911
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/27/23 15:39:08.923
Feb 27 15:39:08.935: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 02/27/23 15:39:10.94
Feb 27 15:39:10.950: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 02/27/23 15:39:12.954
Feb 27 15:39:12.964: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Feb 27 15:39:14.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-3050" for this suite. 02/27/23 15:39:14.97
------------------------------
• [SLOW TEST] [6.078 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:39:08.897
    Feb 27 15:39:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename endpointslicemirroring 02/27/23 15:39:08.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:08.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:08.911
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/27/23 15:39:08.923
    Feb 27 15:39:08.935: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 02/27/23 15:39:10.94
    Feb 27 15:39:10.950: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 02/27/23 15:39:12.954
    Feb 27 15:39:12.964: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:39:14.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-3050" for this suite. 02/27/23 15:39:14.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:39:14.976
Feb 27 15:39:14.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename endpointslice 02/27/23 15:39:14.977
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:14.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:14.992
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 02/27/23 15:39:20.067
STEP: referencing matching pods with named port 02/27/23 15:39:25.073
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/27/23 15:39:30.08
STEP: recreating EndpointSlices after they've been deleted 02/27/23 15:39:35.087
Feb 27 15:39:35.104: INFO: EndpointSlice for Service endpointslice-1966/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 15:39:45.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1966" for this suite. 02/27/23 15:39:45.114
------------------------------
• [SLOW TEST] [30.143 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:39:14.976
    Feb 27 15:39:14.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename endpointslice 02/27/23 15:39:14.977
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:14.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:14.992
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 02/27/23 15:39:20.067
    STEP: referencing matching pods with named port 02/27/23 15:39:25.073
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/27/23 15:39:30.08
    STEP: recreating EndpointSlices after they've been deleted 02/27/23 15:39:35.087
    Feb 27 15:39:35.104: INFO: EndpointSlice for Service endpointslice-1966/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:39:45.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1966" for this suite. 02/27/23 15:39:45.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:39:45.12
Feb 27 15:39:45.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename cronjob 02/27/23 15:39:45.12
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:45.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:45.139
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/27/23 15:39:45.141
STEP: Ensuring no jobs are scheduled 02/27/23 15:39:45.149
STEP: Ensuring no job exists by listing jobs explicitly 02/27/23 15:44:45.155
STEP: Removing cronjob 02/27/23 15:44:45.157
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 15:44:45.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-632" for this suite. 02/27/23 15:44:45.166
------------------------------
• [SLOW TEST] [300.051 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:39:45.12
    Feb 27 15:39:45.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename cronjob 02/27/23 15:39:45.12
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:39:45.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:39:45.139
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/27/23 15:39:45.141
    STEP: Ensuring no jobs are scheduled 02/27/23 15:39:45.149
    STEP: Ensuring no job exists by listing jobs explicitly 02/27/23 15:44:45.155
    STEP: Removing cronjob 02/27/23 15:44:45.157
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:44:45.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-632" for this suite. 02/27/23 15:44:45.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:44:45.171
Feb 27 15:44:45.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:44:45.172
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:44:45.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:44:45.187
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:44:45.194
Feb 27 15:44:45.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34" in namespace "downward-api-5384" to be "Succeeded or Failed"
Feb 27 15:44:45.208: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.687828ms
Feb 27 15:44:47.211: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009931924s
Feb 27 15:44:49.212: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011017746s
STEP: Saw pod success 02/27/23 15:44:49.212
Feb 27 15:44:49.212: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34" satisfied condition "Succeeded or Failed"
Feb 27 15:44:49.215: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34 container client-container: <nil>
STEP: delete the pod 02/27/23 15:44:49.229
Feb 27 15:44:49.240: INFO: Waiting for pod downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34 to disappear
Feb 27 15:44:49.243: INFO: Pod downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 15:44:49.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5384" for this suite. 02/27/23 15:44:49.245
------------------------------
• [4.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:44:45.171
    Feb 27 15:44:45.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:44:45.172
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:44:45.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:44:45.187
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:44:45.194
    Feb 27 15:44:45.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34" in namespace "downward-api-5384" to be "Succeeded or Failed"
    Feb 27 15:44:45.208: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.687828ms
    Feb 27 15:44:47.211: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009931924s
    Feb 27 15:44:49.212: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011017746s
    STEP: Saw pod success 02/27/23 15:44:49.212
    Feb 27 15:44:49.212: INFO: Pod "downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34" satisfied condition "Succeeded or Failed"
    Feb 27 15:44:49.215: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34 container client-container: <nil>
    STEP: delete the pod 02/27/23 15:44:49.229
    Feb 27 15:44:49.240: INFO: Waiting for pod downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34 to disappear
    Feb 27 15:44:49.243: INFO: Pod downwardapi-volume-4d3f85a6-011d-4287-a2cb-414b2d86ce34 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:44:49.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5384" for this suite. 02/27/23 15:44:49.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:44:49.251
Feb 27 15:44:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename podtemplate 02/27/23 15:44:49.252
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:44:49.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:44:49.266
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 27 15:44:49.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5131" for this suite. 02/27/23 15:44:49.295
------------------------------
• [0.049 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:44:49.251
    Feb 27 15:44:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename podtemplate 02/27/23 15:44:49.252
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:44:49.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:44:49.266
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:44:49.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5131" for this suite. 02/27/23 15:44:49.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:44:49.303
Feb 27 15:44:49.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 15:44:49.303
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:44:49.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:44:49.316
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-575 02/27/23 15:44:49.319
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 02/27/23 15:44:49.324
STEP: Creating pod with conflicting port in namespace statefulset-575 02/27/23 15:44:49.332
STEP: Waiting until pod test-pod will start running in namespace statefulset-575 02/27/23 15:44:49.339
Feb 27 15:44:49.339: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-575" to be "running"
Feb 27 15:44:49.342: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588445ms
Feb 27 15:44:51.345: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00607412s
Feb 27 15:44:51.345: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-575 02/27/23 15:44:51.345
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-575 02/27/23 15:44:51.355
Feb 27 15:44:51.368: INFO: Observed stateful pod in namespace: statefulset-575, name: ss-0, uid: 332879d4-f901-41a0-bd53-bc9f63541fa5, status phase: Pending. Waiting for statefulset controller to delete.
Feb 27 15:44:51.381: INFO: Observed stateful pod in namespace: statefulset-575, name: ss-0, uid: 332879d4-f901-41a0-bd53-bc9f63541fa5, status phase: Failed. Waiting for statefulset controller to delete.
Feb 27 15:44:51.391: INFO: Observed stateful pod in namespace: statefulset-575, name: ss-0, uid: 332879d4-f901-41a0-bd53-bc9f63541fa5, status phase: Failed. Waiting for statefulset controller to delete.
Feb 27 15:44:51.396: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-575
STEP: Removing pod with conflicting port in namespace statefulset-575 02/27/23 15:44:51.396
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-575 and will be in running state 02/27/23 15:44:51.417
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 15:44:53.425: INFO: Deleting all statefulset in ns statefulset-575
Feb 27 15:44:53.428: INFO: Scaling statefulset ss to 0
Feb 27 15:45:03.445: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 15:45:03.447: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:45:03.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-575" for this suite. 02/27/23 15:45:03.467
------------------------------
• [SLOW TEST] [14.172 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:44:49.303
    Feb 27 15:44:49.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 15:44:49.303
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:44:49.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:44:49.316
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-575 02/27/23 15:44:49.319
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 02/27/23 15:44:49.324
    STEP: Creating pod with conflicting port in namespace statefulset-575 02/27/23 15:44:49.332
    STEP: Waiting until pod test-pod will start running in namespace statefulset-575 02/27/23 15:44:49.339
    Feb 27 15:44:49.339: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-575" to be "running"
    Feb 27 15:44:49.342: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588445ms
    Feb 27 15:44:51.345: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00607412s
    Feb 27 15:44:51.345: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-575 02/27/23 15:44:51.345
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-575 02/27/23 15:44:51.355
    Feb 27 15:44:51.368: INFO: Observed stateful pod in namespace: statefulset-575, name: ss-0, uid: 332879d4-f901-41a0-bd53-bc9f63541fa5, status phase: Pending. Waiting for statefulset controller to delete.
    Feb 27 15:44:51.381: INFO: Observed stateful pod in namespace: statefulset-575, name: ss-0, uid: 332879d4-f901-41a0-bd53-bc9f63541fa5, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 27 15:44:51.391: INFO: Observed stateful pod in namespace: statefulset-575, name: ss-0, uid: 332879d4-f901-41a0-bd53-bc9f63541fa5, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 27 15:44:51.396: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-575
    STEP: Removing pod with conflicting port in namespace statefulset-575 02/27/23 15:44:51.396
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-575 and will be in running state 02/27/23 15:44:51.417
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 15:44:53.425: INFO: Deleting all statefulset in ns statefulset-575
    Feb 27 15:44:53.428: INFO: Scaling statefulset ss to 0
    Feb 27 15:45:03.445: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 15:45:03.447: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:45:03.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-575" for this suite. 02/27/23 15:45:03.467
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:45:03.475
Feb 27 15:45:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename subpath 02/27/23 15:45:03.476
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:03.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:03.491
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 15:45:03.494
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-wz9t 02/27/23 15:45:03.502
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 15:45:03.502
Feb 27 15:45:03.510: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wz9t" in namespace "subpath-3399" to be "Succeeded or Failed"
Feb 27 15:45:03.512: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.667787ms
Feb 27 15:45:05.519: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.00930992s
Feb 27 15:45:07.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.006705853s
Feb 27 15:45:09.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.006342356s
Feb 27 15:45:11.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.006300999s
Feb 27 15:45:13.517: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.007491691s
Feb 27 15:45:15.515: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.005526677s
Feb 27 15:45:17.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.006329991s
Feb 27 15:45:19.515: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.005547036s
Feb 27 15:45:21.517: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.007284561s
Feb 27 15:45:23.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.006382946s
Feb 27 15:45:25.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=false. Elapsed: 22.006667728s
Feb 27 15:45:27.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006396383s
STEP: Saw pod success 02/27/23 15:45:27.516
Feb 27 15:45:27.516: INFO: Pod "pod-subpath-test-configmap-wz9t" satisfied condition "Succeeded or Failed"
Feb 27 15:45:27.519: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-configmap-wz9t container test-container-subpath-configmap-wz9t: <nil>
STEP: delete the pod 02/27/23 15:45:27.526
Feb 27 15:45:27.536: INFO: Waiting for pod pod-subpath-test-configmap-wz9t to disappear
Feb 27 15:45:27.539: INFO: Pod pod-subpath-test-configmap-wz9t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wz9t 02/27/23 15:45:27.539
Feb 27 15:45:27.539: INFO: Deleting pod "pod-subpath-test-configmap-wz9t" in namespace "subpath-3399"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 15:45:27.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3399" for this suite. 02/27/23 15:45:27.544
------------------------------
• [SLOW TEST] [24.074 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:45:03.475
    Feb 27 15:45:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename subpath 02/27/23 15:45:03.476
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:03.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:03.491
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 15:45:03.494
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-wz9t 02/27/23 15:45:03.502
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 15:45:03.502
    Feb 27 15:45:03.510: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wz9t" in namespace "subpath-3399" to be "Succeeded or Failed"
    Feb 27 15:45:03.512: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.667787ms
    Feb 27 15:45:05.519: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.00930992s
    Feb 27 15:45:07.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.006705853s
    Feb 27 15:45:09.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.006342356s
    Feb 27 15:45:11.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.006300999s
    Feb 27 15:45:13.517: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.007491691s
    Feb 27 15:45:15.515: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.005526677s
    Feb 27 15:45:17.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.006329991s
    Feb 27 15:45:19.515: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.005547036s
    Feb 27 15:45:21.517: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.007284561s
    Feb 27 15:45:23.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.006382946s
    Feb 27 15:45:25.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Running", Reason="", readiness=false. Elapsed: 22.006667728s
    Feb 27 15:45:27.516: INFO: Pod "pod-subpath-test-configmap-wz9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006396383s
    STEP: Saw pod success 02/27/23 15:45:27.516
    Feb 27 15:45:27.516: INFO: Pod "pod-subpath-test-configmap-wz9t" satisfied condition "Succeeded or Failed"
    Feb 27 15:45:27.519: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-configmap-wz9t container test-container-subpath-configmap-wz9t: <nil>
    STEP: delete the pod 02/27/23 15:45:27.526
    Feb 27 15:45:27.536: INFO: Waiting for pod pod-subpath-test-configmap-wz9t to disappear
    Feb 27 15:45:27.539: INFO: Pod pod-subpath-test-configmap-wz9t no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-wz9t 02/27/23 15:45:27.539
    Feb 27 15:45:27.539: INFO: Deleting pod "pod-subpath-test-configmap-wz9t" in namespace "subpath-3399"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:45:27.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3399" for this suite. 02/27/23 15:45:27.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:45:27.55
Feb 27 15:45:27.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 15:45:27.55
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:27.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:27.565
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/27/23 15:45:27.567
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
 02/27/23 15:45:27.572
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
 02/27/23 15:45:27.572
STEP: creating a pod to probe DNS 02/27/23 15:45:27.572
STEP: submitting the pod to kubernetes 02/27/23 15:45:27.572
Feb 27 15:45:27.580: INFO: Waiting up to 15m0s for pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a" in namespace "dns-454" to be "running"
Feb 27 15:45:27.585: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.56994ms
Feb 27 15:45:29.589: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009249605s
Feb 27 15:45:31.589: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009789704s
Feb 27 15:45:33.590: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009997361s
Feb 27 15:45:35.588: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Running", Reason="", readiness=true. Elapsed: 8.008106881s
Feb 27 15:45:35.588: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a" satisfied condition "running"
STEP: retrieving the pod 02/27/23 15:45:35.588
STEP: looking for the results for each expected name from probers 02/27/23 15:45:35.591
Feb 27 15:45:35.602: INFO: DNS probes using dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a succeeded

STEP: deleting the pod 02/27/23 15:45:35.602
STEP: changing the externalName to bar.example.com 02/27/23 15:45:35.615
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
 02/27/23 15:45:35.621
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
 02/27/23 15:45:35.621
STEP: creating a second pod to probe DNS 02/27/23 15:45:35.621
STEP: submitting the pod to kubernetes 02/27/23 15:45:35.621
Feb 27 15:45:35.629: INFO: Waiting up to 15m0s for pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792" in namespace "dns-454" to be "running"
Feb 27 15:45:35.633: INFO: Pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792": Phase="Pending", Reason="", readiness=false. Elapsed: 4.508706ms
Feb 27 15:45:37.637: INFO: Pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063056s
Feb 27 15:45:37.637: INFO: Pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792" satisfied condition "running"
STEP: retrieving the pod 02/27/23 15:45:37.637
STEP: looking for the results for each expected name from probers 02/27/23 15:45:37.639
Feb 27 15:45:37.644: INFO: File wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local from pod  dns-454/dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 15:45:37.648: INFO: File jessie_udp@dns-test-service-3.dns-454.svc.cluster.local from pod  dns-454/dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 15:45:37.648: INFO: Lookups using dns-454/dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 failed for: [wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local jessie_udp@dns-test-service-3.dns-454.svc.cluster.local]

Feb 27 15:45:42.658: INFO: DNS probes using dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 succeeded

STEP: deleting the pod 02/27/23 15:45:42.658
STEP: changing the service to type=ClusterIP 02/27/23 15:45:42.672
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
 02/27/23 15:45:42.683
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
 02/27/23 15:45:42.683
STEP: creating a third pod to probe DNS 02/27/23 15:45:42.683
STEP: submitting the pod to kubernetes 02/27/23 15:45:42.687
Feb 27 15:45:42.695: INFO: Waiting up to 15m0s for pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374" in namespace "dns-454" to be "running"
Feb 27 15:45:42.703: INFO: Pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374": Phase="Pending", Reason="", readiness=false. Elapsed: 7.692354ms
Feb 27 15:45:44.706: INFO: Pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374": Phase="Running", Reason="", readiness=true. Elapsed: 2.011294652s
Feb 27 15:45:44.706: INFO: Pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374" satisfied condition "running"
STEP: retrieving the pod 02/27/23 15:45:44.706
STEP: looking for the results for each expected name from probers 02/27/23 15:45:44.71
Feb 27 15:45:44.717: INFO: DNS probes using dns-test-90f470a6-796b-474e-8f0a-4c826d82f374 succeeded

STEP: deleting the pod 02/27/23 15:45:44.717
STEP: deleting the test externalName service 02/27/23 15:45:44.729
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 15:45:44.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-454" for this suite. 02/27/23 15:45:44.746
------------------------------
• [SLOW TEST] [17.206 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:45:27.55
    Feb 27 15:45:27.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 15:45:27.55
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:27.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:27.565
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/27/23 15:45:27.567
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
     02/27/23 15:45:27.572
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
     02/27/23 15:45:27.572
    STEP: creating a pod to probe DNS 02/27/23 15:45:27.572
    STEP: submitting the pod to kubernetes 02/27/23 15:45:27.572
    Feb 27 15:45:27.580: INFO: Waiting up to 15m0s for pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a" in namespace "dns-454" to be "running"
    Feb 27 15:45:27.585: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.56994ms
    Feb 27 15:45:29.589: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009249605s
    Feb 27 15:45:31.589: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009789704s
    Feb 27 15:45:33.590: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009997361s
    Feb 27 15:45:35.588: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a": Phase="Running", Reason="", readiness=true. Elapsed: 8.008106881s
    Feb 27 15:45:35.588: INFO: Pod "dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 15:45:35.588
    STEP: looking for the results for each expected name from probers 02/27/23 15:45:35.591
    Feb 27 15:45:35.602: INFO: DNS probes using dns-test-7c8e3e19-43c2-454c-8276-f50c90684a5a succeeded

    STEP: deleting the pod 02/27/23 15:45:35.602
    STEP: changing the externalName to bar.example.com 02/27/23 15:45:35.615
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
     02/27/23 15:45:35.621
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
     02/27/23 15:45:35.621
    STEP: creating a second pod to probe DNS 02/27/23 15:45:35.621
    STEP: submitting the pod to kubernetes 02/27/23 15:45:35.621
    Feb 27 15:45:35.629: INFO: Waiting up to 15m0s for pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792" in namespace "dns-454" to be "running"
    Feb 27 15:45:35.633: INFO: Pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792": Phase="Pending", Reason="", readiness=false. Elapsed: 4.508706ms
    Feb 27 15:45:37.637: INFO: Pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792": Phase="Running", Reason="", readiness=true. Elapsed: 2.008063056s
    Feb 27 15:45:37.637: INFO: Pod "dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 15:45:37.637
    STEP: looking for the results for each expected name from probers 02/27/23 15:45:37.639
    Feb 27 15:45:37.644: INFO: File wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local from pod  dns-454/dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 15:45:37.648: INFO: File jessie_udp@dns-test-service-3.dns-454.svc.cluster.local from pod  dns-454/dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 15:45:37.648: INFO: Lookups using dns-454/dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 failed for: [wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local jessie_udp@dns-test-service-3.dns-454.svc.cluster.local]

    Feb 27 15:45:42.658: INFO: DNS probes using dns-test-f1f44336-d303-4c6a-93dc-ee46c29a2792 succeeded

    STEP: deleting the pod 02/27/23 15:45:42.658
    STEP: changing the service to type=ClusterIP 02/27/23 15:45:42.672
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
     02/27/23 15:45:42.683
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-454.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-454.svc.cluster.local; sleep 1; done
     02/27/23 15:45:42.683
    STEP: creating a third pod to probe DNS 02/27/23 15:45:42.683
    STEP: submitting the pod to kubernetes 02/27/23 15:45:42.687
    Feb 27 15:45:42.695: INFO: Waiting up to 15m0s for pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374" in namespace "dns-454" to be "running"
    Feb 27 15:45:42.703: INFO: Pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374": Phase="Pending", Reason="", readiness=false. Elapsed: 7.692354ms
    Feb 27 15:45:44.706: INFO: Pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374": Phase="Running", Reason="", readiness=true. Elapsed: 2.011294652s
    Feb 27 15:45:44.706: INFO: Pod "dns-test-90f470a6-796b-474e-8f0a-4c826d82f374" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 15:45:44.706
    STEP: looking for the results for each expected name from probers 02/27/23 15:45:44.71
    Feb 27 15:45:44.717: INFO: DNS probes using dns-test-90f470a6-796b-474e-8f0a-4c826d82f374 succeeded

    STEP: deleting the pod 02/27/23 15:45:44.717
    STEP: deleting the test externalName service 02/27/23 15:45:44.729
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:45:44.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-454" for this suite. 02/27/23 15:45:44.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:45:44.757
Feb 27 15:45:44.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:45:44.758
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:44.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:44.772
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:45:44.788
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:45:45.22
STEP: Deploying the webhook pod 02/27/23 15:45:45.228
STEP: Wait for the deployment to be ready 02/27/23 15:45:45.245
Feb 27 15:45:45.253: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:45:47.262
STEP: Verifying the service has paired with the endpoint 02/27/23 15:45:47.274
Feb 27 15:45:48.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/27/23 15:45:48.278
Feb 27 15:45:48.292: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook 02/27/23 15:45:48.4
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:45:48.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4138" for this suite. 02/27/23 15:45:48.467
STEP: Destroying namespace "webhook-4138-markers" for this suite. 02/27/23 15:45:48.475
------------------------------
• [3.724 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:45:44.757
    Feb 27 15:45:44.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:45:44.758
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:44.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:44.772
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:45:44.788
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:45:45.22
    STEP: Deploying the webhook pod 02/27/23 15:45:45.228
    STEP: Wait for the deployment to be ready 02/27/23 15:45:45.245
    Feb 27 15:45:45.253: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:45:47.262
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:45:47.274
    Feb 27 15:45:48.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/27/23 15:45:48.278
    Feb 27 15:45:48.292: INFO: Waiting for webhook configuration to be ready...
    STEP: create a configmap that should be updated by the webhook 02/27/23 15:45:48.4
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:45:48.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4138" for this suite. 02/27/23 15:45:48.467
    STEP: Destroying namespace "webhook-4138-markers" for this suite. 02/27/23 15:45:48.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:45:48.482
Feb 27 15:45:48.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename events 02/27/23 15:45:48.482
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:48.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:48.494
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/27/23 15:45:48.499
STEP: listing events in all namespaces 02/27/23 15:45:48.506
STEP: listing events in test namespace 02/27/23 15:45:48.519
STEP: listing events with field selection filtering on source 02/27/23 15:45:48.521
STEP: listing events with field selection filtering on reportingController 02/27/23 15:45:48.524
STEP: getting the test event 02/27/23 15:45:48.527
STEP: patching the test event 02/27/23 15:45:48.529
STEP: getting the test event 02/27/23 15:45:48.538
STEP: updating the test event 02/27/23 15:45:48.541
STEP: getting the test event 02/27/23 15:45:48.546
STEP: deleting the test event 02/27/23 15:45:48.549
STEP: listing events in all namespaces 02/27/23 15:45:48.557
STEP: listing events in test namespace 02/27/23 15:45:48.564
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Feb 27 15:45:48.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-281" for this suite. 02/27/23 15:45:48.57
------------------------------
• [0.094 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:45:48.482
    Feb 27 15:45:48.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename events 02/27/23 15:45:48.482
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:48.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:48.494
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/27/23 15:45:48.499
    STEP: listing events in all namespaces 02/27/23 15:45:48.506
    STEP: listing events in test namespace 02/27/23 15:45:48.519
    STEP: listing events with field selection filtering on source 02/27/23 15:45:48.521
    STEP: listing events with field selection filtering on reportingController 02/27/23 15:45:48.524
    STEP: getting the test event 02/27/23 15:45:48.527
    STEP: patching the test event 02/27/23 15:45:48.529
    STEP: getting the test event 02/27/23 15:45:48.538
    STEP: updating the test event 02/27/23 15:45:48.541
    STEP: getting the test event 02/27/23 15:45:48.546
    STEP: deleting the test event 02/27/23 15:45:48.549
    STEP: listing events in all namespaces 02/27/23 15:45:48.557
    STEP: listing events in test namespace 02/27/23 15:45:48.564
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:45:48.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-281" for this suite. 02/27/23 15:45:48.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:45:48.577
Feb 27 15:45:48.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 15:45:48.577
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:48.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:48.591
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-4099f84f-9797-49ab-88ff-5789c9643be7 02/27/23 15:45:48.593
STEP: Creating a pod to test consume secrets 02/27/23 15:45:48.606
Feb 27 15:45:48.614: INFO: Waiting up to 5m0s for pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7" in namespace "secrets-19" to be "Succeeded or Failed"
Feb 27 15:45:48.619: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409435ms
Feb 27 15:45:50.622: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007455974s
Feb 27 15:45:52.623: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009211284s
STEP: Saw pod success 02/27/23 15:45:52.623
Feb 27 15:45:52.624: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7" satisfied condition "Succeeded or Failed"
Feb 27 15:45:52.626: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:45:52.632
Feb 27 15:45:52.644: INFO: Waiting for pod pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7 to disappear
Feb 27 15:45:52.646: INFO: Pod pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 15:45:52.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-19" for this suite. 02/27/23 15:45:52.649
------------------------------
• [4.077 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:45:48.577
    Feb 27 15:45:48.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 15:45:48.577
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:48.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:48.591
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-4099f84f-9797-49ab-88ff-5789c9643be7 02/27/23 15:45:48.593
    STEP: Creating a pod to test consume secrets 02/27/23 15:45:48.606
    Feb 27 15:45:48.614: INFO: Waiting up to 5m0s for pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7" in namespace "secrets-19" to be "Succeeded or Failed"
    Feb 27 15:45:48.619: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409435ms
    Feb 27 15:45:50.622: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007455974s
    Feb 27 15:45:52.623: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009211284s
    STEP: Saw pod success 02/27/23 15:45:52.623
    Feb 27 15:45:52.624: INFO: Pod "pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7" satisfied condition "Succeeded or Failed"
    Feb 27 15:45:52.626: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:45:52.632
    Feb 27 15:45:52.644: INFO: Waiting for pod pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7 to disappear
    Feb 27 15:45:52.646: INFO: Pod pod-secrets-6aa8f79a-3fe6-4b39-b362-83ac0d6053b7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:45:52.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-19" for this suite. 02/27/23 15:45:52.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:45:52.654
Feb 27 15:45:52.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 15:45:52.655
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:52.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:52.671
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/27/23 15:45:52.677
STEP: waiting for Deployment to be created 02/27/23 15:45:52.682
STEP: waiting for all Replicas to be Ready 02/27/23 15:45:52.683
Feb 27 15:45:52.684: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:52.684: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:52.692: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:52.692: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:52.710: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:52.710: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:52.814: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:52.814: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 15:45:53.921: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 27 15:45:53.921: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 27 15:45:54.127: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/27/23 15:45:54.127
W0227 15:45:54.137220      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 15:45:54.138: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/27/23 15:45:54.138
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.150: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.150: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.166: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.166: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.181: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:54.182: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:54.196: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:54.196: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:54.943: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.943: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:54.968: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
STEP: listing Deployments 02/27/23 15:45:54.968
Feb 27 15:45:54.972: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/27/23 15:45:54.972
Feb 27 15:45:54.983: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/27/23 15:45:54.983
Feb 27 15:45:54.988: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:54.994: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:55.014: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:55.027: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:55.040: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:55.940: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:56.067: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:56.087: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 15:45:57.168: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/27/23 15:45:57.191
STEP: fetching the DeploymentStatus 02/27/23 15:45:57.196
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 3
STEP: deleting the Deployment 02/27/23 15:45:57.2
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
Feb 27 15:45:57.210: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 15:45:57.217: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 27 15:45:57.220: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9705  c69c5c8d-3256-401f-844a-d64fb64d9424 15964 2 2023-02-27 15:45:55 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a5ca236e-522c-41e5-acfc-9e74ba45480e 0xc004234617 0xc004234618}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5ca236e-522c-41e5-acfc-9e74ba45480e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042346a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb 27 15:45:57.224: INFO: pod: "test-deployment-7b7876f9d6-c9gmj":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-c9gmj test-deployment-7b7876f9d6- deployment-9705  65744a94-dbbc-4f98-9c95-444c308dfafb 15918 0 2023-02-27 15:45:55 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c69c5c8d-3256-401f-844a-d64fb64d9424 0xc004235837 0xc004235838}] [] [{kube-controller-manager Update v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c69c5c8d-3256-401f-844a-d64fb64d9424\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8bpf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8bpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.161,StartTime:2023-02-27 15:45:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:45:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e4fe39db83ec3f27b13509d6683854fd5561166c8438dc4545a8eacc7f3d2768,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 15:45:57.224: INFO: pod: "test-deployment-7b7876f9d6-cp54t":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-cp54t test-deployment-7b7876f9d6- deployment-9705  f50969aa-375d-42f3-a7f9-bfd53da2a6c1 15963 0 2023-02-27 15:45:56 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c69c5c8d-3256-401f-844a-d64fb64d9424 0xc004235db7 0xc004235db8}] [] [{kube-controller-manager Update v1 2023-02-27 15:45:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c69c5c8d-3256-401f-844a-d64fb64d9424\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdnpz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdnpz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.139,StartTime:2023-02-27 15:45:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:45:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9fea291f18a809bab8d95f77c1836b3530e024cdf92af1547536473c82278120,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 15:45:57.224: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9705  b793f471-9620-403e-abaf-5f123f7b683e 15975 4 2023-02-27 15:45:54 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment a5ca236e-522c-41e5-acfc-9e74ba45480e 0xc004234707 0xc004234708}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5ca236e-522c-41e5-acfc-9e74ba45480e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004234790 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 27 15:45:57.230: INFO: pod: "test-deployment-7df74c55ff-wmlf6":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-wmlf6 test-deployment-7df74c55ff- deployment-9705  f0bb4ba3-2c7f-468c-82ea-8f0872cb9f87 15969 0 2023-02-27 15:45:54 +0000 UTC 2023-02-27 15:45:58 +0000 UTC 0xc00310d3d8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff b793f471-9620-403e-abaf-5f123f7b683e 0xc00310d517 0xc00310d518}] [] [{kube-controller-manager Update v1 2023-02-27 15:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b793f471-9620-403e-abaf-5f123f7b683e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:45:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68sbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68sbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.152,StartTime:2023-02-27 15:45:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:45:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://d9e97941ff48d53432c3e6f86978c2cd8b0f762d22788df7cc8de947fa6ac08a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 15:45:57.230: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9705  fe8fbab4-99b1-4464-bbd3-4bbc45690ca7 15879 3 2023-02-27 15:45:52 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a5ca236e-522c-41e5-acfc-9e74ba45480e 0xc0042348f7 0xc0042348f8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5ca236e-522c-41e5-acfc-9e74ba45480e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004234bd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 15:45:57.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9705" for this suite. 02/27/23 15:45:57.24
------------------------------
• [4.591 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:45:52.654
    Feb 27 15:45:52.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 15:45:52.655
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:52.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:52.671
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/27/23 15:45:52.677
    STEP: waiting for Deployment to be created 02/27/23 15:45:52.682
    STEP: waiting for all Replicas to be Ready 02/27/23 15:45:52.683
    Feb 27 15:45:52.684: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:52.684: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:52.692: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:52.692: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:52.710: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:52.710: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:52.814: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:52.814: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 15:45:53.921: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 27 15:45:53.921: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 27 15:45:54.127: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/27/23 15:45:54.127
    W0227 15:45:54.137220      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 15:45:54.138: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/27/23 15:45:54.138
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 0
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.140: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.150: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.150: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.166: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.166: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.181: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:54.182: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:54.196: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:54.196: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:54.943: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.943: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:54.968: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    STEP: listing Deployments 02/27/23 15:45:54.968
    Feb 27 15:45:54.972: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/27/23 15:45:54.972
    Feb 27 15:45:54.983: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/27/23 15:45:54.983
    Feb 27 15:45:54.988: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:54.994: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:55.014: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:55.027: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:55.040: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:55.940: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:56.067: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:56.087: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 15:45:57.168: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/27/23 15:45:57.191
    STEP: fetching the DeploymentStatus 02/27/23 15:45:57.196
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 1
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 2
    Feb 27 15:45:57.200: INFO: observed Deployment test-deployment in namespace deployment-9705 with ReadyReplicas 3
    STEP: deleting the Deployment 02/27/23 15:45:57.2
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    Feb 27 15:45:57.210: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 15:45:57.217: INFO: Log out all the ReplicaSets if there is no deployment created
    Feb 27 15:45:57.220: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-9705  c69c5c8d-3256-401f-844a-d64fb64d9424 15964 2 2023-02-27 15:45:55 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a5ca236e-522c-41e5-acfc-9e74ba45480e 0xc004234617 0xc004234618}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5ca236e-522c-41e5-acfc-9e74ba45480e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042346a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Feb 27 15:45:57.224: INFO: pod: "test-deployment-7b7876f9d6-c9gmj":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-c9gmj test-deployment-7b7876f9d6- deployment-9705  65744a94-dbbc-4f98-9c95-444c308dfafb 15918 0 2023-02-27 15:45:55 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c69c5c8d-3256-401f-844a-d64fb64d9424 0xc004235837 0xc004235838}] [] [{kube-controller-manager Update v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c69c5c8d-3256-401f-844a-d64fb64d9424\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8bpf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8bpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.161,StartTime:2023-02-27 15:45:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:45:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e4fe39db83ec3f27b13509d6683854fd5561166c8438dc4545a8eacc7f3d2768,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 15:45:57.224: INFO: pod: "test-deployment-7b7876f9d6-cp54t":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-cp54t test-deployment-7b7876f9d6- deployment-9705  f50969aa-375d-42f3-a7f9-bfd53da2a6c1 15963 0 2023-02-27 15:45:56 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c69c5c8d-3256-401f-844a-d64fb64d9424 0xc004235db7 0xc004235db8}] [] [{kube-controller-manager Update v1 2023-02-27 15:45:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c69c5c8d-3256-401f-844a-d64fb64d9424\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdnpz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdnpz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.139,StartTime:2023-02-27 15:45:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:45:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9fea291f18a809bab8d95f77c1836b3530e024cdf92af1547536473c82278120,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 15:45:57.224: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-9705  b793f471-9620-403e-abaf-5f123f7b683e 15975 4 2023-02-27 15:45:54 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment a5ca236e-522c-41e5-acfc-9e74ba45480e 0xc004234707 0xc004234708}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5ca236e-522c-41e5-acfc-9e74ba45480e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:45:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004234790 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Feb 27 15:45:57.230: INFO: pod: "test-deployment-7df74c55ff-wmlf6":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-wmlf6 test-deployment-7df74c55ff- deployment-9705  f0bb4ba3-2c7f-468c-82ea-8f0872cb9f87 15969 0 2023-02-27 15:45:54 +0000 UTC 2023-02-27 15:45:58 +0000 UTC 0xc00310d3d8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff b793f471-9620-403e-abaf-5f123f7b683e 0xc00310d517 0xc00310d518}] [] [{kube-controller-manager Update v1 2023-02-27 15:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b793f471-9620-403e-abaf-5f123f7b683e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 15:45:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68sbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68sbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 15:45:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.152,StartTime:2023-02-27 15:45:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 15:45:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://d9e97941ff48d53432c3e6f86978c2cd8b0f762d22788df7cc8de947fa6ac08a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 15:45:57.230: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-9705  fe8fbab4-99b1-4464-bbd3-4bbc45690ca7 15879 3 2023-02-27 15:45:52 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a5ca236e-522c-41e5-acfc-9e74ba45480e 0xc0042348f7 0xc0042348f8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 15:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5ca236e-522c-41e5-acfc-9e74ba45480e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 15:45:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004234bd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:45:57.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9705" for this suite. 02/27/23 15:45:57.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:45:57.246
Feb 27 15:45:57.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 15:45:57.246
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:57.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:57.261
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-5acf2703-c30f-447e-a9bf-6a6c3061896c 02/27/23 15:45:57.263
STEP: Creating a pod to test consume secrets 02/27/23 15:45:57.266
Feb 27 15:45:57.274: INFO: Waiting up to 5m0s for pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b" in namespace "secrets-3845" to be "Succeeded or Failed"
Feb 27 15:45:57.276: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518272ms
Feb 27 15:45:59.279: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00569264s
Feb 27 15:46:01.279: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00559351s
STEP: Saw pod success 02/27/23 15:46:01.279
Feb 27 15:46:01.279: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b" satisfied condition "Succeeded or Failed"
Feb 27 15:46:01.282: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:46:01.288
Feb 27 15:46:01.299: INFO: Waiting for pod pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b to disappear
Feb 27 15:46:01.301: INFO: Pod pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:01.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3845" for this suite. 02/27/23 15:46:01.304
------------------------------
• [4.063 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:45:57.246
    Feb 27 15:45:57.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 15:45:57.246
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:45:57.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:45:57.261
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-5acf2703-c30f-447e-a9bf-6a6c3061896c 02/27/23 15:45:57.263
    STEP: Creating a pod to test consume secrets 02/27/23 15:45:57.266
    Feb 27 15:45:57.274: INFO: Waiting up to 5m0s for pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b" in namespace "secrets-3845" to be "Succeeded or Failed"
    Feb 27 15:45:57.276: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518272ms
    Feb 27 15:45:59.279: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00569264s
    Feb 27 15:46:01.279: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00559351s
    STEP: Saw pod success 02/27/23 15:46:01.279
    Feb 27 15:46:01.279: INFO: Pod "pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b" satisfied condition "Succeeded or Failed"
    Feb 27 15:46:01.282: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:46:01.288
    Feb 27 15:46:01.299: INFO: Waiting for pod pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b to disappear
    Feb 27 15:46:01.301: INFO: Pod pod-secrets-051bea2a-6e4f-45cd-a842-d01a492fdc0b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:01.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3845" for this suite. 02/27/23 15:46:01.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:01.31
Feb 27 15:46:01.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 15:46:01.311
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:01.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:01.327
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Feb 27 15:46:01.337: INFO: Waiting up to 2m0s for pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" in namespace "var-expansion-2097" to be "container 0 failed with reason CreateContainerConfigError"
Feb 27 15:46:01.342: INFO: Pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.013525ms
Feb 27 15:46:03.346: INFO: Pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008908744s
Feb 27 15:46:03.346: INFO: Pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 27 15:46:03.346: INFO: Deleting pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" in namespace "var-expansion-2097"
Feb 27 15:46:03.354: INFO: Wait up to 5m0s for pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:05.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2097" for this suite. 02/27/23 15:46:05.365
------------------------------
• [4.064 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:01.31
    Feb 27 15:46:01.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 15:46:01.311
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:01.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:01.327
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Feb 27 15:46:01.337: INFO: Waiting up to 2m0s for pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" in namespace "var-expansion-2097" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 27 15:46:01.342: INFO: Pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.013525ms
    Feb 27 15:46:03.346: INFO: Pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008908744s
    Feb 27 15:46:03.346: INFO: Pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 27 15:46:03.346: INFO: Deleting pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" in namespace "var-expansion-2097"
    Feb 27 15:46:03.354: INFO: Wait up to 5m0s for pod "var-expansion-8d6f52a2-33b1-4799-a827-994169dcf8e7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:05.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2097" for this suite. 02/27/23 15:46:05.365
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:05.377
Feb 27 15:46:05.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:46:05.378
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:05.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:05.392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 15:46:05.395
Feb 27 15:46:05.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 27 15:46:05.451: INFO: stderr: ""
Feb 27 15:46:05.451: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/27/23 15:46:05.451
STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 15:46:10.502
Feb 27 15:46:10.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 get pod e2e-test-httpd-pod -o json'
Feb 27 15:46:10.549: INFO: stderr: ""
Feb 27 15:46:10.549: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-02-27T15:46:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-412\",\n        \"resourceVersion\": \"16164\",\n        \"uid\": \"1a96509e-eddb-491e-9287-7c4a9854aeba\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-ghmvb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-42-40\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-ghmvb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6e6fbbb27e40c2c7aa160e004e57d279996332adace592a1e80838a814c2ccb2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-27T15:46:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.42.40\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.192.176\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.192.176\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-27T15:46:05Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/27/23 15:46:10.549
Feb 27 15:46:10.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 replace -f -'
Feb 27 15:46:11.035: INFO: stderr: ""
Feb 27 15:46:11.035: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 02/27/23 15:46:11.035
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Feb 27 15:46:11.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 delete pods e2e-test-httpd-pod'
Feb 27 15:46:12.988: INFO: stderr: ""
Feb 27 15:46:12.988: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:12.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-412" for this suite. 02/27/23 15:46:13
------------------------------
• [SLOW TEST] [7.632 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:05.377
    Feb 27 15:46:05.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:46:05.378
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:05.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:05.392
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 15:46:05.395
    Feb 27 15:46:05.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 27 15:46:05.451: INFO: stderr: ""
    Feb 27 15:46:05.451: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/27/23 15:46:05.451
    STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 15:46:10.502
    Feb 27 15:46:10.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 get pod e2e-test-httpd-pod -o json'
    Feb 27 15:46:10.549: INFO: stderr: ""
    Feb 27 15:46:10.549: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-02-27T15:46:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-412\",\n        \"resourceVersion\": \"16164\",\n        \"uid\": \"1a96509e-eddb-491e-9287-7c4a9854aeba\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-ghmvb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-42-40\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-ghmvb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T15:46:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6e6fbbb27e40c2c7aa160e004e57d279996332adace592a1e80838a814c2ccb2\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-27T15:46:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.42.40\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.192.176\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.192.176\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-27T15:46:05Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/27/23 15:46:10.549
    Feb 27 15:46:10.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 replace -f -'
    Feb 27 15:46:11.035: INFO: stderr: ""
    Feb 27 15:46:11.035: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 02/27/23 15:46:11.035
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Feb 27 15:46:11.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-412 delete pods e2e-test-httpd-pod'
    Feb 27 15:46:12.988: INFO: stderr: ""
    Feb 27 15:46:12.988: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:12.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-412" for this suite. 02/27/23 15:46:13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:13.008
Feb 27 15:46:13.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:46:13.008
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:13.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:13.026
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:46:13.03
Feb 27 15:46:13.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303" in namespace "projected-3875" to be "Succeeded or Failed"
Feb 27 15:46:13.043: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621407ms
Feb 27 15:46:15.144: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104834331s
Feb 27 15:46:17.046: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006790093s
STEP: Saw pod success 02/27/23 15:46:17.046
Feb 27 15:46:17.046: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303" satisfied condition "Succeeded or Failed"
Feb 27 15:46:17.049: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303 container client-container: <nil>
STEP: delete the pod 02/27/23 15:46:17.055
Feb 27 15:46:17.066: INFO: Waiting for pod downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303 to disappear
Feb 27 15:46:17.069: INFO: Pod downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:17.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3875" for this suite. 02/27/23 15:46:17.072
------------------------------
• [4.069 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:13.008
    Feb 27 15:46:13.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:46:13.008
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:13.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:13.026
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:46:13.03
    Feb 27 15:46:13.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303" in namespace "projected-3875" to be "Succeeded or Failed"
    Feb 27 15:46:13.043: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621407ms
    Feb 27 15:46:15.144: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104834331s
    Feb 27 15:46:17.046: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006790093s
    STEP: Saw pod success 02/27/23 15:46:17.046
    Feb 27 15:46:17.046: INFO: Pod "downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303" satisfied condition "Succeeded or Failed"
    Feb 27 15:46:17.049: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303 container client-container: <nil>
    STEP: delete the pod 02/27/23 15:46:17.055
    Feb 27 15:46:17.066: INFO: Waiting for pod downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303 to disappear
    Feb 27 15:46:17.069: INFO: Pod downwardapi-volume-877eebdd-e786-4184-97b1-ef9df0ccf303 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:17.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3875" for this suite. 02/27/23 15:46:17.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:17.078
Feb 27 15:46:17.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:46:17.079
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:17.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:17.094
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 02/27/23 15:46:17.097
Feb 27 15:46:17.097: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-6229 proxy --unix-socket=/tmp/kubectl-proxy-unix469106367/test'
STEP: retrieving proxy /api/ output 02/27/23 15:46:17.13
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:17.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6229" for this suite. 02/27/23 15:46:17.134
------------------------------
• [0.061 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:17.078
    Feb 27 15:46:17.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:46:17.079
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:17.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:17.094
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 02/27/23 15:46:17.097
    Feb 27 15:46:17.097: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-6229 proxy --unix-socket=/tmp/kubectl-proxy-unix469106367/test'
    STEP: retrieving proxy /api/ output 02/27/23 15:46:17.13
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:17.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6229" for this suite. 02/27/23 15:46:17.134
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:17.14
Feb 27 15:46:17.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:46:17.14
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:17.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:17.154
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:46:17.175
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:46:17.6
STEP: Deploying the webhook pod 02/27/23 15:46:17.607
STEP: Wait for the deployment to be ready 02/27/23 15:46:17.617
Feb 27 15:46:17.626: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:46:19.635
STEP: Verifying the service has paired with the endpoint 02/27/23 15:46:19.65
Feb 27 15:46:20.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/27/23 15:46:20.653
STEP: create a namespace for the webhook 02/27/23 15:46:20.665
STEP: create a configmap should be unconditionally rejected by the webhook 02/27/23 15:46:20.671
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:20.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7850" for this suite. 02/27/23 15:46:20.909
STEP: Destroying namespace "webhook-7850-markers" for this suite. 02/27/23 15:46:20.916
------------------------------
• [3.782 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:17.14
    Feb 27 15:46:17.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:46:17.14
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:17.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:17.154
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:46:17.175
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:46:17.6
    STEP: Deploying the webhook pod 02/27/23 15:46:17.607
    STEP: Wait for the deployment to be ready 02/27/23 15:46:17.617
    Feb 27 15:46:17.626: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:46:19.635
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:46:19.65
    Feb 27 15:46:20.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/27/23 15:46:20.653
    STEP: create a namespace for the webhook 02/27/23 15:46:20.665
    STEP: create a configmap should be unconditionally rejected by the webhook 02/27/23 15:46:20.671
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:20.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7850" for this suite. 02/27/23 15:46:20.909
    STEP: Destroying namespace "webhook-7850-markers" for this suite. 02/27/23 15:46:20.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:20.923
Feb 27 15:46:20.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replication-controller 02/27/23 15:46:20.924
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:20.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:20.936
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d 02/27/23 15:46:20.939
Feb 27 15:46:20.946: INFO: Pod name my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d: Found 0 pods out of 1
Feb 27 15:46:25.951: INFO: Pod name my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d: Found 1 pods out of 1
Feb 27 15:46:25.951: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d" are running
Feb 27 15:46:25.951: INFO: Waiting up to 5m0s for pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx" in namespace "replication-controller-5049" to be "running"
Feb 27 15:46:25.953: INFO: Pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx": Phase="Running", Reason="", readiness=true. Elapsed: 2.697554ms
Feb 27 15:46:25.953: INFO: Pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx" satisfied condition "running"
Feb 27 15:46:25.953: INFO: Pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:21 +0000 UTC Reason: Message:}])
Feb 27 15:46:25.953: INFO: Trying to dial the pod
Feb 27 15:46:30.965: INFO: Controller my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d: Got expected result from replica 1 [my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx]: "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:30.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5049" for this suite. 02/27/23 15:46:30.967
------------------------------
• [SLOW TEST] [10.051 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:20.923
    Feb 27 15:46:20.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replication-controller 02/27/23 15:46:20.924
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:20.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:20.936
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d 02/27/23 15:46:20.939
    Feb 27 15:46:20.946: INFO: Pod name my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d: Found 0 pods out of 1
    Feb 27 15:46:25.951: INFO: Pod name my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d: Found 1 pods out of 1
    Feb 27 15:46:25.951: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d" are running
    Feb 27 15:46:25.951: INFO: Waiting up to 5m0s for pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx" in namespace "replication-controller-5049" to be "running"
    Feb 27 15:46:25.953: INFO: Pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx": Phase="Running", Reason="", readiness=true. Elapsed: 2.697554ms
    Feb 27 15:46:25.953: INFO: Pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx" satisfied condition "running"
    Feb 27 15:46:25.953: INFO: Pod "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 15:46:21 +0000 UTC Reason: Message:}])
    Feb 27 15:46:25.953: INFO: Trying to dial the pod
    Feb 27 15:46:30.965: INFO: Controller my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d: Got expected result from replica 1 [my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx]: "my-hostname-basic-baa20076-c0dc-4416-a1f7-230ccfa6e06d-45wqx", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:30.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5049" for this suite. 02/27/23 15:46:30.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:30.975
Feb 27 15:46:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 15:46:30.975
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:31.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:31.086
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 02/27/23 15:46:31.089
Feb 27 15:46:31.095: INFO: Waiting up to 5m0s for pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2" in namespace "var-expansion-7529" to be "Succeeded or Failed"
Feb 27 15:46:31.098: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724057ms
Feb 27 15:46:33.101: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0054562s
Feb 27 15:46:35.102: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006457941s
STEP: Saw pod success 02/27/23 15:46:35.102
Feb 27 15:46:35.102: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2" satisfied condition "Succeeded or Failed"
Feb 27 15:46:35.104: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2 container dapi-container: <nil>
STEP: delete the pod 02/27/23 15:46:35.11
Feb 27 15:46:35.121: INFO: Waiting for pod var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2 to disappear
Feb 27 15:46:35.123: INFO: Pod var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:35.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7529" for this suite. 02/27/23 15:46:35.126
------------------------------
• [4.156 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:30.975
    Feb 27 15:46:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 15:46:30.975
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:31.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:31.086
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 02/27/23 15:46:31.089
    Feb 27 15:46:31.095: INFO: Waiting up to 5m0s for pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2" in namespace "var-expansion-7529" to be "Succeeded or Failed"
    Feb 27 15:46:31.098: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724057ms
    Feb 27 15:46:33.101: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0054562s
    Feb 27 15:46:35.102: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006457941s
    STEP: Saw pod success 02/27/23 15:46:35.102
    Feb 27 15:46:35.102: INFO: Pod "var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2" satisfied condition "Succeeded or Failed"
    Feb 27 15:46:35.104: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 15:46:35.11
    Feb 27 15:46:35.121: INFO: Waiting for pod var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2 to disappear
    Feb 27 15:46:35.123: INFO: Pod var-expansion-e9390ad6-96ca-41f0-b8ea-493e074334f2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:35.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7529" for this suite. 02/27/23 15:46:35.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:35.131
Feb 27 15:46:35.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename security-context 02/27/23 15:46:35.132
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:35.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:35.143
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 15:46:35.146
Feb 27 15:46:35.153: INFO: Waiting up to 5m0s for pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494" in namespace "security-context-5070" to be "Succeeded or Failed"
Feb 27 15:46:35.157: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416182ms
Feb 27 15:46:37.160: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00682775s
Feb 27 15:46:39.161: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007748862s
STEP: Saw pod success 02/27/23 15:46:39.161
Feb 27 15:46:39.161: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494" satisfied condition "Succeeded or Failed"
Feb 27 15:46:39.164: INFO: Trying to get logs from node ip-172-31-42-40 pod security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494 container test-container: <nil>
STEP: delete the pod 02/27/23 15:46:39.17
Feb 27 15:46:39.181: INFO: Waiting for pod security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494 to disappear
Feb 27 15:46:39.183: INFO: Pod security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:39.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5070" for this suite. 02/27/23 15:46:39.186
------------------------------
• [4.061 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:35.131
    Feb 27 15:46:35.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename security-context 02/27/23 15:46:35.132
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:35.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:35.143
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 15:46:35.146
    Feb 27 15:46:35.153: INFO: Waiting up to 5m0s for pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494" in namespace "security-context-5070" to be "Succeeded or Failed"
    Feb 27 15:46:35.157: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416182ms
    Feb 27 15:46:37.160: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00682775s
    Feb 27 15:46:39.161: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007748862s
    STEP: Saw pod success 02/27/23 15:46:39.161
    Feb 27 15:46:39.161: INFO: Pod "security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494" satisfied condition "Succeeded or Failed"
    Feb 27 15:46:39.164: INFO: Trying to get logs from node ip-172-31-42-40 pod security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:46:39.17
    Feb 27 15:46:39.181: INFO: Waiting for pod security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494 to disappear
    Feb 27 15:46:39.183: INFO: Pod security-context-ecfde24e-4b63-4b68-9ce1-4c1973b94494 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:39.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5070" for this suite. 02/27/23 15:46:39.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:39.193
Feb 27 15:46:39.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 15:46:39.194
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:39.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:39.207
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/27/23 15:46:39.209
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/27/23 15:46:39.217
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/27/23 15:46:39.217
STEP: creating a pod to probe DNS 02/27/23 15:46:39.217
STEP: submitting the pod to kubernetes 02/27/23 15:46:39.217
Feb 27 15:46:39.234: INFO: Waiting up to 15m0s for pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657" in namespace "dns-7931" to be "running"
Feb 27 15:46:39.238: INFO: Pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657": Phase="Pending", Reason="", readiness=false. Elapsed: 3.491305ms
Feb 27 15:46:41.242: INFO: Pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657": Phase="Running", Reason="", readiness=true. Elapsed: 2.007149327s
Feb 27 15:46:41.242: INFO: Pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657" satisfied condition "running"
STEP: retrieving the pod 02/27/23 15:46:41.242
STEP: looking for the results for each expected name from probers 02/27/23 15:46:41.245
Feb 27 15:46:41.259: INFO: DNS probes using dns-7931/dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657 succeeded

STEP: deleting the pod 02/27/23 15:46:41.259
STEP: deleting the test headless service 02/27/23 15:46:41.275
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:41.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7931" for this suite. 02/27/23 15:46:41.295
------------------------------
• [2.111 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:39.193
    Feb 27 15:46:39.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 15:46:39.194
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:39.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:39.207
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/27/23 15:46:39.209
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/27/23 15:46:39.217
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7931.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/27/23 15:46:39.217
    STEP: creating a pod to probe DNS 02/27/23 15:46:39.217
    STEP: submitting the pod to kubernetes 02/27/23 15:46:39.217
    Feb 27 15:46:39.234: INFO: Waiting up to 15m0s for pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657" in namespace "dns-7931" to be "running"
    Feb 27 15:46:39.238: INFO: Pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657": Phase="Pending", Reason="", readiness=false. Elapsed: 3.491305ms
    Feb 27 15:46:41.242: INFO: Pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657": Phase="Running", Reason="", readiness=true. Elapsed: 2.007149327s
    Feb 27 15:46:41.242: INFO: Pod "dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 15:46:41.242
    STEP: looking for the results for each expected name from probers 02/27/23 15:46:41.245
    Feb 27 15:46:41.259: INFO: DNS probes using dns-7931/dns-test-efc8a411-3a92-4e1d-b9e0-d03812397657 succeeded

    STEP: deleting the pod 02/27/23 15:46:41.259
    STEP: deleting the test headless service 02/27/23 15:46:41.275
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:41.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7931" for this suite. 02/27/23 15:46:41.295
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:41.305
Feb 27 15:46:41.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:46:41.305
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:41.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:41.319
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 02/27/23 15:46:41.322
Feb 27 15:46:41.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 27 15:46:41.378: INFO: stderr: ""
Feb 27 15:46:41.378: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 02/27/23 15:46:41.378
Feb 27 15:46:41.378: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 27 15:46:41.378: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9630" to be "running and ready, or succeeded"
Feb 27 15:46:41.381: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804397ms
Feb 27 15:46:41.381: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-42-40' to be 'Running' but was 'Pending'
Feb 27 15:46:43.385: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006856595s
Feb 27 15:46:43.385: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 27 15:46:43.385: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/27/23 15:46:43.385
Feb 27 15:46:43.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator'
Feb 27 15:46:43.446: INFO: stderr: ""
Feb 27 15:46:43.446: INFO: stdout: "I0227 15:46:42.139609       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/vrn 515\nI0227 15:46:42.339713       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/2hqz 405\nI0227 15:46:42.540203       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/x4g 440\nI0227 15:46:42.740490       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/p49q 517\nI0227 15:46:42.939736       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/549 563\nI0227 15:46:43.140014       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f4j 216\nI0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\n"
STEP: limiting log lines 02/27/23 15:46:43.446
Feb 27 15:46:43.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --tail=1'
Feb 27 15:46:43.496: INFO: stderr: ""
Feb 27 15:46:43.496: INFO: stdout: "I0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\n"
Feb 27 15:46:43.496: INFO: got output "I0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\n"
STEP: limiting log bytes 02/27/23 15:46:43.496
Feb 27 15:46:43.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --limit-bytes=1'
Feb 27 15:46:43.546: INFO: stderr: ""
Feb 27 15:46:43.546: INFO: stdout: "I"
Feb 27 15:46:43.546: INFO: got output "I"
STEP: exposing timestamps 02/27/23 15:46:43.546
Feb 27 15:46:43.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 27 15:46:43.609: INFO: stderr: ""
Feb 27 15:46:43.609: INFO: stdout: "2023-02-27T15:46:43.540671906Z I0227 15:46:43.540555       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/47gn 421\n"
Feb 27 15:46:43.609: INFO: got output "2023-02-27T15:46:43.540671906Z I0227 15:46:43.540555       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/47gn 421\n"
STEP: restricting to a time range 02/27/23 15:46:43.609
Feb 27 15:46:46.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --since=1s'
Feb 27 15:46:46.164: INFO: stderr: ""
Feb 27 15:46:46.164: INFO: stdout: "I0227 15:46:45.339835       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/9htk 477\nI0227 15:46:45.540125       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/p65d 386\nI0227 15:46:45.740415       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/26w 355\nI0227 15:46:45.939652       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/tltf 331\nI0227 15:46:46.139944       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/mz2l 423\n"
Feb 27 15:46:46.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --since=24h'
Feb 27 15:46:46.216: INFO: stderr: ""
Feb 27 15:46:46.216: INFO: stdout: "I0227 15:46:42.139609       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/vrn 515\nI0227 15:46:42.339713       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/2hqz 405\nI0227 15:46:42.540203       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/x4g 440\nI0227 15:46:42.740490       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/p49q 517\nI0227 15:46:42.939736       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/549 563\nI0227 15:46:43.140014       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f4j 216\nI0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\nI0227 15:46:43.540555       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/47gn 421\nI0227 15:46:43.739702       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/jsbh 247\nI0227 15:46:43.939995       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/5sz5 591\nI0227 15:46:44.140308       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/cpz9 522\nI0227 15:46:44.340642       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/8d6 262\nI0227 15:46:44.539930       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/bswk 546\nI0227 15:46:44.740022       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/tqd 213\nI0227 15:46:44.940312       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/c59 547\nI0227 15:46:45.140612       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/gm8 365\nI0227 15:46:45.339835       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/9htk 477\nI0227 15:46:45.540125       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/p65d 386\nI0227 15:46:45.740415       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/26w 355\nI0227 15:46:45.939652       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/tltf 331\nI0227 15:46:46.139944       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/mz2l 423\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Feb 27 15:46:46.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 delete pod logs-generator'
Feb 27 15:46:47.072: INFO: stderr: ""
Feb 27 15:46:47.072: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:47.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9630" for this suite. 02/27/23 15:46:47.075
------------------------------
• [SLOW TEST] [5.776 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:41.305
    Feb 27 15:46:41.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:46:41.305
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:41.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:41.319
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 02/27/23 15:46:41.322
    Feb 27 15:46:41.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb 27 15:46:41.378: INFO: stderr: ""
    Feb 27 15:46:41.378: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 02/27/23 15:46:41.378
    Feb 27 15:46:41.378: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb 27 15:46:41.378: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9630" to be "running and ready, or succeeded"
    Feb 27 15:46:41.381: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804397ms
    Feb 27 15:46:41.381: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-42-40' to be 'Running' but was 'Pending'
    Feb 27 15:46:43.385: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006856595s
    Feb 27 15:46:43.385: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb 27 15:46:43.385: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/27/23 15:46:43.385
    Feb 27 15:46:43.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator'
    Feb 27 15:46:43.446: INFO: stderr: ""
    Feb 27 15:46:43.446: INFO: stdout: "I0227 15:46:42.139609       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/vrn 515\nI0227 15:46:42.339713       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/2hqz 405\nI0227 15:46:42.540203       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/x4g 440\nI0227 15:46:42.740490       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/p49q 517\nI0227 15:46:42.939736       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/549 563\nI0227 15:46:43.140014       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f4j 216\nI0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\n"
    STEP: limiting log lines 02/27/23 15:46:43.446
    Feb 27 15:46:43.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --tail=1'
    Feb 27 15:46:43.496: INFO: stderr: ""
    Feb 27 15:46:43.496: INFO: stdout: "I0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\n"
    Feb 27 15:46:43.496: INFO: got output "I0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\n"
    STEP: limiting log bytes 02/27/23 15:46:43.496
    Feb 27 15:46:43.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --limit-bytes=1'
    Feb 27 15:46:43.546: INFO: stderr: ""
    Feb 27 15:46:43.546: INFO: stdout: "I"
    Feb 27 15:46:43.546: INFO: got output "I"
    STEP: exposing timestamps 02/27/23 15:46:43.546
    Feb 27 15:46:43.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb 27 15:46:43.609: INFO: stderr: ""
    Feb 27 15:46:43.609: INFO: stdout: "2023-02-27T15:46:43.540671906Z I0227 15:46:43.540555       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/47gn 421\n"
    Feb 27 15:46:43.609: INFO: got output "2023-02-27T15:46:43.540671906Z I0227 15:46:43.540555       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/47gn 421\n"
    STEP: restricting to a time range 02/27/23 15:46:43.609
    Feb 27 15:46:46.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --since=1s'
    Feb 27 15:46:46.164: INFO: stderr: ""
    Feb 27 15:46:46.164: INFO: stdout: "I0227 15:46:45.339835       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/9htk 477\nI0227 15:46:45.540125       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/p65d 386\nI0227 15:46:45.740415       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/26w 355\nI0227 15:46:45.939652       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/tltf 331\nI0227 15:46:46.139944       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/mz2l 423\n"
    Feb 27 15:46:46.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 logs logs-generator logs-generator --since=24h'
    Feb 27 15:46:46.216: INFO: stderr: ""
    Feb 27 15:46:46.216: INFO: stdout: "I0227 15:46:42.139609       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/vrn 515\nI0227 15:46:42.339713       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/2hqz 405\nI0227 15:46:42.540203       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/x4g 440\nI0227 15:46:42.740490       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/p49q 517\nI0227 15:46:42.939736       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/549 563\nI0227 15:46:43.140014       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/f4j 216\nI0227 15:46:43.340310       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/ztr2 471\nI0227 15:46:43.540555       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/47gn 421\nI0227 15:46:43.739702       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/jsbh 247\nI0227 15:46:43.939995       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/5sz5 591\nI0227 15:46:44.140308       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/cpz9 522\nI0227 15:46:44.340642       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/8d6 262\nI0227 15:46:44.539930       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/bswk 546\nI0227 15:46:44.740022       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/tqd 213\nI0227 15:46:44.940312       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/c59 547\nI0227 15:46:45.140612       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/gm8 365\nI0227 15:46:45.339835       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/9htk 477\nI0227 15:46:45.540125       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/p65d 386\nI0227 15:46:45.740415       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/26w 355\nI0227 15:46:45.939652       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/tltf 331\nI0227 15:46:46.139944       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/mz2l 423\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Feb 27 15:46:46.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9630 delete pod logs-generator'
    Feb 27 15:46:47.072: INFO: stderr: ""
    Feb 27 15:46:47.072: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:47.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9630" for this suite. 02/27/23 15:46:47.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:47.081
Feb 27 15:46:47.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename watch 02/27/23 15:46:47.082
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:47.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:47.098
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/27/23 15:46:47.101
STEP: creating a new configmap 02/27/23 15:46:47.102
STEP: modifying the configmap once 02/27/23 15:46:47.106
STEP: changing the label value of the configmap 02/27/23 15:46:47.113
STEP: Expecting to observe a delete notification for the watched object 02/27/23 15:46:47.119
Feb 27 15:46:47.119: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16604 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:46:47.119: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16606 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:46:47.119: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16607 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/27/23 15:46:47.119
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/27/23 15:46:47.125
STEP: changing the label value of the configmap back 02/27/23 15:46:57.125
STEP: modifying the configmap a third time 02/27/23 15:46:57.135
STEP: deleting the configmap 02/27/23 15:46:57.141
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/27/23 15:46:57.145
Feb 27 15:46:57.145: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16639 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:46:57.145: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16640 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 15:46:57.145: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16641 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 15:46:57.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9573" for this suite. 02/27/23 15:46:57.149
------------------------------
• [SLOW TEST] [10.074 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:47.081
    Feb 27 15:46:47.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename watch 02/27/23 15:46:47.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:47.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:47.098
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/27/23 15:46:47.101
    STEP: creating a new configmap 02/27/23 15:46:47.102
    STEP: modifying the configmap once 02/27/23 15:46:47.106
    STEP: changing the label value of the configmap 02/27/23 15:46:47.113
    STEP: Expecting to observe a delete notification for the watched object 02/27/23 15:46:47.119
    Feb 27 15:46:47.119: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16604 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:46:47.119: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16606 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:46:47.119: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16607 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/27/23 15:46:47.119
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/27/23 15:46:47.125
    STEP: changing the label value of the configmap back 02/27/23 15:46:57.125
    STEP: modifying the configmap a third time 02/27/23 15:46:57.135
    STEP: deleting the configmap 02/27/23 15:46:57.141
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/27/23 15:46:57.145
    Feb 27 15:46:57.145: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16639 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:46:57.145: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16640 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 15:46:57.145: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9573  abafd785-868e-4064-91bf-31e2f609835e 16641 0 2023-02-27 15:46:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 15:46:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:46:57.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9573" for this suite. 02/27/23 15:46:57.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:46:57.156
Feb 27 15:46:57.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:46:57.157
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:57.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:57.176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 15:46:57.178
Feb 27 15:46:57.188: INFO: Waiting up to 5m0s for pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777" in namespace "emptydir-2380" to be "Succeeded or Failed"
Feb 27 15:46:57.193: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809197ms
Feb 27 15:46:59.197: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008938521s
Feb 27 15:47:01.196: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008309512s
STEP: Saw pod success 02/27/23 15:47:01.196
Feb 27 15:47:01.196: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777" satisfied condition "Succeeded or Failed"
Feb 27 15:47:01.198: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-0415ff46-cf9c-4fad-b096-31e672e09777 container test-container: <nil>
STEP: delete the pod 02/27/23 15:47:01.204
Feb 27 15:47:01.215: INFO: Waiting for pod pod-0415ff46-cf9c-4fad-b096-31e672e09777 to disappear
Feb 27 15:47:01.217: INFO: Pod pod-0415ff46-cf9c-4fad-b096-31e672e09777 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:47:01.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2380" for this suite. 02/27/23 15:47:01.22
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:46:57.156
    Feb 27 15:46:57.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:46:57.157
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:46:57.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:46:57.176
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 15:46:57.178
    Feb 27 15:46:57.188: INFO: Waiting up to 5m0s for pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777" in namespace "emptydir-2380" to be "Succeeded or Failed"
    Feb 27 15:46:57.193: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809197ms
    Feb 27 15:46:59.197: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008938521s
    Feb 27 15:47:01.196: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008309512s
    STEP: Saw pod success 02/27/23 15:47:01.196
    Feb 27 15:47:01.196: INFO: Pod "pod-0415ff46-cf9c-4fad-b096-31e672e09777" satisfied condition "Succeeded or Failed"
    Feb 27 15:47:01.198: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-0415ff46-cf9c-4fad-b096-31e672e09777 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:47:01.204
    Feb 27 15:47:01.215: INFO: Waiting for pod pod-0415ff46-cf9c-4fad-b096-31e672e09777 to disappear
    Feb 27 15:47:01.217: INFO: Pod pod-0415ff46-cf9c-4fad-b096-31e672e09777 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:47:01.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2380" for this suite. 02/27/23 15:47:01.22
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:47:01.226
Feb 27 15:47:01.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:47:01.226
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:47:01.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:47:01.24
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 02/27/23 15:47:01.242
Feb 27 15:47:01.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 create -f -'
Feb 27 15:47:01.380: INFO: stderr: ""
Feb 27 15:47:01.380: INFO: stdout: "pod/pause created\n"
Feb 27 15:47:01.380: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 27 15:47:01.380: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4286" to be "running and ready"
Feb 27 15:47:01.383: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122724ms
Feb 27 15:47:01.383: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-42-40' to be 'Running' but was 'Pending'
Feb 27 15:47:03.386: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006381418s
Feb 27 15:47:03.386: INFO: Pod "pause" satisfied condition "running and ready"
Feb 27 15:47:03.386: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 02/27/23 15:47:03.386
Feb 27 15:47:03.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 label pods pause testing-label=testing-label-value'
Feb 27 15:47:03.441: INFO: stderr: ""
Feb 27 15:47:03.441: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/27/23 15:47:03.441
Feb 27 15:47:03.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get pod pause -L testing-label'
Feb 27 15:47:03.490: INFO: stderr: ""
Feb 27 15:47:03.490: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/27/23 15:47:03.49
Feb 27 15:47:03.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 label pods pause testing-label-'
Feb 27 15:47:03.544: INFO: stderr: ""
Feb 27 15:47:03.544: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/27/23 15:47:03.544
Feb 27 15:47:03.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get pod pause -L testing-label'
Feb 27 15:47:03.592: INFO: stderr: ""
Feb 27 15:47:03.592: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 02/27/23 15:47:03.592
Feb 27 15:47:03.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 delete --grace-period=0 --force -f -'
Feb 27 15:47:03.651: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:47:03.651: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 27 15:47:03.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get rc,svc -l name=pause --no-headers'
Feb 27 15:47:03.703: INFO: stderr: "No resources found in kubectl-4286 namespace.\n"
Feb 27 15:47:03.703: INFO: stdout: ""
Feb 27 15:47:03.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 15:47:03.750: INFO: stderr: ""
Feb 27 15:47:03.750: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:47:03.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4286" for this suite. 02/27/23 15:47:03.754
------------------------------
• [2.534 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:47:01.226
    Feb 27 15:47:01.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:47:01.226
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:47:01.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:47:01.24
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 02/27/23 15:47:01.242
    Feb 27 15:47:01.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 create -f -'
    Feb 27 15:47:01.380: INFO: stderr: ""
    Feb 27 15:47:01.380: INFO: stdout: "pod/pause created\n"
    Feb 27 15:47:01.380: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb 27 15:47:01.380: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4286" to be "running and ready"
    Feb 27 15:47:01.383: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122724ms
    Feb 27 15:47:01.383: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-42-40' to be 'Running' but was 'Pending'
    Feb 27 15:47:03.386: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006381418s
    Feb 27 15:47:03.386: INFO: Pod "pause" satisfied condition "running and ready"
    Feb 27 15:47:03.386: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 02/27/23 15:47:03.386
    Feb 27 15:47:03.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 label pods pause testing-label=testing-label-value'
    Feb 27 15:47:03.441: INFO: stderr: ""
    Feb 27 15:47:03.441: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/27/23 15:47:03.441
    Feb 27 15:47:03.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get pod pause -L testing-label'
    Feb 27 15:47:03.490: INFO: stderr: ""
    Feb 27 15:47:03.490: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/27/23 15:47:03.49
    Feb 27 15:47:03.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 label pods pause testing-label-'
    Feb 27 15:47:03.544: INFO: stderr: ""
    Feb 27 15:47:03.544: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/27/23 15:47:03.544
    Feb 27 15:47:03.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get pod pause -L testing-label'
    Feb 27 15:47:03.592: INFO: stderr: ""
    Feb 27 15:47:03.592: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 02/27/23 15:47:03.592
    Feb 27 15:47:03.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 delete --grace-period=0 --force -f -'
    Feb 27 15:47:03.651: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:47:03.651: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb 27 15:47:03.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get rc,svc -l name=pause --no-headers'
    Feb 27 15:47:03.703: INFO: stderr: "No resources found in kubectl-4286 namespace.\n"
    Feb 27 15:47:03.703: INFO: stdout: ""
    Feb 27 15:47:03.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4286 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 15:47:03.750: INFO: stderr: ""
    Feb 27 15:47:03.750: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:47:03.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4286" for this suite. 02/27/23 15:47:03.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:47:03.761
Feb 27 15:47:03.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 15:47:03.761
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:47:03.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:47:03.776
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-d83af05b-0407-4133-b697-4dbb986a842d in namespace container-probe-8105 02/27/23 15:47:03.779
Feb 27 15:47:03.787: INFO: Waiting up to 5m0s for pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d" in namespace "container-probe-8105" to be "not pending"
Feb 27 15:47:03.876: INFO: Pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d": Phase="Pending", Reason="", readiness=false. Elapsed: 88.696178ms
Feb 27 15:47:05.879: INFO: Pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d": Phase="Running", Reason="", readiness=true. Elapsed: 2.092598577s
Feb 27 15:47:05.879: INFO: Pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d" satisfied condition "not pending"
Feb 27 15:47:05.879: INFO: Started pod test-webserver-d83af05b-0407-4133-b697-4dbb986a842d in namespace container-probe-8105
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 15:47:05.879
Feb 27 15:47:05.882: INFO: Initial restart count of pod test-webserver-d83af05b-0407-4133-b697-4dbb986a842d is 0
STEP: deleting the pod 02/27/23 15:51:06.34
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 15:51:06.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8105" for this suite. 02/27/23 15:51:06.357
------------------------------
• [SLOW TEST] [242.604 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:47:03.761
    Feb 27 15:47:03.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 15:47:03.761
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:47:03.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:47:03.776
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-d83af05b-0407-4133-b697-4dbb986a842d in namespace container-probe-8105 02/27/23 15:47:03.779
    Feb 27 15:47:03.787: INFO: Waiting up to 5m0s for pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d" in namespace "container-probe-8105" to be "not pending"
    Feb 27 15:47:03.876: INFO: Pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d": Phase="Pending", Reason="", readiness=false. Elapsed: 88.696178ms
    Feb 27 15:47:05.879: INFO: Pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d": Phase="Running", Reason="", readiness=true. Elapsed: 2.092598577s
    Feb 27 15:47:05.879: INFO: Pod "test-webserver-d83af05b-0407-4133-b697-4dbb986a842d" satisfied condition "not pending"
    Feb 27 15:47:05.879: INFO: Started pod test-webserver-d83af05b-0407-4133-b697-4dbb986a842d in namespace container-probe-8105
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 15:47:05.879
    Feb 27 15:47:05.882: INFO: Initial restart count of pod test-webserver-d83af05b-0407-4133-b697-4dbb986a842d is 0
    STEP: deleting the pod 02/27/23 15:51:06.34
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:51:06.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8105" for this suite. 02/27/23 15:51:06.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:51:06.366
Feb 27 15:51:06.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:51:06.367
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:06.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:06.382
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 15:51:06.384
Feb 27 15:51:06.391: INFO: Waiting up to 5m0s for pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25" in namespace "emptydir-2628" to be "Succeeded or Failed"
Feb 27 15:51:06.397: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514175ms
Feb 27 15:51:08.401: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00968284s
Feb 27 15:51:10.400: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008460273s
STEP: Saw pod success 02/27/23 15:51:10.4
Feb 27 15:51:10.400: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25" satisfied condition "Succeeded or Failed"
Feb 27 15:51:10.403: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25 container test-container: <nil>
STEP: delete the pod 02/27/23 15:51:10.418
Feb 27 15:51:10.427: INFO: Waiting for pod pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25 to disappear
Feb 27 15:51:10.429: INFO: Pod pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:51:10.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2628" for this suite. 02/27/23 15:51:10.432
------------------------------
• [4.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:51:06.366
    Feb 27 15:51:06.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:51:06.367
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:06.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:06.382
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 15:51:06.384
    Feb 27 15:51:06.391: INFO: Waiting up to 5m0s for pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25" in namespace "emptydir-2628" to be "Succeeded or Failed"
    Feb 27 15:51:06.397: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514175ms
    Feb 27 15:51:08.401: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00968284s
    Feb 27 15:51:10.400: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008460273s
    STEP: Saw pod success 02/27/23 15:51:10.4
    Feb 27 15:51:10.400: INFO: Pod "pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25" satisfied condition "Succeeded or Failed"
    Feb 27 15:51:10.403: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:51:10.418
    Feb 27 15:51:10.427: INFO: Waiting for pod pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25 to disappear
    Feb 27 15:51:10.429: INFO: Pod pod-6a474d2c-9bd7-4310-a2a9-8598c2b01b25 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:51:10.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2628" for this suite. 02/27/23 15:51:10.432
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:51:10.439
Feb 27 15:51:10.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 15:51:10.44
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:10.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:10.456
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Feb 27 15:51:10.466: INFO: Waiting up to 5m0s for pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f" in namespace "container-probe-3961" to be "running and ready"
Feb 27 15:51:10.471: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.619181ms
Feb 27 15:51:10.471: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:51:12.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010083708s
Feb 27 15:51:12.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:14.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 4.009837773s
Feb 27 15:51:14.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:16.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 6.009448202s
Feb 27 15:51:16.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:18.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 8.010132255s
Feb 27 15:51:18.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:20.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 10.009250656s
Feb 27 15:51:20.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:22.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 12.010341312s
Feb 27 15:51:22.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:24.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 14.009434097s
Feb 27 15:51:24.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:26.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 16.010227199s
Feb 27 15:51:26.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:28.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 18.009000953s
Feb 27 15:51:28.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:30.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 20.009532554s
Feb 27 15:51:30.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
Feb 27 15:51:32.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=true. Elapsed: 22.010523721s
Feb 27 15:51:32.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = true)
Feb 27 15:51:32.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f" satisfied condition "running and ready"
Feb 27 15:51:32.479: INFO: Container started at 2023-02-27 15:51:11 +0000 UTC, pod became ready at 2023-02-27 15:51:30 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 15:51:32.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3961" for this suite. 02/27/23 15:51:32.482
------------------------------
• [SLOW TEST] [22.048 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:51:10.439
    Feb 27 15:51:10.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 15:51:10.44
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:10.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:10.456
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Feb 27 15:51:10.466: INFO: Waiting up to 5m0s for pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f" in namespace "container-probe-3961" to be "running and ready"
    Feb 27 15:51:10.471: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.619181ms
    Feb 27 15:51:10.471: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:51:12.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 2.010083708s
    Feb 27 15:51:12.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:14.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 4.009837773s
    Feb 27 15:51:14.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:16.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 6.009448202s
    Feb 27 15:51:16.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:18.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 8.010132255s
    Feb 27 15:51:18.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:20.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 10.009250656s
    Feb 27 15:51:20.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:22.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 12.010341312s
    Feb 27 15:51:22.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:24.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 14.009434097s
    Feb 27 15:51:24.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:26.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 16.010227199s
    Feb 27 15:51:26.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:28.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 18.009000953s
    Feb 27 15:51:28.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:30.475: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=false. Elapsed: 20.009532554s
    Feb 27 15:51:30.475: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = false)
    Feb 27 15:51:32.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f": Phase="Running", Reason="", readiness=true. Elapsed: 22.010523721s
    Feb 27 15:51:32.476: INFO: The phase of Pod test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f is Running (Ready = true)
    Feb 27 15:51:32.476: INFO: Pod "test-webserver-8cef98a1-23ff-4d95-bee9-309b0401872f" satisfied condition "running and ready"
    Feb 27 15:51:32.479: INFO: Container started at 2023-02-27 15:51:11 +0000 UTC, pod became ready at 2023-02-27 15:51:30 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:51:32.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3961" for this suite. 02/27/23 15:51:32.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:51:32.489
Feb 27 15:51:32.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:51:32.489
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:32.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:32.506
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-7376 02/27/23 15:51:32.508
STEP: creating service affinity-clusterip-transition in namespace services-7376 02/27/23 15:51:32.508
STEP: creating replication controller affinity-clusterip-transition in namespace services-7376 02/27/23 15:51:32.533
I0227 15:51:32.541940      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7376, replica count: 3
I0227 15:51:35.593088      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 15:51:35.598: INFO: Creating new exec pod
Feb 27 15:51:35.603: INFO: Waiting up to 5m0s for pod "execpod-affinityh2f2t" in namespace "services-7376" to be "running"
Feb 27 15:51:35.606: INFO: Pod "execpod-affinityh2f2t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.92197ms
Feb 27 15:51:37.610: INFO: Pod "execpod-affinityh2f2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.007553591s
Feb 27 15:51:37.610: INFO: Pod "execpod-affinityh2f2t" satisfied condition "running"
Feb 27 15:51:38.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Feb 27 15:51:38.722: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 27 15:51:38.722: INFO: stdout: ""
Feb 27 15:51:38.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c nc -v -z -w 2 10.152.183.237 80'
Feb 27 15:51:38.823: INFO: stderr: "+ nc -v -z -w 2 10.152.183.237 80\nConnection to 10.152.183.237 80 port [tcp/http] succeeded!\n"
Feb 27 15:51:38.823: INFO: stdout: ""
Feb 27 15:51:38.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.237:80/ ; done'
Feb 27 15:51:39.003: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n"
Feb 27 15:51:39.003: INFO: stdout: "\naffinity-clusterip-transition-5l8r4\naffinity-clusterip-transition-5l8r4\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-5l8r4\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5"
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-5l8r4
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-5l8r4
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-5l8r4
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.237:80/ ; done'
Feb 27 15:51:39.209: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n"
Feb 27 15:51:39.209: INFO: stdout: "\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5"
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
Feb 27 15:51:39.209: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7376, will wait for the garbage collector to delete the pods 02/27/23 15:51:39.22
Feb 27 15:51:39.282: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.428784ms
Feb 27 15:51:39.382: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.197459ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:51:41.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7376" for this suite. 02/27/23 15:51:41.603
------------------------------
• [SLOW TEST] [9.120 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:51:32.489
    Feb 27 15:51:32.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:51:32.489
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:32.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:32.506
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-7376 02/27/23 15:51:32.508
    STEP: creating service affinity-clusterip-transition in namespace services-7376 02/27/23 15:51:32.508
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7376 02/27/23 15:51:32.533
    I0227 15:51:32.541940      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7376, replica count: 3
    I0227 15:51:35.593088      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 15:51:35.598: INFO: Creating new exec pod
    Feb 27 15:51:35.603: INFO: Waiting up to 5m0s for pod "execpod-affinityh2f2t" in namespace "services-7376" to be "running"
    Feb 27 15:51:35.606: INFO: Pod "execpod-affinityh2f2t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.92197ms
    Feb 27 15:51:37.610: INFO: Pod "execpod-affinityh2f2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.007553591s
    Feb 27 15:51:37.610: INFO: Pod "execpod-affinityh2f2t" satisfied condition "running"
    Feb 27 15:51:38.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Feb 27 15:51:38.722: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb 27 15:51:38.722: INFO: stdout: ""
    Feb 27 15:51:38.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c nc -v -z -w 2 10.152.183.237 80'
    Feb 27 15:51:38.823: INFO: stderr: "+ nc -v -z -w 2 10.152.183.237 80\nConnection to 10.152.183.237 80 port [tcp/http] succeeded!\n"
    Feb 27 15:51:38.823: INFO: stdout: ""
    Feb 27 15:51:38.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.237:80/ ; done'
    Feb 27 15:51:39.003: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n"
    Feb 27 15:51:39.003: INFO: stdout: "\naffinity-clusterip-transition-5l8r4\naffinity-clusterip-transition-5l8r4\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-5l8r4\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-dw2h7\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5"
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-5l8r4
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-5l8r4
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-5l8r4
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-dw2h7
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.003: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-7376 exec execpod-affinityh2f2t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.237:80/ ; done'
    Feb 27 15:51:39.209: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.237:80/\n"
    Feb 27 15:51:39.209: INFO: stdout: "\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5\naffinity-clusterip-transition-fgps5"
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Received response from host: affinity-clusterip-transition-fgps5
    Feb 27 15:51:39.209: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7376, will wait for the garbage collector to delete the pods 02/27/23 15:51:39.22
    Feb 27 15:51:39.282: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.428784ms
    Feb 27 15:51:39.382: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.197459ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:51:41.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7376" for this suite. 02/27/23 15:51:41.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:51:41.61
Feb 27 15:51:41.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replication-controller 02/27/23 15:51:41.61
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:41.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:41.627
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 02/27/23 15:51:41.632
STEP: waiting for RC to be added 02/27/23 15:51:41.637
STEP: waiting for available Replicas 02/27/23 15:51:41.638
STEP: patching ReplicationController 02/27/23 15:51:42.489
STEP: waiting for RC to be modified 02/27/23 15:51:42.497
STEP: patching ReplicationController status 02/27/23 15:51:42.497
STEP: waiting for RC to be modified 02/27/23 15:51:42.501
STEP: waiting for available Replicas 02/27/23 15:51:42.502
STEP: fetching ReplicationController status 02/27/23 15:51:42.51
STEP: patching ReplicationController scale 02/27/23 15:51:42.513
STEP: waiting for RC to be modified 02/27/23 15:51:42.519
STEP: waiting for ReplicationController's scale to be the max amount 02/27/23 15:51:42.52
STEP: fetching ReplicationController; ensuring that it's patched 02/27/23 15:51:43.656
STEP: updating ReplicationController status 02/27/23 15:51:43.658
STEP: waiting for RC to be modified 02/27/23 15:51:43.665
STEP: listing all ReplicationControllers 02/27/23 15:51:43.665
STEP: checking that ReplicationController has expected values 02/27/23 15:51:43.667
STEP: deleting ReplicationControllers by collection 02/27/23 15:51:43.667
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/27/23 15:51:43.674
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 15:51:43.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5332" for this suite. 02/27/23 15:51:43.7
------------------------------
• [2.098 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:51:41.61
    Feb 27 15:51:41.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replication-controller 02/27/23 15:51:41.61
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:41.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:41.627
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 02/27/23 15:51:41.632
    STEP: waiting for RC to be added 02/27/23 15:51:41.637
    STEP: waiting for available Replicas 02/27/23 15:51:41.638
    STEP: patching ReplicationController 02/27/23 15:51:42.489
    STEP: waiting for RC to be modified 02/27/23 15:51:42.497
    STEP: patching ReplicationController status 02/27/23 15:51:42.497
    STEP: waiting for RC to be modified 02/27/23 15:51:42.501
    STEP: waiting for available Replicas 02/27/23 15:51:42.502
    STEP: fetching ReplicationController status 02/27/23 15:51:42.51
    STEP: patching ReplicationController scale 02/27/23 15:51:42.513
    STEP: waiting for RC to be modified 02/27/23 15:51:42.519
    STEP: waiting for ReplicationController's scale to be the max amount 02/27/23 15:51:42.52
    STEP: fetching ReplicationController; ensuring that it's patched 02/27/23 15:51:43.656
    STEP: updating ReplicationController status 02/27/23 15:51:43.658
    STEP: waiting for RC to be modified 02/27/23 15:51:43.665
    STEP: listing all ReplicationControllers 02/27/23 15:51:43.665
    STEP: checking that ReplicationController has expected values 02/27/23 15:51:43.667
    STEP: deleting ReplicationControllers by collection 02/27/23 15:51:43.667
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/27/23 15:51:43.674
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:51:43.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5332" for this suite. 02/27/23 15:51:43.7
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:51:43.708
Feb 27 15:51:43.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename namespaces 02/27/23 15:51:43.708
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:43.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:43.724
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 02/27/23 15:51:43.727
STEP: patching the Namespace 02/27/23 15:51:43.737
STEP: get the Namespace and ensuring it has the label 02/27/23 15:51:43.744
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:51:43.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-422" for this suite. 02/27/23 15:51:43.753
STEP: Destroying namespace "nspatchtest-4e1adcda-c257-4d0e-a92e-685344feb349-8652" for this suite. 02/27/23 15:51:43.76
------------------------------
• [0.057 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:51:43.708
    Feb 27 15:51:43.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename namespaces 02/27/23 15:51:43.708
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:43.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:43.724
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 02/27/23 15:51:43.727
    STEP: patching the Namespace 02/27/23 15:51:43.737
    STEP: get the Namespace and ensuring it has the label 02/27/23 15:51:43.744
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:51:43.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-422" for this suite. 02/27/23 15:51:43.753
    STEP: Destroying namespace "nspatchtest-4e1adcda-c257-4d0e-a92e-685344feb349-8652" for this suite. 02/27/23 15:51:43.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:51:43.767
Feb 27 15:51:43.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 15:51:43.767
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:43.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:43.781
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-ae305074-9955-4ce1-b16d-e2a729cf43f9 02/27/23 15:51:43.784
STEP: Creating a pod to test consume secrets 02/27/23 15:51:43.788
Feb 27 15:51:43.795: INFO: Waiting up to 5m0s for pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57" in namespace "secrets-9543" to be "Succeeded or Failed"
Feb 27 15:51:43.799: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928722ms
Feb 27 15:51:45.802: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57": Phase="Running", Reason="", readiness=false. Elapsed: 2.007872301s
Feb 27 15:51:47.803: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00850149s
STEP: Saw pod success 02/27/23 15:51:47.803
Feb 27 15:51:47.803: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57" satisfied condition "Succeeded or Failed"
Feb 27 15:51:47.806: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:51:47.812
Feb 27 15:51:47.822: INFO: Waiting for pod pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57 to disappear
Feb 27 15:51:47.824: INFO: Pod pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 15:51:47.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9543" for this suite. 02/27/23 15:51:47.827
------------------------------
• [4.066 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:51:43.767
    Feb 27 15:51:43.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 15:51:43.767
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:43.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:43.781
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-ae305074-9955-4ce1-b16d-e2a729cf43f9 02/27/23 15:51:43.784
    STEP: Creating a pod to test consume secrets 02/27/23 15:51:43.788
    Feb 27 15:51:43.795: INFO: Waiting up to 5m0s for pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57" in namespace "secrets-9543" to be "Succeeded or Failed"
    Feb 27 15:51:43.799: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928722ms
    Feb 27 15:51:45.802: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57": Phase="Running", Reason="", readiness=false. Elapsed: 2.007872301s
    Feb 27 15:51:47.803: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00850149s
    STEP: Saw pod success 02/27/23 15:51:47.803
    Feb 27 15:51:47.803: INFO: Pod "pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57" satisfied condition "Succeeded or Failed"
    Feb 27 15:51:47.806: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:51:47.812
    Feb 27 15:51:47.822: INFO: Waiting for pod pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57 to disappear
    Feb 27 15:51:47.824: INFO: Pod pod-secrets-926433b5-4763-4fd6-bcf3-81673cf24b57 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:51:47.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9543" for this suite. 02/27/23 15:51:47.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:51:47.834
Feb 27 15:51:47.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pod-network-test 02/27/23 15:51:47.834
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:47.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:47.855
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4298 02/27/23 15:51:47.857
STEP: creating a selector 02/27/23 15:51:47.857
STEP: Creating the service pods in kubernetes 02/27/23 15:51:47.857
Feb 27 15:51:47.857: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 15:51:47.887: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4298" to be "running and ready"
Feb 27 15:51:47.902: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.240007ms
Feb 27 15:51:47.902: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:51:49.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019375295s
Feb 27 15:51:49.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:51:51.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018721818s
Feb 27 15:51:51.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:51:53.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018679156s
Feb 27 15:51:53.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:51:55.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019282728s
Feb 27 15:51:55.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:51:57.907: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019894056s
Feb 27 15:51:57.907: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:51:59.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019014976s
Feb 27 15:51:59.906: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 15:51:59.906: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 15:51:59.909: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4298" to be "running and ready"
Feb 27 15:51:59.912: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.64371ms
Feb 27 15:51:59.912: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 15:51:59.912: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 15:51:59.914: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4298" to be "running and ready"
Feb 27 15:51:59.916: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.666655ms
Feb 27 15:51:59.916: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 15:51:59.916: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 15:51:59.919
Feb 27 15:51:59.930: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4298" to be "running"
Feb 27 15:51:59.935: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.033658ms
Feb 27 15:52:01.939: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009443861s
Feb 27 15:52:01.939: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 15:52:01.942: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4298" to be "running"
Feb 27 15:52:01.944: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.278869ms
Feb 27 15:52:01.944: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 27 15:52:01.947: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 15:52:01.947: INFO: Going to poll 192.168.212.142 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 27 15:52:01.949: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.212.142:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4298 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:52:01.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:52:01.949: INFO: ExecWithOptions: Clientset creation
Feb 27 15:52:01.950: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4298/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.212.142%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 15:52:02.025: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 27 15:52:02.025: INFO: Going to poll 192.168.192.181 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 27 15:52:02.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.192.181:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4298 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:52:02.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:52:02.029: INFO: ExecWithOptions: Clientset creation
Feb 27 15:52:02.029: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4298/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.192.181%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 15:52:02.084: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 27 15:52:02.084: INFO: Going to poll 192.168.214.176 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 27 15:52:02.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.214.176:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4298 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:52:02.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:52:02.087: INFO: ExecWithOptions: Clientset creation
Feb 27 15:52:02.087: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4298/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.214.176%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 15:52:02.149: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 15:52:02.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4298" for this suite. 02/27/23 15:52:02.152
------------------------------
• [SLOW TEST] [14.325 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:51:47.834
    Feb 27 15:51:47.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 15:51:47.834
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:51:47.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:51:47.855
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4298 02/27/23 15:51:47.857
    STEP: creating a selector 02/27/23 15:51:47.857
    STEP: Creating the service pods in kubernetes 02/27/23 15:51:47.857
    Feb 27 15:51:47.857: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 15:51:47.887: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4298" to be "running and ready"
    Feb 27 15:51:47.902: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.240007ms
    Feb 27 15:51:47.902: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:51:49.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019375295s
    Feb 27 15:51:49.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:51:51.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018721818s
    Feb 27 15:51:51.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:51:53.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018679156s
    Feb 27 15:51:53.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:51:55.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019282728s
    Feb 27 15:51:55.906: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:51:57.907: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.019894056s
    Feb 27 15:51:57.907: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:51:59.906: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.019014976s
    Feb 27 15:51:59.906: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 15:51:59.906: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 15:51:59.909: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4298" to be "running and ready"
    Feb 27 15:51:59.912: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.64371ms
    Feb 27 15:51:59.912: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 15:51:59.912: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 15:51:59.914: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4298" to be "running and ready"
    Feb 27 15:51:59.916: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.666655ms
    Feb 27 15:51:59.916: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 15:51:59.916: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 15:51:59.919
    Feb 27 15:51:59.930: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4298" to be "running"
    Feb 27 15:51:59.935: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.033658ms
    Feb 27 15:52:01.939: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009443861s
    Feb 27 15:52:01.939: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 15:52:01.942: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4298" to be "running"
    Feb 27 15:52:01.944: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.278869ms
    Feb 27 15:52:01.944: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 27 15:52:01.947: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 15:52:01.947: INFO: Going to poll 192.168.212.142 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 15:52:01.949: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.212.142:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4298 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:52:01.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:52:01.949: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:52:01.950: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4298/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.212.142%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 15:52:02.025: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 27 15:52:02.025: INFO: Going to poll 192.168.192.181 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 15:52:02.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.192.181:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4298 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:52:02.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:52:02.029: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:52:02.029: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4298/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.192.181%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 15:52:02.084: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 27 15:52:02.084: INFO: Going to poll 192.168.214.176 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 15:52:02.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.214.176:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4298 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:52:02.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:52:02.087: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:52:02.087: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4298/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.214.176%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 15:52:02.149: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:52:02.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4298" for this suite. 02/27/23 15:52:02.152
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:52:02.159
Feb 27 15:52:02.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pod-network-test 02/27/23 15:52:02.16
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:52:02.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:52:02.174
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3094 02/27/23 15:52:02.176
STEP: creating a selector 02/27/23 15:52:02.176
STEP: Creating the service pods in kubernetes 02/27/23 15:52:02.176
Feb 27 15:52:02.176: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 15:52:02.202: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3094" to be "running and ready"
Feb 27 15:52:02.208: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960913ms
Feb 27 15:52:02.208: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:52:04.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010323085s
Feb 27 15:52:04.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:52:06.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009398401s
Feb 27 15:52:06.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:52:08.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010560918s
Feb 27 15:52:08.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:52:10.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010379159s
Feb 27 15:52:10.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:52:12.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008955287s
Feb 27 15:52:12.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 15:52:14.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010264415s
Feb 27 15:52:14.212: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 15:52:14.212: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 15:52:14.215: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3094" to be "running and ready"
Feb 27 15:52:14.217: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.285328ms
Feb 27 15:52:14.217: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 15:52:14.217: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 15:52:14.220: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3094" to be "running and ready"
Feb 27 15:52:14.222: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.716499ms
Feb 27 15:52:14.222: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 15:52:14.222: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 15:52:14.224
Feb 27 15:52:14.236: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3094" to be "running"
Feb 27 15:52:14.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108985ms
Feb 27 15:52:16.243: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007010624s
Feb 27 15:52:16.243: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 15:52:16.246: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3094" to be "running"
Feb 27 15:52:16.249: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.587162ms
Feb 27 15:52:16.249: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 27 15:52:16.251: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 15:52:16.251: INFO: Going to poll 192.168.212.148 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 27 15:52:16.255: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.212.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:52:16.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:52:16.255: INFO: ExecWithOptions: Clientset creation
Feb 27 15:52:16.255: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3094/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.212.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 15:52:17.331: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 27 15:52:17.331: INFO: Going to poll 192.168.192.184 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 27 15:52:17.333: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.192.184 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:52:17.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:52:17.334: INFO: ExecWithOptions: Clientset creation
Feb 27 15:52:17.334: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3094/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.192.184+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 15:52:18.406: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 27 15:52:18.407: INFO: Going to poll 192.168.214.177 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 27 15:52:18.410: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.214.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 15:52:18.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 15:52:18.410: INFO: ExecWithOptions: Clientset creation
Feb 27 15:52:18.410: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3094/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.214.177+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 15:52:19.480: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 15:52:19.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3094" for this suite. 02/27/23 15:52:19.483
------------------------------
• [SLOW TEST] [17.330 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:52:02.159
    Feb 27 15:52:02.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 15:52:02.16
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:52:02.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:52:02.174
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3094 02/27/23 15:52:02.176
    STEP: creating a selector 02/27/23 15:52:02.176
    STEP: Creating the service pods in kubernetes 02/27/23 15:52:02.176
    Feb 27 15:52:02.176: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 15:52:02.202: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3094" to be "running and ready"
    Feb 27 15:52:02.208: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.960913ms
    Feb 27 15:52:02.208: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:52:04.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010323085s
    Feb 27 15:52:04.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:52:06.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009398401s
    Feb 27 15:52:06.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:52:08.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010560918s
    Feb 27 15:52:08.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:52:10.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010379159s
    Feb 27 15:52:10.212: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:52:12.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008955287s
    Feb 27 15:52:12.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 15:52:14.212: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010264415s
    Feb 27 15:52:14.212: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 15:52:14.212: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 15:52:14.215: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3094" to be "running and ready"
    Feb 27 15:52:14.217: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.285328ms
    Feb 27 15:52:14.217: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 15:52:14.217: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 15:52:14.220: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3094" to be "running and ready"
    Feb 27 15:52:14.222: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.716499ms
    Feb 27 15:52:14.222: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 15:52:14.222: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 15:52:14.224
    Feb 27 15:52:14.236: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3094" to be "running"
    Feb 27 15:52:14.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108985ms
    Feb 27 15:52:16.243: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007010624s
    Feb 27 15:52:16.243: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 15:52:16.246: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3094" to be "running"
    Feb 27 15:52:16.249: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.587162ms
    Feb 27 15:52:16.249: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 27 15:52:16.251: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 15:52:16.251: INFO: Going to poll 192.168.212.148 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 15:52:16.255: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.212.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:52:16.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:52:16.255: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:52:16.255: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3094/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.212.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 15:52:17.331: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 27 15:52:17.331: INFO: Going to poll 192.168.192.184 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 15:52:17.333: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.192.184 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:52:17.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:52:17.334: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:52:17.334: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3094/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.192.184+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 15:52:18.406: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 27 15:52:18.407: INFO: Going to poll 192.168.214.177 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 15:52:18.410: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.214.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 15:52:18.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 15:52:18.410: INFO: ExecWithOptions: Clientset creation
    Feb 27 15:52:18.410: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3094/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.214.177+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 15:52:19.480: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:52:19.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3094" for this suite. 02/27/23 15:52:19.483
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:52:19.489
Feb 27 15:52:19.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:52:19.49
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:52:19.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:52:19.504
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:52:19.506
Feb 27 15:52:19.514: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e" in namespace "downward-api-8938" to be "Succeeded or Failed"
Feb 27 15:52:19.522: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.784035ms
Feb 27 15:52:21.526: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011852481s
Feb 27 15:52:23.526: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011803175s
STEP: Saw pod success 02/27/23 15:52:23.526
Feb 27 15:52:23.526: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e" satisfied condition "Succeeded or Failed"
Feb 27 15:52:23.529: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e container client-container: <nil>
STEP: delete the pod 02/27/23 15:52:23.534
Feb 27 15:52:23.546: INFO: Waiting for pod downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e to disappear
Feb 27 15:52:23.548: INFO: Pod downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 15:52:23.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8938" for this suite. 02/27/23 15:52:23.551
------------------------------
• [4.066 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:52:19.489
    Feb 27 15:52:19.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:52:19.49
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:52:19.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:52:19.504
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:52:19.506
    Feb 27 15:52:19.514: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e" in namespace "downward-api-8938" to be "Succeeded or Failed"
    Feb 27 15:52:19.522: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.784035ms
    Feb 27 15:52:21.526: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011852481s
    Feb 27 15:52:23.526: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011803175s
    STEP: Saw pod success 02/27/23 15:52:23.526
    Feb 27 15:52:23.526: INFO: Pod "downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e" satisfied condition "Succeeded or Failed"
    Feb 27 15:52:23.529: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e container client-container: <nil>
    STEP: delete the pod 02/27/23 15:52:23.534
    Feb 27 15:52:23.546: INFO: Waiting for pod downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e to disappear
    Feb 27 15:52:23.548: INFO: Pod downwardapi-volume-2aaf7a0c-eaea-4bdd-906b-ade9a6e76d3e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:52:23.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8938" for this suite. 02/27/23 15:52:23.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:52:23.556
Feb 27 15:52:23.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 15:52:23.557
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:52:23.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:52:23.573
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-kqlks" 02/27/23 15:52:23.578
Feb 27 15:52:23.586: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard cpu limit of 500m
Feb 27 15:52:23.586: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-kqlks" /status 02/27/23 15:52:23.586
STEP: Confirm /status for "e2e-rq-status-kqlks" resourceQuota via watch 02/27/23 15:52:23.593
Feb 27 15:52:23.594: INFO: observed resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList(nil)
Feb 27 15:52:23.594: INFO: Found resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Feb 27 15:52:23.594: INFO: ResourceQuota "e2e-rq-status-kqlks" /status was updated
STEP: Patching hard spec values for cpu & memory 02/27/23 15:52:23.597
Feb 27 15:52:23.601: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard cpu limit of 1
Feb 27 15:52:23.601: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-kqlks" /status 02/27/23 15:52:23.601
STEP: Confirm /status for "e2e-rq-status-kqlks" resourceQuota via watch 02/27/23 15:52:23.608
Feb 27 15:52:23.610: INFO: observed resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Feb 27 15:52:23.610: INFO: Found resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Feb 27 15:52:23.610: INFO: ResourceQuota "e2e-rq-status-kqlks" /status was patched
STEP: Get "e2e-rq-status-kqlks" /status 02/27/23 15:52:23.61
Feb 27 15:52:23.619: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard cpu of 1
Feb 27 15:52:23.620: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-kqlks" /status before checking Spec is unchanged 02/27/23 15:52:23.622
Feb 27 15:52:23.629: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard cpu of 2
Feb 27 15:52:23.629: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard memory of 2Gi
Feb 27 15:52:23.631: INFO: Found resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Feb 27 15:53:23.640: INFO: ResourceQuota "e2e-rq-status-kqlks" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 15:53:23.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2227" for this suite. 02/27/23 15:53:23.643
------------------------------
• [SLOW TEST] [60.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:52:23.556
    Feb 27 15:52:23.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 15:52:23.557
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:52:23.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:52:23.573
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-kqlks" 02/27/23 15:52:23.578
    Feb 27 15:52:23.586: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard cpu limit of 500m
    Feb 27 15:52:23.586: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-kqlks" /status 02/27/23 15:52:23.586
    STEP: Confirm /status for "e2e-rq-status-kqlks" resourceQuota via watch 02/27/23 15:52:23.593
    Feb 27 15:52:23.594: INFO: observed resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList(nil)
    Feb 27 15:52:23.594: INFO: Found resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Feb 27 15:52:23.594: INFO: ResourceQuota "e2e-rq-status-kqlks" /status was updated
    STEP: Patching hard spec values for cpu & memory 02/27/23 15:52:23.597
    Feb 27 15:52:23.601: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard cpu limit of 1
    Feb 27 15:52:23.601: INFO: Resource quota "e2e-rq-status-kqlks" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-kqlks" /status 02/27/23 15:52:23.601
    STEP: Confirm /status for "e2e-rq-status-kqlks" resourceQuota via watch 02/27/23 15:52:23.608
    Feb 27 15:52:23.610: INFO: observed resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Feb 27 15:52:23.610: INFO: Found resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Feb 27 15:52:23.610: INFO: ResourceQuota "e2e-rq-status-kqlks" /status was patched
    STEP: Get "e2e-rq-status-kqlks" /status 02/27/23 15:52:23.61
    Feb 27 15:52:23.619: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard cpu of 1
    Feb 27 15:52:23.620: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-kqlks" /status before checking Spec is unchanged 02/27/23 15:52:23.622
    Feb 27 15:52:23.629: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard cpu of 2
    Feb 27 15:52:23.629: INFO: Resourcequota "e2e-rq-status-kqlks" reports status: hard memory of 2Gi
    Feb 27 15:52:23.631: INFO: Found resourceQuota "e2e-rq-status-kqlks" in namespace "resourcequota-2227" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Feb 27 15:53:23.640: INFO: ResourceQuota "e2e-rq-status-kqlks" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:53:23.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2227" for this suite. 02/27/23 15:53:23.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:53:23.65
Feb 27 15:53:23.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename job 02/27/23 15:53:23.65
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:23.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:23.665
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 02/27/23 15:53:23.67
STEP: Patching the Job 02/27/23 15:53:23.678
STEP: Watching for Job to be patched 02/27/23 15:53:23.686
Feb 27 15:53:23.687: INFO: Event ADDED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 27 15:53:23.687: INFO: Event MODIFIED found for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/27/23 15:53:23.687
STEP: Watching for Job to be updated 02/27/23 15:53:23.717
Feb 27 15:53:23.719: INFO: Event MODIFIED found for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 15:53:23.719: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/27/23 15:53:23.719
Feb 27 15:53:23.722: INFO: Job: e2e-sh6wm as labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched]
STEP: Waiting for job to complete 02/27/23 15:53:23.722
STEP: Delete a job collection with a labelselector 02/27/23 15:53:33.727
STEP: Watching for Job to be deleted 02/27/23 15:53:33.735
Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 15:53:33.736: INFO: Event DELETED found for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/27/23 15:53:33.736
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 15:53:33.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4134" for this suite. 02/27/23 15:53:33.745
------------------------------
• [SLOW TEST] [10.103 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:53:23.65
    Feb 27 15:53:23.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename job 02/27/23 15:53:23.65
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:23.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:23.665
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 02/27/23 15:53:23.67
    STEP: Patching the Job 02/27/23 15:53:23.678
    STEP: Watching for Job to be patched 02/27/23 15:53:23.686
    Feb 27 15:53:23.687: INFO: Event ADDED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 27 15:53:23.687: INFO: Event MODIFIED found for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/27/23 15:53:23.687
    STEP: Watching for Job to be updated 02/27/23 15:53:23.717
    Feb 27 15:53:23.719: INFO: Event MODIFIED found for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 15:53:23.719: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/27/23 15:53:23.719
    Feb 27 15:53:23.722: INFO: Job: e2e-sh6wm as labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched]
    STEP: Waiting for job to complete 02/27/23 15:53:23.722
    STEP: Delete a job collection with a labelselector 02/27/23 15:53:33.727
    STEP: Watching for Job to be deleted 02/27/23 15:53:33.735
    Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 15:53:33.736: INFO: Event MODIFIED observed for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 15:53:33.736: INFO: Event DELETED found for Job e2e-sh6wm in namespace job-4134 with labels: map[e2e-job-label:e2e-sh6wm e2e-sh6wm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/27/23 15:53:33.736
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:53:33.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4134" for this suite. 02/27/23 15:53:33.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:53:33.753
Feb 27 15:53:33.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:53:33.753
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:33.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:33.778
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 15:53:33.781
Feb 27 15:53:33.787: INFO: Waiting up to 5m0s for pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53" in namespace "emptydir-2762" to be "Succeeded or Failed"
Feb 27 15:53:33.792: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.557426ms
Feb 27 15:53:35.795: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007536416s
Feb 27 15:53:37.796: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008350767s
STEP: Saw pod success 02/27/23 15:53:37.796
Feb 27 15:53:37.796: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53" satisfied condition "Succeeded or Failed"
Feb 27 15:53:37.800: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-683ba651-e6da-46e8-af59-8472a3a1ee53 container test-container: <nil>
STEP: delete the pod 02/27/23 15:53:37.807
Feb 27 15:53:37.818: INFO: Waiting for pod pod-683ba651-e6da-46e8-af59-8472a3a1ee53 to disappear
Feb 27 15:53:37.821: INFO: Pod pod-683ba651-e6da-46e8-af59-8472a3a1ee53 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:53:37.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2762" for this suite. 02/27/23 15:53:37.824
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:53:33.753
    Feb 27 15:53:33.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:53:33.753
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:33.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:33.778
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 15:53:33.781
    Feb 27 15:53:33.787: INFO: Waiting up to 5m0s for pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53" in namespace "emptydir-2762" to be "Succeeded or Failed"
    Feb 27 15:53:33.792: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.557426ms
    Feb 27 15:53:35.795: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007536416s
    Feb 27 15:53:37.796: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008350767s
    STEP: Saw pod success 02/27/23 15:53:37.796
    Feb 27 15:53:37.796: INFO: Pod "pod-683ba651-e6da-46e8-af59-8472a3a1ee53" satisfied condition "Succeeded or Failed"
    Feb 27 15:53:37.800: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-683ba651-e6da-46e8-af59-8472a3a1ee53 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:53:37.807
    Feb 27 15:53:37.818: INFO: Waiting for pod pod-683ba651-e6da-46e8-af59-8472a3a1ee53 to disappear
    Feb 27 15:53:37.821: INFO: Pod pod-683ba651-e6da-46e8-af59-8472a3a1ee53 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:53:37.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2762" for this suite. 02/27/23 15:53:37.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:53:37.83
Feb 27 15:53:37.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:53:37.831
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:37.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:37.844
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 02/27/23 15:53:37.853
Feb 27 15:53:37.864: INFO: Waiting up to 5m0s for pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d" in namespace "emptydir-8013" to be "Succeeded or Failed"
Feb 27 15:53:37.868: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104611ms
Feb 27 15:53:39.873: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008494257s
Feb 27 15:53:41.876: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012241755s
STEP: Saw pod success 02/27/23 15:53:41.876
Feb 27 15:53:41.877: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d" satisfied condition "Succeeded or Failed"
Feb 27 15:53:41.879: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-1e1dce6d-cd07-4386-9baf-85582b3d765d container test-container: <nil>
STEP: delete the pod 02/27/23 15:53:41.886
Feb 27 15:53:41.897: INFO: Waiting for pod pod-1e1dce6d-cd07-4386-9baf-85582b3d765d to disappear
Feb 27 15:53:41.900: INFO: Pod pod-1e1dce6d-cd07-4386-9baf-85582b3d765d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:53:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8013" for this suite. 02/27/23 15:53:41.904
------------------------------
• [4.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:53:37.83
    Feb 27 15:53:37.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:53:37.831
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:37.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:37.844
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 02/27/23 15:53:37.853
    Feb 27 15:53:37.864: INFO: Waiting up to 5m0s for pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d" in namespace "emptydir-8013" to be "Succeeded or Failed"
    Feb 27 15:53:37.868: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104611ms
    Feb 27 15:53:39.873: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008494257s
    Feb 27 15:53:41.876: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012241755s
    STEP: Saw pod success 02/27/23 15:53:41.876
    Feb 27 15:53:41.877: INFO: Pod "pod-1e1dce6d-cd07-4386-9baf-85582b3d765d" satisfied condition "Succeeded or Failed"
    Feb 27 15:53:41.879: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-1e1dce6d-cd07-4386-9baf-85582b3d765d container test-container: <nil>
    STEP: delete the pod 02/27/23 15:53:41.886
    Feb 27 15:53:41.897: INFO: Waiting for pod pod-1e1dce6d-cd07-4386-9baf-85582b3d765d to disappear
    Feb 27 15:53:41.900: INFO: Pod pod-1e1dce6d-cd07-4386-9baf-85582b3d765d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:53:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8013" for this suite. 02/27/23 15:53:41.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:53:41.91
Feb 27 15:53:41.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-preemption 02/27/23 15:53:41.911
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:41.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:41.931
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 15:53:41.962: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 15:54:41.976: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:54:41.979
Feb 27 15:54:41.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 15:54:41.98
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:41.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:42.002
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Feb 27 15:54:42.016: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb 27 15:54:42.018: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Feb 27 15:54:42.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:54:42.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2291" for this suite. 02/27/23 15:54:42.092
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7209" for this suite. 02/27/23 15:54:42.098
------------------------------
• [SLOW TEST] [60.194 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:53:41.91
    Feb 27 15:53:41.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 15:53:41.911
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:53:41.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:53:41.931
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 15:53:41.962: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 15:54:41.976: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:54:41.979
    Feb 27 15:54:41.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 15:54:41.98
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:41.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:42.002
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Feb 27 15:54:42.016: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb 27 15:54:42.018: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:54:42.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:54:42.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2291" for this suite. 02/27/23 15:54:42.092
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7209" for this suite. 02/27/23 15:54:42.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:54:42.104
Feb 27 15:54:42.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:54:42.105
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:42.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:42.121
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 02/27/23 15:54:42.124
Feb 27 15:54:42.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4361 create -f -'
Feb 27 15:54:42.266: INFO: stderr: ""
Feb 27 15:54:42.266: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/27/23 15:54:42.266
Feb 27 15:54:42.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4361 diff -f -'
Feb 27 15:54:42.488: INFO: rc: 1
Feb 27 15:54:42.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4361 delete -f -'
Feb 27 15:54:42.551: INFO: stderr: ""
Feb 27 15:54:42.551: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:54:42.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4361" for this suite. 02/27/23 15:54:42.557
------------------------------
• [0.458 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:54:42.104
    Feb 27 15:54:42.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:54:42.105
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:42.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:42.121
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 02/27/23 15:54:42.124
    Feb 27 15:54:42.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4361 create -f -'
    Feb 27 15:54:42.266: INFO: stderr: ""
    Feb 27 15:54:42.266: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/27/23 15:54:42.266
    Feb 27 15:54:42.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4361 diff -f -'
    Feb 27 15:54:42.488: INFO: rc: 1
    Feb 27 15:54:42.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-4361 delete -f -'
    Feb 27 15:54:42.551: INFO: stderr: ""
    Feb 27 15:54:42.551: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:54:42.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4361" for this suite. 02/27/23 15:54:42.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:54:42.563
Feb 27 15:54:42.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename containers 02/27/23 15:54:42.563
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:42.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:42.578
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 02/27/23 15:54:42.58
Feb 27 15:54:42.589: INFO: Waiting up to 5m0s for pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e" in namespace "containers-233" to be "Succeeded or Failed"
Feb 27 15:54:42.591: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.60441ms
Feb 27 15:54:44.595: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006134826s
Feb 27 15:54:46.595: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006847404s
STEP: Saw pod success 02/27/23 15:54:46.595
Feb 27 15:54:46.596: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e" satisfied condition "Succeeded or Failed"
Feb 27 15:54:46.598: INFO: Trying to get logs from node ip-172-31-42-40 pod client-containers-463066f7-eaff-467a-abd9-d17a41cb523e container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:54:46.604
Feb 27 15:54:46.615: INFO: Waiting for pod client-containers-463066f7-eaff-467a-abd9-d17a41cb523e to disappear
Feb 27 15:54:46.621: INFO: Pod client-containers-463066f7-eaff-467a-abd9-d17a41cb523e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 15:54:46.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-233" for this suite. 02/27/23 15:54:46.624
------------------------------
• [4.066 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:54:42.563
    Feb 27 15:54:42.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename containers 02/27/23 15:54:42.563
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:42.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:42.578
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 02/27/23 15:54:42.58
    Feb 27 15:54:42.589: INFO: Waiting up to 5m0s for pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e" in namespace "containers-233" to be "Succeeded or Failed"
    Feb 27 15:54:42.591: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.60441ms
    Feb 27 15:54:44.595: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006134826s
    Feb 27 15:54:46.595: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006847404s
    STEP: Saw pod success 02/27/23 15:54:46.595
    Feb 27 15:54:46.596: INFO: Pod "client-containers-463066f7-eaff-467a-abd9-d17a41cb523e" satisfied condition "Succeeded or Failed"
    Feb 27 15:54:46.598: INFO: Trying to get logs from node ip-172-31-42-40 pod client-containers-463066f7-eaff-467a-abd9-d17a41cb523e container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:54:46.604
    Feb 27 15:54:46.615: INFO: Waiting for pod client-containers-463066f7-eaff-467a-abd9-d17a41cb523e to disappear
    Feb 27 15:54:46.621: INFO: Pod client-containers-463066f7-eaff-467a-abd9-d17a41cb523e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:54:46.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-233" for this suite. 02/27/23 15:54:46.624
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:54:46.629
Feb 27 15:54:46.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename limitrange 02/27/23 15:54:46.63
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:46.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:46.644
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 02/27/23 15:54:46.646
STEP: Setting up watch 02/27/23 15:54:46.647
STEP: Submitting a LimitRange 02/27/23 15:54:46.75
STEP: Verifying LimitRange creation was observed 02/27/23 15:54:46.755
STEP: Fetching the LimitRange to ensure it has proper values 02/27/23 15:54:46.755
Feb 27 15:54:46.758: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 27 15:54:46.758: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/27/23 15:54:46.758
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/27/23 15:54:46.763
Feb 27 15:54:46.765: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 27 15:54:46.765: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/27/23 15:54:46.765
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/27/23 15:54:46.773
Feb 27 15:54:46.776: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 27 15:54:46.776: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/27/23 15:54:46.776
STEP: Failing to create a Pod with more than max resources 02/27/23 15:54:46.778
STEP: Updating a LimitRange 02/27/23 15:54:46.78
STEP: Verifying LimitRange updating is effective 02/27/23 15:54:46.786
STEP: Creating a Pod with less than former min resources 02/27/23 15:54:48.79
STEP: Failing to create a Pod with more than max resources 02/27/23 15:54:48.798
STEP: Deleting a LimitRange 02/27/23 15:54:48.799
STEP: Verifying the LimitRange was deleted 02/27/23 15:54:48.808
Feb 27 15:54:53.814: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/27/23 15:54:53.814
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Feb 27 15:54:53.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6665" for this suite. 02/27/23 15:54:53.824
------------------------------
• [SLOW TEST] [7.201 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:54:46.629
    Feb 27 15:54:46.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename limitrange 02/27/23 15:54:46.63
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:46.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:46.644
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 02/27/23 15:54:46.646
    STEP: Setting up watch 02/27/23 15:54:46.647
    STEP: Submitting a LimitRange 02/27/23 15:54:46.75
    STEP: Verifying LimitRange creation was observed 02/27/23 15:54:46.755
    STEP: Fetching the LimitRange to ensure it has proper values 02/27/23 15:54:46.755
    Feb 27 15:54:46.758: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 27 15:54:46.758: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/27/23 15:54:46.758
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/27/23 15:54:46.763
    Feb 27 15:54:46.765: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 27 15:54:46.765: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/27/23 15:54:46.765
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/27/23 15:54:46.773
    Feb 27 15:54:46.776: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb 27 15:54:46.776: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/27/23 15:54:46.776
    STEP: Failing to create a Pod with more than max resources 02/27/23 15:54:46.778
    STEP: Updating a LimitRange 02/27/23 15:54:46.78
    STEP: Verifying LimitRange updating is effective 02/27/23 15:54:46.786
    STEP: Creating a Pod with less than former min resources 02/27/23 15:54:48.79
    STEP: Failing to create a Pod with more than max resources 02/27/23 15:54:48.798
    STEP: Deleting a LimitRange 02/27/23 15:54:48.799
    STEP: Verifying the LimitRange was deleted 02/27/23 15:54:48.808
    Feb 27 15:54:53.814: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/27/23 15:54:53.814
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:54:53.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6665" for this suite. 02/27/23 15:54:53.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:54:53.831
Feb 27 15:54:53.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename disruption 02/27/23 15:54:53.832
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:53.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:53.848
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 02/27/23 15:54:53.851
STEP: Waiting for the pdb to be processed 02/27/23 15:54:53.855
STEP: updating the pdb 02/27/23 15:54:55.861
STEP: Waiting for the pdb to be processed 02/27/23 15:54:55.868
STEP: patching the pdb 02/27/23 15:54:57.874
STEP: Waiting for the pdb to be processed 02/27/23 15:54:57.882
STEP: Waiting for the pdb to be deleted 02/27/23 15:54:57.892
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 15:54:57.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-55" for this suite. 02/27/23 15:54:57.899
------------------------------
• [4.073 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:54:53.831
    Feb 27 15:54:53.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename disruption 02/27/23 15:54:53.832
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:53.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:53.848
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 02/27/23 15:54:53.851
    STEP: Waiting for the pdb to be processed 02/27/23 15:54:53.855
    STEP: updating the pdb 02/27/23 15:54:55.861
    STEP: Waiting for the pdb to be processed 02/27/23 15:54:55.868
    STEP: patching the pdb 02/27/23 15:54:57.874
    STEP: Waiting for the pdb to be processed 02/27/23 15:54:57.882
    STEP: Waiting for the pdb to be deleted 02/27/23 15:54:57.892
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:54:57.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-55" for this suite. 02/27/23 15:54:57.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:54:57.905
Feb 27 15:54:57.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename namespaces 02/27/23 15:54:57.906
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:57.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:57.919
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6326" 02/27/23 15:54:57.931
Feb 27 15:54:57.937: INFO: Namespace "namespaces-6326" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"0e1ee2c8-2a4b-4b26-a946-79230015426d", "kubernetes.io/metadata.name":"namespaces-6326", "namespaces-6326":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:54:57.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6326" for this suite. 02/27/23 15:54:57.94
------------------------------
• [0.040 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:54:57.905
    Feb 27 15:54:57.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename namespaces 02/27/23 15:54:57.906
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:57.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:57.919
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6326" 02/27/23 15:54:57.931
    Feb 27 15:54:57.937: INFO: Namespace "namespaces-6326" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"0e1ee2c8-2a4b-4b26-a946-79230015426d", "kubernetes.io/metadata.name":"namespaces-6326", "namespaces-6326":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:54:57.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6326" for this suite. 02/27/23 15:54:57.94
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:54:57.945
Feb 27 15:54:57.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 15:54:57.946
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:57.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:57.961
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 02/27/23 15:54:57.963
Feb 27 15:54:57.963: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 27 15:54:57.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
Feb 27 15:54:58.115: INFO: stderr: ""
Feb 27 15:54:58.115: INFO: stdout: "service/agnhost-replica created\n"
Feb 27 15:54:58.115: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 27 15:54:58.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
Feb 27 15:54:58.281: INFO: stderr: ""
Feb 27 15:54:58.281: INFO: stdout: "service/agnhost-primary created\n"
Feb 27 15:54:58.281: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 27 15:54:58.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
Feb 27 15:54:58.525: INFO: stderr: ""
Feb 27 15:54:58.525: INFO: stdout: "service/frontend created\n"
Feb 27 15:54:58.525: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 27 15:54:58.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
Feb 27 15:54:58.769: INFO: stderr: ""
Feb 27 15:54:58.769: INFO: stdout: "deployment.apps/frontend created\n"
Feb 27 15:54:58.769: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 27 15:54:58.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
Feb 27 15:54:59.021: INFO: stderr: ""
Feb 27 15:54:59.021: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 27 15:54:59.021: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 27 15:54:59.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
Feb 27 15:54:59.289: INFO: stderr: ""
Feb 27 15:54:59.289: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/27/23 15:54:59.289
Feb 27 15:54:59.289: INFO: Waiting for all frontend pods to be Running.
Feb 27 15:55:04.342: INFO: Waiting for frontend to serve content.
Feb 27 15:55:04.351: INFO: Trying to add a new entry to the guestbook.
Feb 27 15:55:04.361: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/27/23 15:55:04.368
Feb 27 15:55:04.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
Feb 27 15:55:04.436: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:55:04.436: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 15:55:04.436
Feb 27 15:55:04.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
Feb 27 15:55:04.518: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:55:04.518: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 15:55:04.518
Feb 27 15:55:04.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
Feb 27 15:55:04.598: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:55:04.598: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 15:55:04.598
Feb 27 15:55:04.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
Feb 27 15:55:04.653: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:55:04.653: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 15:55:04.653
Feb 27 15:55:04.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
Feb 27 15:55:04.722: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:55:04.722: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 15:55:04.722
Feb 27 15:55:04.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
Feb 27 15:55:04.789: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 15:55:04.789: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:04.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1622" for this suite. 02/27/23 15:55:04.793
------------------------------
• [SLOW TEST] [6.869 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:54:57.945
    Feb 27 15:54:57.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 15:54:57.946
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:54:57.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:54:57.961
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 02/27/23 15:54:57.963
    Feb 27 15:54:57.963: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb 27 15:54:57.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
    Feb 27 15:54:58.115: INFO: stderr: ""
    Feb 27 15:54:58.115: INFO: stdout: "service/agnhost-replica created\n"
    Feb 27 15:54:58.115: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb 27 15:54:58.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
    Feb 27 15:54:58.281: INFO: stderr: ""
    Feb 27 15:54:58.281: INFO: stdout: "service/agnhost-primary created\n"
    Feb 27 15:54:58.281: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb 27 15:54:58.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
    Feb 27 15:54:58.525: INFO: stderr: ""
    Feb 27 15:54:58.525: INFO: stdout: "service/frontend created\n"
    Feb 27 15:54:58.525: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb 27 15:54:58.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
    Feb 27 15:54:58.769: INFO: stderr: ""
    Feb 27 15:54:58.769: INFO: stdout: "deployment.apps/frontend created\n"
    Feb 27 15:54:58.769: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 27 15:54:58.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
    Feb 27 15:54:59.021: INFO: stderr: ""
    Feb 27 15:54:59.021: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb 27 15:54:59.021: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 27 15:54:59.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 create -f -'
    Feb 27 15:54:59.289: INFO: stderr: ""
    Feb 27 15:54:59.289: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/27/23 15:54:59.289
    Feb 27 15:54:59.289: INFO: Waiting for all frontend pods to be Running.
    Feb 27 15:55:04.342: INFO: Waiting for frontend to serve content.
    Feb 27 15:55:04.351: INFO: Trying to add a new entry to the guestbook.
    Feb 27 15:55:04.361: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/27/23 15:55:04.368
    Feb 27 15:55:04.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
    Feb 27 15:55:04.436: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:55:04.436: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 15:55:04.436
    Feb 27 15:55:04.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
    Feb 27 15:55:04.518: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:55:04.518: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 15:55:04.518
    Feb 27 15:55:04.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
    Feb 27 15:55:04.598: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:55:04.598: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 15:55:04.598
    Feb 27 15:55:04.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
    Feb 27 15:55:04.653: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:55:04.653: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 15:55:04.653
    Feb 27 15:55:04.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
    Feb 27 15:55:04.722: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:55:04.722: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 15:55:04.722
    Feb 27 15:55:04.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1622 delete --grace-period=0 --force -f -'
    Feb 27 15:55:04.789: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 15:55:04.789: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:04.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1622" for this suite. 02/27/23 15:55:04.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:04.818
Feb 27 15:55:04.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:55:04.818
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:04.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:04.832
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:55:04.851
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:55:05.3
STEP: Deploying the webhook pod 02/27/23 15:55:05.307
STEP: Wait for the deployment to be ready 02/27/23 15:55:05.318
Feb 27 15:55:05.325: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 27 15:55:07.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/27/23 15:55:09.337
STEP: Verifying the service has paired with the endpoint 02/27/23 15:55:09.349
Feb 27 15:55:10.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Feb 27 15:55:10.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4608-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 15:55:10.862
STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 15:55:10.877
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6771" for this suite. 02/27/23 15:55:13.491
STEP: Destroying namespace "webhook-6771-markers" for this suite. 02/27/23 15:55:13.497
------------------------------
• [SLOW TEST] [8.689 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:04.818
    Feb 27 15:55:04.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:55:04.818
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:04.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:04.832
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:55:04.851
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:55:05.3
    STEP: Deploying the webhook pod 02/27/23 15:55:05.307
    STEP: Wait for the deployment to be ready 02/27/23 15:55:05.318
    Feb 27 15:55:05.325: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 27 15:55:07.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 15, 55, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/27/23 15:55:09.337
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:55:09.349
    Feb 27 15:55:10.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Feb 27 15:55:10.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4608-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 15:55:10.862
    STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 15:55:10.877
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6771" for this suite. 02/27/23 15:55:13.491
    STEP: Destroying namespace "webhook-6771-markers" for this suite. 02/27/23 15:55:13.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:13.507
Feb 27 15:55:13.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:55:13.508
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:13.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:13.523
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 02/27/23 15:55:13.525
Feb 27 15:55:13.530: INFO: Waiting up to 5m0s for pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770" in namespace "downward-api-5810" to be "Succeeded or Failed"
Feb 27 15:55:13.533: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970524ms
Feb 27 15:55:15.537: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006686437s
Feb 27 15:55:17.537: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006641692s
STEP: Saw pod success 02/27/23 15:55:17.537
Feb 27 15:55:17.537: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770" satisfied condition "Succeeded or Failed"
Feb 27 15:55:17.540: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-71257d66-e21f-46a5-9b4c-615ee7341770 container dapi-container: <nil>
STEP: delete the pod 02/27/23 15:55:17.546
Feb 27 15:55:17.555: INFO: Waiting for pod downward-api-71257d66-e21f-46a5-9b4c-615ee7341770 to disappear
Feb 27 15:55:17.558: INFO: Pod downward-api-71257d66-e21f-46a5-9b4c-615ee7341770 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:17.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5810" for this suite. 02/27/23 15:55:17.562
------------------------------
• [4.060 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:13.507
    Feb 27 15:55:13.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:55:13.508
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:13.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:13.523
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 02/27/23 15:55:13.525
    Feb 27 15:55:13.530: INFO: Waiting up to 5m0s for pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770" in namespace "downward-api-5810" to be "Succeeded or Failed"
    Feb 27 15:55:13.533: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970524ms
    Feb 27 15:55:15.537: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006686437s
    Feb 27 15:55:17.537: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006641692s
    STEP: Saw pod success 02/27/23 15:55:17.537
    Feb 27 15:55:17.537: INFO: Pod "downward-api-71257d66-e21f-46a5-9b4c-615ee7341770" satisfied condition "Succeeded or Failed"
    Feb 27 15:55:17.540: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-71257d66-e21f-46a5-9b4c-615ee7341770 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 15:55:17.546
    Feb 27 15:55:17.555: INFO: Waiting for pod downward-api-71257d66-e21f-46a5-9b4c-615ee7341770 to disappear
    Feb 27 15:55:17.558: INFO: Pod downward-api-71257d66-e21f-46a5-9b4c-615ee7341770 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:17.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5810" for this suite. 02/27/23 15:55:17.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:17.568
Feb 27 15:55:17.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:55:17.569
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:17.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:17.631
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 02/27/23 15:55:17.634
Feb 27 15:55:17.642: INFO: Waiting up to 5m0s for pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b" in namespace "downward-api-1573" to be "Succeeded or Failed"
Feb 27 15:55:17.649: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.801645ms
Feb 27 15:55:19.652: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010077245s
Feb 27 15:55:21.653: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0104663s
STEP: Saw pod success 02/27/23 15:55:21.653
Feb 27 15:55:21.653: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b" satisfied condition "Succeeded or Failed"
Feb 27 15:55:21.656: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b container dapi-container: <nil>
STEP: delete the pod 02/27/23 15:55:21.662
Feb 27 15:55:21.674: INFO: Waiting for pod downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b to disappear
Feb 27 15:55:21.677: INFO: Pod downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:21.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1573" for this suite. 02/27/23 15:55:21.68
------------------------------
• [4.118 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:17.568
    Feb 27 15:55:17.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:55:17.569
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:17.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:17.631
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 02/27/23 15:55:17.634
    Feb 27 15:55:17.642: INFO: Waiting up to 5m0s for pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b" in namespace "downward-api-1573" to be "Succeeded or Failed"
    Feb 27 15:55:17.649: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.801645ms
    Feb 27 15:55:19.652: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010077245s
    Feb 27 15:55:21.653: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0104663s
    STEP: Saw pod success 02/27/23 15:55:21.653
    Feb 27 15:55:21.653: INFO: Pod "downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b" satisfied condition "Succeeded or Failed"
    Feb 27 15:55:21.656: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b container dapi-container: <nil>
    STEP: delete the pod 02/27/23 15:55:21.662
    Feb 27 15:55:21.674: INFO: Waiting for pod downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b to disappear
    Feb 27 15:55:21.677: INFO: Pod downward-api-f56d3920-af22-495b-88cd-0b9392e81a6b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:21.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1573" for this suite. 02/27/23 15:55:21.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:21.688
Feb 27 15:55:21.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 15:55:21.688
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:21.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:21.701
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 02/27/23 15:55:21.705
Feb 27 15:55:21.715: INFO: Waiting up to 5m0s for pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7" in namespace "var-expansion-2025" to be "Succeeded or Failed"
Feb 27 15:55:21.718: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373898ms
Feb 27 15:55:23.722: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006917869s
Feb 27 15:55:25.722: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006548389s
STEP: Saw pod success 02/27/23 15:55:25.722
Feb 27 15:55:25.722: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7" satisfied condition "Succeeded or Failed"
Feb 27 15:55:25.725: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7 container dapi-container: <nil>
STEP: delete the pod 02/27/23 15:55:25.73
Feb 27 15:55:25.743: INFO: Waiting for pod var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7 to disappear
Feb 27 15:55:25.746: INFO: Pod var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:25.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2025" for this suite. 02/27/23 15:55:25.749
------------------------------
• [4.067 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:21.688
    Feb 27 15:55:21.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 15:55:21.688
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:21.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:21.701
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 02/27/23 15:55:21.705
    Feb 27 15:55:21.715: INFO: Waiting up to 5m0s for pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7" in namespace "var-expansion-2025" to be "Succeeded or Failed"
    Feb 27 15:55:21.718: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373898ms
    Feb 27 15:55:23.722: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006917869s
    Feb 27 15:55:25.722: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006548389s
    STEP: Saw pod success 02/27/23 15:55:25.722
    Feb 27 15:55:25.722: INFO: Pod "var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7" satisfied condition "Succeeded or Failed"
    Feb 27 15:55:25.725: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 15:55:25.73
    Feb 27 15:55:25.743: INFO: Waiting for pod var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7 to disappear
    Feb 27 15:55:25.746: INFO: Pod var-expansion-1a64869e-55ad-43d8-af80-a6c08de884f7 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:25.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2025" for this suite. 02/27/23 15:55:25.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:25.755
Feb 27 15:55:25.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename runtimeclass 02/27/23 15:55:25.755
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:25.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:25.768
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5388-delete-me 02/27/23 15:55:25.777
STEP: Waiting for the RuntimeClass to disappear 02/27/23 15:55:25.784
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:25.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5388" for this suite. 02/27/23 15:55:25.794
------------------------------
• [0.048 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:25.755
    Feb 27 15:55:25.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 15:55:25.755
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:25.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:25.768
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5388-delete-me 02/27/23 15:55:25.777
    STEP: Waiting for the RuntimeClass to disappear 02/27/23 15:55:25.784
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:25.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5388" for this suite. 02/27/23 15:55:25.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:25.804
Feb 27 15:55:25.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:55:25.804
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:25.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:25.818
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:29.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9640" for this suite. 02/27/23 15:55:29.839
------------------------------
• [4.041 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:25.804
    Feb 27 15:55:25.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:55:25.804
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:25.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:25.818
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:29.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9640" for this suite. 02/27/23 15:55:29.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:29.846
Feb 27 15:55:29.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 15:55:29.847
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:29.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:29.861
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 02/27/23 15:55:29.864
Feb 27 15:55:29.871: INFO: Waiting up to 5m0s for pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840" in namespace "pods-4904" to be "running and ready"
Feb 27 15:55:29.877: INFO: Pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942629ms
Feb 27 15:55:29.877: INFO: The phase of Pod pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:55:31.880: INFO: Pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840": Phase="Running", Reason="", readiness=true. Elapsed: 2.008791288s
Feb 27 15:55:31.880: INFO: The phase of Pod pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840 is Running (Ready = true)
Feb 27 15:55:31.880: INFO: Pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840" satisfied condition "running and ready"
Feb 27 15:55:31.885: INFO: Pod pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840 has hostIP: 172.31.42.40
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:31.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4904" for this suite. 02/27/23 15:55:31.888
------------------------------
• [2.048 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:29.846
    Feb 27 15:55:29.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 15:55:29.847
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:29.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:29.861
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 02/27/23 15:55:29.864
    Feb 27 15:55:29.871: INFO: Waiting up to 5m0s for pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840" in namespace "pods-4904" to be "running and ready"
    Feb 27 15:55:29.877: INFO: Pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942629ms
    Feb 27 15:55:29.877: INFO: The phase of Pod pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:55:31.880: INFO: Pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840": Phase="Running", Reason="", readiness=true. Elapsed: 2.008791288s
    Feb 27 15:55:31.880: INFO: The phase of Pod pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840 is Running (Ready = true)
    Feb 27 15:55:31.880: INFO: Pod "pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840" satisfied condition "running and ready"
    Feb 27 15:55:31.885: INFO: Pod pod-hostip-721f7ec1-2da6-4620-a3e5-ff339dd18840 has hostIP: 172.31.42.40
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:31.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4904" for this suite. 02/27/23 15:55:31.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:31.895
Feb 27 15:55:31.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replication-controller 02/27/23 15:55:31.896
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:31.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:31.911
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 02/27/23 15:55:31.914
STEP: When the matched label of one of its pods change 02/27/23 15:55:31.921
Feb 27 15:55:31.924: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 27 15:55:36.927: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/27/23 15:55:36.937
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:37.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-758" for this suite. 02/27/23 15:55:37.946
------------------------------
• [SLOW TEST] [6.058 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:31.895
    Feb 27 15:55:31.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replication-controller 02/27/23 15:55:31.896
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:31.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:31.911
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 02/27/23 15:55:31.914
    STEP: When the matched label of one of its pods change 02/27/23 15:55:31.921
    Feb 27 15:55:31.924: INFO: Pod name pod-release: Found 0 pods out of 1
    Feb 27 15:55:36.927: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/27/23 15:55:36.937
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:37.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-758" for this suite. 02/27/23 15:55:37.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:37.954
Feb 27 15:55:37.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:55:37.954
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:37.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:37.975
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:55:37.979
Feb 27 15:55:37.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2" in namespace "projected-145" to be "Succeeded or Failed"
Feb 27 15:55:38.004: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925363ms
Feb 27 15:55:40.007: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009300713s
Feb 27 15:55:42.007: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009287347s
STEP: Saw pod success 02/27/23 15:55:42.007
Feb 27 15:55:42.008: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2" satisfied condition "Succeeded or Failed"
Feb 27 15:55:42.010: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2 container client-container: <nil>
STEP: delete the pod 02/27/23 15:55:42.015
Feb 27 15:55:42.028: INFO: Waiting for pod downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2 to disappear
Feb 27 15:55:42.030: INFO: Pod downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:42.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-145" for this suite. 02/27/23 15:55:42.033
------------------------------
• [4.085 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:37.954
    Feb 27 15:55:37.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:55:37.954
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:37.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:37.975
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:55:37.979
    Feb 27 15:55:37.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2" in namespace "projected-145" to be "Succeeded or Failed"
    Feb 27 15:55:38.004: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925363ms
    Feb 27 15:55:40.007: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009300713s
    Feb 27 15:55:42.007: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009287347s
    STEP: Saw pod success 02/27/23 15:55:42.007
    Feb 27 15:55:42.008: INFO: Pod "downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2" satisfied condition "Succeeded or Failed"
    Feb 27 15:55:42.010: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2 container client-container: <nil>
    STEP: delete the pod 02/27/23 15:55:42.015
    Feb 27 15:55:42.028: INFO: Waiting for pod downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2 to disappear
    Feb 27 15:55:42.030: INFO: Pod downwardapi-volume-6dda2f6d-c0c5-4b68-945c-49ba35bdc8a2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:42.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-145" for this suite. 02/27/23 15:55:42.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:42.04
Feb 27 15:55:42.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:55:42.04
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:42.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:42.052
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-a4ae5ea6-2dce-4d1f-976b-ca686cde0826 02/27/23 15:55:42.056
STEP: Creating a pod to test consume secrets 02/27/23 15:55:42.059
Feb 27 15:55:42.066: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87" in namespace "projected-5286" to be "Succeeded or Failed"
Feb 27 15:55:42.146: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87": Phase="Pending", Reason="", readiness=false. Elapsed: 80.135903ms
Feb 27 15:55:44.150: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083676559s
Feb 27 15:55:46.150: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083871557s
STEP: Saw pod success 02/27/23 15:55:46.15
Feb 27 15:55:46.150: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87" satisfied condition "Succeeded or Failed"
Feb 27 15:55:46.153: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:55:46.158
Feb 27 15:55:46.174: INFO: Waiting for pod pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87 to disappear
Feb 27 15:55:46.177: INFO: Pod pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:46.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5286" for this suite. 02/27/23 15:55:46.18
------------------------------
• [4.148 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:42.04
    Feb 27 15:55:42.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:55:42.04
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:42.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:42.052
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-a4ae5ea6-2dce-4d1f-976b-ca686cde0826 02/27/23 15:55:42.056
    STEP: Creating a pod to test consume secrets 02/27/23 15:55:42.059
    Feb 27 15:55:42.066: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87" in namespace "projected-5286" to be "Succeeded or Failed"
    Feb 27 15:55:42.146: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87": Phase="Pending", Reason="", readiness=false. Elapsed: 80.135903ms
    Feb 27 15:55:44.150: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083676559s
    Feb 27 15:55:46.150: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083871557s
    STEP: Saw pod success 02/27/23 15:55:46.15
    Feb 27 15:55:46.150: INFO: Pod "pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87" satisfied condition "Succeeded or Failed"
    Feb 27 15:55:46.153: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:55:46.158
    Feb 27 15:55:46.174: INFO: Waiting for pod pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87 to disappear
    Feb 27 15:55:46.177: INFO: Pod pod-projected-secrets-80c362d8-c4bb-4afe-b30a-7198e2df7f87 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:46.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5286" for this suite. 02/27/23 15:55:46.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:46.189
Feb 27 15:55:46.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 15:55:46.19
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:46.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:46.209
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1766/configmap-test-fe1d48a1-f41d-4549-a4c3-a4893a6db5f1 02/27/23 15:55:46.212
STEP: Creating a pod to test consume configMaps 02/27/23 15:55:46.216
Feb 27 15:55:46.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688" in namespace "configmap-1766" to be "Succeeded or Failed"
Feb 27 15:55:46.229: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688": Phase="Pending", Reason="", readiness=false. Elapsed: 3.143838ms
Feb 27 15:55:48.233: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007003709s
Feb 27 15:55:50.232: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006246153s
STEP: Saw pod success 02/27/23 15:55:50.232
Feb 27 15:55:50.232: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688" satisfied condition "Succeeded or Failed"
Feb 27 15:55:50.235: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688 container env-test: <nil>
STEP: delete the pod 02/27/23 15:55:50.24
Feb 27 15:55:50.252: INFO: Waiting for pod pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688 to disappear
Feb 27 15:55:50.254: INFO: Pod pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:55:50.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1766" for this suite. 02/27/23 15:55:50.257
------------------------------
• [4.073 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:46.189
    Feb 27 15:55:46.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 15:55:46.19
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:46.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:46.209
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1766/configmap-test-fe1d48a1-f41d-4549-a4c3-a4893a6db5f1 02/27/23 15:55:46.212
    STEP: Creating a pod to test consume configMaps 02/27/23 15:55:46.216
    Feb 27 15:55:46.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688" in namespace "configmap-1766" to be "Succeeded or Failed"
    Feb 27 15:55:46.229: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688": Phase="Pending", Reason="", readiness=false. Elapsed: 3.143838ms
    Feb 27 15:55:48.233: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007003709s
    Feb 27 15:55:50.232: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006246153s
    STEP: Saw pod success 02/27/23 15:55:50.232
    Feb 27 15:55:50.232: INFO: Pod "pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688" satisfied condition "Succeeded or Failed"
    Feb 27 15:55:50.235: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688 container env-test: <nil>
    STEP: delete the pod 02/27/23 15:55:50.24
    Feb 27 15:55:50.252: INFO: Waiting for pod pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688 to disappear
    Feb 27 15:55:50.254: INFO: Pod pod-configmaps-258b80e8-d56c-469e-88a6-c599e8794688 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:55:50.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1766" for this suite. 02/27/23 15:55:50.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:55:50.262
Feb 27 15:55:50.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 15:55:50.263
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:50.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:50.278
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4703 02/27/23 15:55:50.28
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 02/27/23 15:55:50.284
STEP: Creating stateful set ss in namespace statefulset-4703 02/27/23 15:55:50.291
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4703 02/27/23 15:55:50.298
Feb 27 15:55:50.302: INFO: Found 0 stateful pods, waiting for 1
Feb 27 15:56:00.306: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/27/23 15:56:00.306
Feb 27 15:56:00.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 15:56:00.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 15:56:00.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 15:56:00.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 15:56:00.442: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 27 15:56:10.446: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 15:56:10.446: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 15:56:10.468: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999843s
Feb 27 15:56:11.472: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987750248s
Feb 27 15:56:12.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984878961s
Feb 27 15:56:13.479: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981216755s
Feb 27 15:56:14.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977963922s
Feb 27 15:56:15.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974942389s
Feb 27 15:56:16.489: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971251113s
Feb 27 15:56:17.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967743753s
Feb 27 15:56:18.497: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963642773s
Feb 27 15:56:19.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.253381ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4703 02/27/23 15:56:20.5
Feb 27 15:56:20.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 15:56:20.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 15:56:20.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 15:56:20.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 15:56:20.624: INFO: Found 1 stateful pods, waiting for 3
Feb 27 15:56:30.628: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 15:56:30.628: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 15:56:30.628: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/27/23 15:56:30.628
STEP: Scale down will halt with unhealthy stateful pod 02/27/23 15:56:30.628
Feb 27 15:56:30.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 15:56:30.753: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 15:56:30.753: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 15:56:30.753: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 15:56:30.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 15:56:30.847: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 15:56:30.847: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 15:56:30.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 15:56:30.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 15:56:30.969: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 15:56:30.969: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 15:56:30.969: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 15:56:30.969: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 15:56:30.972: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 27 15:56:40.978: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 15:56:40.978: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 15:56:40.978: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 15:56:40.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999838s
Feb 27 15:56:42.000: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993616484s
Feb 27 15:56:43.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989651653s
Feb 27 15:56:44.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986909491s
Feb 27 15:56:45.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983143604s
Feb 27 15:56:46.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979716764s
Feb 27 15:56:47.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975189047s
Feb 27 15:56:48.023: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971416959s
Feb 27 15:56:49.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967470247s
Feb 27 15:56:50.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.768991ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4703 02/27/23 15:56:51.031
Feb 27 15:56:51.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 15:56:51.162: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 15:56:51.162: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 15:56:51.162: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 15:56:51.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 15:56:51.289: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 15:56:51.289: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 15:56:51.289: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 15:56:51.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 15:56:51.419: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 15:56:51.419: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 15:56:51.419: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 15:56:51.419: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/27/23 15:57:01.432
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 15:57:01.432: INFO: Deleting all statefulset in ns statefulset-4703
Feb 27 15:57:01.435: INFO: Scaling statefulset ss to 0
Feb 27 15:57:01.443: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 15:57:01.445: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:01.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4703" for this suite. 02/27/23 15:57:01.468
------------------------------
• [SLOW TEST] [71.211 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:55:50.262
    Feb 27 15:55:50.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 15:55:50.263
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:55:50.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:55:50.278
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4703 02/27/23 15:55:50.28
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/27/23 15:55:50.284
    STEP: Creating stateful set ss in namespace statefulset-4703 02/27/23 15:55:50.291
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4703 02/27/23 15:55:50.298
    Feb 27 15:55:50.302: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 15:56:00.306: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/27/23 15:56:00.306
    Feb 27 15:56:00.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 15:56:00.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 15:56:00.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 15:56:00.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 15:56:00.442: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 27 15:56:10.446: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 15:56:10.446: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 15:56:10.468: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999843s
    Feb 27 15:56:11.472: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987750248s
    Feb 27 15:56:12.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984878961s
    Feb 27 15:56:13.479: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981216755s
    Feb 27 15:56:14.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977963922s
    Feb 27 15:56:15.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974942389s
    Feb 27 15:56:16.489: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971251113s
    Feb 27 15:56:17.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967743753s
    Feb 27 15:56:18.497: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963642773s
    Feb 27 15:56:19.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.253381ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4703 02/27/23 15:56:20.5
    Feb 27 15:56:20.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 15:56:20.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 15:56:20.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 15:56:20.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 15:56:20.624: INFO: Found 1 stateful pods, waiting for 3
    Feb 27 15:56:30.628: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 15:56:30.628: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 15:56:30.628: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/27/23 15:56:30.628
    STEP: Scale down will halt with unhealthy stateful pod 02/27/23 15:56:30.628
    Feb 27 15:56:30.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 15:56:30.753: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 15:56:30.753: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 15:56:30.753: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 15:56:30.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 15:56:30.847: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 15:56:30.847: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 15:56:30.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 15:56:30.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 15:56:30.969: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 15:56:30.969: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 15:56:30.969: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 15:56:30.969: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 15:56:30.972: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 27 15:56:40.978: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 15:56:40.978: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 15:56:40.978: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 15:56:40.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999838s
    Feb 27 15:56:42.000: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993616484s
    Feb 27 15:56:43.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989651653s
    Feb 27 15:56:44.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986909491s
    Feb 27 15:56:45.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983143604s
    Feb 27 15:56:46.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979716764s
    Feb 27 15:56:47.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975189047s
    Feb 27 15:56:48.023: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971416959s
    Feb 27 15:56:49.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967470247s
    Feb 27 15:56:50.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.768991ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4703 02/27/23 15:56:51.031
    Feb 27 15:56:51.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 15:56:51.162: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 15:56:51.162: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 15:56:51.162: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 15:56:51.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 15:56:51.289: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 15:56:51.289: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 15:56:51.289: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 15:56:51.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-4703 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 15:56:51.419: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 15:56:51.419: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 15:56:51.419: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 15:56:51.419: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/27/23 15:57:01.432
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 15:57:01.432: INFO: Deleting all statefulset in ns statefulset-4703
    Feb 27 15:57:01.435: INFO: Scaling statefulset ss to 0
    Feb 27 15:57:01.443: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 15:57:01.445: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:01.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4703" for this suite. 02/27/23 15:57:01.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:01.474
Feb 27 15:57:01.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 15:57:01.475
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:01.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:01.489
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Feb 27 15:57:01.502: INFO: Waiting up to 5m0s for pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476" in namespace "pods-4917" to be "running and ready"
Feb 27 15:57:01.506: INFO: Pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162205ms
Feb 27 15:57:01.506: INFO: The phase of Pod server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:57:03.510: INFO: Pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476": Phase="Running", Reason="", readiness=true. Elapsed: 2.007769253s
Feb 27 15:57:03.510: INFO: The phase of Pod server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476 is Running (Ready = true)
Feb 27 15:57:03.510: INFO: Pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476" satisfied condition "running and ready"
Feb 27 15:57:03.527: INFO: Waiting up to 5m0s for pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408" in namespace "pods-4917" to be "Succeeded or Failed"
Feb 27 15:57:03.539: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083346ms
Feb 27 15:57:05.545: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017606108s
Feb 27 15:57:07.543: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015853568s
STEP: Saw pod success 02/27/23 15:57:07.543
Feb 27 15:57:07.543: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408" satisfied condition "Succeeded or Failed"
Feb 27 15:57:07.546: INFO: Trying to get logs from node ip-172-31-42-40 pod client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408 container env3cont: <nil>
STEP: delete the pod 02/27/23 15:57:07.556
Feb 27 15:57:07.567: INFO: Waiting for pod client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408 to disappear
Feb 27 15:57:07.570: INFO: Pod client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:07.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4917" for this suite. 02/27/23 15:57:07.572
------------------------------
• [SLOW TEST] [6.104 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:01.474
    Feb 27 15:57:01.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 15:57:01.475
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:01.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:01.489
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Feb 27 15:57:01.502: INFO: Waiting up to 5m0s for pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476" in namespace "pods-4917" to be "running and ready"
    Feb 27 15:57:01.506: INFO: Pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476": Phase="Pending", Reason="", readiness=false. Elapsed: 3.162205ms
    Feb 27 15:57:01.506: INFO: The phase of Pod server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:57:03.510: INFO: Pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476": Phase="Running", Reason="", readiness=true. Elapsed: 2.007769253s
    Feb 27 15:57:03.510: INFO: The phase of Pod server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476 is Running (Ready = true)
    Feb 27 15:57:03.510: INFO: Pod "server-envvars-510c827d-7f20-458c-8464-ae6de9aa7476" satisfied condition "running and ready"
    Feb 27 15:57:03.527: INFO: Waiting up to 5m0s for pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408" in namespace "pods-4917" to be "Succeeded or Failed"
    Feb 27 15:57:03.539: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083346ms
    Feb 27 15:57:05.545: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017606108s
    Feb 27 15:57:07.543: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015853568s
    STEP: Saw pod success 02/27/23 15:57:07.543
    Feb 27 15:57:07.543: INFO: Pod "client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408" satisfied condition "Succeeded or Failed"
    Feb 27 15:57:07.546: INFO: Trying to get logs from node ip-172-31-42-40 pod client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408 container env3cont: <nil>
    STEP: delete the pod 02/27/23 15:57:07.556
    Feb 27 15:57:07.567: INFO: Waiting for pod client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408 to disappear
    Feb 27 15:57:07.570: INFO: Pod client-envvars-73fa0d71-5ea0-4c4a-a08f-dbf573a80408 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:07.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4917" for this suite. 02/27/23 15:57:07.572
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:07.579
Feb 27 15:57:07.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:57:07.579
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:07.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:07.594
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:57:07.596
Feb 27 15:57:07.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c" in namespace "downward-api-510" to be "Succeeded or Failed"
Feb 27 15:57:07.605: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70979ms
Feb 27 15:57:09.609: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007669976s
Feb 27 15:57:11.611: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009493326s
STEP: Saw pod success 02/27/23 15:57:11.611
Feb 27 15:57:11.611: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c" satisfied condition "Succeeded or Failed"
Feb 27 15:57:11.614: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c container client-container: <nil>
STEP: delete the pod 02/27/23 15:57:11.619
Feb 27 15:57:11.632: INFO: Waiting for pod downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c to disappear
Feb 27 15:57:11.635: INFO: Pod downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:11.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-510" for this suite. 02/27/23 15:57:11.64
------------------------------
• [4.071 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:07.579
    Feb 27 15:57:07.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:57:07.579
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:07.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:07.594
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:57:07.596
    Feb 27 15:57:07.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c" in namespace "downward-api-510" to be "Succeeded or Failed"
    Feb 27 15:57:07.605: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70979ms
    Feb 27 15:57:09.609: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007669976s
    Feb 27 15:57:11.611: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009493326s
    STEP: Saw pod success 02/27/23 15:57:11.611
    Feb 27 15:57:11.611: INFO: Pod "downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c" satisfied condition "Succeeded or Failed"
    Feb 27 15:57:11.614: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c container client-container: <nil>
    STEP: delete the pod 02/27/23 15:57:11.619
    Feb 27 15:57:11.632: INFO: Waiting for pod downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c to disappear
    Feb 27 15:57:11.635: INFO: Pod downwardapi-volume-51f548f8-b84e-4630-bda6-faa0eafb720c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:11.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-510" for this suite. 02/27/23 15:57:11.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:11.651
Feb 27 15:57:11.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 15:57:11.651
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:11.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:11.668
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 15:57:11.678
Feb 27 15:57:11.686: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6319" to be "running and ready"
Feb 27 15:57:11.688: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223294ms
Feb 27 15:57:11.688: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:57:13.693: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006424472s
Feb 27 15:57:13.693: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 15:57:13.693: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 02/27/23 15:57:13.695
Feb 27 15:57:13.700: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6319" to be "running and ready"
Feb 27 15:57:13.707: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.524526ms
Feb 27 15:57:13.708: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:57:15.711: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011529545s
Feb 27 15:57:15.712: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb 27 15:57:15.712: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/27/23 15:57:15.714
Feb 27 15:57:15.723: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 15:57:15.725: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 27 15:57:17.725: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 15:57:17.729: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 27 15:57:19.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 15:57:19.730: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/27/23 15:57:19.73
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:19.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6319" for this suite. 02/27/23 15:57:19.739
------------------------------
• [SLOW TEST] [8.094 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:11.651
    Feb 27 15:57:11.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 15:57:11.651
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:11.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:11.668
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 15:57:11.678
    Feb 27 15:57:11.686: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6319" to be "running and ready"
    Feb 27 15:57:11.688: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223294ms
    Feb 27 15:57:11.688: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:57:13.693: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006424472s
    Feb 27 15:57:13.693: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 15:57:13.693: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 02/27/23 15:57:13.695
    Feb 27 15:57:13.700: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6319" to be "running and ready"
    Feb 27 15:57:13.707: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.524526ms
    Feb 27 15:57:13.708: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:57:15.711: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011529545s
    Feb 27 15:57:15.712: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb 27 15:57:15.712: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/27/23 15:57:15.714
    Feb 27 15:57:15.723: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 15:57:15.725: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 27 15:57:17.725: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 15:57:17.729: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 27 15:57:19.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 15:57:19.730: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/27/23 15:57:19.73
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:19.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6319" for this suite. 02/27/23 15:57:19.739
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:19.745
Feb 27 15:57:19.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 15:57:19.746
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:19.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:19.761
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 02/27/23 15:57:19.763
Feb 27 15:57:19.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc" in namespace "downward-api-994" to be "Succeeded or Failed"
Feb 27 15:57:19.775: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351085ms
Feb 27 15:57:21.780: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00772517s
Feb 27 15:57:23.780: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00783503s
STEP: Saw pod success 02/27/23 15:57:23.78
Feb 27 15:57:23.780: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc" satisfied condition "Succeeded or Failed"
Feb 27 15:57:23.783: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc container client-container: <nil>
STEP: delete the pod 02/27/23 15:57:23.788
Feb 27 15:57:23.798: INFO: Waiting for pod downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc to disappear
Feb 27 15:57:23.801: INFO: Pod downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:23.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-994" for this suite. 02/27/23 15:57:23.804
------------------------------
• [4.068 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:19.745
    Feb 27 15:57:19.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 15:57:19.746
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:19.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:19.761
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 02/27/23 15:57:19.763
    Feb 27 15:57:19.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc" in namespace "downward-api-994" to be "Succeeded or Failed"
    Feb 27 15:57:19.775: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351085ms
    Feb 27 15:57:21.780: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00772517s
    Feb 27 15:57:23.780: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00783503s
    STEP: Saw pod success 02/27/23 15:57:23.78
    Feb 27 15:57:23.780: INFO: Pod "downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc" satisfied condition "Succeeded or Failed"
    Feb 27 15:57:23.783: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc container client-container: <nil>
    STEP: delete the pod 02/27/23 15:57:23.788
    Feb 27 15:57:23.798: INFO: Waiting for pod downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc to disappear
    Feb 27 15:57:23.801: INFO: Pod downwardapi-volume-d28192cc-0165-434d-8576-ecfa8f152bbc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:23.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-994" for this suite. 02/27/23 15:57:23.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:23.814
Feb 27 15:57:23.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:57:23.815
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:23.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:23.831
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8711 02/27/23 15:57:23.833
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 15:57:23.846
STEP: creating service externalsvc in namespace services-8711 02/27/23 15:57:23.846
STEP: creating replication controller externalsvc in namespace services-8711 02/27/23 15:57:23.861
I0227 15:57:23.866840      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8711, replica count: 2
I0227 15:57:26.917418      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/27/23 15:57:26.92
Feb 27 15:57:26.936: INFO: Creating new exec pod
Feb 27 15:57:26.944: INFO: Waiting up to 5m0s for pod "execpodl48d9" in namespace "services-8711" to be "running"
Feb 27 15:57:26.950: INFO: Pod "execpodl48d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962394ms
Feb 27 15:57:28.952: INFO: Pod "execpodl48d9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007625648s
Feb 27 15:57:28.952: INFO: Pod "execpodl48d9" satisfied condition "running"
Feb 27 15:57:28.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8711 exec execpodl48d9 -- /bin/sh -x -c nslookup nodeport-service.services-8711.svc.cluster.local'
Feb 27 15:57:29.095: INFO: stderr: "+ nslookup nodeport-service.services-8711.svc.cluster.local\n"
Feb 27 15:57:29.095: INFO: stdout: "Server:\t\t10.152.183.116\nAddress:\t10.152.183.116#53\n\nnodeport-service.services-8711.svc.cluster.local\tcanonical name = externalsvc.services-8711.svc.cluster.local.\nName:\texternalsvc.services-8711.svc.cluster.local\nAddress: 10.152.183.119\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8711, will wait for the garbage collector to delete the pods 02/27/23 15:57:29.095
Feb 27 15:57:29.155: INFO: Deleting ReplicationController externalsvc took: 6.09772ms
Feb 27 15:57:29.255: INFO: Terminating ReplicationController externalsvc pods took: 100.350673ms
Feb 27 15:57:31.274: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:31.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8711" for this suite. 02/27/23 15:57:31.291
------------------------------
• [SLOW TEST] [7.482 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:23.814
    Feb 27 15:57:23.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:57:23.815
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:23.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:23.831
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8711 02/27/23 15:57:23.833
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 15:57:23.846
    STEP: creating service externalsvc in namespace services-8711 02/27/23 15:57:23.846
    STEP: creating replication controller externalsvc in namespace services-8711 02/27/23 15:57:23.861
    I0227 15:57:23.866840      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8711, replica count: 2
    I0227 15:57:26.917418      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/27/23 15:57:26.92
    Feb 27 15:57:26.936: INFO: Creating new exec pod
    Feb 27 15:57:26.944: INFO: Waiting up to 5m0s for pod "execpodl48d9" in namespace "services-8711" to be "running"
    Feb 27 15:57:26.950: INFO: Pod "execpodl48d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962394ms
    Feb 27 15:57:28.952: INFO: Pod "execpodl48d9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007625648s
    Feb 27 15:57:28.952: INFO: Pod "execpodl48d9" satisfied condition "running"
    Feb 27 15:57:28.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8711 exec execpodl48d9 -- /bin/sh -x -c nslookup nodeport-service.services-8711.svc.cluster.local'
    Feb 27 15:57:29.095: INFO: stderr: "+ nslookup nodeport-service.services-8711.svc.cluster.local\n"
    Feb 27 15:57:29.095: INFO: stdout: "Server:\t\t10.152.183.116\nAddress:\t10.152.183.116#53\n\nnodeport-service.services-8711.svc.cluster.local\tcanonical name = externalsvc.services-8711.svc.cluster.local.\nName:\texternalsvc.services-8711.svc.cluster.local\nAddress: 10.152.183.119\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8711, will wait for the garbage collector to delete the pods 02/27/23 15:57:29.095
    Feb 27 15:57:29.155: INFO: Deleting ReplicationController externalsvc took: 6.09772ms
    Feb 27 15:57:29.255: INFO: Terminating ReplicationController externalsvc pods took: 100.350673ms
    Feb 27 15:57:31.274: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:31.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8711" for this suite. 02/27/23 15:57:31.291
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:31.297
Feb 27 15:57:31.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:57:31.297
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:31.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:31.362
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-bf149b12-f2ba-4093-81d7-22517e9c89c4 02/27/23 15:57:31.364
STEP: Creating a pod to test consume secrets 02/27/23 15:57:31.368
Feb 27 15:57:31.374: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80" in namespace "projected-5047" to be "Succeeded or Failed"
Feb 27 15:57:31.377: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886975ms
Feb 27 15:57:33.381: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80": Phase="Running", Reason="", readiness=false. Elapsed: 2.006856929s
Feb 27 15:57:35.380: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006171698s
STEP: Saw pod success 02/27/23 15:57:35.38
Feb 27 15:57:35.381: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80" satisfied condition "Succeeded or Failed"
Feb 27 15:57:35.383: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:57:35.389
Feb 27 15:57:35.400: INFO: Waiting for pod pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80 to disappear
Feb 27 15:57:35.403: INFO: Pod pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:35.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5047" for this suite. 02/27/23 15:57:35.406
------------------------------
• [4.114 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:31.297
    Feb 27 15:57:31.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:57:31.297
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:31.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:31.362
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-bf149b12-f2ba-4093-81d7-22517e9c89c4 02/27/23 15:57:31.364
    STEP: Creating a pod to test consume secrets 02/27/23 15:57:31.368
    Feb 27 15:57:31.374: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80" in namespace "projected-5047" to be "Succeeded or Failed"
    Feb 27 15:57:31.377: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886975ms
    Feb 27 15:57:33.381: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80": Phase="Running", Reason="", readiness=false. Elapsed: 2.006856929s
    Feb 27 15:57:35.380: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006171698s
    STEP: Saw pod success 02/27/23 15:57:35.38
    Feb 27 15:57:35.381: INFO: Pod "pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80" satisfied condition "Succeeded or Failed"
    Feb 27 15:57:35.383: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:57:35.389
    Feb 27 15:57:35.400: INFO: Waiting for pod pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80 to disappear
    Feb 27 15:57:35.403: INFO: Pod pod-projected-secrets-4011abc2-e6ed-42bc-8373-d4a6d7097e80 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:35.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5047" for this suite. 02/27/23 15:57:35.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:35.411
Feb 27 15:57:35.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 15:57:35.412
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:35.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:35.431
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 02/27/23 15:57:35.433
Feb 27 15:57:35.440: INFO: Waiting up to 5m0s for pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0" in namespace "emptydir-5042" to be "Succeeded or Failed"
Feb 27 15:57:35.448: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.40179ms
Feb 27 15:57:37.451: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010805241s
Feb 27 15:57:39.452: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011794747s
STEP: Saw pod success 02/27/23 15:57:39.452
Feb 27 15:57:39.452: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0" satisfied condition "Succeeded or Failed"
Feb 27 15:57:39.455: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0 container test-container: <nil>
STEP: delete the pod 02/27/23 15:57:39.461
Feb 27 15:57:39.472: INFO: Waiting for pod pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0 to disappear
Feb 27 15:57:39.474: INFO: Pod pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:39.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5042" for this suite. 02/27/23 15:57:39.477
------------------------------
• [4.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:35.411
    Feb 27 15:57:35.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 15:57:35.412
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:35.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:35.431
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/27/23 15:57:35.433
    Feb 27 15:57:35.440: INFO: Waiting up to 5m0s for pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0" in namespace "emptydir-5042" to be "Succeeded or Failed"
    Feb 27 15:57:35.448: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.40179ms
    Feb 27 15:57:37.451: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010805241s
    Feb 27 15:57:39.452: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011794747s
    STEP: Saw pod success 02/27/23 15:57:39.452
    Feb 27 15:57:39.452: INFO: Pod "pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0" satisfied condition "Succeeded or Failed"
    Feb 27 15:57:39.455: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0 container test-container: <nil>
    STEP: delete the pod 02/27/23 15:57:39.461
    Feb 27 15:57:39.472: INFO: Waiting for pod pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0 to disappear
    Feb 27 15:57:39.474: INFO: Pod pod-ea7668a6-52b4-45bf-b78b-bc80be4c80e0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:39.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5042" for this suite. 02/27/23 15:57:39.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:39.485
Feb 27 15:57:39.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 15:57:39.485
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:39.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:39.499
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8029 02/27/23 15:57:39.501
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Feb 27 15:57:39.514: INFO: Found 0 stateful pods, waiting for 1
Feb 27 15:57:49.521: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/27/23 15:57:49.526
W0227 15:57:49.536105      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 15:57:49.541: INFO: Found 1 stateful pods, waiting for 2
Feb 27 15:57:59.550: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 15:57:59.550: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/27/23 15:57:59.555
STEP: Delete all of the StatefulSets 02/27/23 15:57:59.558
STEP: Verify that StatefulSets have been deleted 02/27/23 15:57:59.565
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 15:57:59.567: INFO: Deleting all statefulset in ns statefulset-8029
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:57:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8029" for this suite. 02/27/23 15:57:59.587
------------------------------
• [SLOW TEST] [20.110 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:39.485
    Feb 27 15:57:39.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 15:57:39.485
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:39.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:39.499
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8029 02/27/23 15:57:39.501
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Feb 27 15:57:39.514: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 15:57:49.521: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/27/23 15:57:49.526
    W0227 15:57:49.536105      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 15:57:49.541: INFO: Found 1 stateful pods, waiting for 2
    Feb 27 15:57:59.550: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 15:57:59.550: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/27/23 15:57:59.555
    STEP: Delete all of the StatefulSets 02/27/23 15:57:59.558
    STEP: Verify that StatefulSets have been deleted 02/27/23 15:57:59.565
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 15:57:59.567: INFO: Deleting all statefulset in ns statefulset-8029
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:57:59.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8029" for this suite. 02/27/23 15:57:59.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:57:59.597
Feb 27 15:57:59.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:57:59.597
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:59.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:59.611
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:57:59.628
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:58:00.042
STEP: Deploying the webhook pod 02/27/23 15:58:00.05
STEP: Wait for the deployment to be ready 02/27/23 15:58:00.061
Feb 27 15:58:00.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:58:02.083
STEP: Verifying the service has paired with the endpoint 02/27/23 15:58:02.098
Feb 27 15:58:03.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 02/27/23 15:58:03.102
Feb 27 15:58:03.118: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/27/23 15:58:03.228
STEP: Creating a configMap that should not be mutated 02/27/23 15:58:03.236
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/27/23 15:58:03.246
STEP: Creating a configMap that should be mutated 02/27/23 15:58:03.251
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:03.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6129" for this suite. 02/27/23 15:58:03.422
STEP: Destroying namespace "webhook-6129-markers" for this suite. 02/27/23 15:58:03.432
------------------------------
• [3.842 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:57:59.597
    Feb 27 15:57:59.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:57:59.597
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:57:59.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:57:59.611
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:57:59.628
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:58:00.042
    STEP: Deploying the webhook pod 02/27/23 15:58:00.05
    STEP: Wait for the deployment to be ready 02/27/23 15:58:00.061
    Feb 27 15:58:00.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:58:02.083
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:58:02.098
    Feb 27 15:58:03.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 02/27/23 15:58:03.102
    Feb 27 15:58:03.118: INFO: Waiting for webhook configuration to be ready...
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/27/23 15:58:03.228
    STEP: Creating a configMap that should not be mutated 02/27/23 15:58:03.236
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/27/23 15:58:03.246
    STEP: Creating a configMap that should be mutated 02/27/23 15:58:03.251
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:03.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6129" for this suite. 02/27/23 15:58:03.422
    STEP: Destroying namespace "webhook-6129-markers" for this suite. 02/27/23 15:58:03.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:03.44
Feb 27 15:58:03.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 15:58:03.44
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:03.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:03.457
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-4a2320be-9820-4279-8a95-e76eaddf2ee7 02/27/23 15:58:03.46
STEP: Creating a pod to test consume configMaps 02/27/23 15:58:03.464
Feb 27 15:58:03.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c" in namespace "configmap-7544" to be "Succeeded or Failed"
Feb 27 15:58:03.473: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033415ms
Feb 27 15:58:05.476: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c": Phase="Running", Reason="", readiness=false. Elapsed: 2.006853989s
Feb 27 15:58:07.476: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006395042s
STEP: Saw pod success 02/27/23 15:58:07.476
Feb 27 15:58:07.476: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c" satisfied condition "Succeeded or Failed"
Feb 27 15:58:07.479: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:58:07.484
Feb 27 15:58:07.498: INFO: Waiting for pod pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c to disappear
Feb 27 15:58:07.500: INFO: Pod pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:07.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7544" for this suite. 02/27/23 15:58:07.504
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:03.44
    Feb 27 15:58:03.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 15:58:03.44
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:03.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:03.457
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-4a2320be-9820-4279-8a95-e76eaddf2ee7 02/27/23 15:58:03.46
    STEP: Creating a pod to test consume configMaps 02/27/23 15:58:03.464
    Feb 27 15:58:03.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c" in namespace "configmap-7544" to be "Succeeded or Failed"
    Feb 27 15:58:03.473: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.033415ms
    Feb 27 15:58:05.476: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c": Phase="Running", Reason="", readiness=false. Elapsed: 2.006853989s
    Feb 27 15:58:07.476: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006395042s
    STEP: Saw pod success 02/27/23 15:58:07.476
    Feb 27 15:58:07.476: INFO: Pod "pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c" satisfied condition "Succeeded or Failed"
    Feb 27 15:58:07.479: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:58:07.484
    Feb 27 15:58:07.498: INFO: Waiting for pod pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c to disappear
    Feb 27 15:58:07.500: INFO: Pod pod-configmaps-7ef3ddc0-0ad0-48a6-afc4-0896d7d8d07c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:07.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7544" for this suite. 02/27/23 15:58:07.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:07.509
Feb 27 15:58:07.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:58:07.51
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:07.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:07.526
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-6ad026b6-8920-4b96-b93d-8350eba727b7 02/27/23 15:58:07.528
STEP: Creating a pod to test consume secrets 02/27/23 15:58:07.533
Feb 27 15:58:07.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4" in namespace "projected-8046" to be "Succeeded or Failed"
Feb 27 15:58:07.547: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845933ms
Feb 27 15:58:09.550: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4": Phase="Running", Reason="", readiness=false. Elapsed: 2.006991267s
Feb 27 15:58:11.551: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007712066s
STEP: Saw pod success 02/27/23 15:58:11.551
Feb 27 15:58:11.551: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4" satisfied condition "Succeeded or Failed"
Feb 27 15:58:11.554: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 15:58:11.559
Feb 27 15:58:11.571: INFO: Waiting for pod pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4 to disappear
Feb 27 15:58:11.574: INFO: Pod pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8046" for this suite. 02/27/23 15:58:11.577
------------------------------
• [4.073 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:07.509
    Feb 27 15:58:07.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:58:07.51
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:07.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:07.526
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-6ad026b6-8920-4b96-b93d-8350eba727b7 02/27/23 15:58:07.528
    STEP: Creating a pod to test consume secrets 02/27/23 15:58:07.533
    Feb 27 15:58:07.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4" in namespace "projected-8046" to be "Succeeded or Failed"
    Feb 27 15:58:07.547: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845933ms
    Feb 27 15:58:09.550: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4": Phase="Running", Reason="", readiness=false. Elapsed: 2.006991267s
    Feb 27 15:58:11.551: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007712066s
    STEP: Saw pod success 02/27/23 15:58:11.551
    Feb 27 15:58:11.551: INFO: Pod "pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4" satisfied condition "Succeeded or Failed"
    Feb 27 15:58:11.554: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 15:58:11.559
    Feb 27 15:58:11.571: INFO: Waiting for pod pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4 to disappear
    Feb 27 15:58:11.574: INFO: Pod pod-projected-secrets-68f361af-1f93-48c9-bcaf-b621eec7c5e4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8046" for this suite. 02/27/23 15:58:11.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:11.583
Feb 27 15:58:11.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 15:58:11.584
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:11.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:11.596
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb 27 15:58:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4487" for this suite. 02/27/23 15:58:12.143
------------------------------
• [0.565 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:11.583
    Feb 27 15:58:11.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 15:58:11.584
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:11.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:11.596
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb 27 15:58:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4487" for this suite. 02/27/23 15:58:12.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:12.149
Feb 27 15:58:12.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 15:58:12.15
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:12.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:12.215
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb 27 15:58:12.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:18.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7473" for this suite. 02/27/23 15:58:18.39
------------------------------
• [SLOW TEST] [6.251 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:12.149
    Feb 27 15:58:12.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 15:58:12.15
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:12.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:12.215
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb 27 15:58:12.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:18.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7473" for this suite. 02/27/23 15:58:18.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:18.401
Feb 27 15:58:18.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename podtemplate 02/27/23 15:58:18.402
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:18.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:18.417
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/27/23 15:58:18.42
Feb 27 15:58:18.427: INFO: created test-podtemplate-1
Feb 27 15:58:18.431: INFO: created test-podtemplate-2
Feb 27 15:58:18.435: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/27/23 15:58:18.435
STEP: delete collection of pod templates 02/27/23 15:58:18.438
Feb 27 15:58:18.438: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/27/23 15:58:18.452
Feb 27 15:58:18.452: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:18.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9507" for this suite. 02/27/23 15:58:18.458
------------------------------
• [0.063 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:18.401
    Feb 27 15:58:18.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename podtemplate 02/27/23 15:58:18.402
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:18.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:18.417
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/27/23 15:58:18.42
    Feb 27 15:58:18.427: INFO: created test-podtemplate-1
    Feb 27 15:58:18.431: INFO: created test-podtemplate-2
    Feb 27 15:58:18.435: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/27/23 15:58:18.435
    STEP: delete collection of pod templates 02/27/23 15:58:18.438
    Feb 27 15:58:18.438: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/27/23 15:58:18.452
    Feb 27 15:58:18.452: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:18.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9507" for this suite. 02/27/23 15:58:18.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:18.464
Feb 27 15:58:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:58:18.465
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:18.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:18.478
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Feb 27 15:58:18.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 15:58:19.971
Feb 27 15:58:19.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 create -f -'
Feb 27 15:58:20.525: INFO: stderr: ""
Feb 27 15:58:20.525: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 27 15:58:20.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 delete e2e-test-crd-publish-openapi-1654-crds test-cr'
Feb 27 15:58:20.599: INFO: stderr: ""
Feb 27 15:58:20.599: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 27 15:58:20.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 apply -f -'
Feb 27 15:58:20.738: INFO: stderr: ""
Feb 27 15:58:20.738: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 27 15:58:20.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 delete e2e-test-crd-publish-openapi-1654-crds test-cr'
Feb 27 15:58:20.789: INFO: stderr: ""
Feb 27 15:58:20.789: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/27/23 15:58:20.789
Feb 27 15:58:20.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 explain e2e-test-crd-publish-openapi-1654-crds'
Feb 27 15:58:21.233: INFO: stderr: ""
Feb 27 15:58:21.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1654-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:22.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2075" for this suite. 02/27/23 15:58:22.904
------------------------------
• [4.447 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:18.464
    Feb 27 15:58:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 15:58:18.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:18.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:18.478
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Feb 27 15:58:18.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 15:58:19.971
    Feb 27 15:58:19.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 create -f -'
    Feb 27 15:58:20.525: INFO: stderr: ""
    Feb 27 15:58:20.525: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 27 15:58:20.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 delete e2e-test-crd-publish-openapi-1654-crds test-cr'
    Feb 27 15:58:20.599: INFO: stderr: ""
    Feb 27 15:58:20.599: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb 27 15:58:20.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 apply -f -'
    Feb 27 15:58:20.738: INFO: stderr: ""
    Feb 27 15:58:20.738: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 27 15:58:20.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 --namespace=crd-publish-openapi-2075 delete e2e-test-crd-publish-openapi-1654-crds test-cr'
    Feb 27 15:58:20.789: INFO: stderr: ""
    Feb 27 15:58:20.789: INFO: stdout: "e2e-test-crd-publish-openapi-1654-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/27/23 15:58:20.789
    Feb 27 15:58:20.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-2075 explain e2e-test-crd-publish-openapi-1654-crds'
    Feb 27 15:58:21.233: INFO: stderr: ""
    Feb 27 15:58:21.233: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1654-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:22.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2075" for this suite. 02/27/23 15:58:22.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:22.913
Feb 27 15:58:22.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 15:58:22.914
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:22.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:22.938
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-9642 02/27/23 15:58:22.941
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[] 02/27/23 15:58:22.951
Feb 27 15:58:22.957: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 27 15:58:23.966: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9642 02/27/23 15:58:23.966
Feb 27 15:58:23.974: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9642" to be "running and ready"
Feb 27 15:58:23.981: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.412822ms
Feb 27 15:58:23.981: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:58:25.986: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01181144s
Feb 27 15:58:25.986: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 15:58:25.986: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[pod1:[100]] 02/27/23 15:58:25.99
Feb 27 15:58:26.000: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9642 02/27/23 15:58:26
Feb 27 15:58:26.007: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9642" to be "running and ready"
Feb 27 15:58:26.014: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.171781ms
Feb 27 15:58:26.014: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:58:28.017: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010036508s
Feb 27 15:58:28.018: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 15:58:28.018: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[pod1:[100] pod2:[101]] 02/27/23 15:58:28.021
Feb 27 15:58:28.038: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/27/23 15:58:28.038
Feb 27 15:58:28.038: INFO: Creating new exec pod
Feb 27 15:58:28.043: INFO: Waiting up to 5m0s for pod "execpodb882p" in namespace "services-9642" to be "running"
Feb 27 15:58:28.048: INFO: Pod "execpodb882p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095341ms
Feb 27 15:58:30.052: INFO: Pod "execpodb882p": Phase="Running", Reason="", readiness=true. Elapsed: 2.008468232s
Feb 27 15:58:30.052: INFO: Pod "execpodb882p" satisfied condition "running"
Feb 27 15:58:31.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Feb 27 15:58:31.184: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 27 15:58:31.184: INFO: stdout: ""
Feb 27 15:58:31.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 10.152.183.202 80'
Feb 27 15:58:31.283: INFO: stderr: "+ nc -v -z -w 2 10.152.183.202 80\nConnection to 10.152.183.202 80 port [tcp/http] succeeded!\n"
Feb 27 15:58:31.283: INFO: stdout: ""
Feb 27 15:58:31.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Feb 27 15:58:31.392: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 27 15:58:31.392: INFO: stdout: ""
Feb 27 15:58:31.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 10.152.183.202 81'
Feb 27 15:58:31.522: INFO: stderr: "+ nc -v -z -w 2 10.152.183.202 81\nConnection to 10.152.183.202 81 port [tcp/*] succeeded!\n"
Feb 27 15:58:31.522: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9642 02/27/23 15:58:31.522
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[pod2:[101]] 02/27/23 15:58:31.541
Feb 27 15:58:32.566: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9642 02/27/23 15:58:32.567
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[] 02/27/23 15:58:32.583
Feb 27 15:58:32.600: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:32.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9642" for this suite. 02/27/23 15:58:32.623
------------------------------
• [SLOW TEST] [9.719 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:22.913
    Feb 27 15:58:22.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 15:58:22.914
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:22.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:22.938
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-9642 02/27/23 15:58:22.941
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[] 02/27/23 15:58:22.951
    Feb 27 15:58:22.957: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Feb 27 15:58:23.966: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9642 02/27/23 15:58:23.966
    Feb 27 15:58:23.974: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9642" to be "running and ready"
    Feb 27 15:58:23.981: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.412822ms
    Feb 27 15:58:23.981: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:58:25.986: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01181144s
    Feb 27 15:58:25.986: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 15:58:25.986: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[pod1:[100]] 02/27/23 15:58:25.99
    Feb 27 15:58:26.000: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9642 02/27/23 15:58:26
    Feb 27 15:58:26.007: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9642" to be "running and ready"
    Feb 27 15:58:26.014: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.171781ms
    Feb 27 15:58:26.014: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:58:28.017: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010036508s
    Feb 27 15:58:28.018: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 15:58:28.018: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[pod1:[100] pod2:[101]] 02/27/23 15:58:28.021
    Feb 27 15:58:28.038: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/27/23 15:58:28.038
    Feb 27 15:58:28.038: INFO: Creating new exec pod
    Feb 27 15:58:28.043: INFO: Waiting up to 5m0s for pod "execpodb882p" in namespace "services-9642" to be "running"
    Feb 27 15:58:28.048: INFO: Pod "execpodb882p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095341ms
    Feb 27 15:58:30.052: INFO: Pod "execpodb882p": Phase="Running", Reason="", readiness=true. Elapsed: 2.008468232s
    Feb 27 15:58:30.052: INFO: Pod "execpodb882p" satisfied condition "running"
    Feb 27 15:58:31.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Feb 27 15:58:31.184: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb 27 15:58:31.184: INFO: stdout: ""
    Feb 27 15:58:31.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 10.152.183.202 80'
    Feb 27 15:58:31.283: INFO: stderr: "+ nc -v -z -w 2 10.152.183.202 80\nConnection to 10.152.183.202 80 port [tcp/http] succeeded!\n"
    Feb 27 15:58:31.283: INFO: stdout: ""
    Feb 27 15:58:31.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Feb 27 15:58:31.392: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb 27 15:58:31.392: INFO: stdout: ""
    Feb 27 15:58:31.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9642 exec execpodb882p -- /bin/sh -x -c nc -v -z -w 2 10.152.183.202 81'
    Feb 27 15:58:31.522: INFO: stderr: "+ nc -v -z -w 2 10.152.183.202 81\nConnection to 10.152.183.202 81 port [tcp/*] succeeded!\n"
    Feb 27 15:58:31.522: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9642 02/27/23 15:58:31.522
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[pod2:[101]] 02/27/23 15:58:31.541
    Feb 27 15:58:32.566: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9642 02/27/23 15:58:32.567
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9642 to expose endpoints map[] 02/27/23 15:58:32.583
    Feb 27 15:58:32.600: INFO: successfully validated that service multi-endpoint-test in namespace services-9642 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:32.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9642" for this suite. 02/27/23 15:58:32.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:32.633
Feb 27 15:58:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 15:58:32.633
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:32.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:32.65
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-d2c139ae-39e4-4cb5-a012-206ef615d603 02/27/23 15:58:32.654
STEP: Creating a pod to test consume configMaps 02/27/23 15:58:32.659
Feb 27 15:58:32.668: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679" in namespace "projected-3975" to be "Succeeded or Failed"
Feb 27 15:58:32.672: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002781ms
Feb 27 15:58:34.676: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008124829s
Feb 27 15:58:36.676: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008329448s
STEP: Saw pod success 02/27/23 15:58:36.676
Feb 27 15:58:36.676: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679" satisfied condition "Succeeded or Failed"
Feb 27 15:58:36.681: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 15:58:36.691
Feb 27 15:58:36.703: INFO: Waiting for pod pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679 to disappear
Feb 27 15:58:36.706: INFO: Pod pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:36.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3975" for this suite. 02/27/23 15:58:36.71
------------------------------
• [4.083 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:32.633
    Feb 27 15:58:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 15:58:32.633
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:32.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:32.65
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-d2c139ae-39e4-4cb5-a012-206ef615d603 02/27/23 15:58:32.654
    STEP: Creating a pod to test consume configMaps 02/27/23 15:58:32.659
    Feb 27 15:58:32.668: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679" in namespace "projected-3975" to be "Succeeded or Failed"
    Feb 27 15:58:32.672: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002781ms
    Feb 27 15:58:34.676: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008124829s
    Feb 27 15:58:36.676: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008329448s
    STEP: Saw pod success 02/27/23 15:58:36.676
    Feb 27 15:58:36.676: INFO: Pod "pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679" satisfied condition "Succeeded or Failed"
    Feb 27 15:58:36.681: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 15:58:36.691
    Feb 27 15:58:36.703: INFO: Waiting for pod pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679 to disappear
    Feb 27 15:58:36.706: INFO: Pod pod-projected-configmaps-0e05ea71-3fc0-4421-b273-ff3bfe6de679 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:36.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3975" for this suite. 02/27/23 15:58:36.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:36.717
Feb 27 15:58:36.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:58:36.718
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:36.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:36.736
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:58:36.754
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:58:36.967
STEP: Deploying the webhook pod 02/27/23 15:58:36.976
STEP: Wait for the deployment to be ready 02/27/23 15:58:36.987
Feb 27 15:58:36.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:58:39.006
STEP: Verifying the service has paired with the endpoint 02/27/23 15:58:39.018
Feb 27 15:58:40.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 15:58:40.022
STEP: create a pod that should be denied by the webhook 02/27/23 15:58:40.034
STEP: create a pod that causes the webhook to hang 02/27/23 15:58:40.043
STEP: create a configmap that should be denied by the webhook 02/27/23 15:58:50.051
STEP: create a configmap that should be admitted by the webhook 02/27/23 15:58:50.084
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 15:58:50.092
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 15:58:50.1
STEP: create a namespace that bypass the webhook 02/27/23 15:58:50.104
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/27/23 15:58:50.11
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:50.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5693" for this suite. 02/27/23 15:58:50.177
STEP: Destroying namespace "webhook-5693-markers" for this suite. 02/27/23 15:58:50.184
------------------------------
• [SLOW TEST] [13.476 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:36.717
    Feb 27 15:58:36.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:58:36.718
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:36.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:36.736
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:58:36.754
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:58:36.967
    STEP: Deploying the webhook pod 02/27/23 15:58:36.976
    STEP: Wait for the deployment to be ready 02/27/23 15:58:36.987
    Feb 27 15:58:36.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:58:39.006
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:58:39.018
    Feb 27 15:58:40.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 15:58:40.022
    STEP: create a pod that should be denied by the webhook 02/27/23 15:58:40.034
    STEP: create a pod that causes the webhook to hang 02/27/23 15:58:40.043
    STEP: create a configmap that should be denied by the webhook 02/27/23 15:58:50.051
    STEP: create a configmap that should be admitted by the webhook 02/27/23 15:58:50.084
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 15:58:50.092
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 15:58:50.1
    STEP: create a namespace that bypass the webhook 02/27/23 15:58:50.104
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/27/23 15:58:50.11
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:50.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5693" for this suite. 02/27/23 15:58:50.177
    STEP: Destroying namespace "webhook-5693-markers" for this suite. 02/27/23 15:58:50.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:50.194
Feb 27 15:58:50.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename endpointslice 02/27/23 15:58:50.195
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:50.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:50.215
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 02/27/23 15:58:50.218
STEP: getting /apis/discovery.k8s.io 02/27/23 15:58:50.221
STEP: getting /apis/discovery.k8s.iov1 02/27/23 15:58:50.222
STEP: creating 02/27/23 15:58:50.223
STEP: getting 02/27/23 15:58:50.236
STEP: listing 02/27/23 15:58:50.24
STEP: watching 02/27/23 15:58:50.243
Feb 27 15:58:50.243: INFO: starting watch
STEP: cluster-wide listing 02/27/23 15:58:50.244
STEP: cluster-wide watching 02/27/23 15:58:50.247
Feb 27 15:58:50.247: INFO: starting watch
STEP: patching 02/27/23 15:58:50.248
STEP: updating 02/27/23 15:58:50.253
Feb 27 15:58:50.261: INFO: waiting for watch events with expected annotations
Feb 27 15:58:50.261: INFO: saw patched and updated annotations
STEP: deleting 02/27/23 15:58:50.261
STEP: deleting a collection 02/27/23 15:58:50.274
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:50.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6539" for this suite. 02/27/23 15:58:50.293
------------------------------
• [0.105 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:50.194
    Feb 27 15:58:50.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename endpointslice 02/27/23 15:58:50.195
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:50.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:50.215
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 02/27/23 15:58:50.218
    STEP: getting /apis/discovery.k8s.io 02/27/23 15:58:50.221
    STEP: getting /apis/discovery.k8s.iov1 02/27/23 15:58:50.222
    STEP: creating 02/27/23 15:58:50.223
    STEP: getting 02/27/23 15:58:50.236
    STEP: listing 02/27/23 15:58:50.24
    STEP: watching 02/27/23 15:58:50.243
    Feb 27 15:58:50.243: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 15:58:50.244
    STEP: cluster-wide watching 02/27/23 15:58:50.247
    Feb 27 15:58:50.247: INFO: starting watch
    STEP: patching 02/27/23 15:58:50.248
    STEP: updating 02/27/23 15:58:50.253
    Feb 27 15:58:50.261: INFO: waiting for watch events with expected annotations
    Feb 27 15:58:50.261: INFO: saw patched and updated annotations
    STEP: deleting 02/27/23 15:58:50.261
    STEP: deleting a collection 02/27/23 15:58:50.274
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:50.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6539" for this suite. 02/27/23 15:58:50.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:50.3
Feb 27 15:58:50.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:58:50.3
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:50.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:50.321
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb 27 15:58:50.334: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45" in namespace "kubelet-test-6459" to be "running and ready"
Feb 27 15:58:50.336: INFO: Pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832283ms
Feb 27 15:58:50.336: INFO: The phase of Pod busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 15:58:52.341: INFO: Pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45": Phase="Running", Reason="", readiness=true. Elapsed: 2.007838576s
Feb 27 15:58:52.341: INFO: The phase of Pod busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45 is Running (Ready = true)
Feb 27 15:58:52.341: INFO: Pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:52.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6459" for this suite. 02/27/23 15:58:52.354
------------------------------
• [2.063 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:50.3
    Feb 27 15:58:50.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 15:58:50.3
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:50.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:50.321
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb 27 15:58:50.334: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45" in namespace "kubelet-test-6459" to be "running and ready"
    Feb 27 15:58:50.336: INFO: Pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832283ms
    Feb 27 15:58:50.336: INFO: The phase of Pod busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 15:58:52.341: INFO: Pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45": Phase="Running", Reason="", readiness=true. Elapsed: 2.007838576s
    Feb 27 15:58:52.341: INFO: The phase of Pod busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45 is Running (Ready = true)
    Feb 27 15:58:52.341: INFO: Pod "busybox-readonly-fsc4b569dc-aade-486d-850b-c8a510105d45" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:52.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6459" for this suite. 02/27/23 15:58:52.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:52.363
Feb 27 15:58:52.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 15:58:52.363
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:52.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:52.38
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 15:58:52.396
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:58:52.759
STEP: Deploying the webhook pod 02/27/23 15:58:52.764
STEP: Wait for the deployment to be ready 02/27/23 15:58:52.777
Feb 27 15:58:52.786: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 15:58:54.797
STEP: Verifying the service has paired with the endpoint 02/27/23 15:58:54.807
Feb 27 15:58:55.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 15:58:55.811
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 15:58:55.823
STEP: Creating a dummy validating-webhook-configuration object 02/27/23 15:58:55.834
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/27/23 15:58:55.843
STEP: Creating a dummy mutating-webhook-configuration object 02/27/23 15:58:55.849
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/27/23 15:58:55.856
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 15:58:55.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4919" for this suite. 02/27/23 15:58:55.914
STEP: Destroying namespace "webhook-4919-markers" for this suite. 02/27/23 15:58:55.921
------------------------------
• [3.566 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:52.363
    Feb 27 15:58:52.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 15:58:52.363
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:52.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:52.38
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 15:58:52.396
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 15:58:52.759
    STEP: Deploying the webhook pod 02/27/23 15:58:52.764
    STEP: Wait for the deployment to be ready 02/27/23 15:58:52.777
    Feb 27 15:58:52.786: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 15:58:54.797
    STEP: Verifying the service has paired with the endpoint 02/27/23 15:58:54.807
    Feb 27 15:58:55.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 15:58:55.811
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 15:58:55.823
    STEP: Creating a dummy validating-webhook-configuration object 02/27/23 15:58:55.834
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/27/23 15:58:55.843
    STEP: Creating a dummy mutating-webhook-configuration object 02/27/23 15:58:55.849
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/27/23 15:58:55.856
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:58:55.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4919" for this suite. 02/27/23 15:58:55.914
    STEP: Destroying namespace "webhook-4919-markers" for this suite. 02/27/23 15:58:55.921
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:58:55.929
Feb 27 15:58:55.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 15:58:55.93
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:55.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:55.959
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 02/27/23 15:58:55.962
STEP: Counting existing ResourceQuota 02/27/23 15:59:00.966
STEP: Creating a ResourceQuota 02/27/23 15:59:05.97
STEP: Ensuring resource quota status is calculated 02/27/23 15:59:05.976
STEP: Creating a Secret 02/27/23 15:59:07.98
STEP: Ensuring resource quota status captures secret creation 02/27/23 15:59:07.992
STEP: Deleting a secret 02/27/23 15:59:09.997
STEP: Ensuring resource quota status released usage 02/27/23 15:59:10.005
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 15:59:12.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7389" for this suite. 02/27/23 15:59:12.014
------------------------------
• [SLOW TEST] [16.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:58:55.929
    Feb 27 15:58:55.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 15:58:55.93
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:58:55.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:58:55.959
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 02/27/23 15:58:55.962
    STEP: Counting existing ResourceQuota 02/27/23 15:59:00.966
    STEP: Creating a ResourceQuota 02/27/23 15:59:05.97
    STEP: Ensuring resource quota status is calculated 02/27/23 15:59:05.976
    STEP: Creating a Secret 02/27/23 15:59:07.98
    STEP: Ensuring resource quota status captures secret creation 02/27/23 15:59:07.992
    STEP: Deleting a secret 02/27/23 15:59:09.997
    STEP: Ensuring resource quota status released usage 02/27/23 15:59:10.005
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:59:12.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7389" for this suite. 02/27/23 15:59:12.014
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:59:12.021
Feb 27 15:59:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 15:59:12.021
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:59:12.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:59:12.039
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 02/27/23 15:59:12.042
STEP: Getting a ResourceQuota 02/27/23 15:59:12.047
STEP: Listing all ResourceQuotas with LabelSelector 02/27/23 15:59:12.053
STEP: Patching the ResourceQuota 02/27/23 15:59:12.056
STEP: Deleting a Collection of ResourceQuotas 02/27/23 15:59:12.062
STEP: Verifying the deleted ResourceQuota 02/27/23 15:59:12.071
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 15:59:12.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7647" for this suite. 02/27/23 15:59:12.078
------------------------------
• [0.065 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:59:12.021
    Feb 27 15:59:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 15:59:12.021
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:59:12.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:59:12.039
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 02/27/23 15:59:12.042
    STEP: Getting a ResourceQuota 02/27/23 15:59:12.047
    STEP: Listing all ResourceQuotas with LabelSelector 02/27/23 15:59:12.053
    STEP: Patching the ResourceQuota 02/27/23 15:59:12.056
    STEP: Deleting a Collection of ResourceQuotas 02/27/23 15:59:12.062
    STEP: Verifying the deleted ResourceQuota 02/27/23 15:59:12.071
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 15:59:12.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7647" for this suite. 02/27/23 15:59:12.078
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 15:59:12.086
Feb 27 15:59:12.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 15:59:12.087
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:59:12.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:59:12.107
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/27/23 15:59:12.11
STEP: Creating RC which spawns configmap-volume pods 02/27/23 15:59:12.342
Feb 27 15:59:12.562: INFO: Pod name wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 15:59:12.562
Feb 27 15:59:12.562: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:12.569: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.031128ms
Feb 27 15:59:14.573: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011696696s
Feb 27 15:59:16.575: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013298748s
Feb 27 15:59:18.573: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011323362s
Feb 27 15:59:20.574: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011998903s
Feb 27 15:59:22.574: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01244487s
Feb 27 15:59:24.574: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011902868s
Feb 27 15:59:26.653: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Running", Reason="", readiness=true. Elapsed: 14.091106609s
Feb 27 15:59:26.653: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn" satisfied condition "running"
Feb 27 15:59:26.653: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-cb8nl" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:26.657: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-cb8nl": Phase="Running", Reason="", readiness=true. Elapsed: 3.587036ms
Feb 27 15:59:26.657: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-cb8nl" satisfied condition "running"
Feb 27 15:59:26.657: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-ntkdq" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:26.660: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-ntkdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.997173ms
Feb 27 15:59:26.660: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-ntkdq" satisfied condition "running"
Feb 27 15:59:26.660: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-q2dmc" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:26.663: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-q2dmc": Phase="Running", Reason="", readiness=true. Elapsed: 3.792806ms
Feb 27 15:59:26.663: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-q2dmc" satisfied condition "running"
Feb 27 15:59:26.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-r4n7s" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:26.667: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-r4n7s": Phase="Running", Reason="", readiness=true. Elapsed: 3.464187ms
Feb 27 15:59:26.667: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-r4n7s" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5 in namespace emptydir-wrapper-1176, will wait for the garbage collector to delete the pods 02/27/23 15:59:26.667
Feb 27 15:59:26.729: INFO: Deleting ReplicationController wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5 took: 8.110037ms
Feb 27 15:59:26.830: INFO: Terminating ReplicationController wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5 pods took: 100.68337ms
STEP: Creating RC which spawns configmap-volume pods 02/27/23 15:59:31.934
Feb 27 15:59:31.946: INFO: Pod name wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987: Found 0 pods out of 5
Feb 27 15:59:36.954: INFO: Pod name wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 15:59:36.954
Feb 27 15:59:36.954: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:36.958: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114718ms
Feb 27 15:59:38.963: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008963224s
Feb 27 15:59:40.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007910219s
Feb 27 15:59:42.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008357128s
Feb 27 15:59:44.961: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007427224s
Feb 27 15:59:46.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008098222s
Feb 27 15:59:48.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Running", Reason="", readiness=true. Elapsed: 12.007893013s
Feb 27 15:59:48.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6" satisfied condition "running"
Feb 27 15:59:48.962: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-9plwt" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:48.966: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-9plwt": Phase="Running", Reason="", readiness=true. Elapsed: 4.124943ms
Feb 27 15:59:48.966: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-9plwt" satisfied condition "running"
Feb 27 15:59:48.966: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-hxczc" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:48.970: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-hxczc": Phase="Running", Reason="", readiness=true. Elapsed: 3.437838ms
Feb 27 15:59:48.970: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-hxczc" satisfied condition "running"
Feb 27 15:59:48.970: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-n2k25" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:48.973: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-n2k25": Phase="Running", Reason="", readiness=true. Elapsed: 3.028213ms
Feb 27 15:59:48.973: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-n2k25" satisfied condition "running"
Feb 27 15:59:48.973: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-s8tl5" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:48.977: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-s8tl5": Phase="Running", Reason="", readiness=true. Elapsed: 4.740719ms
Feb 27 15:59:48.977: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-s8tl5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987 in namespace emptydir-wrapper-1176, will wait for the garbage collector to delete the pods 02/27/23 15:59:48.977
Feb 27 15:59:49.042: INFO: Deleting ReplicationController wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987 took: 6.385065ms
Feb 27 15:59:49.142: INFO: Terminating ReplicationController wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987 pods took: 100.301013ms
STEP: Creating RC which spawns configmap-volume pods 02/27/23 15:59:52.246
Feb 27 15:59:52.260: INFO: Pod name wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4: Found 0 pods out of 5
Feb 27 15:59:57.269: INFO: Pod name wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 15:59:57.269
Feb 27 15:59:57.269: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 15:59:57.272: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328807ms
Feb 27 15:59:59.278: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008761258s
Feb 27 16:00:01.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007746924s
Feb 27 16:00:03.276: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007178762s
Feb 27 16:00:05.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008250851s
Feb 27 16:00:07.278: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009079052s
Feb 27 16:00:09.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Running", Reason="", readiness=true. Elapsed: 12.008370888s
Feb 27 16:00:09.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s" satisfied condition "running"
Feb 27 16:00:09.277: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-lpwxf" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 16:00:09.281: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-lpwxf": Phase="Running", Reason="", readiness=true. Elapsed: 3.900373ms
Feb 27 16:00:09.281: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-lpwxf" satisfied condition "running"
Feb 27 16:00:09.281: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-m9t5x" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 16:00:09.285: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-m9t5x": Phase="Running", Reason="", readiness=true. Elapsed: 3.811629ms
Feb 27 16:00:09.285: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-m9t5x" satisfied condition "running"
Feb 27 16:00:09.285: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-wt8j7" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 16:00:09.290: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-wt8j7": Phase="Running", Reason="", readiness=true. Elapsed: 4.582023ms
Feb 27 16:00:09.290: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-wt8j7" satisfied condition "running"
Feb 27 16:00:09.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-x4rhp" in namespace "emptydir-wrapper-1176" to be "running"
Feb 27 16:00:09.295: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-x4rhp": Phase="Running", Reason="", readiness=true. Elapsed: 4.885051ms
Feb 27 16:00:09.295: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-x4rhp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4 in namespace emptydir-wrapper-1176, will wait for the garbage collector to delete the pods 02/27/23 16:00:09.295
Feb 27 16:00:09.357: INFO: Deleting ReplicationController wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4 took: 7.375587ms
Feb 27 16:00:09.458: INFO: Terminating ReplicationController wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4 pods took: 100.935915ms
STEP: Cleaning up the configMaps 02/27/23 16:00:12.858
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:13.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1176" for this suite. 02/27/23 16:00:13.2
------------------------------
• [SLOW TEST] [61.125 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 15:59:12.086
    Feb 27 15:59:12.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 15:59:12.087
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 15:59:12.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 15:59:12.107
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/27/23 15:59:12.11
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 15:59:12.342
    Feb 27 15:59:12.562: INFO: Pod name wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 15:59:12.562
    Feb 27 15:59:12.562: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:12.569: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.031128ms
    Feb 27 15:59:14.573: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011696696s
    Feb 27 15:59:16.575: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013298748s
    Feb 27 15:59:18.573: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011323362s
    Feb 27 15:59:20.574: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011998903s
    Feb 27 15:59:22.574: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01244487s
    Feb 27 15:59:24.574: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011902868s
    Feb 27 15:59:26.653: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn": Phase="Running", Reason="", readiness=true. Elapsed: 14.091106609s
    Feb 27 15:59:26.653: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-b8xsn" satisfied condition "running"
    Feb 27 15:59:26.653: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-cb8nl" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:26.657: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-cb8nl": Phase="Running", Reason="", readiness=true. Elapsed: 3.587036ms
    Feb 27 15:59:26.657: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-cb8nl" satisfied condition "running"
    Feb 27 15:59:26.657: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-ntkdq" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:26.660: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-ntkdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.997173ms
    Feb 27 15:59:26.660: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-ntkdq" satisfied condition "running"
    Feb 27 15:59:26.660: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-q2dmc" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:26.663: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-q2dmc": Phase="Running", Reason="", readiness=true. Elapsed: 3.792806ms
    Feb 27 15:59:26.663: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-q2dmc" satisfied condition "running"
    Feb 27 15:59:26.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-r4n7s" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:26.667: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-r4n7s": Phase="Running", Reason="", readiness=true. Elapsed: 3.464187ms
    Feb 27 15:59:26.667: INFO: Pod "wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5-r4n7s" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5 in namespace emptydir-wrapper-1176, will wait for the garbage collector to delete the pods 02/27/23 15:59:26.667
    Feb 27 15:59:26.729: INFO: Deleting ReplicationController wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5 took: 8.110037ms
    Feb 27 15:59:26.830: INFO: Terminating ReplicationController wrapped-volume-race-671434ad-0a69-4a7a-b3ef-394ff590a9d5 pods took: 100.68337ms
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 15:59:31.934
    Feb 27 15:59:31.946: INFO: Pod name wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987: Found 0 pods out of 5
    Feb 27 15:59:36.954: INFO: Pod name wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 15:59:36.954
    Feb 27 15:59:36.954: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:36.958: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114718ms
    Feb 27 15:59:38.963: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008963224s
    Feb 27 15:59:40.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007910219s
    Feb 27 15:59:42.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008357128s
    Feb 27 15:59:44.961: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007427224s
    Feb 27 15:59:46.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008098222s
    Feb 27 15:59:48.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6": Phase="Running", Reason="", readiness=true. Elapsed: 12.007893013s
    Feb 27 15:59:48.962: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-8fdc6" satisfied condition "running"
    Feb 27 15:59:48.962: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-9plwt" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:48.966: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-9plwt": Phase="Running", Reason="", readiness=true. Elapsed: 4.124943ms
    Feb 27 15:59:48.966: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-9plwt" satisfied condition "running"
    Feb 27 15:59:48.966: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-hxczc" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:48.970: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-hxczc": Phase="Running", Reason="", readiness=true. Elapsed: 3.437838ms
    Feb 27 15:59:48.970: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-hxczc" satisfied condition "running"
    Feb 27 15:59:48.970: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-n2k25" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:48.973: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-n2k25": Phase="Running", Reason="", readiness=true. Elapsed: 3.028213ms
    Feb 27 15:59:48.973: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-n2k25" satisfied condition "running"
    Feb 27 15:59:48.973: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-s8tl5" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:48.977: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-s8tl5": Phase="Running", Reason="", readiness=true. Elapsed: 4.740719ms
    Feb 27 15:59:48.977: INFO: Pod "wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987-s8tl5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987 in namespace emptydir-wrapper-1176, will wait for the garbage collector to delete the pods 02/27/23 15:59:48.977
    Feb 27 15:59:49.042: INFO: Deleting ReplicationController wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987 took: 6.385065ms
    Feb 27 15:59:49.142: INFO: Terminating ReplicationController wrapped-volume-race-86e7280d-b5d9-4a2d-b051-b740fe642987 pods took: 100.301013ms
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 15:59:52.246
    Feb 27 15:59:52.260: INFO: Pod name wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4: Found 0 pods out of 5
    Feb 27 15:59:57.269: INFO: Pod name wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 15:59:57.269
    Feb 27 15:59:57.269: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 15:59:57.272: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328807ms
    Feb 27 15:59:59.278: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008761258s
    Feb 27 16:00:01.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007746924s
    Feb 27 16:00:03.276: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007178762s
    Feb 27 16:00:05.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008250851s
    Feb 27 16:00:07.278: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009079052s
    Feb 27 16:00:09.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s": Phase="Running", Reason="", readiness=true. Elapsed: 12.008370888s
    Feb 27 16:00:09.277: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-h6w6s" satisfied condition "running"
    Feb 27 16:00:09.277: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-lpwxf" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 16:00:09.281: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-lpwxf": Phase="Running", Reason="", readiness=true. Elapsed: 3.900373ms
    Feb 27 16:00:09.281: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-lpwxf" satisfied condition "running"
    Feb 27 16:00:09.281: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-m9t5x" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 16:00:09.285: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-m9t5x": Phase="Running", Reason="", readiness=true. Elapsed: 3.811629ms
    Feb 27 16:00:09.285: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-m9t5x" satisfied condition "running"
    Feb 27 16:00:09.285: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-wt8j7" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 16:00:09.290: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-wt8j7": Phase="Running", Reason="", readiness=true. Elapsed: 4.582023ms
    Feb 27 16:00:09.290: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-wt8j7" satisfied condition "running"
    Feb 27 16:00:09.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-x4rhp" in namespace "emptydir-wrapper-1176" to be "running"
    Feb 27 16:00:09.295: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-x4rhp": Phase="Running", Reason="", readiness=true. Elapsed: 4.885051ms
    Feb 27 16:00:09.295: INFO: Pod "wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4-x4rhp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4 in namespace emptydir-wrapper-1176, will wait for the garbage collector to delete the pods 02/27/23 16:00:09.295
    Feb 27 16:00:09.357: INFO: Deleting ReplicationController wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4 took: 7.375587ms
    Feb 27 16:00:09.458: INFO: Terminating ReplicationController wrapped-volume-race-d5f514fe-cbb5-4b2c-865f-6cada26797e4 pods took: 100.935915ms
    STEP: Cleaning up the configMaps 02/27/23 16:00:12.858
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:13.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1176" for this suite. 02/27/23 16:00:13.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:13.212
Feb 27 16:00:13.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 16:00:13.212
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:13.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:13.284
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-9239 02/27/23 16:00:13.288
STEP: creating service affinity-nodeport in namespace services-9239 02/27/23 16:00:13.288
STEP: creating replication controller affinity-nodeport in namespace services-9239 02/27/23 16:00:13.303
I0227 16:00:13.315185      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9239, replica count: 3
I0227 16:00:16.365761      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 16:00:16.378: INFO: Creating new exec pod
Feb 27 16:00:16.386: INFO: Waiting up to 5m0s for pod "execpod-affinityqxmrr" in namespace "services-9239" to be "running"
Feb 27 16:00:16.391: INFO: Pod "execpod-affinityqxmrr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.964262ms
Feb 27 16:00:18.397: INFO: Pod "execpod-affinityqxmrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010504756s
Feb 27 16:00:18.397: INFO: Pod "execpod-affinityqxmrr" satisfied condition "running"
Feb 27 16:00:19.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Feb 27 16:00:19.521: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 27 16:00:19.521: INFO: stdout: ""
Feb 27 16:00:19.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 10.152.183.30 80'
Feb 27 16:00:19.615: INFO: stderr: "+ nc -v -z -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
Feb 27 16:00:19.615: INFO: stdout: ""
Feb 27 16:00:19.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 172.31.84.171 32240'
Feb 27 16:00:19.722: INFO: stderr: "+ nc -v -z -w 2 172.31.84.171 32240\nConnection to 172.31.84.171 32240 port [tcp/*] succeeded!\n"
Feb 27 16:00:19.722: INFO: stdout: ""
Feb 27 16:00:19.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 32240'
Feb 27 16:00:19.824: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 32240\nConnection to 172.31.42.40 32240 port [tcp/*] succeeded!\n"
Feb 27 16:00:19.824: INFO: stdout: ""
Feb 27 16:00:19.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.182:32240/ ; done'
Feb 27 16:00:20.010: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n"
Feb 27 16:00:20.010: INFO: stdout: "\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn"
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
Feb 27 16:00:20.010: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9239, will wait for the garbage collector to delete the pods 02/27/23 16:00:20.023
Feb 27 16:00:20.089: INFO: Deleting ReplicationController affinity-nodeport took: 11.670472ms
Feb 27 16:00:20.189: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.123807ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:21.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9239" for this suite. 02/27/23 16:00:21.814
------------------------------
• [SLOW TEST] [8.609 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:13.212
    Feb 27 16:00:13.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 16:00:13.212
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:13.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:13.284
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-9239 02/27/23 16:00:13.288
    STEP: creating service affinity-nodeport in namespace services-9239 02/27/23 16:00:13.288
    STEP: creating replication controller affinity-nodeport in namespace services-9239 02/27/23 16:00:13.303
    I0227 16:00:13.315185      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9239, replica count: 3
    I0227 16:00:16.365761      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 16:00:16.378: INFO: Creating new exec pod
    Feb 27 16:00:16.386: INFO: Waiting up to 5m0s for pod "execpod-affinityqxmrr" in namespace "services-9239" to be "running"
    Feb 27 16:00:16.391: INFO: Pod "execpod-affinityqxmrr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.964262ms
    Feb 27 16:00:18.397: INFO: Pod "execpod-affinityqxmrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010504756s
    Feb 27 16:00:18.397: INFO: Pod "execpod-affinityqxmrr" satisfied condition "running"
    Feb 27 16:00:19.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Feb 27 16:00:19.521: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb 27 16:00:19.521: INFO: stdout: ""
    Feb 27 16:00:19.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 10.152.183.30 80'
    Feb 27 16:00:19.615: INFO: stderr: "+ nc -v -z -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
    Feb 27 16:00:19.615: INFO: stdout: ""
    Feb 27 16:00:19.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 172.31.84.171 32240'
    Feb 27 16:00:19.722: INFO: stderr: "+ nc -v -z -w 2 172.31.84.171 32240\nConnection to 172.31.84.171 32240 port [tcp/*] succeeded!\n"
    Feb 27 16:00:19.722: INFO: stdout: ""
    Feb 27 16:00:19.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 32240'
    Feb 27 16:00:19.824: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 32240\nConnection to 172.31.42.40 32240 port [tcp/*] succeeded!\n"
    Feb 27 16:00:19.824: INFO: stdout: ""
    Feb 27 16:00:19.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9239 exec execpod-affinityqxmrr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.3.182:32240/ ; done'
    Feb 27 16:00:20.010: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.3.182:32240/\n"
    Feb 27 16:00:20.010: INFO: stdout: "\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn\naffinity-nodeport-mjzbn"
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Received response from host: affinity-nodeport-mjzbn
    Feb 27 16:00:20.010: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9239, will wait for the garbage collector to delete the pods 02/27/23 16:00:20.023
    Feb 27 16:00:20.089: INFO: Deleting ReplicationController affinity-nodeport took: 11.670472ms
    Feb 27 16:00:20.189: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.123807ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:21.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9239" for this suite. 02/27/23 16:00:21.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:21.822
Feb 27 16:00:21.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:00:21.822
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:21.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:21.842
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 02/27/23 16:00:21.845
Feb 27 16:00:21.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 create -f -'
Feb 27 16:00:22.010: INFO: stderr: ""
Feb 27 16:00:22.010: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 16:00:22.01
Feb 27 16:00:22.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 16:00:22.066: INFO: stderr: ""
Feb 27 16:00:22.066: INFO: stdout: "update-demo-nautilus-8pn4b update-demo-nautilus-9rvrt "
Feb 27 16:00:22.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-8pn4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 16:00:22.112: INFO: stderr: ""
Feb 27 16:00:22.112: INFO: stdout: ""
Feb 27 16:00:22.112: INFO: update-demo-nautilus-8pn4b is created but not running
Feb 27 16:00:27.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 16:00:27.163: INFO: stderr: ""
Feb 27 16:00:27.163: INFO: stdout: "update-demo-nautilus-8pn4b update-demo-nautilus-9rvrt "
Feb 27 16:00:27.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-8pn4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 16:00:27.209: INFO: stderr: ""
Feb 27 16:00:27.209: INFO: stdout: "true"
Feb 27 16:00:27.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-8pn4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 16:00:27.257: INFO: stderr: ""
Feb 27 16:00:27.257: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 16:00:27.257: INFO: validating pod update-demo-nautilus-8pn4b
Feb 27 16:00:27.263: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 16:00:27.263: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 16:00:27.263: INFO: update-demo-nautilus-8pn4b is verified up and running
Feb 27 16:00:27.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-9rvrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 16:00:27.317: INFO: stderr: ""
Feb 27 16:00:27.317: INFO: stdout: "true"
Feb 27 16:00:27.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-9rvrt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 16:00:27.368: INFO: stderr: ""
Feb 27 16:00:27.368: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Feb 27 16:00:27.368: INFO: validating pod update-demo-nautilus-9rvrt
Feb 27 16:00:27.373: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 16:00:27.373: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 16:00:27.373: INFO: update-demo-nautilus-9rvrt is verified up and running
STEP: using delete to clean up resources 02/27/23 16:00:27.373
Feb 27 16:00:27.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 delete --grace-period=0 --force -f -'
Feb 27 16:00:27.425: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 16:00:27.425: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 27 16:00:27.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get rc,svc -l name=update-demo --no-headers'
Feb 27 16:00:27.494: INFO: stderr: "No resources found in kubectl-497 namespace.\n"
Feb 27 16:00:27.494: INFO: stdout: ""
Feb 27 16:00:27.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 16:00:27.560: INFO: stderr: ""
Feb 27 16:00:27.560: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:27.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-497" for this suite. 02/27/23 16:00:27.564
------------------------------
• [SLOW TEST] [5.750 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:21.822
    Feb 27 16:00:21.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:00:21.822
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:21.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:21.842
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 02/27/23 16:00:21.845
    Feb 27 16:00:21.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 create -f -'
    Feb 27 16:00:22.010: INFO: stderr: ""
    Feb 27 16:00:22.010: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 16:00:22.01
    Feb 27 16:00:22.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 16:00:22.066: INFO: stderr: ""
    Feb 27 16:00:22.066: INFO: stdout: "update-demo-nautilus-8pn4b update-demo-nautilus-9rvrt "
    Feb 27 16:00:22.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-8pn4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 16:00:22.112: INFO: stderr: ""
    Feb 27 16:00:22.112: INFO: stdout: ""
    Feb 27 16:00:22.112: INFO: update-demo-nautilus-8pn4b is created but not running
    Feb 27 16:00:27.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 16:00:27.163: INFO: stderr: ""
    Feb 27 16:00:27.163: INFO: stdout: "update-demo-nautilus-8pn4b update-demo-nautilus-9rvrt "
    Feb 27 16:00:27.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-8pn4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 16:00:27.209: INFO: stderr: ""
    Feb 27 16:00:27.209: INFO: stdout: "true"
    Feb 27 16:00:27.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-8pn4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 16:00:27.257: INFO: stderr: ""
    Feb 27 16:00:27.257: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 16:00:27.257: INFO: validating pod update-demo-nautilus-8pn4b
    Feb 27 16:00:27.263: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 16:00:27.263: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 16:00:27.263: INFO: update-demo-nautilus-8pn4b is verified up and running
    Feb 27 16:00:27.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-9rvrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 16:00:27.317: INFO: stderr: ""
    Feb 27 16:00:27.317: INFO: stdout: "true"
    Feb 27 16:00:27.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods update-demo-nautilus-9rvrt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 16:00:27.368: INFO: stderr: ""
    Feb 27 16:00:27.368: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Feb 27 16:00:27.368: INFO: validating pod update-demo-nautilus-9rvrt
    Feb 27 16:00:27.373: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 16:00:27.373: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 16:00:27.373: INFO: update-demo-nautilus-9rvrt is verified up and running
    STEP: using delete to clean up resources 02/27/23 16:00:27.373
    Feb 27 16:00:27.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 delete --grace-period=0 --force -f -'
    Feb 27 16:00:27.425: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 16:00:27.425: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 27 16:00:27.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get rc,svc -l name=update-demo --no-headers'
    Feb 27 16:00:27.494: INFO: stderr: "No resources found in kubectl-497 namespace.\n"
    Feb 27 16:00:27.494: INFO: stdout: ""
    Feb 27 16:00:27.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-497 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 16:00:27.560: INFO: stderr: ""
    Feb 27 16:00:27.560: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:27.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-497" for this suite. 02/27/23 16:00:27.564
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:27.572
Feb 27 16:00:27.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename endpointslice 02/27/23 16:00:27.573
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:27.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:27.596
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:29.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7907" for this suite. 02/27/23 16:00:29.652
------------------------------
• [2.088 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:27.572
    Feb 27 16:00:27.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename endpointslice 02/27/23 16:00:27.573
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:27.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:27.596
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:29.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7907" for this suite. 02/27/23 16:00:29.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:29.661
Feb 27 16:00:29.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename lease-test 02/27/23 16:00:29.662
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:29.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:29.685
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:29.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6156" for this suite. 02/27/23 16:00:29.761
------------------------------
• [0.107 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:29.661
    Feb 27 16:00:29.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename lease-test 02/27/23 16:00:29.662
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:29.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:29.685
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:29.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6156" for this suite. 02/27/23 16:00:29.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:29.768
Feb 27 16:00:29.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 16:00:29.769
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:29.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:29.794
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 02/27/23 16:00:29.796
Feb 27 16:00:29.809: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411" in namespace "emptydir-8578" to be "running"
Feb 27 16:00:29.814: INFO: Pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868818ms
Feb 27 16:00:31.819: INFO: Pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411": Phase="Running", Reason="", readiness=false. Elapsed: 2.010397314s
Feb 27 16:00:31.819: INFO: Pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/27/23 16:00:31.819
Feb 27 16:00:31.819: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8578 PodName:pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 16:00:31.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 16:00:31.820: INFO: ExecWithOptions: Clientset creation
Feb 27 16:00:31.820: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-8578/pods/pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb 27 16:00:31.890: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:31.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8578" for this suite. 02/27/23 16:00:31.894
------------------------------
• [2.132 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:29.768
    Feb 27 16:00:29.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 16:00:29.769
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:29.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:29.794
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 02/27/23 16:00:29.796
    Feb 27 16:00:29.809: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411" in namespace "emptydir-8578" to be "running"
    Feb 27 16:00:29.814: INFO: Pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868818ms
    Feb 27 16:00:31.819: INFO: Pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411": Phase="Running", Reason="", readiness=false. Elapsed: 2.010397314s
    Feb 27 16:00:31.819: INFO: Pod "pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/27/23 16:00:31.819
    Feb 27 16:00:31.819: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8578 PodName:pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 16:00:31.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 16:00:31.820: INFO: ExecWithOptions: Clientset creation
    Feb 27 16:00:31.820: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-8578/pods/pod-sharedvolume-e080ec0f-806e-4244-b837-25c210d31411/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb 27 16:00:31.890: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:31.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8578" for this suite. 02/27/23 16:00:31.894
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:31.901
Feb 27 16:00:31.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:00:31.902
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:31.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:31.923
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 02/27/23 16:00:31.926
STEP: Creating a ResourceQuota 02/27/23 16:00:36.93
STEP: Ensuring resource quota status is calculated 02/27/23 16:00:36.935
STEP: Creating a Service 02/27/23 16:00:38.938
STEP: Creating a NodePort Service 02/27/23 16:00:38.955
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/27/23 16:00:38.978
STEP: Ensuring resource quota status captures service creation 02/27/23 16:00:38.998
STEP: Deleting Services 02/27/23 16:00:41.003
STEP: Ensuring resource quota status released usage 02/27/23 16:00:41.038
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:43.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9885" for this suite. 02/27/23 16:00:43.046
------------------------------
• [SLOW TEST] [11.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:31.901
    Feb 27 16:00:31.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:00:31.902
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:31.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:31.923
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 02/27/23 16:00:31.926
    STEP: Creating a ResourceQuota 02/27/23 16:00:36.93
    STEP: Ensuring resource quota status is calculated 02/27/23 16:00:36.935
    STEP: Creating a Service 02/27/23 16:00:38.938
    STEP: Creating a NodePort Service 02/27/23 16:00:38.955
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/27/23 16:00:38.978
    STEP: Ensuring resource quota status captures service creation 02/27/23 16:00:38.998
    STEP: Deleting Services 02/27/23 16:00:41.003
    STEP: Ensuring resource quota status released usage 02/27/23 16:00:41.038
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:43.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9885" for this suite. 02/27/23 16:00:43.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:43.056
Feb 27 16:00:43.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 16:00:43.057
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:43.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:43.075
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/27/23 16:00:43.078
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9493;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9493;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +notcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_tcp@PTR;sleep 1; done
 02/27/23 16:00:43.096
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9493;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9493;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +notcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_tcp@PTR;sleep 1; done
 02/27/23 16:00:43.096
STEP: creating a pod to probe DNS 02/27/23 16:00:43.096
STEP: submitting the pod to kubernetes 02/27/23 16:00:43.097
Feb 27 16:00:43.108: INFO: Waiting up to 15m0s for pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387" in namespace "dns-9493" to be "running"
Feb 27 16:00:43.114: INFO: Pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59769ms
Feb 27 16:00:45.118: INFO: Pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387": Phase="Running", Reason="", readiness=true. Elapsed: 2.010120781s
Feb 27 16:00:45.118: INFO: Pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387" satisfied condition "running"
STEP: retrieving the pod 02/27/23 16:00:45.118
STEP: looking for the results for each expected name from probers 02/27/23 16:00:45.122
Feb 27 16:00:45.126: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.132: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.137: INFO: Unable to read wheezy_udp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.140: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.144: INFO: Unable to read wheezy_udp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.148: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.151: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.154: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.171: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.175: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.178: INFO: Unable to read jessie_udp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.181: INFO: Unable to read jessie_tcp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.185: INFO: Unable to read jessie_udp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.191: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.195: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
Feb 27 16:00:45.208: INFO: Lookups using dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9493 wheezy_tcp@dns-test-service.dns-9493 wheezy_udp@dns-test-service.dns-9493.svc wheezy_tcp@dns-test-service.dns-9493.svc wheezy_udp@_http._tcp.dns-test-service.dns-9493.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9493.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9493 jessie_tcp@dns-test-service.dns-9493 jessie_udp@dns-test-service.dns-9493.svc jessie_tcp@dns-test-service.dns-9493.svc jessie_udp@_http._tcp.dns-test-service.dns-9493.svc jessie_tcp@_http._tcp.dns-test-service.dns-9493.svc]

Feb 27 16:00:50.296: INFO: DNS probes using dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387 succeeded

STEP: deleting the pod 02/27/23 16:00:50.296
STEP: deleting the test service 02/27/23 16:00:50.31
STEP: deleting the test headless service 02/27/23 16:00:50.329
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 16:00:50.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9493" for this suite. 02/27/23 16:00:50.348
------------------------------
• [SLOW TEST] [7.303 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:43.056
    Feb 27 16:00:43.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 16:00:43.057
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:43.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:43.075
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/27/23 16:00:43.078
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9493;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9493;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +notcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_tcp@PTR;sleep 1; done
     02/27/23 16:00:43.096
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9493;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9493;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9493.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9493.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9493.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9493.svc;check="$$(dig +notcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_udp@PTR;check="$$(dig +tcp +noall +answer +search 209.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.209_tcp@PTR;sleep 1; done
     02/27/23 16:00:43.096
    STEP: creating a pod to probe DNS 02/27/23 16:00:43.096
    STEP: submitting the pod to kubernetes 02/27/23 16:00:43.097
    Feb 27 16:00:43.108: INFO: Waiting up to 15m0s for pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387" in namespace "dns-9493" to be "running"
    Feb 27 16:00:43.114: INFO: Pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59769ms
    Feb 27 16:00:45.118: INFO: Pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387": Phase="Running", Reason="", readiness=true. Elapsed: 2.010120781s
    Feb 27 16:00:45.118: INFO: Pod "dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 16:00:45.118
    STEP: looking for the results for each expected name from probers 02/27/23 16:00:45.122
    Feb 27 16:00:45.126: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.132: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.137: INFO: Unable to read wheezy_udp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.140: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.144: INFO: Unable to read wheezy_udp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.148: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.151: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.154: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.171: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.175: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.178: INFO: Unable to read jessie_udp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.181: INFO: Unable to read jessie_tcp@dns-test-service.dns-9493 from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.185: INFO: Unable to read jessie_udp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.191: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.195: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9493.svc from pod dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387: the server could not find the requested resource (get pods dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387)
    Feb 27 16:00:45.208: INFO: Lookups using dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9493 wheezy_tcp@dns-test-service.dns-9493 wheezy_udp@dns-test-service.dns-9493.svc wheezy_tcp@dns-test-service.dns-9493.svc wheezy_udp@_http._tcp.dns-test-service.dns-9493.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9493.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9493 jessie_tcp@dns-test-service.dns-9493 jessie_udp@dns-test-service.dns-9493.svc jessie_tcp@dns-test-service.dns-9493.svc jessie_udp@_http._tcp.dns-test-service.dns-9493.svc jessie_tcp@_http._tcp.dns-test-service.dns-9493.svc]

    Feb 27 16:00:50.296: INFO: DNS probes using dns-9493/dns-test-a7360bbd-189d-4ab3-9249-5bddba13e387 succeeded

    STEP: deleting the pod 02/27/23 16:00:50.296
    STEP: deleting the test service 02/27/23 16:00:50.31
    STEP: deleting the test headless service 02/27/23 16:00:50.329
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:00:50.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9493" for this suite. 02/27/23 16:00:50.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:00:50.36
Feb 27 16:00:50.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-watch 02/27/23 16:00:50.36
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:50.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:50.38
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb 27 16:00:50.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Creating first CR  02/27/23 16:00:52.929
Feb 27 16:00:52.933: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:00:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:00:52Z]] name:name1 resourceVersion:22841 uid:d9366799-d349-4cd1-a3ec-7e83013d06f0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/27/23 16:01:02.934
Feb 27 16:01:02.941: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:01:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:02Z]] name:name2 resourceVersion:22885 uid:849f4411-0d2e-4aac-957e-05122b8b0816] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/27/23 16:01:12.942
Feb 27 16:01:12.949: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:00:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:12Z]] name:name1 resourceVersion:22906 uid:d9366799-d349-4cd1-a3ec-7e83013d06f0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/27/23 16:01:22.952
Feb 27 16:01:22.960: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:01:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:22Z]] name:name2 resourceVersion:22928 uid:849f4411-0d2e-4aac-957e-05122b8b0816] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/27/23 16:01:32.962
Feb 27 16:01:32.971: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:00:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:12Z]] name:name1 resourceVersion:22950 uid:d9366799-d349-4cd1-a3ec-7e83013d06f0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/27/23 16:01:42.973
Feb 27 16:01:42.981: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:01:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:22Z]] name:name2 resourceVersion:22972 uid:849f4411-0d2e-4aac-957e-05122b8b0816] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:01:53.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-979" for this suite. 02/27/23 16:01:53.502
------------------------------
• [SLOW TEST] [63.149 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:00:50.36
    Feb 27 16:00:50.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-watch 02/27/23 16:00:50.36
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:00:50.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:00:50.38
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb 27 16:00:50.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Creating first CR  02/27/23 16:00:52.929
    Feb 27 16:00:52.933: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:00:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:00:52Z]] name:name1 resourceVersion:22841 uid:d9366799-d349-4cd1-a3ec-7e83013d06f0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/27/23 16:01:02.934
    Feb 27 16:01:02.941: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:01:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:02Z]] name:name2 resourceVersion:22885 uid:849f4411-0d2e-4aac-957e-05122b8b0816] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/27/23 16:01:12.942
    Feb 27 16:01:12.949: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:00:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:12Z]] name:name1 resourceVersion:22906 uid:d9366799-d349-4cd1-a3ec-7e83013d06f0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/27/23 16:01:22.952
    Feb 27 16:01:22.960: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:01:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:22Z]] name:name2 resourceVersion:22928 uid:849f4411-0d2e-4aac-957e-05122b8b0816] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/27/23 16:01:32.962
    Feb 27 16:01:32.971: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:00:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:12Z]] name:name1 resourceVersion:22950 uid:d9366799-d349-4cd1-a3ec-7e83013d06f0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/27/23 16:01:42.973
    Feb 27 16:01:42.981: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T16:01:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T16:01:22Z]] name:name2 resourceVersion:22972 uid:849f4411-0d2e-4aac-957e-05122b8b0816] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:01:53.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-979" for this suite. 02/27/23 16:01:53.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:01:53.509
Feb 27 16:01:53.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:01:53.51
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:01:53.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:01:53.53
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 02/27/23 16:01:53.532
Feb 27 16:01:53.542: INFO: Waiting up to 5m0s for pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df" in namespace "downward-api-8002" to be "Succeeded or Failed"
Feb 27 16:01:53.547: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.976248ms
Feb 27 16:01:55.551: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009615891s
Feb 27 16:01:57.552: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010300567s
STEP: Saw pod success 02/27/23 16:01:57.552
Feb 27 16:01:57.552: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df" satisfied condition "Succeeded or Failed"
Feb 27 16:01:57.555: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df container dapi-container: <nil>
STEP: delete the pod 02/27/23 16:01:57.568
Feb 27 16:01:57.582: INFO: Waiting for pod downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df to disappear
Feb 27 16:01:57.585: INFO: Pod downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 16:01:57.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8002" for this suite. 02/27/23 16:01:57.589
------------------------------
• [4.086 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:01:53.509
    Feb 27 16:01:53.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:01:53.51
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:01:53.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:01:53.53
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 02/27/23 16:01:53.532
    Feb 27 16:01:53.542: INFO: Waiting up to 5m0s for pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df" in namespace "downward-api-8002" to be "Succeeded or Failed"
    Feb 27 16:01:53.547: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.976248ms
    Feb 27 16:01:55.551: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009615891s
    Feb 27 16:01:57.552: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010300567s
    STEP: Saw pod success 02/27/23 16:01:57.552
    Feb 27 16:01:57.552: INFO: Pod "downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df" satisfied condition "Succeeded or Failed"
    Feb 27 16:01:57.555: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df container dapi-container: <nil>
    STEP: delete the pod 02/27/23 16:01:57.568
    Feb 27 16:01:57.582: INFO: Waiting for pod downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df to disappear
    Feb 27 16:01:57.585: INFO: Pod downward-api-f5623661-1848-4bce-ab2e-87c57e2ef0df no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:01:57.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8002" for this suite. 02/27/23 16:01:57.589
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:01:57.595
Feb 27 16:01:57.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:01:57.596
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:01:57.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:01:57.613
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-fd2ec35f-48c2-4ea5-9b3e-92c2ea36848b 02/27/23 16:01:57.62
STEP: Creating the pod 02/27/23 16:01:57.624
Feb 27 16:01:57.632: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8" in namespace "configmap-1995" to be "running"
Feb 27 16:01:57.638: INFO: Pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888536ms
Feb 27 16:01:59.642: INFO: Pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8": Phase="Running", Reason="", readiness=false. Elapsed: 2.009438066s
Feb 27 16:01:59.642: INFO: Pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8" satisfied condition "running"
STEP: Waiting for pod with text data 02/27/23 16:01:59.642
STEP: Waiting for pod with binary data 02/27/23 16:01:59.649
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:01:59.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1995" for this suite. 02/27/23 16:01:59.658
------------------------------
• [2.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:01:57.595
    Feb 27 16:01:57.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:01:57.596
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:01:57.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:01:57.613
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-fd2ec35f-48c2-4ea5-9b3e-92c2ea36848b 02/27/23 16:01:57.62
    STEP: Creating the pod 02/27/23 16:01:57.624
    Feb 27 16:01:57.632: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8" in namespace "configmap-1995" to be "running"
    Feb 27 16:01:57.638: INFO: Pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888536ms
    Feb 27 16:01:59.642: INFO: Pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8": Phase="Running", Reason="", readiness=false. Elapsed: 2.009438066s
    Feb 27 16:01:59.642: INFO: Pod "pod-configmaps-8a9b2803-0609-4f01-9edb-1d9b62b811a8" satisfied condition "running"
    STEP: Waiting for pod with text data 02/27/23 16:01:59.642
    STEP: Waiting for pod with binary data 02/27/23 16:01:59.649
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:01:59.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1995" for this suite. 02/27/23 16:01:59.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:01:59.666
Feb 27 16:01:59.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename disruption 02/27/23 16:01:59.666
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:01:59.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:01:59.684
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 02/27/23 16:01:59.687
STEP: Waiting for the pdb to be processed 02/27/23 16:01:59.692
STEP: First trying to evict a pod which shouldn't be evictable 02/27/23 16:02:01.705
STEP: Waiting for all pods to be running 02/27/23 16:02:01.705
Feb 27 16:02:01.708: INFO: pods: 0 < 3
STEP: locating a running pod 02/27/23 16:02:03.714
STEP: Updating the pdb to allow a pod to be evicted 02/27/23 16:02:03.723
STEP: Waiting for the pdb to be processed 02/27/23 16:02:03.735
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 16:02:05.745
STEP: Waiting for all pods to be running 02/27/23 16:02:05.745
STEP: Waiting for the pdb to observed all healthy pods 02/27/23 16:02:05.75
STEP: Patching the pdb to disallow a pod to be evicted 02/27/23 16:02:05.787
STEP: Waiting for the pdb to be processed 02/27/23 16:02:05.818
STEP: Waiting for all pods to be running 02/27/23 16:02:05.822
Feb 27 16:02:05.829: INFO: running pods: 2 < 3
STEP: locating a running pod 02/27/23 16:02:07.834
STEP: Deleting the pdb to allow a pod to be evicted 02/27/23 16:02:07.843
STEP: Waiting for the pdb to be deleted 02/27/23 16:02:07.849
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 16:02:07.852
STEP: Waiting for all pods to be running 02/27/23 16:02:07.853
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 16:02:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7844" for this suite. 02/27/23 16:02:07.877
------------------------------
• [SLOW TEST] [8.228 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:01:59.666
    Feb 27 16:01:59.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename disruption 02/27/23 16:01:59.666
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:01:59.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:01:59.684
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 02/27/23 16:01:59.687
    STEP: Waiting for the pdb to be processed 02/27/23 16:01:59.692
    STEP: First trying to evict a pod which shouldn't be evictable 02/27/23 16:02:01.705
    STEP: Waiting for all pods to be running 02/27/23 16:02:01.705
    Feb 27 16:02:01.708: INFO: pods: 0 < 3
    STEP: locating a running pod 02/27/23 16:02:03.714
    STEP: Updating the pdb to allow a pod to be evicted 02/27/23 16:02:03.723
    STEP: Waiting for the pdb to be processed 02/27/23 16:02:03.735
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 16:02:05.745
    STEP: Waiting for all pods to be running 02/27/23 16:02:05.745
    STEP: Waiting for the pdb to observed all healthy pods 02/27/23 16:02:05.75
    STEP: Patching the pdb to disallow a pod to be evicted 02/27/23 16:02:05.787
    STEP: Waiting for the pdb to be processed 02/27/23 16:02:05.818
    STEP: Waiting for all pods to be running 02/27/23 16:02:05.822
    Feb 27 16:02:05.829: INFO: running pods: 2 < 3
    STEP: locating a running pod 02/27/23 16:02:07.834
    STEP: Deleting the pdb to allow a pod to be evicted 02/27/23 16:02:07.843
    STEP: Waiting for the pdb to be deleted 02/27/23 16:02:07.849
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 16:02:07.852
    STEP: Waiting for all pods to be running 02/27/23 16:02:07.853
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:02:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7844" for this suite. 02/27/23 16:02:07.877
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:02:07.894
Feb 27 16:02:07.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename ingressclass 02/27/23 16:02:07.894
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:02:07.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:02:07.917
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/27/23 16:02:07.92
STEP: getting /apis/networking.k8s.io 02/27/23 16:02:07.922
STEP: getting /apis/networking.k8s.iov1 02/27/23 16:02:07.923
STEP: creating 02/27/23 16:02:07.925
STEP: getting 02/27/23 16:02:08.003
STEP: listing 02/27/23 16:02:08.007
STEP: watching 02/27/23 16:02:08.01
Feb 27 16:02:08.010: INFO: starting watch
STEP: patching 02/27/23 16:02:08.011
STEP: updating 02/27/23 16:02:08.016
Feb 27 16:02:08.022: INFO: waiting for watch events with expected annotations
Feb 27 16:02:08.022: INFO: saw patched and updated annotations
STEP: deleting 02/27/23 16:02:08.022
STEP: deleting a collection 02/27/23 16:02:08.034
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Feb 27 16:02:08.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3140" for this suite. 02/27/23 16:02:08.053
------------------------------
• [0.166 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:02:07.894
    Feb 27 16:02:07.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename ingressclass 02/27/23 16:02:07.894
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:02:07.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:02:07.917
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/27/23 16:02:07.92
    STEP: getting /apis/networking.k8s.io 02/27/23 16:02:07.922
    STEP: getting /apis/networking.k8s.iov1 02/27/23 16:02:07.923
    STEP: creating 02/27/23 16:02:07.925
    STEP: getting 02/27/23 16:02:08.003
    STEP: listing 02/27/23 16:02:08.007
    STEP: watching 02/27/23 16:02:08.01
    Feb 27 16:02:08.010: INFO: starting watch
    STEP: patching 02/27/23 16:02:08.011
    STEP: updating 02/27/23 16:02:08.016
    Feb 27 16:02:08.022: INFO: waiting for watch events with expected annotations
    Feb 27 16:02:08.022: INFO: saw patched and updated annotations
    STEP: deleting 02/27/23 16:02:08.022
    STEP: deleting a collection 02/27/23 16:02:08.034
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:02:08.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3140" for this suite. 02/27/23 16:02:08.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:02:08.06
Feb 27 16:02:08.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-preemption 02/27/23 16:02:08.061
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:02:08.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:02:08.076
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 16:02:08.092: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 16:03:08.109: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 02/27/23 16:03:08.113
Feb 27 16:03:08.134: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 27 16:03:08.142: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 27 16:03:08.159: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 27 16:03:08.166: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 27 16:03:08.186: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 27 16:03:08.195: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/27/23 16:03:08.195
Feb 27 16:03:08.195: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9188" to be "running"
Feb 27 16:03:08.200: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.627109ms
Feb 27 16:03:10.205: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009953579s
Feb 27 16:03:10.205: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 27 16:03:10.205: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
Feb 27 16:03:10.209: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.252112ms
Feb 27 16:03:10.209: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:03:10.209: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
Feb 27 16:03:10.213: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.542731ms
Feb 27 16:03:10.213: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:03:10.213: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
Feb 27 16:03:10.216: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.025907ms
Feb 27 16:03:10.216: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:03:10.216: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
Feb 27 16:03:10.220: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.927407ms
Feb 27 16:03:10.220: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:03:10.220: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
Feb 27 16:03:10.223: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.420391ms
Feb 27 16:03:10.223: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/27/23 16:03:10.223
Feb 27 16:03:10.234: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb 27 16:03:10.238: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95196ms
Feb 27 16:03:12.242: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008509669s
Feb 27 16:03:14.241: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007588766s
Feb 27 16:03:16.243: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009092165s
Feb 27 16:03:16.243: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:03:16.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9188" for this suite. 02/27/23 16:03:16.326
------------------------------
• [SLOW TEST] [68.273 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:02:08.06
    Feb 27 16:02:08.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 16:02:08.061
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:02:08.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:02:08.076
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 16:02:08.092: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 16:03:08.109: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 02/27/23 16:03:08.113
    Feb 27 16:03:08.134: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 27 16:03:08.142: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 27 16:03:08.159: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 27 16:03:08.166: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 27 16:03:08.186: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 27 16:03:08.195: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/27/23 16:03:08.195
    Feb 27 16:03:08.195: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9188" to be "running"
    Feb 27 16:03:08.200: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.627109ms
    Feb 27 16:03:10.205: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009953579s
    Feb 27 16:03:10.205: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 27 16:03:10.205: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
    Feb 27 16:03:10.209: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.252112ms
    Feb 27 16:03:10.209: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:03:10.209: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
    Feb 27 16:03:10.213: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.542731ms
    Feb 27 16:03:10.213: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:03:10.213: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
    Feb 27 16:03:10.216: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.025907ms
    Feb 27 16:03:10.216: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:03:10.216: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
    Feb 27 16:03:10.220: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.927407ms
    Feb 27 16:03:10.220: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:03:10.220: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9188" to be "running"
    Feb 27 16:03:10.223: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.420391ms
    Feb 27 16:03:10.223: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/27/23 16:03:10.223
    Feb 27 16:03:10.234: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb 27 16:03:10.238: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.95196ms
    Feb 27 16:03:12.242: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008509669s
    Feb 27 16:03:14.241: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007588766s
    Feb 27 16:03:16.243: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009092165s
    Feb 27 16:03:16.243: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:03:16.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9188" for this suite. 02/27/23 16:03:16.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:03:16.334
Feb 27 16:03:16.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 16:03:16.334
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:16.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:16.351
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/27/23 16:03:16.354
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_udp@PTR;check="$$(dig +tcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_tcp@PTR;sleep 1; done
 02/27/23 16:03:16.371
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_udp@PTR;check="$$(dig +tcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_tcp@PTR;sleep 1; done
 02/27/23 16:03:16.371
STEP: creating a pod to probe DNS 02/27/23 16:03:16.371
STEP: submitting the pod to kubernetes 02/27/23 16:03:16.371
Feb 27 16:03:16.386: INFO: Waiting up to 15m0s for pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb" in namespace "dns-3068" to be "running"
Feb 27 16:03:16.394: INFO: Pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.381523ms
Feb 27 16:03:18.398: INFO: Pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012665962s
Feb 27 16:03:18.398: INFO: Pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb" satisfied condition "running"
STEP: retrieving the pod 02/27/23 16:03:18.398
STEP: looking for the results for each expected name from probers 02/27/23 16:03:18.403
Feb 27 16:03:18.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.415: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.422: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.427: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.462: INFO: Unable to read jessie_udp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.471: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.481: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
Feb 27 16:03:18.499: INFO: Lookups using dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb failed for: [wheezy_udp@dns-test-service.dns-3068.svc.cluster.local wheezy_tcp@dns-test-service.dns-3068.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local jessie_udp@dns-test-service.dns-3068.svc.cluster.local jessie_tcp@dns-test-service.dns-3068.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local]

Feb 27 16:03:23.561: INFO: DNS probes using dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb succeeded

STEP: deleting the pod 02/27/23 16:03:23.561
STEP: deleting the test service 02/27/23 16:03:23.637
STEP: deleting the test headless service 02/27/23 16:03:23.669
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 16:03:23.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3068" for this suite. 02/27/23 16:03:23.688
------------------------------
• [SLOW TEST] [7.361 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:03:16.334
    Feb 27 16:03:16.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 16:03:16.334
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:16.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:16.351
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/27/23 16:03:16.354
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_udp@PTR;check="$$(dig +tcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_tcp@PTR;sleep 1; done
     02/27/23 16:03:16.371
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3068.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3068.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3068.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_udp@PTR;check="$$(dig +tcp +noall +answer +search 49.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.49_tcp@PTR;sleep 1; done
     02/27/23 16:03:16.371
    STEP: creating a pod to probe DNS 02/27/23 16:03:16.371
    STEP: submitting the pod to kubernetes 02/27/23 16:03:16.371
    Feb 27 16:03:16.386: INFO: Waiting up to 15m0s for pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb" in namespace "dns-3068" to be "running"
    Feb 27 16:03:16.394: INFO: Pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.381523ms
    Feb 27 16:03:18.398: INFO: Pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012665962s
    Feb 27 16:03:18.398: INFO: Pod "dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 16:03:18.398
    STEP: looking for the results for each expected name from probers 02/27/23 16:03:18.403
    Feb 27 16:03:18.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.415: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.422: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.427: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.462: INFO: Unable to read jessie_udp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.467: INFO: Unable to read jessie_tcp@dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.471: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.481: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local from pod dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb: the server could not find the requested resource (get pods dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb)
    Feb 27 16:03:18.499: INFO: Lookups using dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb failed for: [wheezy_udp@dns-test-service.dns-3068.svc.cluster.local wheezy_tcp@dns-test-service.dns-3068.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local jessie_udp@dns-test-service.dns-3068.svc.cluster.local jessie_tcp@dns-test-service.dns-3068.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3068.svc.cluster.local]

    Feb 27 16:03:23.561: INFO: DNS probes using dns-3068/dns-test-a2b3b5f1-8cf9-4dc8-8536-e26ed2ed97fb succeeded

    STEP: deleting the pod 02/27/23 16:03:23.561
    STEP: deleting the test service 02/27/23 16:03:23.637
    STEP: deleting the test headless service 02/27/23 16:03:23.669
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:03:23.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3068" for this suite. 02/27/23 16:03:23.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:03:23.696
Feb 27 16:03:23.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename subpath 02/27/23 16:03:23.696
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:23.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:23.716
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 16:03:23.72
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-hnjr 02/27/23 16:03:23.735
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 16:03:23.735
Feb 27 16:03:23.745: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hnjr" in namespace "subpath-8717" to be "Succeeded or Failed"
Feb 27 16:03:23.748: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.557742ms
Feb 27 16:03:25.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008096599s
Feb 27 16:03:27.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 4.008530014s
Feb 27 16:03:29.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 6.008671129s
Feb 27 16:03:31.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 8.007701718s
Feb 27 16:03:33.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 10.008548553s
Feb 27 16:03:35.754: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 12.009076858s
Feb 27 16:03:37.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 14.007711741s
Feb 27 16:03:39.754: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 16.00886212s
Feb 27 16:03:41.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 18.007991896s
Feb 27 16:03:43.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 20.007852114s
Feb 27 16:03:45.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=false. Elapsed: 22.007889347s
Feb 27 16:03:47.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007599426s
STEP: Saw pod success 02/27/23 16:03:47.752
Feb 27 16:03:47.752: INFO: Pod "pod-subpath-test-projected-hnjr" satisfied condition "Succeeded or Failed"
Feb 27 16:03:47.755: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-projected-hnjr container test-container-subpath-projected-hnjr: <nil>
STEP: delete the pod 02/27/23 16:03:47.77
Feb 27 16:03:47.783: INFO: Waiting for pod pod-subpath-test-projected-hnjr to disappear
Feb 27 16:03:47.786: INFO: Pod pod-subpath-test-projected-hnjr no longer exists
STEP: Deleting pod pod-subpath-test-projected-hnjr 02/27/23 16:03:47.786
Feb 27 16:03:47.786: INFO: Deleting pod "pod-subpath-test-projected-hnjr" in namespace "subpath-8717"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 16:03:47.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8717" for this suite. 02/27/23 16:03:47.793
------------------------------
• [SLOW TEST] [24.105 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:03:23.696
    Feb 27 16:03:23.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename subpath 02/27/23 16:03:23.696
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:23.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:23.716
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 16:03:23.72
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-hnjr 02/27/23 16:03:23.735
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 16:03:23.735
    Feb 27 16:03:23.745: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hnjr" in namespace "subpath-8717" to be "Succeeded or Failed"
    Feb 27 16:03:23.748: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.557742ms
    Feb 27 16:03:25.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008096599s
    Feb 27 16:03:27.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 4.008530014s
    Feb 27 16:03:29.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 6.008671129s
    Feb 27 16:03:31.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 8.007701718s
    Feb 27 16:03:33.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 10.008548553s
    Feb 27 16:03:35.754: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 12.009076858s
    Feb 27 16:03:37.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 14.007711741s
    Feb 27 16:03:39.754: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 16.00886212s
    Feb 27 16:03:41.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 18.007991896s
    Feb 27 16:03:43.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=true. Elapsed: 20.007852114s
    Feb 27 16:03:45.753: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Running", Reason="", readiness=false. Elapsed: 22.007889347s
    Feb 27 16:03:47.752: INFO: Pod "pod-subpath-test-projected-hnjr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007599426s
    STEP: Saw pod success 02/27/23 16:03:47.752
    Feb 27 16:03:47.752: INFO: Pod "pod-subpath-test-projected-hnjr" satisfied condition "Succeeded or Failed"
    Feb 27 16:03:47.755: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-projected-hnjr container test-container-subpath-projected-hnjr: <nil>
    STEP: delete the pod 02/27/23 16:03:47.77
    Feb 27 16:03:47.783: INFO: Waiting for pod pod-subpath-test-projected-hnjr to disappear
    Feb 27 16:03:47.786: INFO: Pod pod-subpath-test-projected-hnjr no longer exists
    STEP: Deleting pod pod-subpath-test-projected-hnjr 02/27/23 16:03:47.786
    Feb 27 16:03:47.786: INFO: Deleting pod "pod-subpath-test-projected-hnjr" in namespace "subpath-8717"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:03:47.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8717" for this suite. 02/27/23 16:03:47.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:03:47.801
Feb 27 16:03:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename proxy 02/27/23 16:03:47.802
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:47.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:47.823
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/27/23 16:03:47.837
STEP: creating replication controller proxy-service-rbkx7 in namespace proxy-9208 02/27/23 16:03:47.837
I0227 16:03:47.845792      19 runners.go:193] Created replication controller with name: proxy-service-rbkx7, namespace: proxy-9208, replica count: 1
I0227 16:03:48.896974      19 runners.go:193] proxy-service-rbkx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0227 16:03:49.897334      19 runners.go:193] proxy-service-rbkx7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0227 16:03:50.897465      19 runners.go:193] proxy-service-rbkx7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 16:03:50.901: INFO: setup took 3.075649503s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/27/23 16:03:50.901
Feb 27 16:03:50.906: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.703601ms)
Feb 27 16:03:50.907: INFO: (0) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.530671ms)
Feb 27 16:03:50.908: INFO: (0) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.561967ms)
Feb 27 16:03:50.908: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.615246ms)
Feb 27 16:03:50.909: INFO: (0) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.966258ms)
Feb 27 16:03:50.911: INFO: (0) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.628427ms)
Feb 27 16:03:50.911: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.424396ms)
Feb 27 16:03:50.911: INFO: (0) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.62786ms)
Feb 27 16:03:50.914: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 11.970215ms)
Feb 27 16:03:50.915: INFO: (0) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 12.959117ms)
Feb 27 16:03:50.915: INFO: (0) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 13.712167ms)
Feb 27 16:03:50.915: INFO: (0) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 13.524527ms)
Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 13.842848ms)
Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 14.046193ms)
Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 14.137547ms)
Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 14.157873ms)
Feb 27 16:03:50.921: INFO: (1) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 4.438083ms)
Feb 27 16:03:50.921: INFO: (1) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 4.444917ms)
Feb 27 16:03:50.922: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.882163ms)
Feb 27 16:03:50.922: INFO: (1) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.097631ms)
Feb 27 16:03:50.922: INFO: (1) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 6.302032ms)
Feb 27 16:03:50.923: INFO: (1) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.065966ms)
Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.533218ms)
Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.422443ms)
Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.611221ms)
Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.928644ms)
Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.072717ms)
Feb 27 16:03:50.925: INFO: (1) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.378239ms)
Feb 27 16:03:50.925: INFO: (1) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.68684ms)
Feb 27 16:03:50.925: INFO: (1) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.93257ms)
Feb 27 16:03:50.928: INFO: (1) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 11.252915ms)
Feb 27 16:03:50.928: INFO: (1) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 11.253241ms)
Feb 27 16:03:50.931: INFO: (2) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.826665ms)
Feb 27 16:03:50.933: INFO: (2) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.021965ms)
Feb 27 16:03:50.933: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 4.94135ms)
Feb 27 16:03:50.934: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.015594ms)
Feb 27 16:03:50.934: INFO: (2) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.850236ms)
Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.778385ms)
Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.052639ms)
Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.33812ms)
Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.564231ms)
Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.812289ms)
Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.241718ms)
Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 7.918549ms)
Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.226574ms)
Feb 27 16:03:50.937: INFO: (2) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.598296ms)
Feb 27 16:03:50.937: INFO: (2) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.095134ms)
Feb 27 16:03:50.937: INFO: (2) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 8.916112ms)
Feb 27 16:03:50.941: INFO: (3) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.017765ms)
Feb 27 16:03:50.942: INFO: (3) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 4.741861ms)
Feb 27 16:03:50.942: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 4.577163ms)
Feb 27 16:03:50.943: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 5.628775ms)
Feb 27 16:03:50.943: INFO: (3) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.630095ms)
Feb 27 16:03:50.943: INFO: (3) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.869647ms)
Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 6.826388ms)
Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.916143ms)
Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.123367ms)
Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.252219ms)
Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.522042ms)
Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.845282ms)
Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.083858ms)
Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.217584ms)
Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.29759ms)
Feb 27 16:03:50.946: INFO: (3) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.896446ms)
Feb 27 16:03:50.950: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 3.952974ms)
Feb 27 16:03:50.950: INFO: (4) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 3.913522ms)
Feb 27 16:03:50.951: INFO: (4) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.469601ms)
Feb 27 16:03:50.951: INFO: (4) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.496983ms)
Feb 27 16:03:50.951: INFO: (4) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.123949ms)
Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 5.362139ms)
Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.490548ms)
Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.91119ms)
Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.030911ms)
Feb 27 16:03:50.953: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.972462ms)
Feb 27 16:03:50.953: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.010957ms)
Feb 27 16:03:50.954: INFO: (4) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.226628ms)
Feb 27 16:03:50.954: INFO: (4) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.464054ms)
Feb 27 16:03:50.954: INFO: (4) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.848793ms)
Feb 27 16:03:50.955: INFO: (4) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.500476ms)
Feb 27 16:03:50.955: INFO: (4) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.605734ms)
Feb 27 16:03:50.960: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.203157ms)
Feb 27 16:03:50.961: INFO: (5) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.430233ms)
Feb 27 16:03:50.961: INFO: (5) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.274809ms)
Feb 27 16:03:50.961: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 5.526244ms)
Feb 27 16:03:50.962: INFO: (5) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 6.592959ms)
Feb 27 16:03:50.962: INFO: (5) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.862068ms)
Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 7.347218ms)
Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 7.527458ms)
Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 7.786966ms)
Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.69079ms)
Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 7.925095ms)
Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 8.763406ms)
Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.025196ms)
Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.939494ms)
Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.149427ms)
Feb 27 16:03:50.965: INFO: (5) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.419187ms)
Feb 27 16:03:50.969: INFO: (6) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.2373ms)
Feb 27 16:03:50.969: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.526651ms)
Feb 27 16:03:50.970: INFO: (6) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.647191ms)
Feb 27 16:03:50.971: INFO: (6) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.5529ms)
Feb 27 16:03:50.972: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.76384ms)
Feb 27 16:03:50.972: INFO: (6) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.645097ms)
Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.422644ms)
Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 7.91101ms)
Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 8.155859ms)
Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.674018ms)
Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 8.767297ms)
Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.12168ms)
Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 9.037049ms)
Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.300903ms)
Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.665459ms)
Feb 27 16:03:50.975: INFO: (6) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.725987ms)
Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.547929ms)
Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.628142ms)
Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 4.919618ms)
Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.299259ms)
Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.343251ms)
Feb 27 16:03:50.981: INFO: (7) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 6.395397ms)
Feb 27 16:03:50.981: INFO: (7) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.106446ms)
Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.026233ms)
Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.22785ms)
Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.166579ms)
Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 7.418362ms)
Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.854879ms)
Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 8.034651ms)
Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.14228ms)
Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.088375ms)
Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.325421ms)
Feb 27 16:03:50.987: INFO: (8) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 3.545557ms)
Feb 27 16:03:50.988: INFO: (8) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 3.932794ms)
Feb 27 16:03:50.988: INFO: (8) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.707007ms)
Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.805186ms)
Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 6.116911ms)
Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.528817ms)
Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.500634ms)
Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.617981ms)
Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 6.854798ms)
Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.210272ms)
Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.474451ms)
Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.788973ms)
Feb 27 16:03:50.992: INFO: (8) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.079501ms)
Feb 27 16:03:50.992: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.049934ms)
Feb 27 16:03:50.992: INFO: (8) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.593235ms)
Feb 27 16:03:50.993: INFO: (8) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.768793ms)
Feb 27 16:03:50.996: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.32099ms)
Feb 27 16:03:50.996: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 3.731124ms)
Feb 27 16:03:50.997: INFO: (9) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.248461ms)
Feb 27 16:03:50.997: INFO: (9) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.543371ms)
Feb 27 16:03:50.998: INFO: (9) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.054406ms)
Feb 27 16:03:50.999: INFO: (9) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.230658ms)
Feb 27 16:03:50.999: INFO: (9) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 6.458125ms)
Feb 27 16:03:51.000: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.688111ms)
Feb 27 16:03:51.000: INFO: (9) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.395267ms)
Feb 27 16:03:51.001: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.735763ms)
Feb 27 16:03:51.001: INFO: (9) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.847119ms)
Feb 27 16:03:51.001: INFO: (9) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.402006ms)
Feb 27 16:03:51.002: INFO: (9) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.082372ms)
Feb 27 16:03:51.003: INFO: (9) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.711809ms)
Feb 27 16:03:51.003: INFO: (9) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 10.137262ms)
Feb 27 16:03:51.003: INFO: (9) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 10.179056ms)
Feb 27 16:03:51.009: INFO: (10) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.453654ms)
Feb 27 16:03:51.009: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.831252ms)
Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.335598ms)
Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.384529ms)
Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.415731ms)
Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.944489ms)
Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.094102ms)
Feb 27 16:03:51.012: INFO: (10) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.517232ms)
Feb 27 16:03:51.012: INFO: (10) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 8.710859ms)
Feb 27 16:03:51.012: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.839508ms)
Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.244649ms)
Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 9.432105ms)
Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.391541ms)
Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 9.50933ms)
Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 9.727798ms)
Feb 27 16:03:51.014: INFO: (10) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.948104ms)
Feb 27 16:03:51.017: INFO: (11) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 3.71172ms)
Feb 27 16:03:51.019: INFO: (11) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.039562ms)
Feb 27 16:03:51.020: INFO: (11) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.103144ms)
Feb 27 16:03:51.021: INFO: (11) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.674436ms)
Feb 27 16:03:51.021: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.894118ms)
Feb 27 16:03:51.021: INFO: (11) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.896096ms)
Feb 27 16:03:51.022: INFO: (11) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.120888ms)
Feb 27 16:03:51.022: INFO: (11) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.577579ms)
Feb 27 16:03:51.022: INFO: (11) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.547452ms)
Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 8.913026ms)
Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.236176ms)
Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 9.253486ms)
Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.602652ms)
Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 9.481243ms)
Feb 27 16:03:51.024: INFO: (11) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.838488ms)
Feb 27 16:03:51.024: INFO: (11) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 10.054392ms)
Feb 27 16:03:51.028: INFO: (12) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 4.13138ms)
Feb 27 16:03:51.029: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 4.761681ms)
Feb 27 16:03:51.029: INFO: (12) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.985091ms)
Feb 27 16:03:51.030: INFO: (12) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.128017ms)
Feb 27 16:03:51.031: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.521782ms)
Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.480093ms)
Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.81411ms)
Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.904063ms)
Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.271785ms)
Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.597053ms)
Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.556149ms)
Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 8.617705ms)
Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.597826ms)
Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.974472ms)
Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 9.116952ms)
Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 8.903775ms)
Feb 27 16:03:51.038: INFO: (13) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.123402ms)
Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 6.213711ms)
Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 6.400303ms)
Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.083589ms)
Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.844396ms)
Feb 27 16:03:51.041: INFO: (13) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.210153ms)
Feb 27 16:03:51.041: INFO: (13) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 8.092932ms)
Feb 27 16:03:51.041: INFO: (13) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.036662ms)
Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.489636ms)
Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 8.610188ms)
Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.577683ms)
Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.891642ms)
Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 9.361635ms)
Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 9.573169ms)
Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.825217ms)
Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.573228ms)
Feb 27 16:03:51.047: INFO: (14) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.786453ms)
Feb 27 16:03:51.049: INFO: (14) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 5.339727ms)
Feb 27 16:03:51.049: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 5.638224ms)
Feb 27 16:03:51.050: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.855466ms)
Feb 27 16:03:51.051: INFO: (14) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 7.494258ms)
Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.541801ms)
Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.2299ms)
Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 8.667735ms)
Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 8.679398ms)
Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.009098ms)
Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.474086ms)
Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.387402ms)
Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.648617ms)
Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.887796ms)
Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 10.255443ms)
Feb 27 16:03:51.054: INFO: (14) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 10.32537ms)
Feb 27 16:03:51.058: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.85537ms)
Feb 27 16:03:51.058: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.272656ms)
Feb 27 16:03:51.059: INFO: (15) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.905675ms)
Feb 27 16:03:51.059: INFO: (15) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.168771ms)
Feb 27 16:03:51.060: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.59083ms)
Feb 27 16:03:51.060: INFO: (15) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.81471ms)
Feb 27 16:03:51.060: INFO: (15) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 6.411598ms)
Feb 27 16:03:51.061: INFO: (15) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 6.806217ms)
Feb 27 16:03:51.061: INFO: (15) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.672585ms)
Feb 27 16:03:51.061: INFO: (15) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 7.579615ms)
Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 8.1577ms)
Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 7.911206ms)
Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.48346ms)
Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.533181ms)
Feb 27 16:03:51.063: INFO: (15) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.70146ms)
Feb 27 16:03:51.064: INFO: (15) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 10.233097ms)
Feb 27 16:03:51.068: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 3.707809ms)
Feb 27 16:03:51.068: INFO: (16) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 4.028626ms)
Feb 27 16:03:51.070: INFO: (16) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.336249ms)
Feb 27 16:03:51.070: INFO: (16) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.570424ms)
Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 6.220578ms)
Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.16896ms)
Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.325105ms)
Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.558283ms)
Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 7.067762ms)
Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.443839ms)
Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.518202ms)
Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.7412ms)
Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.956628ms)
Feb 27 16:03:51.073: INFO: (16) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.062479ms)
Feb 27 16:03:51.073: INFO: (16) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.852302ms)
Feb 27 16:03:51.074: INFO: (16) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.861797ms)
Feb 27 16:03:51.077: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 3.752544ms)
Feb 27 16:03:51.079: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.804041ms)
Feb 27 16:03:51.079: INFO: (17) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.311712ms)
Feb 27 16:03:51.080: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.783002ms)
Feb 27 16:03:51.081: INFO: (17) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.419705ms)
Feb 27 16:03:51.081: INFO: (17) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.803164ms)
Feb 27 16:03:51.081: INFO: (17) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.75188ms)
Feb 27 16:03:51.082: INFO: (17) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 8.131503ms)
Feb 27 16:03:51.082: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.256ms)
Feb 27 16:03:51.082: INFO: (17) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.493766ms)
Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 8.62577ms)
Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.842695ms)
Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.939581ms)
Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.153905ms)
Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.425602ms)
Feb 27 16:03:51.084: INFO: (17) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.455142ms)
Feb 27 16:03:51.087: INFO: (18) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 3.359553ms)
Feb 27 16:03:51.088: INFO: (18) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.114553ms)
Feb 27 16:03:51.088: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.521595ms)
Feb 27 16:03:51.089: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.873757ms)
Feb 27 16:03:51.089: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.458882ms)
Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.866355ms)
Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.25577ms)
Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.599254ms)
Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 6.696308ms)
Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.910772ms)
Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 7.014538ms)
Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 7.00252ms)
Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.422722ms)
Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.747822ms)
Feb 27 16:03:51.092: INFO: (18) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.266524ms)
Feb 27 16:03:51.093: INFO: (18) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.144406ms)
Feb 27 16:03:51.096: INFO: (19) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 3.715273ms)
Feb 27 16:03:51.097: INFO: (19) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.002918ms)
Feb 27 16:03:51.098: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.937316ms)
Feb 27 16:03:51.099: INFO: (19) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.68669ms)
Feb 27 16:03:51.099: INFO: (19) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.023143ms)
Feb 27 16:03:51.099: INFO: (19) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.835173ms)
Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 6.981555ms)
Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.771593ms)
Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 6.735781ms)
Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.888984ms)
Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.15926ms)
Feb 27 16:03:51.101: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 7.588801ms)
Feb 27 16:03:51.101: INFO: (19) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.714683ms)
Feb 27 16:03:51.101: INFO: (19) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 7.729975ms)
Feb 27 16:03:51.102: INFO: (19) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.697563ms)
Feb 27 16:03:51.102: INFO: (19) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.515523ms)
STEP: deleting ReplicationController proxy-service-rbkx7 in namespace proxy-9208, will wait for the garbage collector to delete the pods 02/27/23 16:03:51.103
Feb 27 16:03:51.164: INFO: Deleting ReplicationController proxy-service-rbkx7 took: 7.10364ms
Feb 27 16:03:51.264: INFO: Terminating ReplicationController proxy-service-rbkx7 pods took: 100.588216ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 27 16:03:53.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9208" for this suite. 02/27/23 16:03:53.07
------------------------------
• [SLOW TEST] [5.274 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:03:47.801
    Feb 27 16:03:47.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename proxy 02/27/23 16:03:47.802
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:47.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:47.823
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/27/23 16:03:47.837
    STEP: creating replication controller proxy-service-rbkx7 in namespace proxy-9208 02/27/23 16:03:47.837
    I0227 16:03:47.845792      19 runners.go:193] Created replication controller with name: proxy-service-rbkx7, namespace: proxy-9208, replica count: 1
    I0227 16:03:48.896974      19 runners.go:193] proxy-service-rbkx7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0227 16:03:49.897334      19 runners.go:193] proxy-service-rbkx7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0227 16:03:50.897465      19 runners.go:193] proxy-service-rbkx7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 16:03:50.901: INFO: setup took 3.075649503s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/27/23 16:03:50.901
    Feb 27 16:03:50.906: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.703601ms)
    Feb 27 16:03:50.907: INFO: (0) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.530671ms)
    Feb 27 16:03:50.908: INFO: (0) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.561967ms)
    Feb 27 16:03:50.908: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.615246ms)
    Feb 27 16:03:50.909: INFO: (0) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.966258ms)
    Feb 27 16:03:50.911: INFO: (0) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.628427ms)
    Feb 27 16:03:50.911: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.424396ms)
    Feb 27 16:03:50.911: INFO: (0) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.62786ms)
    Feb 27 16:03:50.914: INFO: (0) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 11.970215ms)
    Feb 27 16:03:50.915: INFO: (0) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 12.959117ms)
    Feb 27 16:03:50.915: INFO: (0) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 13.712167ms)
    Feb 27 16:03:50.915: INFO: (0) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 13.524527ms)
    Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 13.842848ms)
    Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 14.046193ms)
    Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 14.137547ms)
    Feb 27 16:03:50.916: INFO: (0) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 14.157873ms)
    Feb 27 16:03:50.921: INFO: (1) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 4.438083ms)
    Feb 27 16:03:50.921: INFO: (1) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 4.444917ms)
    Feb 27 16:03:50.922: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.882163ms)
    Feb 27 16:03:50.922: INFO: (1) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.097631ms)
    Feb 27 16:03:50.922: INFO: (1) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 6.302032ms)
    Feb 27 16:03:50.923: INFO: (1) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.065966ms)
    Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.533218ms)
    Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.422443ms)
    Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.611221ms)
    Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.928644ms)
    Feb 27 16:03:50.924: INFO: (1) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.072717ms)
    Feb 27 16:03:50.925: INFO: (1) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.378239ms)
    Feb 27 16:03:50.925: INFO: (1) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.68684ms)
    Feb 27 16:03:50.925: INFO: (1) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.93257ms)
    Feb 27 16:03:50.928: INFO: (1) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 11.252915ms)
    Feb 27 16:03:50.928: INFO: (1) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 11.253241ms)
    Feb 27 16:03:50.931: INFO: (2) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.826665ms)
    Feb 27 16:03:50.933: INFO: (2) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.021965ms)
    Feb 27 16:03:50.933: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 4.94135ms)
    Feb 27 16:03:50.934: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.015594ms)
    Feb 27 16:03:50.934: INFO: (2) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.850236ms)
    Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.778385ms)
    Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.052639ms)
    Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.33812ms)
    Feb 27 16:03:50.935: INFO: (2) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.564231ms)
    Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.812289ms)
    Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.241718ms)
    Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 7.918549ms)
    Feb 27 16:03:50.936: INFO: (2) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.226574ms)
    Feb 27 16:03:50.937: INFO: (2) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.598296ms)
    Feb 27 16:03:50.937: INFO: (2) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.095134ms)
    Feb 27 16:03:50.937: INFO: (2) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 8.916112ms)
    Feb 27 16:03:50.941: INFO: (3) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.017765ms)
    Feb 27 16:03:50.942: INFO: (3) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 4.741861ms)
    Feb 27 16:03:50.942: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 4.577163ms)
    Feb 27 16:03:50.943: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 5.628775ms)
    Feb 27 16:03:50.943: INFO: (3) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.630095ms)
    Feb 27 16:03:50.943: INFO: (3) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.869647ms)
    Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 6.826388ms)
    Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.916143ms)
    Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.123367ms)
    Feb 27 16:03:50.944: INFO: (3) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.252219ms)
    Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.522042ms)
    Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.845282ms)
    Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.083858ms)
    Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.217584ms)
    Feb 27 16:03:50.945: INFO: (3) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.29759ms)
    Feb 27 16:03:50.946: INFO: (3) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.896446ms)
    Feb 27 16:03:50.950: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 3.952974ms)
    Feb 27 16:03:50.950: INFO: (4) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 3.913522ms)
    Feb 27 16:03:50.951: INFO: (4) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.469601ms)
    Feb 27 16:03:50.951: INFO: (4) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.496983ms)
    Feb 27 16:03:50.951: INFO: (4) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.123949ms)
    Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 5.362139ms)
    Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.490548ms)
    Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.91119ms)
    Feb 27 16:03:50.952: INFO: (4) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.030911ms)
    Feb 27 16:03:50.953: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.972462ms)
    Feb 27 16:03:50.953: INFO: (4) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.010957ms)
    Feb 27 16:03:50.954: INFO: (4) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.226628ms)
    Feb 27 16:03:50.954: INFO: (4) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.464054ms)
    Feb 27 16:03:50.954: INFO: (4) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.848793ms)
    Feb 27 16:03:50.955: INFO: (4) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.500476ms)
    Feb 27 16:03:50.955: INFO: (4) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.605734ms)
    Feb 27 16:03:50.960: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.203157ms)
    Feb 27 16:03:50.961: INFO: (5) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.430233ms)
    Feb 27 16:03:50.961: INFO: (5) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.274809ms)
    Feb 27 16:03:50.961: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 5.526244ms)
    Feb 27 16:03:50.962: INFO: (5) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 6.592959ms)
    Feb 27 16:03:50.962: INFO: (5) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.862068ms)
    Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 7.347218ms)
    Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 7.527458ms)
    Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 7.786966ms)
    Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.69079ms)
    Feb 27 16:03:50.963: INFO: (5) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 7.925095ms)
    Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 8.763406ms)
    Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.025196ms)
    Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.939494ms)
    Feb 27 16:03:50.964: INFO: (5) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.149427ms)
    Feb 27 16:03:50.965: INFO: (5) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.419187ms)
    Feb 27 16:03:50.969: INFO: (6) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.2373ms)
    Feb 27 16:03:50.969: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.526651ms)
    Feb 27 16:03:50.970: INFO: (6) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.647191ms)
    Feb 27 16:03:50.971: INFO: (6) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.5529ms)
    Feb 27 16:03:50.972: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.76384ms)
    Feb 27 16:03:50.972: INFO: (6) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.645097ms)
    Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.422644ms)
    Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 7.91101ms)
    Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 8.155859ms)
    Feb 27 16:03:50.973: INFO: (6) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.674018ms)
    Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 8.767297ms)
    Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.12168ms)
    Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 9.037049ms)
    Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.300903ms)
    Feb 27 16:03:50.974: INFO: (6) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.665459ms)
    Feb 27 16:03:50.975: INFO: (6) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.725987ms)
    Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.547929ms)
    Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.628142ms)
    Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 4.919618ms)
    Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.299259ms)
    Feb 27 16:03:50.980: INFO: (7) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 5.343251ms)
    Feb 27 16:03:50.981: INFO: (7) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 6.395397ms)
    Feb 27 16:03:50.981: INFO: (7) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.106446ms)
    Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.026233ms)
    Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.22785ms)
    Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.166579ms)
    Feb 27 16:03:50.982: INFO: (7) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 7.418362ms)
    Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.854879ms)
    Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 8.034651ms)
    Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.14228ms)
    Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.088375ms)
    Feb 27 16:03:50.983: INFO: (7) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.325421ms)
    Feb 27 16:03:50.987: INFO: (8) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 3.545557ms)
    Feb 27 16:03:50.988: INFO: (8) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 3.932794ms)
    Feb 27 16:03:50.988: INFO: (8) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.707007ms)
    Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.805186ms)
    Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 6.116911ms)
    Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.528817ms)
    Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.500634ms)
    Feb 27 16:03:50.990: INFO: (8) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.617981ms)
    Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 6.854798ms)
    Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.210272ms)
    Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 7.474451ms)
    Feb 27 16:03:50.991: INFO: (8) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.788973ms)
    Feb 27 16:03:50.992: INFO: (8) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.079501ms)
    Feb 27 16:03:50.992: INFO: (8) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.049934ms)
    Feb 27 16:03:50.992: INFO: (8) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.593235ms)
    Feb 27 16:03:50.993: INFO: (8) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.768793ms)
    Feb 27 16:03:50.996: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.32099ms)
    Feb 27 16:03:50.996: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 3.731124ms)
    Feb 27 16:03:50.997: INFO: (9) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.248461ms)
    Feb 27 16:03:50.997: INFO: (9) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.543371ms)
    Feb 27 16:03:50.998: INFO: (9) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.054406ms)
    Feb 27 16:03:50.999: INFO: (9) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.230658ms)
    Feb 27 16:03:50.999: INFO: (9) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 6.458125ms)
    Feb 27 16:03:51.000: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.688111ms)
    Feb 27 16:03:51.000: INFO: (9) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.395267ms)
    Feb 27 16:03:51.001: INFO: (9) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.735763ms)
    Feb 27 16:03:51.001: INFO: (9) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.847119ms)
    Feb 27 16:03:51.001: INFO: (9) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.402006ms)
    Feb 27 16:03:51.002: INFO: (9) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.082372ms)
    Feb 27 16:03:51.003: INFO: (9) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.711809ms)
    Feb 27 16:03:51.003: INFO: (9) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 10.137262ms)
    Feb 27 16:03:51.003: INFO: (9) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 10.179056ms)
    Feb 27 16:03:51.009: INFO: (10) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.453654ms)
    Feb 27 16:03:51.009: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.831252ms)
    Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.335598ms)
    Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.384529ms)
    Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.415731ms)
    Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.944489ms)
    Feb 27 16:03:51.011: INFO: (10) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.094102ms)
    Feb 27 16:03:51.012: INFO: (10) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.517232ms)
    Feb 27 16:03:51.012: INFO: (10) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 8.710859ms)
    Feb 27 16:03:51.012: INFO: (10) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.839508ms)
    Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.244649ms)
    Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 9.432105ms)
    Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.391541ms)
    Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 9.50933ms)
    Feb 27 16:03:51.013: INFO: (10) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 9.727798ms)
    Feb 27 16:03:51.014: INFO: (10) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.948104ms)
    Feb 27 16:03:51.017: INFO: (11) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 3.71172ms)
    Feb 27 16:03:51.019: INFO: (11) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.039562ms)
    Feb 27 16:03:51.020: INFO: (11) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.103144ms)
    Feb 27 16:03:51.021: INFO: (11) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.674436ms)
    Feb 27 16:03:51.021: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.894118ms)
    Feb 27 16:03:51.021: INFO: (11) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.896096ms)
    Feb 27 16:03:51.022: INFO: (11) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.120888ms)
    Feb 27 16:03:51.022: INFO: (11) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.577579ms)
    Feb 27 16:03:51.022: INFO: (11) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.547452ms)
    Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 8.913026ms)
    Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.236176ms)
    Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 9.253486ms)
    Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.602652ms)
    Feb 27 16:03:51.023: INFO: (11) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 9.481243ms)
    Feb 27 16:03:51.024: INFO: (11) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.838488ms)
    Feb 27 16:03:51.024: INFO: (11) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 10.054392ms)
    Feb 27 16:03:51.028: INFO: (12) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 4.13138ms)
    Feb 27 16:03:51.029: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 4.761681ms)
    Feb 27 16:03:51.029: INFO: (12) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.985091ms)
    Feb 27 16:03:51.030: INFO: (12) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.128017ms)
    Feb 27 16:03:51.031: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.521782ms)
    Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.480093ms)
    Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.81411ms)
    Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 7.904063ms)
    Feb 27 16:03:51.032: INFO: (12) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.271785ms)
    Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.597053ms)
    Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.556149ms)
    Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 8.617705ms)
    Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.597826ms)
    Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.974472ms)
    Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 9.116952ms)
    Feb 27 16:03:51.033: INFO: (12) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 8.903775ms)
    Feb 27 16:03:51.038: INFO: (13) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.123402ms)
    Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 6.213711ms)
    Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 6.400303ms)
    Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.083589ms)
    Feb 27 16:03:51.040: INFO: (13) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.844396ms)
    Feb 27 16:03:51.041: INFO: (13) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.210153ms)
    Feb 27 16:03:51.041: INFO: (13) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 8.092932ms)
    Feb 27 16:03:51.041: INFO: (13) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.036662ms)
    Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.489636ms)
    Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 8.610188ms)
    Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.577683ms)
    Feb 27 16:03:51.042: INFO: (13) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.891642ms)
    Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 9.361635ms)
    Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 9.573169ms)
    Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.825217ms)
    Feb 27 16:03:51.043: INFO: (13) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.573228ms)
    Feb 27 16:03:51.047: INFO: (14) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.786453ms)
    Feb 27 16:03:51.049: INFO: (14) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 5.339727ms)
    Feb 27 16:03:51.049: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 5.638224ms)
    Feb 27 16:03:51.050: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.855466ms)
    Feb 27 16:03:51.051: INFO: (14) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 7.494258ms)
    Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 8.541801ms)
    Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.2299ms)
    Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 8.667735ms)
    Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 8.679398ms)
    Feb 27 16:03:51.052: INFO: (14) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 9.009098ms)
    Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.474086ms)
    Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.387402ms)
    Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.648617ms)
    Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 9.887796ms)
    Feb 27 16:03:51.053: INFO: (14) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 10.255443ms)
    Feb 27 16:03:51.054: INFO: (14) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 10.32537ms)
    Feb 27 16:03:51.058: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 3.85537ms)
    Feb 27 16:03:51.058: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.272656ms)
    Feb 27 16:03:51.059: INFO: (15) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.905675ms)
    Feb 27 16:03:51.059: INFO: (15) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.168771ms)
    Feb 27 16:03:51.060: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 5.59083ms)
    Feb 27 16:03:51.060: INFO: (15) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.81471ms)
    Feb 27 16:03:51.060: INFO: (15) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 6.411598ms)
    Feb 27 16:03:51.061: INFO: (15) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 6.806217ms)
    Feb 27 16:03:51.061: INFO: (15) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.672585ms)
    Feb 27 16:03:51.061: INFO: (15) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 7.579615ms)
    Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 8.1577ms)
    Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 7.911206ms)
    Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.48346ms)
    Feb 27 16:03:51.062: INFO: (15) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.533181ms)
    Feb 27 16:03:51.063: INFO: (15) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.70146ms)
    Feb 27 16:03:51.064: INFO: (15) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 10.233097ms)
    Feb 27 16:03:51.068: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 3.707809ms)
    Feb 27 16:03:51.068: INFO: (16) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 4.028626ms)
    Feb 27 16:03:51.070: INFO: (16) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.336249ms)
    Feb 27 16:03:51.070: INFO: (16) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.570424ms)
    Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 6.220578ms)
    Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.16896ms)
    Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.325105ms)
    Feb 27 16:03:51.071: INFO: (16) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.558283ms)
    Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 7.067762ms)
    Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.443839ms)
    Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 7.518202ms)
    Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 7.7412ms)
    Feb 27 16:03:51.072: INFO: (16) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.956628ms)
    Feb 27 16:03:51.073: INFO: (16) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.062479ms)
    Feb 27 16:03:51.073: INFO: (16) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.852302ms)
    Feb 27 16:03:51.074: INFO: (16) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 8.861797ms)
    Feb 27 16:03:51.077: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 3.752544ms)
    Feb 27 16:03:51.079: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.804041ms)
    Feb 27 16:03:51.079: INFO: (17) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 5.311712ms)
    Feb 27 16:03:51.080: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.783002ms)
    Feb 27 16:03:51.081: INFO: (17) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 6.419705ms)
    Feb 27 16:03:51.081: INFO: (17) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.803164ms)
    Feb 27 16:03:51.081: INFO: (17) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 7.75188ms)
    Feb 27 16:03:51.082: INFO: (17) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 8.131503ms)
    Feb 27 16:03:51.082: INFO: (17) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 8.256ms)
    Feb 27 16:03:51.082: INFO: (17) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 8.493766ms)
    Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 8.62577ms)
    Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 8.842695ms)
    Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.939581ms)
    Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 9.153905ms)
    Feb 27 16:03:51.083: INFO: (17) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 9.425602ms)
    Feb 27 16:03:51.084: INFO: (17) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.455142ms)
    Feb 27 16:03:51.087: INFO: (18) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 3.359553ms)
    Feb 27 16:03:51.088: INFO: (18) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.114553ms)
    Feb 27 16:03:51.088: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.521595ms)
    Feb 27 16:03:51.089: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 4.873757ms)
    Feb 27 16:03:51.089: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.458882ms)
    Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 5.866355ms)
    Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 6.25577ms)
    Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.599254ms)
    Feb 27 16:03:51.090: INFO: (18) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 6.696308ms)
    Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 6.910772ms)
    Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 7.014538ms)
    Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 7.00252ms)
    Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 7.422722ms)
    Feb 27 16:03:51.091: INFO: (18) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.747822ms)
    Feb 27 16:03:51.092: INFO: (18) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 8.266524ms)
    Feb 27 16:03:51.093: INFO: (18) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 9.144406ms)
    Feb 27 16:03:51.096: INFO: (19) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:460/proxy/: tls baz (200; 3.715273ms)
    Feb 27 16:03:51.097: INFO: (19) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 4.002918ms)
    Feb 27 16:03:51.098: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 4.937316ms)
    Feb 27 16:03:51.099: INFO: (19) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">... (200; 5.68669ms)
    Feb 27 16:03:51.099: INFO: (19) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:443/proxy/tlsrewritem... (200; 6.023143ms)
    Feb 27 16:03:51.099: INFO: (19) /api/v1/namespaces/proxy-9208/pods/http:proxy-service-rbkx7-g9cjf:162/proxy/: bar (200; 5.835173ms)
    Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname2/proxy/: bar (200; 6.981555ms)
    Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:160/proxy/: foo (200; 6.771593ms)
    Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf/proxy/rewriteme">test</a> (200; 6.735781ms)
    Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/pods/https:proxy-service-rbkx7-g9cjf:462/proxy/: tls qux (200; 6.888984ms)
    Feb 27 16:03:51.100: INFO: (19) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname2/proxy/: tls qux (200; 7.15926ms)
    Feb 27 16:03:51.101: INFO: (19) /api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/: <a href="/api/v1/namespaces/proxy-9208/pods/proxy-service-rbkx7-g9cjf:1080/proxy/rewriteme">test<... (200; 7.588801ms)
    Feb 27 16:03:51.101: INFO: (19) /api/v1/namespaces/proxy-9208/services/http:proxy-service-rbkx7:portname1/proxy/: foo (200; 7.714683ms)
    Feb 27 16:03:51.101: INFO: (19) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname2/proxy/: bar (200; 7.729975ms)
    Feb 27 16:03:51.102: INFO: (19) /api/v1/namespaces/proxy-9208/services/proxy-service-rbkx7:portname1/proxy/: foo (200; 8.697563ms)
    Feb 27 16:03:51.102: INFO: (19) /api/v1/namespaces/proxy-9208/services/https:proxy-service-rbkx7:tlsportname1/proxy/: tls baz (200; 9.515523ms)
    STEP: deleting ReplicationController proxy-service-rbkx7 in namespace proxy-9208, will wait for the garbage collector to delete the pods 02/27/23 16:03:51.103
    Feb 27 16:03:51.164: INFO: Deleting ReplicationController proxy-service-rbkx7 took: 7.10364ms
    Feb 27 16:03:51.264: INFO: Terminating ReplicationController proxy-service-rbkx7 pods took: 100.588216ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:03:53.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9208" for this suite. 02/27/23 16:03:53.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:03:53.076
Feb 27 16:03:53.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 16:03:53.077
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:53.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:53.095
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 02/27/23 16:03:53.099
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 16:03:53.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4767" for this suite. 02/27/23 16:03:53.105
------------------------------
• [0.035 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:03:53.076
    Feb 27 16:03:53.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 16:03:53.077
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:53.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:53.095
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 02/27/23 16:03:53.099
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:03:53.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4767" for this suite. 02/27/23 16:03:53.105
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:03:53.111
Feb 27 16:03:53.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename gc 02/27/23 16:03:53.112
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:53.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:53.13
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb 27 16:03:53.162: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d79cc1b5-42f0-4291-b878-c227fa132036", Controller:(*bool)(0xc00522af56), BlockOwnerDeletion:(*bool)(0xc00522af57)}}
Feb 27 16:03:53.170: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"85d79bce-7167-492b-a061-4379224ce5c2", Controller:(*bool)(0xc00522b196), BlockOwnerDeletion:(*bool)(0xc00522b197)}}
Feb 27 16:03:53.180: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"af8cfc7e-c5d4-4e7e-a36b-f56992e19f30", Controller:(*bool)(0xc00531c17a), BlockOwnerDeletion:(*bool)(0xc00531c17b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 16:03:58.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5553" for this suite. 02/27/23 16:03:58.198
------------------------------
• [SLOW TEST] [5.094 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:03:53.111
    Feb 27 16:03:53.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename gc 02/27/23 16:03:53.112
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:53.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:53.13
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb 27 16:03:53.162: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d79cc1b5-42f0-4291-b878-c227fa132036", Controller:(*bool)(0xc00522af56), BlockOwnerDeletion:(*bool)(0xc00522af57)}}
    Feb 27 16:03:53.170: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"85d79bce-7167-492b-a061-4379224ce5c2", Controller:(*bool)(0xc00522b196), BlockOwnerDeletion:(*bool)(0xc00522b197)}}
    Feb 27 16:03:53.180: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"af8cfc7e-c5d4-4e7e-a36b-f56992e19f30", Controller:(*bool)(0xc00531c17a), BlockOwnerDeletion:(*bool)(0xc00531c17b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:03:58.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5553" for this suite. 02/27/23 16:03:58.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:03:58.206
Feb 27 16:03:58.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 16:03:58.206
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:58.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:58.235
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb 27 16:03:58.246: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 27 16:04:03.252: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 16:04:03.252
Feb 27 16:04:03.252: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/27/23 16:04:03.263
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 16:04:03.274: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5946  75be3ff1-cbb8-4d29-832c-3452742ef64f 23949 1 2023-02-27 16:04:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055479b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 27 16:04:03.278: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5946  c11d3afc-de56-4344-8567-12707e9e8874 23951 1 2023-02-27 16:04:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 75be3ff1-cbb8-4d29-832c-3452742ef64f 0xc0053af567 0xc0053af568}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75be3ff1-cbb8-4d29-832c-3452742ef64f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053af5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 16:04:03.278: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 27 16:04:03.278: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5946  789cdc7c-8b95-4b91-8281-7ad1ba60951d 23950 1 2023-02-27 16:03:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 75be3ff1-cbb8-4d29-832c-3452742ef64f 0xc0053af437 0xc0053af438}] [] [{e2e.test Update apps/v1 2023-02-27 16:03:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:03:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"75be3ff1-cbb8-4d29-832c-3452742ef64f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0053af4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 16:04:03.292: INFO: Pod "test-cleanup-controller-ctp66" is available:
&Pod{ObjectMeta:{test-cleanup-controller-ctp66 test-cleanup-controller- deployment-5946  8bf795ad-0af0-4a4a-a277-0890da9c5f78 23932 0 2023-02-27 16:03:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 789cdc7c-8b95-4b91-8281-7ad1ba60951d 0xc0053afba7 0xc0053afba8}] [] [{kube-controller-manager Update v1 2023-02-27 16:03:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"789cdc7c-8b95-4b91-8281-7ad1ba60951d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:03:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vf5sh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vf5sh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.136,StartTime:2023-02-27 16:03:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:03:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://14dedb07c08451a0393f6d617f83dea7a69c012a33abf4cb0582ad043df5bbe1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:04:03.292: INFO: Pod "test-cleanup-deployment-7698ff6f6b-52ft5" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-52ft5 test-cleanup-deployment-7698ff6f6b- deployment-5946  058e441b-3fce-4ea8-9139-c2d3b3d58639 23953 0 2023-02-27 16:04:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b c11d3afc-de56-4344-8567-12707e9e8874 0xc0053afda7 0xc0053afda8}] [] [{kube-controller-manager Update v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c11d3afc-de56-4344-8567-12707e9e8874\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cw2bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cw2bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:03.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5946" for this suite. 02/27/23 16:04:03.297
------------------------------
• [SLOW TEST] [5.099 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:03:58.206
    Feb 27 16:03:58.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 16:03:58.206
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:03:58.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:03:58.235
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb 27 16:03:58.246: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb 27 16:04:03.252: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 16:04:03.252
    Feb 27 16:04:03.252: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/27/23 16:04:03.263
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 16:04:03.274: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5946  75be3ff1-cbb8-4d29-832c-3452742ef64f 23949 1 2023-02-27 16:04:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055479b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 27 16:04:03.278: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-5946  c11d3afc-de56-4344-8567-12707e9e8874 23951 1 2023-02-27 16:04:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 75be3ff1-cbb8-4d29-832c-3452742ef64f 0xc0053af567 0xc0053af568}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75be3ff1-cbb8-4d29-832c-3452742ef64f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053af5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 16:04:03.278: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Feb 27 16:04:03.278: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5946  789cdc7c-8b95-4b91-8281-7ad1ba60951d 23950 1 2023-02-27 16:03:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 75be3ff1-cbb8-4d29-832c-3452742ef64f 0xc0053af437 0xc0053af438}] [] [{e2e.test Update apps/v1 2023-02-27 16:03:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:03:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"75be3ff1-cbb8-4d29-832c-3452742ef64f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0053af4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 16:04:03.292: INFO: Pod "test-cleanup-controller-ctp66" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-ctp66 test-cleanup-controller- deployment-5946  8bf795ad-0af0-4a4a-a277-0890da9c5f78 23932 0 2023-02-27 16:03:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 789cdc7c-8b95-4b91-8281-7ad1ba60951d 0xc0053afba7 0xc0053afba8}] [] [{kube-controller-manager Update v1 2023-02-27 16:03:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"789cdc7c-8b95-4b91-8281-7ad1ba60951d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:03:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vf5sh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vf5sh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:03:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.136,StartTime:2023-02-27 16:03:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:03:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://14dedb07c08451a0393f6d617f83dea7a69c012a33abf4cb0582ad043df5bbe1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:04:03.292: INFO: Pod "test-cleanup-deployment-7698ff6f6b-52ft5" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-52ft5 test-cleanup-deployment-7698ff6f6b- deployment-5946  058e441b-3fce-4ea8-9139-c2d3b3d58639 23953 0 2023-02-27 16:04:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b c11d3afc-de56-4344-8567-12707e9e8874 0xc0053afda7 0xc0053afda8}] [] [{kube-controller-manager Update v1 2023-02-27 16:04:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c11d3afc-de56-4344-8567-12707e9e8874\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cw2bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cw2bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:03.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5946" for this suite. 02/27/23 16:04:03.297
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:03.305
Feb 27 16:04:03.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:04:03.306
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:03.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:03.329
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-659d499e-fea0-464e-b0ae-a58e00b8e2e9 02/27/23 16:04:03.335
STEP: Creating the pod 02/27/23 16:04:03.34
Feb 27 16:04:03.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c" in namespace "projected-5241" to be "running and ready"
Feb 27 16:04:03.354: INFO: Pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.815149ms
Feb 27 16:04:03.354: INFO: The phase of Pod pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:04:05.358: INFO: Pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009520948s
Feb 27 16:04:05.358: INFO: The phase of Pod pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c is Running (Ready = true)
Feb 27 16:04:05.358: INFO: Pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-659d499e-fea0-464e-b0ae-a58e00b8e2e9 02/27/23 16:04:05.375
STEP: waiting to observe update in volume 02/27/23 16:04:05.381
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:07.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5241" for this suite. 02/27/23 16:04:07.399
------------------------------
• [4.102 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:03.305
    Feb 27 16:04:03.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:04:03.306
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:03.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:03.329
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-659d499e-fea0-464e-b0ae-a58e00b8e2e9 02/27/23 16:04:03.335
    STEP: Creating the pod 02/27/23 16:04:03.34
    Feb 27 16:04:03.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c" in namespace "projected-5241" to be "running and ready"
    Feb 27 16:04:03.354: INFO: Pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.815149ms
    Feb 27 16:04:03.354: INFO: The phase of Pod pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:04:05.358: INFO: Pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009520948s
    Feb 27 16:04:05.358: INFO: The phase of Pod pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c is Running (Ready = true)
    Feb 27 16:04:05.358: INFO: Pod "pod-projected-configmaps-06ceb9e1-3bad-4151-8089-4a6882a24d0c" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-659d499e-fea0-464e-b0ae-a58e00b8e2e9 02/27/23 16:04:05.375
    STEP: waiting to observe update in volume 02/27/23 16:04:05.381
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:07.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5241" for this suite. 02/27/23 16:04:07.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:07.407
Feb 27 16:04:07.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 16:04:07.408
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:07.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:07.427
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-5c181cb9-c24b-4209-8fae-693b82bfef43 02/27/23 16:04:07.431
STEP: Creating a pod to test consume secrets 02/27/23 16:04:07.436
Feb 27 16:04:07.443: INFO: Waiting up to 5m0s for pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3" in namespace "secrets-8270" to be "Succeeded or Failed"
Feb 27 16:04:07.449: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.430977ms
Feb 27 16:04:09.453: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010631906s
Feb 27 16:04:11.455: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01210909s
STEP: Saw pod success 02/27/23 16:04:11.455
Feb 27 16:04:11.455: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3" satisfied condition "Succeeded or Failed"
Feb 27 16:04:11.459: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 16:04:11.465
Feb 27 16:04:11.479: INFO: Waiting for pod pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3 to disappear
Feb 27 16:04:11.483: INFO: Pod pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:11.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8270" for this suite. 02/27/23 16:04:11.487
------------------------------
• [4.086 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:07.407
    Feb 27 16:04:07.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 16:04:07.408
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:07.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:07.427
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-5c181cb9-c24b-4209-8fae-693b82bfef43 02/27/23 16:04:07.431
    STEP: Creating a pod to test consume secrets 02/27/23 16:04:07.436
    Feb 27 16:04:07.443: INFO: Waiting up to 5m0s for pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3" in namespace "secrets-8270" to be "Succeeded or Failed"
    Feb 27 16:04:07.449: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.430977ms
    Feb 27 16:04:09.453: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010631906s
    Feb 27 16:04:11.455: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01210909s
    STEP: Saw pod success 02/27/23 16:04:11.455
    Feb 27 16:04:11.455: INFO: Pod "pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3" satisfied condition "Succeeded or Failed"
    Feb 27 16:04:11.459: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 16:04:11.465
    Feb 27 16:04:11.479: INFO: Waiting for pod pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3 to disappear
    Feb 27 16:04:11.483: INFO: Pod pod-secrets-3faefca4-d46c-4f5a-a579-35f998b4e6d3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:11.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8270" for this suite. 02/27/23 16:04:11.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:11.502
Feb 27 16:04:11.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename proxy 02/27/23 16:04:11.506
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:11.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:11.532
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb 27 16:04:11.536: INFO: Creating pod...
Feb 27 16:04:11.551: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7044" to be "running"
Feb 27 16:04:11.555: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.305195ms
Feb 27 16:04:13.560: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00913796s
Feb 27 16:04:13.560: INFO: Pod "agnhost" satisfied condition "running"
Feb 27 16:04:13.560: INFO: Creating service...
Feb 27 16:04:13.575: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/DELETE
Feb 27 16:04:13.582: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 16:04:13.582: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/GET
Feb 27 16:04:13.587: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 27 16:04:13.587: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/HEAD
Feb 27 16:04:13.591: INFO: http.Client request:HEAD | StatusCode:200
Feb 27 16:04:13.591: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 27 16:04:13.596: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 16:04:13.596: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/PATCH
Feb 27 16:04:13.600: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 16:04:13.600: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/POST
Feb 27 16:04:13.604: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 16:04:13.604: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/PUT
Feb 27 16:04:13.609: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 16:04:13.609: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/DELETE
Feb 27 16:04:13.614: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 16:04:13.615: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/GET
Feb 27 16:04:13.621: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 27 16:04:13.621: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/HEAD
Feb 27 16:04:13.628: INFO: http.Client request:HEAD | StatusCode:200
Feb 27 16:04:13.628: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/OPTIONS
Feb 27 16:04:13.633: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 16:04:13.633: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/PATCH
Feb 27 16:04:13.639: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 16:04:13.639: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/POST
Feb 27 16:04:13.645: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 16:04:13.645: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/PUT
Feb 27 16:04:13.650: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:13.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7044" for this suite. 02/27/23 16:04:13.654
------------------------------
• [2.159 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:11.502
    Feb 27 16:04:11.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename proxy 02/27/23 16:04:11.506
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:11.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:11.532
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb 27 16:04:11.536: INFO: Creating pod...
    Feb 27 16:04:11.551: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7044" to be "running"
    Feb 27 16:04:11.555: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.305195ms
    Feb 27 16:04:13.560: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00913796s
    Feb 27 16:04:13.560: INFO: Pod "agnhost" satisfied condition "running"
    Feb 27 16:04:13.560: INFO: Creating service...
    Feb 27 16:04:13.575: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/DELETE
    Feb 27 16:04:13.582: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 16:04:13.582: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/GET
    Feb 27 16:04:13.587: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 27 16:04:13.587: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/HEAD
    Feb 27 16:04:13.591: INFO: http.Client request:HEAD | StatusCode:200
    Feb 27 16:04:13.591: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb 27 16:04:13.596: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 16:04:13.596: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/PATCH
    Feb 27 16:04:13.600: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 16:04:13.600: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/POST
    Feb 27 16:04:13.604: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 16:04:13.604: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/pods/agnhost/proxy/some/path/with/PUT
    Feb 27 16:04:13.609: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 16:04:13.609: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/DELETE
    Feb 27 16:04:13.614: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 16:04:13.615: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/GET
    Feb 27 16:04:13.621: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 27 16:04:13.621: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/HEAD
    Feb 27 16:04:13.628: INFO: http.Client request:HEAD | StatusCode:200
    Feb 27 16:04:13.628: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/OPTIONS
    Feb 27 16:04:13.633: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 16:04:13.633: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/PATCH
    Feb 27 16:04:13.639: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 16:04:13.639: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/POST
    Feb 27 16:04:13.645: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 16:04:13.645: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7044/services/test-service/proxy/some/path/with/PUT
    Feb 27 16:04:13.650: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:13.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7044" for this suite. 02/27/23 16:04:13.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:13.661
Feb 27 16:04:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:04:13.662
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:13.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:13.686
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 02/27/23 16:04:13.69
STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:04:13.695
STEP: Creating a ResourceQuota with not terminating scope 02/27/23 16:04:15.699
STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:04:15.706
STEP: Creating a long running pod 02/27/23 16:04:17.711
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/27/23 16:04:17.724
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/27/23 16:04:19.73
STEP: Deleting the pod 02/27/23 16:04:21.735
STEP: Ensuring resource quota status released the pod usage 02/27/23 16:04:21.75
STEP: Creating a terminating pod 02/27/23 16:04:23.755
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/27/23 16:04:23.866
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/27/23 16:04:25.871
STEP: Deleting the pod 02/27/23 16:04:27.875
STEP: Ensuring resource quota status released the pod usage 02/27/23 16:04:27.891
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:29.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6163" for this suite. 02/27/23 16:04:29.901
------------------------------
• [SLOW TEST] [16.246 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:13.661
    Feb 27 16:04:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:04:13.662
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:13.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:13.686
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 02/27/23 16:04:13.69
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:04:13.695
    STEP: Creating a ResourceQuota with not terminating scope 02/27/23 16:04:15.699
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:04:15.706
    STEP: Creating a long running pod 02/27/23 16:04:17.711
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/27/23 16:04:17.724
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/27/23 16:04:19.73
    STEP: Deleting the pod 02/27/23 16:04:21.735
    STEP: Ensuring resource quota status released the pod usage 02/27/23 16:04:21.75
    STEP: Creating a terminating pod 02/27/23 16:04:23.755
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/27/23 16:04:23.866
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/27/23 16:04:25.871
    STEP: Deleting the pod 02/27/23 16:04:27.875
    STEP: Ensuring resource quota status released the pod usage 02/27/23 16:04:27.891
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:29.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6163" for this suite. 02/27/23 16:04:29.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:29.908
Feb 27 16:04:29.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:04:29.909
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:29.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:29.927
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 02/27/23 16:04:29.931
Feb 27 16:04:29.940: INFO: Waiting up to 5m0s for pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34" in namespace "downward-api-2269" to be "running and ready"
Feb 27 16:04:29.946: INFO: Pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.962118ms
Feb 27 16:04:29.946: INFO: The phase of Pod labelsupdateabcff489-36dd-41c7-8261-b8717969fe34 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:04:31.950: INFO: Pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34": Phase="Running", Reason="", readiness=true. Elapsed: 2.010209284s
Feb 27 16:04:31.950: INFO: The phase of Pod labelsupdateabcff489-36dd-41c7-8261-b8717969fe34 is Running (Ready = true)
Feb 27 16:04:31.950: INFO: Pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34" satisfied condition "running and ready"
Feb 27 16:04:32.471: INFO: Successfully updated pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:36.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2269" for this suite. 02/27/23 16:04:36.497
------------------------------
• [SLOW TEST] [6.596 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:29.908
    Feb 27 16:04:29.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:04:29.909
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:29.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:29.927
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 02/27/23 16:04:29.931
    Feb 27 16:04:29.940: INFO: Waiting up to 5m0s for pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34" in namespace "downward-api-2269" to be "running and ready"
    Feb 27 16:04:29.946: INFO: Pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.962118ms
    Feb 27 16:04:29.946: INFO: The phase of Pod labelsupdateabcff489-36dd-41c7-8261-b8717969fe34 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:04:31.950: INFO: Pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34": Phase="Running", Reason="", readiness=true. Elapsed: 2.010209284s
    Feb 27 16:04:31.950: INFO: The phase of Pod labelsupdateabcff489-36dd-41c7-8261-b8717969fe34 is Running (Ready = true)
    Feb 27 16:04:31.950: INFO: Pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34" satisfied condition "running and ready"
    Feb 27 16:04:32.471: INFO: Successfully updated pod "labelsupdateabcff489-36dd-41c7-8261-b8717969fe34"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:36.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2269" for this suite. 02/27/23 16:04:36.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:36.505
Feb 27 16:04:36.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 16:04:36.505
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:36.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:36.527
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 16:04:36.53
Feb 27 16:04:36.538: INFO: Waiting up to 5m0s for pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e" in namespace "emptydir-599" to be "Succeeded or Failed"
Feb 27 16:04:36.544: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.932051ms
Feb 27 16:04:38.549: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010703567s
Feb 27 16:04:40.548: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009565559s
STEP: Saw pod success 02/27/23 16:04:40.548
Feb 27 16:04:40.548: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e" satisfied condition "Succeeded or Failed"
Feb 27 16:04:40.552: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e container test-container: <nil>
STEP: delete the pod 02/27/23 16:04:40.558
Feb 27 16:04:40.569: INFO: Waiting for pod pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e to disappear
Feb 27 16:04:40.572: INFO: Pod pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-599" for this suite. 02/27/23 16:04:40.577
------------------------------
• [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:36.505
    Feb 27 16:04:36.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 16:04:36.505
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:36.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:36.527
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 16:04:36.53
    Feb 27 16:04:36.538: INFO: Waiting up to 5m0s for pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e" in namespace "emptydir-599" to be "Succeeded or Failed"
    Feb 27 16:04:36.544: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.932051ms
    Feb 27 16:04:38.549: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010703567s
    Feb 27 16:04:40.548: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009565559s
    STEP: Saw pod success 02/27/23 16:04:40.548
    Feb 27 16:04:40.548: INFO: Pod "pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e" satisfied condition "Succeeded or Failed"
    Feb 27 16:04:40.552: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e container test-container: <nil>
    STEP: delete the pod 02/27/23 16:04:40.558
    Feb 27 16:04:40.569: INFO: Waiting for pod pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e to disappear
    Feb 27 16:04:40.572: INFO: Pod pod-6a90a8e7-eefe-4eb8-b8bf-248c11e75f4e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-599" for this suite. 02/27/23 16:04:40.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:40.586
Feb 27 16:04:40.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:04:40.586
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:40.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:40.613
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:40.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2488" for this suite. 02/27/23 16:04:40.637
------------------------------
• [0.058 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:40.586
    Feb 27 16:04:40.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:04:40.586
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:40.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:40.613
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:40.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2488" for this suite. 02/27/23 16:04:40.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:40.646
Feb 27 16:04:40.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:04:40.647
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:40.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:40.668
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:04:40.675
Feb 27 16:04:40.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5" in namespace "downward-api-5505" to be "Succeeded or Failed"
Feb 27 16:04:40.689: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.87296ms
Feb 27 16:04:42.694: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008767999s
Feb 27 16:04:44.693: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007917775s
STEP: Saw pod success 02/27/23 16:04:44.693
Feb 27 16:04:44.693: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5" satisfied condition "Succeeded or Failed"
Feb 27 16:04:44.697: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5 container client-container: <nil>
STEP: delete the pod 02/27/23 16:04:44.704
Feb 27 16:04:44.717: INFO: Waiting for pod downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5 to disappear
Feb 27 16:04:44.721: INFO: Pod downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 16:04:44.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5505" for this suite. 02/27/23 16:04:44.725
------------------------------
• [4.086 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:40.646
    Feb 27 16:04:40.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:04:40.647
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:40.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:40.668
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:04:40.675
    Feb 27 16:04:40.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5" in namespace "downward-api-5505" to be "Succeeded or Failed"
    Feb 27 16:04:40.689: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.87296ms
    Feb 27 16:04:42.694: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008767999s
    Feb 27 16:04:44.693: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007917775s
    STEP: Saw pod success 02/27/23 16:04:44.693
    Feb 27 16:04:44.693: INFO: Pod "downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5" satisfied condition "Succeeded or Failed"
    Feb 27 16:04:44.697: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5 container client-container: <nil>
    STEP: delete the pod 02/27/23 16:04:44.704
    Feb 27 16:04:44.717: INFO: Waiting for pod downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5 to disappear
    Feb 27 16:04:44.721: INFO: Pod downwardapi-volume-6df364f5-4933-4c18-a4d9-dec729d5c8b5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:04:44.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5505" for this suite. 02/27/23 16:04:44.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:04:44.733
Feb 27 16:04:44.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sched-preemption 02/27/23 16:04:44.734
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:44.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:44.753
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 16:04:44.770: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 16:05:44.787: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 02/27/23 16:05:44.791
Feb 27 16:05:44.805: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 27 16:05:44.811: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 27 16:05:44.827: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 27 16:05:44.834: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 27 16:05:44.848: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 27 16:05:44.855: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/27/23 16:05:44.855
Feb 27 16:05:44.855: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7323" to be "running"
Feb 27 16:05:44.859: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929825ms
Feb 27 16:05:46.864: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008516687s
Feb 27 16:05:46.864: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 27 16:05:46.864: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
Feb 27 16:05:46.867: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.861846ms
Feb 27 16:05:46.867: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:05:46.867: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
Feb 27 16:05:46.871: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.259381ms
Feb 27 16:05:46.871: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:05:46.871: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
Feb 27 16:05:46.874: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.76851ms
Feb 27 16:05:46.874: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:05:46.874: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
Feb 27 16:05:46.877: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.538892ms
Feb 27 16:05:46.877: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 16:05:46.877: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
Feb 27 16:05:46.880: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.086075ms
Feb 27 16:05:46.880: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/27/23 16:05:46.88
Feb 27 16:05:46.885: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7323" to be "running"
Feb 27 16:05:46.889: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813332ms
Feb 27 16:05:48.894: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008627913s
Feb 27 16:05:50.893: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007980821s
Feb 27 16:05:52.893: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.008221011s
Feb 27 16:05:52.893: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:05:52.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7323" for this suite. 02/27/23 16:05:52.961
------------------------------
• [SLOW TEST] [68.234 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:04:44.733
    Feb 27 16:04:44.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 16:04:44.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:04:44.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:04:44.753
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 16:04:44.770: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 16:05:44.787: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 02/27/23 16:05:44.791
    Feb 27 16:05:44.805: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 27 16:05:44.811: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 27 16:05:44.827: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 27 16:05:44.834: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 27 16:05:44.848: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 27 16:05:44.855: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/27/23 16:05:44.855
    Feb 27 16:05:44.855: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7323" to be "running"
    Feb 27 16:05:44.859: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929825ms
    Feb 27 16:05:46.864: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008516687s
    Feb 27 16:05:46.864: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 27 16:05:46.864: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
    Feb 27 16:05:46.867: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.861846ms
    Feb 27 16:05:46.867: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:05:46.867: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
    Feb 27 16:05:46.871: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.259381ms
    Feb 27 16:05:46.871: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:05:46.871: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
    Feb 27 16:05:46.874: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.76851ms
    Feb 27 16:05:46.874: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:05:46.874: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
    Feb 27 16:05:46.877: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.538892ms
    Feb 27 16:05:46.877: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 16:05:46.877: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7323" to be "running"
    Feb 27 16:05:46.880: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.086075ms
    Feb 27 16:05:46.880: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/27/23 16:05:46.88
    Feb 27 16:05:46.885: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7323" to be "running"
    Feb 27 16:05:46.889: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813332ms
    Feb 27 16:05:48.894: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008627913s
    Feb 27 16:05:50.893: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007980821s
    Feb 27 16:05:52.893: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.008221011s
    Feb 27 16:05:52.893: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:05:52.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7323" for this suite. 02/27/23 16:05:52.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:05:52.968
Feb 27 16:05:52.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:05:52.968
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:05:52.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:05:52.986
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-ca9c9320-6719-4889-8cf1-1deb57e765cb 02/27/23 16:05:52.99
STEP: Creating a pod to test consume configMaps 02/27/23 16:05:52.996
Feb 27 16:05:53.003: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1" in namespace "projected-330" to be "Succeeded or Failed"
Feb 27 16:05:53.007: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512598ms
Feb 27 16:05:55.010: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007008489s
Feb 27 16:05:57.011: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007661761s
STEP: Saw pod success 02/27/23 16:05:57.011
Feb 27 16:05:57.011: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1" satisfied condition "Succeeded or Failed"
Feb 27 16:05:57.014: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 16:05:57.021
Feb 27 16:05:57.035: INFO: Waiting for pod pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1 to disappear
Feb 27 16:05:57.038: INFO: Pod pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:05:57.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-330" for this suite. 02/27/23 16:05:57.042
------------------------------
• [4.082 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:05:52.968
    Feb 27 16:05:52.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:05:52.968
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:05:52.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:05:52.986
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-ca9c9320-6719-4889-8cf1-1deb57e765cb 02/27/23 16:05:52.99
    STEP: Creating a pod to test consume configMaps 02/27/23 16:05:52.996
    Feb 27 16:05:53.003: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1" in namespace "projected-330" to be "Succeeded or Failed"
    Feb 27 16:05:53.007: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512598ms
    Feb 27 16:05:55.010: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007008489s
    Feb 27 16:05:57.011: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007661761s
    STEP: Saw pod success 02/27/23 16:05:57.011
    Feb 27 16:05:57.011: INFO: Pod "pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1" satisfied condition "Succeeded or Failed"
    Feb 27 16:05:57.014: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 16:05:57.021
    Feb 27 16:05:57.035: INFO: Waiting for pod pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1 to disappear
    Feb 27 16:05:57.038: INFO: Pod pod-projected-configmaps-63f75461-d8c0-442f-a69f-6bcd602bbec1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:05:57.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-330" for this suite. 02/27/23 16:05:57.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:05:57.05
Feb 27 16:05:57.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 16:05:57.05
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:05:57.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:05:57.072
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb 27 16:05:57.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:05:58.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4535" for this suite. 02/27/23 16:05:58.099
------------------------------
• [1.058 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:05:57.05
    Feb 27 16:05:57.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 16:05:57.05
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:05:57.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:05:57.072
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb 27 16:05:57.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:05:58.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4535" for this suite. 02/27/23 16:05:58.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:05:58.108
Feb 27 16:05:58.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 16:05:58.109
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:05:58.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:05:58.128
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 16:05:58.131
Feb 27 16:05:58.139: INFO: Waiting up to 5m0s for pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455" in namespace "emptydir-9645" to be "Succeeded or Failed"
Feb 27 16:05:58.144: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455": Phase="Pending", Reason="", readiness=false. Elapsed: 5.252226ms
Feb 27 16:06:00.149: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010083873s
Feb 27 16:06:02.148: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009138009s
STEP: Saw pod success 02/27/23 16:06:02.148
Feb 27 16:06:02.148: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455" satisfied condition "Succeeded or Failed"
Feb 27 16:06:02.152: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455 container test-container: <nil>
STEP: delete the pod 02/27/23 16:06:02.159
Feb 27 16:06:02.173: INFO: Waiting for pod pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455 to disappear
Feb 27 16:06:02.176: INFO: Pod pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:02.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9645" for this suite. 02/27/23 16:06:02.18
------------------------------
• [4.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:05:58.108
    Feb 27 16:05:58.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 16:05:58.109
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:05:58.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:05:58.128
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 16:05:58.131
    Feb 27 16:05:58.139: INFO: Waiting up to 5m0s for pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455" in namespace "emptydir-9645" to be "Succeeded or Failed"
    Feb 27 16:05:58.144: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455": Phase="Pending", Reason="", readiness=false. Elapsed: 5.252226ms
    Feb 27 16:06:00.149: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010083873s
    Feb 27 16:06:02.148: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009138009s
    STEP: Saw pod success 02/27/23 16:06:02.148
    Feb 27 16:06:02.148: INFO: Pod "pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455" satisfied condition "Succeeded or Failed"
    Feb 27 16:06:02.152: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455 container test-container: <nil>
    STEP: delete the pod 02/27/23 16:06:02.159
    Feb 27 16:06:02.173: INFO: Waiting for pod pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455 to disappear
    Feb 27 16:06:02.176: INFO: Pod pod-8d03b04b-d33e-4592-b998-3ba7ba7f9455 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:02.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9645" for this suite. 02/27/23 16:06:02.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:02.188
Feb 27 16:06:02.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename daemonsets 02/27/23 16:06:02.188
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:02.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:02.206
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 16:06:02.227
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:06:02.234
Feb 27 16:06:02.239: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:06:02.239: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:06:02.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:02.242: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:06:03.246: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:06:03.246: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:06:03.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 16:06:03.249: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 02/27/23 16:06:03.252
STEP: DeleteCollection of the DaemonSets 02/27/23 16:06:03.256
STEP: Verify that ReplicaSets have been deleted 02/27/23 16:06:03.266
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Feb 27 16:06:03.278: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24828"},"items":null}

Feb 27 16:06:03.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24828"},"items":[{"metadata":{"name":"daemon-set-cflb4","generateName":"daemon-set-","namespace":"daemonsets-1384","uid":"93bc9b33-7cad-4e03-9097-f0d8223fa57c","resourceVersion":"24818","creationTimestamp":"2023-02-27T16:06:02Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8fab609d-5174-41d8-a952-0c259fda9a0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fab609d-5174-41d8-a952-0c259fda9a0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6wq9g","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6wq9g","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-3-182","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-3-182"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"}],"hostIP":"172.31.3.182","podIP":"192.168.212.149","podIPs":[{"ip":"192.168.212.149"}],"startTime":"2023-02-27T16:06:02Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T16:06:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://19c30dc7fe33411063d3c37d1d65f1b6fddf70f0db5713734f52ebcb047bf994","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jxc4p","generateName":"daemon-set-","namespace":"daemonsets-1384","uid":"c8ffce16-9402-4953-8161-2061c0d049b7","resourceVersion":"24823","creationTimestamp":"2023-02-27T16:06:02Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8fab609d-5174-41d8-a952-0c259fda9a0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fab609d-5174-41d8-a952-0c259fda9a0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rbxcp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rbxcp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-42-40","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-42-40"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"}],"hostIP":"172.31.42.40","podIP":"192.168.192.145","podIPs":[{"ip":"192.168.192.145"}],"startTime":"2023-02-27T16:06:02Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T16:06:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://86bfbfc9f6a3adec4d491cba7b3f73f3fff35309f880da23aaf326ac264c3583","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w7vsq","generateName":"daemon-set-","namespace":"daemonsets-1384","uid":"215cf993-33bc-4d87-9163-9874f5803ea7","resourceVersion":"24821","creationTimestamp":"2023-02-27T16:06:02Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8fab609d-5174-41d8-a952-0c259fda9a0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fab609d-5174-41d8-a952-0c259fda9a0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g4hq7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g4hq7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-84-171","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-84-171"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"}],"hostIP":"172.31.84.171","podIP":"192.168.214.186","podIPs":[{"ip":"192.168.214.186"}],"startTime":"2023-02-27T16:06:02Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T16:06:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e9a17517a8ece7b2cd98f3541998e2bb09b63bd73dae1d31ab0adb5f08be74eb","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:03.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1384" for this suite. 02/27/23 16:06:03.307
------------------------------
• [1.127 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:02.188
    Feb 27 16:06:02.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename daemonsets 02/27/23 16:06:02.188
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:02.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:02.206
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 16:06:02.227
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:06:02.234
    Feb 27 16:06:02.239: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:06:02.239: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:06:02.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:02.242: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:06:03.246: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:06:03.246: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:06:03.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 16:06:03.249: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 02/27/23 16:06:03.252
    STEP: DeleteCollection of the DaemonSets 02/27/23 16:06:03.256
    STEP: Verify that ReplicaSets have been deleted 02/27/23 16:06:03.266
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Feb 27 16:06:03.278: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24828"},"items":null}

    Feb 27 16:06:03.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24828"},"items":[{"metadata":{"name":"daemon-set-cflb4","generateName":"daemon-set-","namespace":"daemonsets-1384","uid":"93bc9b33-7cad-4e03-9097-f0d8223fa57c","resourceVersion":"24818","creationTimestamp":"2023-02-27T16:06:02Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8fab609d-5174-41d8-a952-0c259fda9a0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fab609d-5174-41d8-a952-0c259fda9a0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6wq9g","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6wq9g","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-3-182","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-3-182"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"}],"hostIP":"172.31.3.182","podIP":"192.168.212.149","podIPs":[{"ip":"192.168.212.149"}],"startTime":"2023-02-27T16:06:02Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T16:06:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://19c30dc7fe33411063d3c37d1d65f1b6fddf70f0db5713734f52ebcb047bf994","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jxc4p","generateName":"daemon-set-","namespace":"daemonsets-1384","uid":"c8ffce16-9402-4953-8161-2061c0d049b7","resourceVersion":"24823","creationTimestamp":"2023-02-27T16:06:02Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8fab609d-5174-41d8-a952-0c259fda9a0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fab609d-5174-41d8-a952-0c259fda9a0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rbxcp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rbxcp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-42-40","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-42-40"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"}],"hostIP":"172.31.42.40","podIP":"192.168.192.145","podIPs":[{"ip":"192.168.192.145"}],"startTime":"2023-02-27T16:06:02Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T16:06:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://86bfbfc9f6a3adec4d491cba7b3f73f3fff35309f880da23aaf326ac264c3583","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w7vsq","generateName":"daemon-set-","namespace":"daemonsets-1384","uid":"215cf993-33bc-4d87-9163-9874f5803ea7","resourceVersion":"24821","creationTimestamp":"2023-02-27T16:06:02Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8fab609d-5174-41d8-a952-0c259fda9a0f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fab609d-5174-41d8-a952-0c259fda9a0f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T16:06:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g4hq7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g4hq7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-84-171","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-84-171"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T16:06:02Z"}],"hostIP":"172.31.84.171","podIP":"192.168.214.186","podIPs":[{"ip":"192.168.214.186"}],"startTime":"2023-02-27T16:06:02Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T16:06:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e9a17517a8ece7b2cd98f3541998e2bb09b63bd73dae1d31ab0adb5f08be74eb","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:03.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1384" for this suite. 02/27/23 16:06:03.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:03.315
Feb 27 16:06:03.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:06:03.316
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:03.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:03.336
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 02/27/23 16:06:03.339
STEP: Creating a ResourceQuota 02/27/23 16:06:08.343
STEP: Ensuring resource quota status is calculated 02/27/23 16:06:08.349
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:10.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3671" for this suite. 02/27/23 16:06:10.357
------------------------------
• [SLOW TEST] [7.052 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:03.315
    Feb 27 16:06:03.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:06:03.316
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:03.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:03.336
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 02/27/23 16:06:03.339
    STEP: Creating a ResourceQuota 02/27/23 16:06:08.343
    STEP: Ensuring resource quota status is calculated 02/27/23 16:06:08.349
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:10.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3671" for this suite. 02/27/23 16:06:10.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:10.368
Feb 27 16:06:10.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:06:10.369
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:10.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:10.438
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 02/27/23 16:06:10.441
Feb 27 16:06:10.441: INFO: namespace kubectl-9375
Feb 27 16:06:10.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 create -f -'
Feb 27 16:06:10.984: INFO: stderr: ""
Feb 27 16:06:10.984: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 16:06:10.984
Feb 27 16:06:11.989: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 16:06:11.989: INFO: Found 0 / 1
Feb 27 16:06:12.988: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 16:06:12.988: INFO: Found 1 / 1
Feb 27 16:06:12.988: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 27 16:06:12.992: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 16:06:12.992: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 16:06:12.992: INFO: wait on agnhost-primary startup in kubectl-9375 
Feb 27 16:06:12.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 logs agnhost-primary-cvlkn agnhost-primary'
Feb 27 16:06:13.049: INFO: stderr: ""
Feb 27 16:06:13.049: INFO: stdout: "Paused\n"
STEP: exposing RC 02/27/23 16:06:13.049
Feb 27 16:06:13.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 27 16:06:13.111: INFO: stderr: ""
Feb 27 16:06:13.111: INFO: stdout: "service/rm2 exposed\n"
Feb 27 16:06:13.117: INFO: Service rm2 in namespace kubectl-9375 found.
STEP: exposing service 02/27/23 16:06:15.125
Feb 27 16:06:15.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 27 16:06:15.187: INFO: stderr: ""
Feb 27 16:06:15.187: INFO: stdout: "service/rm3 exposed\n"
Feb 27 16:06:15.193: INFO: Service rm3 in namespace kubectl-9375 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:17.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9375" for this suite. 02/27/23 16:06:17.205
------------------------------
• [SLOW TEST] [6.844 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:10.368
    Feb 27 16:06:10.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:06:10.369
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:10.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:10.438
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 02/27/23 16:06:10.441
    Feb 27 16:06:10.441: INFO: namespace kubectl-9375
    Feb 27 16:06:10.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 create -f -'
    Feb 27 16:06:10.984: INFO: stderr: ""
    Feb 27 16:06:10.984: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 16:06:10.984
    Feb 27 16:06:11.989: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 16:06:11.989: INFO: Found 0 / 1
    Feb 27 16:06:12.988: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 16:06:12.988: INFO: Found 1 / 1
    Feb 27 16:06:12.988: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 27 16:06:12.992: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 16:06:12.992: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 16:06:12.992: INFO: wait on agnhost-primary startup in kubectl-9375 
    Feb 27 16:06:12.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 logs agnhost-primary-cvlkn agnhost-primary'
    Feb 27 16:06:13.049: INFO: stderr: ""
    Feb 27 16:06:13.049: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/27/23 16:06:13.049
    Feb 27 16:06:13.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb 27 16:06:13.111: INFO: stderr: ""
    Feb 27 16:06:13.111: INFO: stdout: "service/rm2 exposed\n"
    Feb 27 16:06:13.117: INFO: Service rm2 in namespace kubectl-9375 found.
    STEP: exposing service 02/27/23 16:06:15.125
    Feb 27 16:06:15.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9375 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb 27 16:06:15.187: INFO: stderr: ""
    Feb 27 16:06:15.187: INFO: stdout: "service/rm3 exposed\n"
    Feb 27 16:06:15.193: INFO: Service rm3 in namespace kubectl-9375 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:17.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9375" for this suite. 02/27/23 16:06:17.205
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:17.212
Feb 27 16:06:17.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 16:06:17.213
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:17.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:17.234
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-4391/secret-test-08d95e9d-3867-4e4a-b531-4dac147ca28f 02/27/23 16:06:17.238
STEP: Creating a pod to test consume secrets 02/27/23 16:06:17.242
Feb 27 16:06:17.250: INFO: Waiting up to 5m0s for pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330" in namespace "secrets-4391" to be "Succeeded or Failed"
Feb 27 16:06:17.254: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595619ms
Feb 27 16:06:19.259: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00890138s
Feb 27 16:06:21.260: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009581349s
STEP: Saw pod success 02/27/23 16:06:21.26
Feb 27 16:06:21.260: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330" satisfied condition "Succeeded or Failed"
Feb 27 16:06:21.263: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330 container env-test: <nil>
STEP: delete the pod 02/27/23 16:06:21.27
Feb 27 16:06:21.281: INFO: Waiting for pod pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330 to disappear
Feb 27 16:06:21.285: INFO: Pod pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:21.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4391" for this suite. 02/27/23 16:06:21.289
------------------------------
• [4.083 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:17.212
    Feb 27 16:06:17.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 16:06:17.213
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:17.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:17.234
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-4391/secret-test-08d95e9d-3867-4e4a-b531-4dac147ca28f 02/27/23 16:06:17.238
    STEP: Creating a pod to test consume secrets 02/27/23 16:06:17.242
    Feb 27 16:06:17.250: INFO: Waiting up to 5m0s for pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330" in namespace "secrets-4391" to be "Succeeded or Failed"
    Feb 27 16:06:17.254: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595619ms
    Feb 27 16:06:19.259: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00890138s
    Feb 27 16:06:21.260: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009581349s
    STEP: Saw pod success 02/27/23 16:06:21.26
    Feb 27 16:06:21.260: INFO: Pod "pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330" satisfied condition "Succeeded or Failed"
    Feb 27 16:06:21.263: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330 container env-test: <nil>
    STEP: delete the pod 02/27/23 16:06:21.27
    Feb 27 16:06:21.281: INFO: Waiting for pod pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330 to disappear
    Feb 27 16:06:21.285: INFO: Pod pod-configmaps-6a8b4c91-614c-4c84-858a-e480c4d82330 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:21.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4391" for this suite. 02/27/23 16:06:21.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:21.297
Feb 27 16:06:21.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 16:06:21.297
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:21.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:21.32
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/27/23 16:06:21.324
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/27/23 16:06:21.325
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 16:06:21.326
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/27/23 16:06:21.326
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/27/23 16:06:21.327
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 16:06:21.327
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 16:06:21.329
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:21.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9085" for this suite. 02/27/23 16:06:21.333
------------------------------
• [0.046 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:21.297
    Feb 27 16:06:21.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 16:06:21.297
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:21.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:21.32
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/27/23 16:06:21.324
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/27/23 16:06:21.325
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 16:06:21.326
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/27/23 16:06:21.326
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/27/23 16:06:21.327
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 16:06:21.327
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 16:06:21.329
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:21.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9085" for this suite. 02/27/23 16:06:21.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:21.343
Feb 27 16:06:21.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:06:21.344
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:21.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:21.365
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 02/27/23 16:06:21.368
STEP: Creating a ResourceQuota 02/27/23 16:06:26.374
STEP: Ensuring resource quota status is calculated 02/27/23 16:06:26.379
STEP: Creating a ReplicationController 02/27/23 16:06:28.384
STEP: Ensuring resource quota status captures replication controller creation 02/27/23 16:06:28.405
STEP: Deleting a ReplicationController 02/27/23 16:06:30.409
STEP: Ensuring resource quota status released usage 02/27/23 16:06:30.416
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:32.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5692" for this suite. 02/27/23 16:06:32.424
------------------------------
• [SLOW TEST] [11.089 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:21.343
    Feb 27 16:06:21.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:06:21.344
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:21.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:21.365
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 02/27/23 16:06:21.368
    STEP: Creating a ResourceQuota 02/27/23 16:06:26.374
    STEP: Ensuring resource quota status is calculated 02/27/23 16:06:26.379
    STEP: Creating a ReplicationController 02/27/23 16:06:28.384
    STEP: Ensuring resource quota status captures replication controller creation 02/27/23 16:06:28.405
    STEP: Deleting a ReplicationController 02/27/23 16:06:30.409
    STEP: Ensuring resource quota status released usage 02/27/23 16:06:30.416
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:32.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5692" for this suite. 02/27/23 16:06:32.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:32.433
Feb 27 16:06:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename daemonsets 02/27/23 16:06:32.434
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:32.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:32.456
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Feb 27 16:06:32.479: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/27/23 16:06:32.484
Feb 27 16:06:32.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:32.486: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/27/23 16:06:32.486
Feb 27 16:06:32.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:32.509: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
Feb 27 16:06:33.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 16:06:33.514: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/27/23 16:06:33.518
Feb 27 16:06:33.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 16:06:33.541: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb 27 16:06:34.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:34.546: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/27/23 16:06:34.546
Feb 27 16:06:34.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:34.561: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
Feb 27 16:06:35.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:35.567: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
Feb 27 16:06:36.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:36.565: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
Feb 27 16:06:37.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 16:06:37.565: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:06:37.571
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-839, will wait for the garbage collector to delete the pods 02/27/23 16:06:37.572
Feb 27 16:06:37.633: INFO: Deleting DaemonSet.extensions daemon-set took: 8.522602ms
Feb 27 16:06:37.735: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.143086ms
Feb 27 16:06:40.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:06:40.339: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 16:06:40.342: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25169"},"items":null}

Feb 27 16:06:40.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25169"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:40.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-839" for this suite. 02/27/23 16:06:40.373
------------------------------
• [SLOW TEST] [7.946 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:32.433
    Feb 27 16:06:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename daemonsets 02/27/23 16:06:32.434
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:32.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:32.456
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Feb 27 16:06:32.479: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/27/23 16:06:32.484
    Feb 27 16:06:32.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:32.486: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/27/23 16:06:32.486
    Feb 27 16:06:32.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:32.509: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
    Feb 27 16:06:33.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 16:06:33.514: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/27/23 16:06:33.518
    Feb 27 16:06:33.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 16:06:33.541: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Feb 27 16:06:34.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:34.546: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/27/23 16:06:34.546
    Feb 27 16:06:34.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:34.561: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
    Feb 27 16:06:35.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:35.567: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
    Feb 27 16:06:36.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:36.565: INFO: Node ip-172-31-42-40 is running 0 daemon pod, expected 1
    Feb 27 16:06:37.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 16:06:37.565: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:06:37.571
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-839, will wait for the garbage collector to delete the pods 02/27/23 16:06:37.572
    Feb 27 16:06:37.633: INFO: Deleting DaemonSet.extensions daemon-set took: 8.522602ms
    Feb 27 16:06:37.735: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.143086ms
    Feb 27 16:06:40.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:06:40.339: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 16:06:40.342: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25169"},"items":null}

    Feb 27 16:06:40.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25169"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:40.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-839" for this suite. 02/27/23 16:06:40.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:40.38
Feb 27 16:06:40.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename server-version 02/27/23 16:06:40.381
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:40.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:40.404
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/27/23 16:06:40.407
STEP: Confirm major version 02/27/23 16:06:40.409
Feb 27 16:06:40.409: INFO: Major version: 1
STEP: Confirm minor version 02/27/23 16:06:40.409
Feb 27 16:06:40.409: INFO: cleanMinorVersion: 26
Feb 27 16:06:40.409: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:40.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-9411" for this suite. 02/27/23 16:06:40.417
------------------------------
• [0.043 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:40.38
    Feb 27 16:06:40.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename server-version 02/27/23 16:06:40.381
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:40.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:40.404
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/27/23 16:06:40.407
    STEP: Confirm major version 02/27/23 16:06:40.409
    Feb 27 16:06:40.409: INFO: Major version: 1
    STEP: Confirm minor version 02/27/23 16:06:40.409
    Feb 27 16:06:40.409: INFO: cleanMinorVersion: 26
    Feb 27 16:06:40.409: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:40.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-9411" for this suite. 02/27/23 16:06:40.417
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:40.423
Feb 27 16:06:40.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 16:06:40.424
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:40.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:40.442
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-3574 02/27/23 16:06:40.445
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[] 02/27/23 16:06:40.455
Feb 27 16:06:40.471: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3574 02/27/23 16:06:40.471
Feb 27 16:06:40.481: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3574" to be "running and ready"
Feb 27 16:06:40.487: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295377ms
Feb 27 16:06:40.487: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:06:42.491: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009254024s
Feb 27 16:06:42.491: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 16:06:42.491: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[pod1:[80]] 02/27/23 16:06:42.495
Feb 27 16:06:42.505: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/27/23 16:06:42.505
Feb 27 16:06:42.505: INFO: Creating new exec pod
Feb 27 16:06:42.510: INFO: Waiting up to 5m0s for pod "execpod6nfnh" in namespace "services-3574" to be "running"
Feb 27 16:06:42.516: INFO: Pod "execpod6nfnh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800584ms
Feb 27 16:06:44.521: INFO: Pod "execpod6nfnh": Phase="Running", Reason="", readiness=true. Elapsed: 2.011322645s
Feb 27 16:06:44.521: INFO: Pod "execpod6nfnh" satisfied condition "running"
Feb 27 16:06:45.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 27 16:06:45.650: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 16:06:45.650: INFO: stdout: ""
Feb 27 16:06:45.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.120 80'
Feb 27 16:06:45.751: INFO: stderr: "+ nc -v -z -w 2 10.152.183.120 80\nConnection to 10.152.183.120 80 port [tcp/http] succeeded!\n"
Feb 27 16:06:45.751: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-3574 02/27/23 16:06:45.751
Feb 27 16:06:45.759: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3574" to be "running and ready"
Feb 27 16:06:45.762: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238191ms
Feb 27 16:06:45.762: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:06:47.765: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006503079s
Feb 27 16:06:47.765: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 16:06:47.765: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[pod1:[80] pod2:[80]] 02/27/23 16:06:47.769
Feb 27 16:06:47.782: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/27/23 16:06:47.782
Feb 27 16:06:48.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 27 16:06:48.902: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 16:06:48.902: INFO: stdout: ""
Feb 27 16:06:48.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.120 80'
Feb 27 16:06:49.010: INFO: stderr: "+ nc -v -z -w 2 10.152.183.120 80\nConnection to 10.152.183.120 80 port [tcp/http] succeeded!\n"
Feb 27 16:06:49.010: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3574 02/27/23 16:06:49.01
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[pod2:[80]] 02/27/23 16:06:49.027
Feb 27 16:06:49.040: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/27/23 16:06:49.04
Feb 27 16:06:50.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 27 16:06:50.178: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 16:06:50.178: INFO: stdout: ""
Feb 27 16:06:50.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.120 80'
Feb 27 16:06:50.279: INFO: stderr: "+ nc -v -z -w 2 10.152.183.120 80\nConnection to 10.152.183.120 80 port [tcp/http] succeeded!\n"
Feb 27 16:06:50.279: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-3574 02/27/23 16:06:50.279
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[] 02/27/23 16:06:50.301
Feb 27 16:06:50.312: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:50.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3574" for this suite. 02/27/23 16:06:50.334
------------------------------
• [SLOW TEST] [9.917 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:40.423
    Feb 27 16:06:40.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 16:06:40.424
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:40.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:40.442
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-3574 02/27/23 16:06:40.445
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[] 02/27/23 16:06:40.455
    Feb 27 16:06:40.471: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3574 02/27/23 16:06:40.471
    Feb 27 16:06:40.481: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3574" to be "running and ready"
    Feb 27 16:06:40.487: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295377ms
    Feb 27 16:06:40.487: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:06:42.491: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009254024s
    Feb 27 16:06:42.491: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 16:06:42.491: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[pod1:[80]] 02/27/23 16:06:42.495
    Feb 27 16:06:42.505: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/27/23 16:06:42.505
    Feb 27 16:06:42.505: INFO: Creating new exec pod
    Feb 27 16:06:42.510: INFO: Waiting up to 5m0s for pod "execpod6nfnh" in namespace "services-3574" to be "running"
    Feb 27 16:06:42.516: INFO: Pod "execpod6nfnh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800584ms
    Feb 27 16:06:44.521: INFO: Pod "execpod6nfnh": Phase="Running", Reason="", readiness=true. Elapsed: 2.011322645s
    Feb 27 16:06:44.521: INFO: Pod "execpod6nfnh" satisfied condition "running"
    Feb 27 16:06:45.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 27 16:06:45.650: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 16:06:45.650: INFO: stdout: ""
    Feb 27 16:06:45.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.120 80'
    Feb 27 16:06:45.751: INFO: stderr: "+ nc -v -z -w 2 10.152.183.120 80\nConnection to 10.152.183.120 80 port [tcp/http] succeeded!\n"
    Feb 27 16:06:45.751: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-3574 02/27/23 16:06:45.751
    Feb 27 16:06:45.759: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3574" to be "running and ready"
    Feb 27 16:06:45.762: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238191ms
    Feb 27 16:06:45.762: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:06:47.765: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006503079s
    Feb 27 16:06:47.765: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 16:06:47.765: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[pod1:[80] pod2:[80]] 02/27/23 16:06:47.769
    Feb 27 16:06:47.782: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/27/23 16:06:47.782
    Feb 27 16:06:48.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 27 16:06:48.902: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 16:06:48.902: INFO: stdout: ""
    Feb 27 16:06:48.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.120 80'
    Feb 27 16:06:49.010: INFO: stderr: "+ nc -v -z -w 2 10.152.183.120 80\nConnection to 10.152.183.120 80 port [tcp/http] succeeded!\n"
    Feb 27 16:06:49.010: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3574 02/27/23 16:06:49.01
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[pod2:[80]] 02/27/23 16:06:49.027
    Feb 27 16:06:49.040: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/27/23 16:06:49.04
    Feb 27 16:06:50.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 27 16:06:50.178: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 16:06:50.178: INFO: stdout: ""
    Feb 27 16:06:50.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-3574 exec execpod6nfnh -- /bin/sh -x -c nc -v -z -w 2 10.152.183.120 80'
    Feb 27 16:06:50.279: INFO: stderr: "+ nc -v -z -w 2 10.152.183.120 80\nConnection to 10.152.183.120 80 port [tcp/http] succeeded!\n"
    Feb 27 16:06:50.279: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-3574 02/27/23 16:06:50.279
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3574 to expose endpoints map[] 02/27/23 16:06:50.301
    Feb 27 16:06:50.312: INFO: successfully validated that service endpoint-test2 in namespace services-3574 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:50.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3574" for this suite. 02/27/23 16:06:50.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:50.341
Feb 27 16:06:50.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replication-controller 02/27/23 16:06:50.342
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:50.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:50.361
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 02/27/23 16:06:50.364
Feb 27 16:06:50.375: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2427" to be "running and ready"
Feb 27 16:06:50.380: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.535993ms
Feb 27 16:06:50.380: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:06:52.385: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010304752s
Feb 27 16:06:52.385: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb 27 16:06:52.385: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/27/23 16:06:52.388
STEP: Then the orphan pod is adopted 02/27/23 16:06:52.398
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 16:06:53.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2427" for this suite. 02/27/23 16:06:53.409
------------------------------
• [3.077 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:50.341
    Feb 27 16:06:50.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replication-controller 02/27/23 16:06:50.342
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:50.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:50.361
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/27/23 16:06:50.364
    Feb 27 16:06:50.375: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2427" to be "running and ready"
    Feb 27 16:06:50.380: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.535993ms
    Feb 27 16:06:50.380: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:06:52.385: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010304752s
    Feb 27 16:06:52.385: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb 27 16:06:52.385: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/27/23 16:06:52.388
    STEP: Then the orphan pod is adopted 02/27/23 16:06:52.398
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:06:53.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2427" for this suite. 02/27/23 16:06:53.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:06:53.421
Feb 27 16:06:53.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename job 02/27/23 16:06:53.421
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:53.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:53.439
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 02/27/23 16:06:53.443
STEP: Ensuring job reaches completions 02/27/23 16:06:53.449
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:03.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9165" for this suite. 02/27/23 16:07:03.457
------------------------------
• [SLOW TEST] [10.043 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:06:53.421
    Feb 27 16:06:53.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename job 02/27/23 16:06:53.421
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:06:53.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:06:53.439
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 02/27/23 16:06:53.443
    STEP: Ensuring job reaches completions 02/27/23 16:06:53.449
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:03.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9165" for this suite. 02/27/23 16:07:03.457
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:03.464
Feb 27 16:07:03.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 16:07:03.465
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:03.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:03.484
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 16:07:03.491
Feb 27 16:07:03.499: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4180" to be "running and ready"
Feb 27 16:07:03.504: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670928ms
Feb 27 16:07:03.504: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:07:05.508: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.0088891s
Feb 27 16:07:05.508: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 16:07:05.508: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 02/27/23 16:07:05.511
Feb 27 16:07:05.517: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4180" to be "running and ready"
Feb 27 16:07:05.522: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.79936ms
Feb 27 16:07:05.522: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:07:07.526: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008863904s
Feb 27 16:07:07.526: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb 27 16:07:07.526: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/27/23 16:07:07.529
STEP: delete the pod with lifecycle hook 02/27/23 16:07:07.544
Feb 27 16:07:07.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 16:07:07.554: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 27 16:07:09.554: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 16:07:09.559: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:09.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4180" for this suite. 02/27/23 16:07:09.563
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:03.464
    Feb 27 16:07:03.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 16:07:03.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:03.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:03.484
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 16:07:03.491
    Feb 27 16:07:03.499: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4180" to be "running and ready"
    Feb 27 16:07:03.504: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670928ms
    Feb 27 16:07:03.504: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:07:05.508: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.0088891s
    Feb 27 16:07:05.508: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 16:07:05.508: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 02/27/23 16:07:05.511
    Feb 27 16:07:05.517: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4180" to be "running and ready"
    Feb 27 16:07:05.522: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.79936ms
    Feb 27 16:07:05.522: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:07:07.526: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008863904s
    Feb 27 16:07:07.526: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb 27 16:07:07.526: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/27/23 16:07:07.529
    STEP: delete the pod with lifecycle hook 02/27/23 16:07:07.544
    Feb 27 16:07:07.550: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 16:07:07.554: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 27 16:07:09.554: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 16:07:09.559: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:09.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4180" for this suite. 02/27/23 16:07:09.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:09.571
Feb 27 16:07:09.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:07:09.572
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:09.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:09.596
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-81eb31f0-830b-432d-91b8-a3fd8a2e4138 02/27/23 16:07:09.599
STEP: Creating a pod to test consume secrets 02/27/23 16:07:09.604
Feb 27 16:07:09.614: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014" in namespace "projected-2079" to be "Succeeded or Failed"
Feb 27 16:07:09.617: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895123ms
Feb 27 16:07:11.622: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008192154s
Feb 27 16:07:13.621: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007289091s
STEP: Saw pod success 02/27/23 16:07:13.621
Feb 27 16:07:13.621: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014" satisfied condition "Succeeded or Failed"
Feb 27 16:07:13.625: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 16:07:13.632
Feb 27 16:07:13.649: INFO: Waiting for pod pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014 to disappear
Feb 27 16:07:13.652: INFO: Pod pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:13.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2079" for this suite. 02/27/23 16:07:13.656
------------------------------
• [4.091 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:09.571
    Feb 27 16:07:09.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:07:09.572
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:09.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:09.596
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-81eb31f0-830b-432d-91b8-a3fd8a2e4138 02/27/23 16:07:09.599
    STEP: Creating a pod to test consume secrets 02/27/23 16:07:09.604
    Feb 27 16:07:09.614: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014" in namespace "projected-2079" to be "Succeeded or Failed"
    Feb 27 16:07:09.617: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895123ms
    Feb 27 16:07:11.622: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008192154s
    Feb 27 16:07:13.621: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007289091s
    STEP: Saw pod success 02/27/23 16:07:13.621
    Feb 27 16:07:13.621: INFO: Pod "pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014" satisfied condition "Succeeded or Failed"
    Feb 27 16:07:13.625: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 16:07:13.632
    Feb 27 16:07:13.649: INFO: Waiting for pod pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014 to disappear
    Feb 27 16:07:13.652: INFO: Pod pod-projected-secrets-e715d878-b2dc-4f9a-9c7e-067faa5c4014 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:13.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2079" for this suite. 02/27/23 16:07:13.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:13.663
Feb 27 16:07:13.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 16:07:13.664
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:13.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:13.685
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 16:07:13.702
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:07:13.919
STEP: Deploying the webhook pod 02/27/23 16:07:13.927
STEP: Wait for the deployment to be ready 02/27/23 16:07:13.941
Feb 27 16:07:13.949: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 16:07:15.961
STEP: Verifying the service has paired with the endpoint 02/27/23 16:07:15.972
Feb 27 16:07:16.972: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Feb 27 16:07:16.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/27/23 16:07:17.487
STEP: Creating a custom resource that should be denied by the webhook 02/27/23 16:07:17.501
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/27/23 16:07:19.528
STEP: Updating the custom resource with disallowed data should be denied 02/27/23 16:07:19.535
STEP: Deleting the custom resource should be denied 02/27/23 16:07:19.544
STEP: Remove the offending key and value from the custom resource data 02/27/23 16:07:19.55
STEP: Deleting the updated custom resource should be successful 02/27/23 16:07:19.56
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:20.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-853" for this suite. 02/27/23 16:07:20.132
STEP: Destroying namespace "webhook-853-markers" for this suite. 02/27/23 16:07:20.142
------------------------------
• [SLOW TEST] [6.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:13.663
    Feb 27 16:07:13.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 16:07:13.664
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:13.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:13.685
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 16:07:13.702
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:07:13.919
    STEP: Deploying the webhook pod 02/27/23 16:07:13.927
    STEP: Wait for the deployment to be ready 02/27/23 16:07:13.941
    Feb 27 16:07:13.949: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 16:07:15.961
    STEP: Verifying the service has paired with the endpoint 02/27/23 16:07:15.972
    Feb 27 16:07:16.972: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Feb 27 16:07:16.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/27/23 16:07:17.487
    STEP: Creating a custom resource that should be denied by the webhook 02/27/23 16:07:17.501
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/27/23 16:07:19.528
    STEP: Updating the custom resource with disallowed data should be denied 02/27/23 16:07:19.535
    STEP: Deleting the custom resource should be denied 02/27/23 16:07:19.544
    STEP: Remove the offending key and value from the custom resource data 02/27/23 16:07:19.55
    STEP: Deleting the updated custom resource should be successful 02/27/23 16:07:19.56
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:20.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-853" for this suite. 02/27/23 16:07:20.132
    STEP: Destroying namespace "webhook-853-markers" for this suite. 02/27/23 16:07:20.142
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:20.152
Feb 27 16:07:20.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replicaset 02/27/23 16:07:20.153
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:20.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:20.172
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb 27 16:07:20.175: INFO: Creating ReplicaSet my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e
Feb 27 16:07:20.185: INFO: Pod name my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e: Found 0 pods out of 1
Feb 27 16:07:25.191: INFO: Pod name my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e: Found 1 pods out of 1
Feb 27 16:07:25.191: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e" is running
Feb 27 16:07:25.191: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht" in namespace "replicaset-1125" to be "running"
Feb 27 16:07:25.194: INFO: Pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht": Phase="Running", Reason="", readiness=true. Elapsed: 3.835436ms
Feb 27 16:07:25.194: INFO: Pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht" satisfied condition "running"
Feb 27 16:07:25.194: INFO: Pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:20 +0000 UTC Reason: Message:}])
Feb 27 16:07:25.194: INFO: Trying to dial the pod
Feb 27 16:07:30.206: INFO: Controller my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e: Got expected result from replica 1 [my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht]: "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:30.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1125" for this suite. 02/27/23 16:07:30.21
------------------------------
• [SLOW TEST] [10.065 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:20.152
    Feb 27 16:07:20.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replicaset 02/27/23 16:07:20.153
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:20.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:20.172
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb 27 16:07:20.175: INFO: Creating ReplicaSet my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e
    Feb 27 16:07:20.185: INFO: Pod name my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e: Found 0 pods out of 1
    Feb 27 16:07:25.191: INFO: Pod name my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e: Found 1 pods out of 1
    Feb 27 16:07:25.191: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e" is running
    Feb 27 16:07:25.191: INFO: Waiting up to 5m0s for pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht" in namespace "replicaset-1125" to be "running"
    Feb 27 16:07:25.194: INFO: Pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht": Phase="Running", Reason="", readiness=true. Elapsed: 3.835436ms
    Feb 27 16:07:25.194: INFO: Pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht" satisfied condition "running"
    Feb 27 16:07:25.194: INFO: Pod "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 16:07:20 +0000 UTC Reason: Message:}])
    Feb 27 16:07:25.194: INFO: Trying to dial the pod
    Feb 27 16:07:30.206: INFO: Controller my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e: Got expected result from replica 1 [my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht]: "my-hostname-basic-0d51afb2-ce9f-4e6a-9d8b-ead5e6f7ed9e-ptqht", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:30.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1125" for this suite. 02/27/23 16:07:30.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:30.217
Feb 27 16:07:30.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:07:30.218
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:30.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:30.237
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 02/27/23 16:07:30.24
Feb 27 16:07:30.249: INFO: Waiting up to 5m0s for pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049" in namespace "projected-7847" to be "running and ready"
Feb 27 16:07:30.254: INFO: Pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903858ms
Feb 27 16:07:30.254: INFO: The phase of Pod annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:07:32.278: INFO: Pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049": Phase="Running", Reason="", readiness=true. Elapsed: 2.028747546s
Feb 27 16:07:32.278: INFO: The phase of Pod annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049 is Running (Ready = true)
Feb 27 16:07:32.278: INFO: Pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049" satisfied condition "running and ready"
Feb 27 16:07:32.801: INFO: Successfully updated pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:36.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7847" for this suite. 02/27/23 16:07:36.83
------------------------------
• [SLOW TEST] [6.621 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:30.217
    Feb 27 16:07:30.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:07:30.218
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:30.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:30.237
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 02/27/23 16:07:30.24
    Feb 27 16:07:30.249: INFO: Waiting up to 5m0s for pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049" in namespace "projected-7847" to be "running and ready"
    Feb 27 16:07:30.254: INFO: Pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903858ms
    Feb 27 16:07:30.254: INFO: The phase of Pod annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:07:32.278: INFO: Pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049": Phase="Running", Reason="", readiness=true. Elapsed: 2.028747546s
    Feb 27 16:07:32.278: INFO: The phase of Pod annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049 is Running (Ready = true)
    Feb 27 16:07:32.278: INFO: Pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049" satisfied condition "running and ready"
    Feb 27 16:07:32.801: INFO: Successfully updated pod "annotationupdatea3a1691e-1ebe-4b7c-b229-8db224087049"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:36.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7847" for this suite. 02/27/23 16:07:36.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:36.842
Feb 27 16:07:36.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 16:07:36.843
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:36.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:36.864
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1188.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/27/23 16:07:36.867
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1188.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/27/23 16:07:36.867
STEP: creating a pod to probe /etc/hosts 02/27/23 16:07:36.867
STEP: submitting the pod to kubernetes 02/27/23 16:07:36.872
Feb 27 16:07:36.880: INFO: Waiting up to 15m0s for pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339" in namespace "dns-1188" to be "running"
Feb 27 16:07:36.885: INFO: Pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078107ms
Feb 27 16:07:38.890: INFO: Pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339": Phase="Running", Reason="", readiness=true. Elapsed: 2.00966838s
Feb 27 16:07:38.890: INFO: Pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339" satisfied condition "running"
STEP: retrieving the pod 02/27/23 16:07:38.89
STEP: looking for the results for each expected name from probers 02/27/23 16:07:38.894
Feb 27 16:07:38.909: INFO: DNS probes using dns-1188/dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339 succeeded

STEP: deleting the pod 02/27/23 16:07:38.909
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:38.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1188" for this suite. 02/27/23 16:07:38.926
------------------------------
• [2.090 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:36.842
    Feb 27 16:07:36.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 16:07:36.843
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:36.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:36.864
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1188.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/27/23 16:07:36.867
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1188.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/27/23 16:07:36.867
    STEP: creating a pod to probe /etc/hosts 02/27/23 16:07:36.867
    STEP: submitting the pod to kubernetes 02/27/23 16:07:36.872
    Feb 27 16:07:36.880: INFO: Waiting up to 15m0s for pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339" in namespace "dns-1188" to be "running"
    Feb 27 16:07:36.885: INFO: Pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078107ms
    Feb 27 16:07:38.890: INFO: Pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339": Phase="Running", Reason="", readiness=true. Elapsed: 2.00966838s
    Feb 27 16:07:38.890: INFO: Pod "dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 16:07:38.89
    STEP: looking for the results for each expected name from probers 02/27/23 16:07:38.894
    Feb 27 16:07:38.909: INFO: DNS probes using dns-1188/dns-test-3b4a0a96-febe-4f4d-a693-31138fb07339 succeeded

    STEP: deleting the pod 02/27/23 16:07:38.909
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:38.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1188" for this suite. 02/27/23 16:07:38.926
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:38.932
Feb 27 16:07:38.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:07:38.933
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:38.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:38.95
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-2e772b61-6022-446f-8494-9a5fdece86df 02/27/23 16:07:38.953
STEP: Creating a pod to test consume configMaps 02/27/23 16:07:38.957
Feb 27 16:07:38.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084" in namespace "configmap-814" to be "Succeeded or Failed"
Feb 27 16:07:38.971: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084": Phase="Pending", Reason="", readiness=false. Elapsed: 5.102546ms
Feb 27 16:07:40.976: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009262778s
Feb 27 16:07:42.976: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00925962s
STEP: Saw pod success 02/27/23 16:07:42.976
Feb 27 16:07:42.976: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084" satisfied condition "Succeeded or Failed"
Feb 27 16:07:42.980: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 16:07:42.985
Feb 27 16:07:42.996: INFO: Waiting for pod pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084 to disappear
Feb 27 16:07:42.999: INFO: Pod pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:42.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-814" for this suite. 02/27/23 16:07:43.003
------------------------------
• [4.077 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:38.932
    Feb 27 16:07:38.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:07:38.933
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:38.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:38.95
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-2e772b61-6022-446f-8494-9a5fdece86df 02/27/23 16:07:38.953
    STEP: Creating a pod to test consume configMaps 02/27/23 16:07:38.957
    Feb 27 16:07:38.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084" in namespace "configmap-814" to be "Succeeded or Failed"
    Feb 27 16:07:38.971: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084": Phase="Pending", Reason="", readiness=false. Elapsed: 5.102546ms
    Feb 27 16:07:40.976: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009262778s
    Feb 27 16:07:42.976: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00925962s
    STEP: Saw pod success 02/27/23 16:07:42.976
    Feb 27 16:07:42.976: INFO: Pod "pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084" satisfied condition "Succeeded or Failed"
    Feb 27 16:07:42.980: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 16:07:42.985
    Feb 27 16:07:42.996: INFO: Waiting for pod pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084 to disappear
    Feb 27 16:07:42.999: INFO: Pod pod-configmaps-e9fab482-8b03-48b0-ad97-9029f9422084 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:42.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-814" for this suite. 02/27/23 16:07:43.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:43.01
Feb 27 16:07:43.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:07:43.01
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:43.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:43.028
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:07:43.031
Feb 27 16:07:43.042: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1" in namespace "projected-552" to be "Succeeded or Failed"
Feb 27 16:07:43.049: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570363ms
Feb 27 16:07:45.054: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011893539s
Feb 27 16:07:47.055: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0122373s
STEP: Saw pod success 02/27/23 16:07:47.055
Feb 27 16:07:47.055: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1" satisfied condition "Succeeded or Failed"
Feb 27 16:07:47.058: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1 container client-container: <nil>
STEP: delete the pod 02/27/23 16:07:47.063
Feb 27 16:07:47.077: INFO: Waiting for pod downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1 to disappear
Feb 27 16:07:47.080: INFO: Pod downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-552" for this suite. 02/27/23 16:07:47.084
------------------------------
• [4.080 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:43.01
    Feb 27 16:07:43.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:07:43.01
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:43.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:43.028
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:07:43.031
    Feb 27 16:07:43.042: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1" in namespace "projected-552" to be "Succeeded or Failed"
    Feb 27 16:07:43.049: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570363ms
    Feb 27 16:07:45.054: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011893539s
    Feb 27 16:07:47.055: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0122373s
    STEP: Saw pod success 02/27/23 16:07:47.055
    Feb 27 16:07:47.055: INFO: Pod "downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1" satisfied condition "Succeeded or Failed"
    Feb 27 16:07:47.058: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1 container client-container: <nil>
    STEP: delete the pod 02/27/23 16:07:47.063
    Feb 27 16:07:47.077: INFO: Waiting for pod downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1 to disappear
    Feb 27 16:07:47.080: INFO: Pod downwardapi-volume-d18e211c-984b-4b32-a77e-017129ca41c1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-552" for this suite. 02/27/23 16:07:47.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:47.09
Feb 27 16:07:47.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 16:07:47.091
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:47.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:47.112
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8437 02/27/23 16:07:47.115
STEP: changing the ExternalName service to type=NodePort 02/27/23 16:07:47.122
STEP: creating replication controller externalname-service in namespace services-8437 02/27/23 16:07:47.142
I0227 16:07:47.147868      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8437, replica count: 2
I0227 16:07:50.198471      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 16:07:50.198: INFO: Creating new exec pod
Feb 27 16:07:50.206: INFO: Waiting up to 5m0s for pod "execpodcstk4" in namespace "services-8437" to be "running"
Feb 27 16:07:50.211: INFO: Pod "execpodcstk4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40652ms
Feb 27 16:07:52.214: INFO: Pod "execpodcstk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007565276s
Feb 27 16:07:52.214: INFO: Pod "execpodcstk4" satisfied condition "running"
Feb 27 16:07:53.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Feb 27 16:07:53.339: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 27 16:07:53.339: INFO: stdout: ""
Feb 27 16:07:53.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 10.152.183.55 80'
Feb 27 16:07:53.442: INFO: stderr: "+ nc -v -z -w 2 10.152.183.55 80\nConnection to 10.152.183.55 80 port [tcp/http] succeeded!\n"
Feb 27 16:07:53.442: INFO: stdout: ""
Feb 27 16:07:53.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 31218'
Feb 27 16:07:53.547: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 31218\nConnection to 172.31.42.40 31218 port [tcp/*] succeeded!\n"
Feb 27 16:07:53.547: INFO: stdout: ""
Feb 27 16:07:53.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 172.31.3.182 31218'
Feb 27 16:07:53.650: INFO: stderr: "+ nc -v -z -w 2 172.31.3.182 31218\nConnection to 172.31.3.182 31218 port [tcp/*] succeeded!\n"
Feb 27 16:07:53.650: INFO: stdout: ""
Feb 27 16:07:53.650: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:53.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8437" for this suite. 02/27/23 16:07:53.678
------------------------------
• [SLOW TEST] [6.594 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:47.09
    Feb 27 16:07:47.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 16:07:47.091
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:47.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:47.112
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8437 02/27/23 16:07:47.115
    STEP: changing the ExternalName service to type=NodePort 02/27/23 16:07:47.122
    STEP: creating replication controller externalname-service in namespace services-8437 02/27/23 16:07:47.142
    I0227 16:07:47.147868      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8437, replica count: 2
    I0227 16:07:50.198471      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 16:07:50.198: INFO: Creating new exec pod
    Feb 27 16:07:50.206: INFO: Waiting up to 5m0s for pod "execpodcstk4" in namespace "services-8437" to be "running"
    Feb 27 16:07:50.211: INFO: Pod "execpodcstk4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40652ms
    Feb 27 16:07:52.214: INFO: Pod "execpodcstk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007565276s
    Feb 27 16:07:52.214: INFO: Pod "execpodcstk4" satisfied condition "running"
    Feb 27 16:07:53.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Feb 27 16:07:53.339: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 27 16:07:53.339: INFO: stdout: ""
    Feb 27 16:07:53.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 10.152.183.55 80'
    Feb 27 16:07:53.442: INFO: stderr: "+ nc -v -z -w 2 10.152.183.55 80\nConnection to 10.152.183.55 80 port [tcp/http] succeeded!\n"
    Feb 27 16:07:53.442: INFO: stdout: ""
    Feb 27 16:07:53.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 172.31.42.40 31218'
    Feb 27 16:07:53.547: INFO: stderr: "+ nc -v -z -w 2 172.31.42.40 31218\nConnection to 172.31.42.40 31218 port [tcp/*] succeeded!\n"
    Feb 27 16:07:53.547: INFO: stdout: ""
    Feb 27 16:07:53.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-8437 exec execpodcstk4 -- /bin/sh -x -c nc -v -z -w 2 172.31.3.182 31218'
    Feb 27 16:07:53.650: INFO: stderr: "+ nc -v -z -w 2 172.31.3.182 31218\nConnection to 172.31.3.182 31218 port [tcp/*] succeeded!\n"
    Feb 27 16:07:53.650: INFO: stdout: ""
    Feb 27 16:07:53.650: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:53.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8437" for this suite. 02/27/23 16:07:53.678
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:53.685
Feb 27 16:07:53.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-runtime 02/27/23 16:07:53.686
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:53.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:53.71
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 02/27/23 16:07:53.713
STEP: wait for the container to reach Failed 02/27/23 16:07:53.72
STEP: get the container status 02/27/23 16:07:57.741
STEP: the container should be terminated 02/27/23 16:07:57.745
STEP: the termination message should be set 02/27/23 16:07:57.745
Feb 27 16:07:57.745: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/27/23 16:07:57.745
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:57.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5257" for this suite. 02/27/23 16:07:57.767
------------------------------
• [4.092 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:53.685
    Feb 27 16:07:53.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-runtime 02/27/23 16:07:53.686
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:53.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:53.71
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 02/27/23 16:07:53.713
    STEP: wait for the container to reach Failed 02/27/23 16:07:53.72
    STEP: get the container status 02/27/23 16:07:57.741
    STEP: the container should be terminated 02/27/23 16:07:57.745
    STEP: the termination message should be set 02/27/23 16:07:57.745
    Feb 27 16:07:57.745: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/27/23 16:07:57.745
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:57.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5257" for this suite. 02/27/23 16:07:57.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:57.778
Feb 27 16:07:57.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename conformance-tests 02/27/23 16:07:57.778
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:57.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:57.797
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/27/23 16:07:57.8
Feb 27 16:07:57.800: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:57.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-2573" for this suite. 02/27/23 16:07:57.809
------------------------------
• [0.038 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:57.778
    Feb 27 16:07:57.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename conformance-tests 02/27/23 16:07:57.778
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:57.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:57.797
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/27/23 16:07:57.8
    Feb 27 16:07:57.800: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:57.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-2573" for this suite. 02/27/23 16:07:57.809
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:57.816
Feb 27 16:07:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:07:57.817
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:57.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:57.836
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 02/27/23 16:07:57.839
STEP: Getting a ResourceQuota 02/27/23 16:07:57.845
STEP: Updating a ResourceQuota 02/27/23 16:07:57.851
STEP: Verifying a ResourceQuota was modified 02/27/23 16:07:57.856
STEP: Deleting a ResourceQuota 02/27/23 16:07:57.862
STEP: Verifying the deleted ResourceQuota 02/27/23 16:07:57.868
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:07:57.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5451" for this suite. 02/27/23 16:07:57.876
------------------------------
• [0.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:57.816
    Feb 27 16:07:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:07:57.817
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:57.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:57.836
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 02/27/23 16:07:57.839
    STEP: Getting a ResourceQuota 02/27/23 16:07:57.845
    STEP: Updating a ResourceQuota 02/27/23 16:07:57.851
    STEP: Verifying a ResourceQuota was modified 02/27/23 16:07:57.856
    STEP: Deleting a ResourceQuota 02/27/23 16:07:57.862
    STEP: Verifying the deleted ResourceQuota 02/27/23 16:07:57.868
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:07:57.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5451" for this suite. 02/27/23 16:07:57.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:07:57.884
Feb 27 16:07:57.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename init-container 02/27/23 16:07:57.885
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:57.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:57.907
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 02/27/23 16:07:57.909
Feb 27 16:07:57.909: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:03.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3378" for this suite. 02/27/23 16:08:03.116
------------------------------
• [SLOW TEST] [5.240 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:07:57.884
    Feb 27 16:07:57.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename init-container 02/27/23 16:07:57.885
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:07:57.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:07:57.907
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 02/27/23 16:07:57.909
    Feb 27 16:07:57.909: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:03.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3378" for this suite. 02/27/23 16:08:03.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:03.125
Feb 27 16:08:03.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 16:08:03.126
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:03.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:03.151
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb 27 16:08:03.175: INFO: Waiting up to 5m0s for pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0" in namespace "emptydir-wrapper-4949" to be "running and ready"
Feb 27 16:08:03.182: INFO: Pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957273ms
Feb 27 16:08:03.182: INFO: The phase of Pod pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:08:05.186: INFO: Pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010830329s
Feb 27 16:08:05.186: INFO: The phase of Pod pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0 is Running (Ready = true)
Feb 27 16:08:05.186: INFO: Pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/27/23 16:08:05.19
STEP: Cleaning up the configmap 02/27/23 16:08:05.196
STEP: Cleaning up the pod 02/27/23 16:08:05.202
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:05.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-4949" for this suite. 02/27/23 16:08:05.221
------------------------------
• [2.102 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:03.125
    Feb 27 16:08:03.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 16:08:03.126
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:03.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:03.151
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb 27 16:08:03.175: INFO: Waiting up to 5m0s for pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0" in namespace "emptydir-wrapper-4949" to be "running and ready"
    Feb 27 16:08:03.182: INFO: Pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957273ms
    Feb 27 16:08:03.182: INFO: The phase of Pod pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:08:05.186: INFO: Pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010830329s
    Feb 27 16:08:05.186: INFO: The phase of Pod pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0 is Running (Ready = true)
    Feb 27 16:08:05.186: INFO: Pod "pod-secrets-4d903841-3855-4f9b-a3cd-cd0773bd55b0" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/27/23 16:08:05.19
    STEP: Cleaning up the configmap 02/27/23 16:08:05.196
    STEP: Cleaning up the pod 02/27/23 16:08:05.202
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:05.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-4949" for this suite. 02/27/23 16:08:05.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:05.228
Feb 27 16:08:05.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename sysctl 02/27/23 16:08:05.229
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:05.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:05.251
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/27/23 16:08:05.254
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:05.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-1631" for this suite. 02/27/23 16:08:05.263
------------------------------
• [0.041 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:05.228
    Feb 27 16:08:05.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename sysctl 02/27/23 16:08:05.229
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:05.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:05.251
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/27/23 16:08:05.254
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:05.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-1631" for this suite. 02/27/23 16:08:05.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:05.27
Feb 27 16:08:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svc-latency 02/27/23 16:08:05.27
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:05.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:05.289
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb 27 16:08:05.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: creating replication controller svc-latency-rc in namespace svc-latency-519 02/27/23 16:08:05.292
I0227 16:08:05.298913      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-519, replica count: 1
I0227 16:08:06.349753      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0227 16:08:07.349881      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 16:08:07.461: INFO: Created: latency-svc-4gczl
Feb 27 16:08:07.473: INFO: Got endpoints: latency-svc-4gczl [22.884914ms]
Feb 27 16:08:07.488: INFO: Created: latency-svc-5k5pr
Feb 27 16:08:07.497: INFO: Got endpoints: latency-svc-5k5pr [23.716938ms]
Feb 27 16:08:07.498: INFO: Created: latency-svc-qkt8q
Feb 27 16:08:07.503: INFO: Created: latency-svc-pqnhm
Feb 27 16:08:07.505: INFO: Got endpoints: latency-svc-qkt8q [31.189919ms]
Feb 27 16:08:07.513: INFO: Got endpoints: latency-svc-pqnhm [39.653362ms]
Feb 27 16:08:07.514: INFO: Created: latency-svc-pkhtl
Feb 27 16:08:07.519: INFO: Got endpoints: latency-svc-pkhtl [45.081053ms]
Feb 27 16:08:07.526: INFO: Created: latency-svc-d7jbd
Feb 27 16:08:07.529: INFO: Got endpoints: latency-svc-d7jbd [55.579589ms]
Feb 27 16:08:07.538: INFO: Created: latency-svc-v6lfm
Feb 27 16:08:07.541: INFO: Got endpoints: latency-svc-v6lfm [67.148481ms]
Feb 27 16:08:07.545: INFO: Created: latency-svc-wckst
Feb 27 16:08:07.549: INFO: Got endpoints: latency-svc-wckst [74.624983ms]
Feb 27 16:08:07.553: INFO: Created: latency-svc-dkktj
Feb 27 16:08:07.558: INFO: Got endpoints: latency-svc-dkktj [83.513675ms]
Feb 27 16:08:07.564: INFO: Created: latency-svc-fk7pb
Feb 27 16:08:07.567: INFO: Got endpoints: latency-svc-fk7pb [93.30419ms]
Feb 27 16:08:07.572: INFO: Created: latency-svc-t9j78
Feb 27 16:08:07.576: INFO: Got endpoints: latency-svc-t9j78 [102.043683ms]
Feb 27 16:08:07.579: INFO: Created: latency-svc-psrkb
Feb 27 16:08:07.584: INFO: Got endpoints: latency-svc-psrkb [110.351143ms]
Feb 27 16:08:07.588: INFO: Created: latency-svc-svmzw
Feb 27 16:08:07.591: INFO: Got endpoints: latency-svc-svmzw [117.23641ms]
Feb 27 16:08:07.592: INFO: Created: latency-svc-f669h
Feb 27 16:08:07.600: INFO: Got endpoints: latency-svc-f669h [125.540219ms]
Feb 27 16:08:07.604: INFO: Created: latency-svc-r6xk6
Feb 27 16:08:07.609: INFO: Got endpoints: latency-svc-r6xk6 [135.152836ms]
Feb 27 16:08:07.614: INFO: Created: latency-svc-2n9ql
Feb 27 16:08:07.618: INFO: Got endpoints: latency-svc-2n9ql [144.067556ms]
Feb 27 16:08:07.621: INFO: Created: latency-svc-76xlp
Feb 27 16:08:07.625: INFO: Got endpoints: latency-svc-76xlp [127.643295ms]
Feb 27 16:08:07.629: INFO: Created: latency-svc-4nvc5
Feb 27 16:08:07.632: INFO: Got endpoints: latency-svc-4nvc5 [126.952232ms]
Feb 27 16:08:07.638: INFO: Created: latency-svc-pjxtz
Feb 27 16:08:07.646: INFO: Got endpoints: latency-svc-pjxtz [132.443709ms]
Feb 27 16:08:07.647: INFO: Created: latency-svc-7bgn8
Feb 27 16:08:07.651: INFO: Got endpoints: latency-svc-7bgn8 [132.411872ms]
Feb 27 16:08:07.656: INFO: Created: latency-svc-spsk5
Feb 27 16:08:07.659: INFO: Got endpoints: latency-svc-spsk5 [129.219386ms]
Feb 27 16:08:07.666: INFO: Created: latency-svc-6nq4w
Feb 27 16:08:07.669: INFO: Got endpoints: latency-svc-6nq4w [128.307689ms]
Feb 27 16:08:07.671: INFO: Created: latency-svc-57lp4
Feb 27 16:08:07.674: INFO: Got endpoints: latency-svc-57lp4 [125.242637ms]
Feb 27 16:08:07.680: INFO: Created: latency-svc-4zm4l
Feb 27 16:08:07.685: INFO: Got endpoints: latency-svc-4zm4l [127.035886ms]
Feb 27 16:08:07.685: INFO: Created: latency-svc-8xsd5
Feb 27 16:08:07.690: INFO: Created: latency-svc-b94nv
Feb 27 16:08:07.693: INFO: Got endpoints: latency-svc-8xsd5 [125.524559ms]
Feb 27 16:08:07.694: INFO: Got endpoints: latency-svc-b94nv [117.769173ms]
Feb 27 16:08:07.710: INFO: Created: latency-svc-f8mtq
Feb 27 16:08:07.711: INFO: Got endpoints: latency-svc-f8mtq [126.774816ms]
Feb 27 16:08:07.718: INFO: Created: latency-svc-bmqhn
Feb 27 16:08:07.720: INFO: Got endpoints: latency-svc-bmqhn [128.462246ms]
Feb 27 16:08:07.722: INFO: Created: latency-svc-fhksg
Feb 27 16:08:07.726: INFO: Got endpoints: latency-svc-fhksg [126.482942ms]
Feb 27 16:08:07.732: INFO: Created: latency-svc-vbprn
Feb 27 16:08:07.735: INFO: Got endpoints: latency-svc-vbprn [126.005442ms]
Feb 27 16:08:07.742: INFO: Created: latency-svc-8m8hk
Feb 27 16:08:07.748: INFO: Got endpoints: latency-svc-8m8hk [129.822485ms]
Feb 27 16:08:07.752: INFO: Created: latency-svc-dcgj8
Feb 27 16:08:07.756: INFO: Got endpoints: latency-svc-dcgj8 [131.432105ms]
Feb 27 16:08:07.762: INFO: Created: latency-svc-ffbmp
Feb 27 16:08:07.768: INFO: Got endpoints: latency-svc-ffbmp [135.386327ms]
Feb 27 16:08:07.770: INFO: Created: latency-svc-c97pt
Feb 27 16:08:07.774: INFO: Got endpoints: latency-svc-c97pt [127.734315ms]
Feb 27 16:08:07.777: INFO: Created: latency-svc-gcswx
Feb 27 16:08:07.781: INFO: Got endpoints: latency-svc-gcswx [129.130718ms]
Feb 27 16:08:07.785: INFO: Created: latency-svc-g9f4w
Feb 27 16:08:07.790: INFO: Got endpoints: latency-svc-g9f4w [131.608893ms]
Feb 27 16:08:07.792: INFO: Created: latency-svc-z7jwr
Feb 27 16:08:07.797: INFO: Created: latency-svc-bzk9k
Feb 27 16:08:07.804: INFO: Created: latency-svc-bvgrb
Feb 27 16:08:07.810: INFO: Created: latency-svc-nqzhw
Feb 27 16:08:07.817: INFO: Got endpoints: latency-svc-z7jwr [147.009489ms]
Feb 27 16:08:07.822: INFO: Created: latency-svc-bmzvl
Feb 27 16:08:07.829: INFO: Created: latency-svc-xg6mt
Feb 27 16:08:07.836: INFO: Created: latency-svc-kt65z
Feb 27 16:08:07.844: INFO: Created: latency-svc-xg9g2
Feb 27 16:08:07.850: INFO: Created: latency-svc-wppjn
Feb 27 16:08:07.857: INFO: Created: latency-svc-5zgs8
Feb 27 16:08:07.862: INFO: Created: latency-svc-hbdwn
Feb 27 16:08:07.865: INFO: Got endpoints: latency-svc-bzk9k [191.238209ms]
Feb 27 16:08:07.869: INFO: Created: latency-svc-48flf
Feb 27 16:08:07.877: INFO: Created: latency-svc-pqm62
Feb 27 16:08:07.882: INFO: Created: latency-svc-vwxw8
Feb 27 16:08:07.884: INFO: Created: latency-svc-l2rn7
Feb 27 16:08:07.891: INFO: Created: latency-svc-lrhnr
Feb 27 16:08:07.897: INFO: Created: latency-svc-9fstj
Feb 27 16:08:07.914: INFO: Got endpoints: latency-svc-bvgrb [229.275942ms]
Feb 27 16:08:07.926: INFO: Created: latency-svc-wlrr9
Feb 27 16:08:07.965: INFO: Got endpoints: latency-svc-nqzhw [272.13657ms]
Feb 27 16:08:07.977: INFO: Created: latency-svc-wqwtx
Feb 27 16:08:08.014: INFO: Got endpoints: latency-svc-bmzvl [302.741186ms]
Feb 27 16:08:08.024: INFO: Created: latency-svc-cm6jd
Feb 27 16:08:08.065: INFO: Got endpoints: latency-svc-xg6mt [344.877787ms]
Feb 27 16:08:08.077: INFO: Created: latency-svc-ngv6c
Feb 27 16:08:08.115: INFO: Got endpoints: latency-svc-kt65z [388.894228ms]
Feb 27 16:08:08.134: INFO: Created: latency-svc-k6gbs
Feb 27 16:08:08.165: INFO: Got endpoints: latency-svc-xg9g2 [429.103785ms]
Feb 27 16:08:08.175: INFO: Created: latency-svc-wv4s9
Feb 27 16:08:08.217: INFO: Got endpoints: latency-svc-wppjn [468.579479ms]
Feb 27 16:08:08.229: INFO: Created: latency-svc-h95zn
Feb 27 16:08:08.266: INFO: Got endpoints: latency-svc-5zgs8 [509.339211ms]
Feb 27 16:08:08.277: INFO: Created: latency-svc-mgzkx
Feb 27 16:08:08.317: INFO: Got endpoints: latency-svc-hbdwn [549.230289ms]
Feb 27 16:08:08.327: INFO: Created: latency-svc-z6ptf
Feb 27 16:08:08.365: INFO: Got endpoints: latency-svc-48flf [591.369015ms]
Feb 27 16:08:08.379: INFO: Created: latency-svc-p5dnl
Feb 27 16:08:08.416: INFO: Got endpoints: latency-svc-pqm62 [635.775074ms]
Feb 27 16:08:08.427: INFO: Created: latency-svc-jh8vz
Feb 27 16:08:08.465: INFO: Got endpoints: latency-svc-vwxw8 [674.882001ms]
Feb 27 16:08:08.480: INFO: Created: latency-svc-wbtpl
Feb 27 16:08:08.514: INFO: Got endpoints: latency-svc-l2rn7 [819.994614ms]
Feb 27 16:08:08.525: INFO: Created: latency-svc-vsmxj
Feb 27 16:08:08.565: INFO: Got endpoints: latency-svc-lrhnr [748.084258ms]
Feb 27 16:08:08.574: INFO: Created: latency-svc-znsj9
Feb 27 16:08:08.616: INFO: Got endpoints: latency-svc-9fstj [750.963973ms]
Feb 27 16:08:08.627: INFO: Created: latency-svc-c6b92
Feb 27 16:08:08.666: INFO: Got endpoints: latency-svc-wlrr9 [751.690924ms]
Feb 27 16:08:08.677: INFO: Created: latency-svc-6fkbh
Feb 27 16:08:08.717: INFO: Got endpoints: latency-svc-wqwtx [751.389833ms]
Feb 27 16:08:08.729: INFO: Created: latency-svc-2kplj
Feb 27 16:08:08.765: INFO: Got endpoints: latency-svc-cm6jd [751.097989ms]
Feb 27 16:08:08.815: INFO: Got endpoints: latency-svc-ngv6c [749.68779ms]
Feb 27 16:08:08.848: INFO: Created: latency-svc-zqsdw
Feb 27 16:08:08.855: INFO: Created: latency-svc-lmv6x
Feb 27 16:08:08.866: INFO: Got endpoints: latency-svc-k6gbs [750.954917ms]
Feb 27 16:08:08.876: INFO: Created: latency-svc-m5w4j
Feb 27 16:08:08.914: INFO: Got endpoints: latency-svc-wv4s9 [749.544729ms]
Feb 27 16:08:08.926: INFO: Created: latency-svc-zswnl
Feb 27 16:08:08.965: INFO: Got endpoints: latency-svc-h95zn [748.142455ms]
Feb 27 16:08:08.978: INFO: Created: latency-svc-48vzg
Feb 27 16:08:09.016: INFO: Got endpoints: latency-svc-mgzkx [749.855268ms]
Feb 27 16:08:09.025: INFO: Created: latency-svc-xclhr
Feb 27 16:08:09.067: INFO: Got endpoints: latency-svc-z6ptf [750.503381ms]
Feb 27 16:08:09.084: INFO: Created: latency-svc-kl2tl
Feb 27 16:08:09.116: INFO: Got endpoints: latency-svc-p5dnl [750.635992ms]
Feb 27 16:08:09.129: INFO: Created: latency-svc-ct5mq
Feb 27 16:08:09.165: INFO: Got endpoints: latency-svc-jh8vz [748.74472ms]
Feb 27 16:08:09.177: INFO: Created: latency-svc-7dw4d
Feb 27 16:08:09.215: INFO: Got endpoints: latency-svc-wbtpl [749.478743ms]
Feb 27 16:08:09.227: INFO: Created: latency-svc-2t9xz
Feb 27 16:08:09.264: INFO: Got endpoints: latency-svc-vsmxj [750.37299ms]
Feb 27 16:08:09.277: INFO: Created: latency-svc-zbvmp
Feb 27 16:08:09.316: INFO: Got endpoints: latency-svc-znsj9 [751.245357ms]
Feb 27 16:08:09.327: INFO: Created: latency-svc-jr7gw
Feb 27 16:08:09.365: INFO: Got endpoints: latency-svc-c6b92 [748.328131ms]
Feb 27 16:08:09.375: INFO: Created: latency-svc-lpbpz
Feb 27 16:08:09.415: INFO: Got endpoints: latency-svc-6fkbh [748.923397ms]
Feb 27 16:08:09.429: INFO: Created: latency-svc-kpmp8
Feb 27 16:08:09.465: INFO: Got endpoints: latency-svc-2kplj [748.586898ms]
Feb 27 16:08:09.477: INFO: Created: latency-svc-d542m
Feb 27 16:08:09.518: INFO: Got endpoints: latency-svc-zqsdw [752.515378ms]
Feb 27 16:08:09.528: INFO: Created: latency-svc-mrz7b
Feb 27 16:08:09.565: INFO: Got endpoints: latency-svc-lmv6x [750.151644ms]
Feb 27 16:08:09.576: INFO: Created: latency-svc-x75tw
Feb 27 16:08:09.616: INFO: Got endpoints: latency-svc-m5w4j [749.406057ms]
Feb 27 16:08:09.626: INFO: Created: latency-svc-xbk7v
Feb 27 16:08:09.665: INFO: Got endpoints: latency-svc-zswnl [750.421804ms]
Feb 27 16:08:09.678: INFO: Created: latency-svc-76bpf
Feb 27 16:08:09.715: INFO: Got endpoints: latency-svc-48vzg [749.964816ms]
Feb 27 16:08:09.738: INFO: Created: latency-svc-4pt6g
Feb 27 16:08:09.767: INFO: Got endpoints: latency-svc-xclhr [751.135664ms]
Feb 27 16:08:09.776: INFO: Created: latency-svc-ndmqh
Feb 27 16:08:09.816: INFO: Got endpoints: latency-svc-kl2tl [748.974926ms]
Feb 27 16:08:09.826: INFO: Created: latency-svc-xq2r7
Feb 27 16:08:09.865: INFO: Got endpoints: latency-svc-ct5mq [749.576346ms]
Feb 27 16:08:09.877: INFO: Created: latency-svc-9276j
Feb 27 16:08:09.915: INFO: Got endpoints: latency-svc-7dw4d [750.080776ms]
Feb 27 16:08:09.927: INFO: Created: latency-svc-s2d6m
Feb 27 16:08:09.966: INFO: Got endpoints: latency-svc-2t9xz [750.971815ms]
Feb 27 16:08:09.977: INFO: Created: latency-svc-265b5
Feb 27 16:08:10.014: INFO: Got endpoints: latency-svc-zbvmp [749.948788ms]
Feb 27 16:08:10.025: INFO: Created: latency-svc-klbb8
Feb 27 16:08:10.067: INFO: Got endpoints: latency-svc-jr7gw [750.878575ms]
Feb 27 16:08:10.078: INFO: Created: latency-svc-xzfb9
Feb 27 16:08:10.115: INFO: Got endpoints: latency-svc-lpbpz [750.375157ms]
Feb 27 16:08:10.127: INFO: Created: latency-svc-q9rlx
Feb 27 16:08:10.165: INFO: Got endpoints: latency-svc-kpmp8 [750.764724ms]
Feb 27 16:08:10.178: INFO: Created: latency-svc-v98j4
Feb 27 16:08:10.217: INFO: Got endpoints: latency-svc-d542m [751.068309ms]
Feb 27 16:08:10.228: INFO: Created: latency-svc-d9s7m
Feb 27 16:08:10.269: INFO: Got endpoints: latency-svc-mrz7b [751.268107ms]
Feb 27 16:08:10.279: INFO: Created: latency-svc-r28sr
Feb 27 16:08:10.315: INFO: Got endpoints: latency-svc-x75tw [749.958443ms]
Feb 27 16:08:10.327: INFO: Created: latency-svc-4mxlk
Feb 27 16:08:10.364: INFO: Got endpoints: latency-svc-xbk7v [748.26897ms]
Feb 27 16:08:10.374: INFO: Created: latency-svc-9k27d
Feb 27 16:08:10.418: INFO: Got endpoints: latency-svc-76bpf [752.927173ms]
Feb 27 16:08:10.430: INFO: Created: latency-svc-kh6n7
Feb 27 16:08:10.469: INFO: Got endpoints: latency-svc-4pt6g [754.270138ms]
Feb 27 16:08:10.485: INFO: Created: latency-svc-zr6dx
Feb 27 16:08:10.520: INFO: Got endpoints: latency-svc-ndmqh [753.060658ms]
Feb 27 16:08:10.530: INFO: Created: latency-svc-tmdkb
Feb 27 16:08:10.565: INFO: Got endpoints: latency-svc-xq2r7 [748.64882ms]
Feb 27 16:08:10.575: INFO: Created: latency-svc-5xdn7
Feb 27 16:08:10.615: INFO: Got endpoints: latency-svc-9276j [749.863816ms]
Feb 27 16:08:10.628: INFO: Created: latency-svc-29rvs
Feb 27 16:08:10.664: INFO: Got endpoints: latency-svc-s2d6m [748.676164ms]
Feb 27 16:08:10.674: INFO: Created: latency-svc-j265w
Feb 27 16:08:10.716: INFO: Got endpoints: latency-svc-265b5 [750.322066ms]
Feb 27 16:08:10.726: INFO: Created: latency-svc-gfvg2
Feb 27 16:08:10.765: INFO: Got endpoints: latency-svc-klbb8 [750.752034ms]
Feb 27 16:08:10.776: INFO: Created: latency-svc-f6brp
Feb 27 16:08:10.817: INFO: Got endpoints: latency-svc-xzfb9 [749.661114ms]
Feb 27 16:08:10.828: INFO: Created: latency-svc-zkrsz
Feb 27 16:08:10.866: INFO: Got endpoints: latency-svc-q9rlx [750.048085ms]
Feb 27 16:08:10.876: INFO: Created: latency-svc-vzk4c
Feb 27 16:08:10.918: INFO: Got endpoints: latency-svc-v98j4 [752.784973ms]
Feb 27 16:08:10.931: INFO: Created: latency-svc-9v759
Feb 27 16:08:10.964: INFO: Got endpoints: latency-svc-d9s7m [747.49943ms]
Feb 27 16:08:10.980: INFO: Created: latency-svc-6xzkv
Feb 27 16:08:11.015: INFO: Got endpoints: latency-svc-r28sr [745.738127ms]
Feb 27 16:08:11.027: INFO: Created: latency-svc-mv9k9
Feb 27 16:08:11.072: INFO: Got endpoints: latency-svc-4mxlk [756.668324ms]
Feb 27 16:08:11.085: INFO: Created: latency-svc-zq9wm
Feb 27 16:08:11.114: INFO: Got endpoints: latency-svc-9k27d [750.3787ms]
Feb 27 16:08:11.126: INFO: Created: latency-svc-86kfl
Feb 27 16:08:11.165: INFO: Got endpoints: latency-svc-kh6n7 [747.086116ms]
Feb 27 16:08:11.175: INFO: Created: latency-svc-92z58
Feb 27 16:08:11.217: INFO: Got endpoints: latency-svc-zr6dx [747.407665ms]
Feb 27 16:08:11.227: INFO: Created: latency-svc-t4htm
Feb 27 16:08:11.266: INFO: Got endpoints: latency-svc-tmdkb [745.744165ms]
Feb 27 16:08:11.277: INFO: Created: latency-svc-2nrq9
Feb 27 16:08:11.315: INFO: Got endpoints: latency-svc-5xdn7 [749.703134ms]
Feb 27 16:08:11.327: INFO: Created: latency-svc-5g46z
Feb 27 16:08:11.366: INFO: Got endpoints: latency-svc-29rvs [750.425389ms]
Feb 27 16:08:11.377: INFO: Created: latency-svc-hbw5v
Feb 27 16:08:11.417: INFO: Got endpoints: latency-svc-j265w [752.608516ms]
Feb 27 16:08:11.427: INFO: Created: latency-svc-qpkgj
Feb 27 16:08:11.465: INFO: Got endpoints: latency-svc-gfvg2 [748.402618ms]
Feb 27 16:08:11.478: INFO: Created: latency-svc-4h46s
Feb 27 16:08:11.517: INFO: Got endpoints: latency-svc-f6brp [751.458319ms]
Feb 27 16:08:11.527: INFO: Created: latency-svc-plxl2
Feb 27 16:08:11.566: INFO: Got endpoints: latency-svc-zkrsz [749.015565ms]
Feb 27 16:08:11.575: INFO: Created: latency-svc-c4tsj
Feb 27 16:08:11.616: INFO: Got endpoints: latency-svc-vzk4c [749.821953ms]
Feb 27 16:08:11.627: INFO: Created: latency-svc-kjhdc
Feb 27 16:08:11.666: INFO: Got endpoints: latency-svc-9v759 [747.752113ms]
Feb 27 16:08:11.677: INFO: Created: latency-svc-4gl4w
Feb 27 16:08:11.717: INFO: Got endpoints: latency-svc-6xzkv [752.775391ms]
Feb 27 16:08:11.727: INFO: Created: latency-svc-d66b8
Feb 27 16:08:11.765: INFO: Got endpoints: latency-svc-mv9k9 [750.262657ms]
Feb 27 16:08:11.777: INFO: Created: latency-svc-plz8j
Feb 27 16:08:11.816: INFO: Got endpoints: latency-svc-zq9wm [744.246997ms]
Feb 27 16:08:11.829: INFO: Created: latency-svc-xvfbt
Feb 27 16:08:11.866: INFO: Got endpoints: latency-svc-86kfl [751.270101ms]
Feb 27 16:08:11.877: INFO: Created: latency-svc-6r6pr
Feb 27 16:08:11.916: INFO: Got endpoints: latency-svc-92z58 [751.206791ms]
Feb 27 16:08:11.927: INFO: Created: latency-svc-wn857
Feb 27 16:08:11.968: INFO: Got endpoints: latency-svc-t4htm [750.822627ms]
Feb 27 16:08:11.979: INFO: Created: latency-svc-5wckr
Feb 27 16:08:12.018: INFO: Got endpoints: latency-svc-2nrq9 [752.059766ms]
Feb 27 16:08:12.027: INFO: Created: latency-svc-f4dr2
Feb 27 16:08:12.065: INFO: Got endpoints: latency-svc-5g46z [750.359591ms]
Feb 27 16:08:12.078: INFO: Created: latency-svc-fw8wp
Feb 27 16:08:12.115: INFO: Got endpoints: latency-svc-hbw5v [749.450417ms]
Feb 27 16:08:12.130: INFO: Created: latency-svc-7mjtl
Feb 27 16:08:12.167: INFO: Got endpoints: latency-svc-qpkgj [750.262789ms]
Feb 27 16:08:12.177: INFO: Created: latency-svc-vfqr9
Feb 27 16:08:12.216: INFO: Got endpoints: latency-svc-4h46s [750.878407ms]
Feb 27 16:08:12.232: INFO: Created: latency-svc-z4lrq
Feb 27 16:08:12.267: INFO: Got endpoints: latency-svc-plxl2 [750.295974ms]
Feb 27 16:08:12.281: INFO: Created: latency-svc-srvj4
Feb 27 16:08:12.316: INFO: Got endpoints: latency-svc-c4tsj [750.118667ms]
Feb 27 16:08:12.326: INFO: Created: latency-svc-qsjwd
Feb 27 16:08:12.365: INFO: Got endpoints: latency-svc-kjhdc [749.266975ms]
Feb 27 16:08:12.378: INFO: Created: latency-svc-wl248
Feb 27 16:08:12.417: INFO: Got endpoints: latency-svc-4gl4w [751.184226ms]
Feb 27 16:08:12.428: INFO: Created: latency-svc-pd52w
Feb 27 16:08:12.465: INFO: Got endpoints: latency-svc-d66b8 [747.738868ms]
Feb 27 16:08:12.477: INFO: Created: latency-svc-djq78
Feb 27 16:08:12.517: INFO: Got endpoints: latency-svc-plz8j [751.435691ms]
Feb 27 16:08:12.530: INFO: Created: latency-svc-gms4t
Feb 27 16:08:12.567: INFO: Got endpoints: latency-svc-xvfbt [751.044891ms]
Feb 27 16:08:12.578: INFO: Created: latency-svc-hsfgv
Feb 27 16:08:12.615: INFO: Got endpoints: latency-svc-6r6pr [749.044625ms]
Feb 27 16:08:12.625: INFO: Created: latency-svc-9l7vl
Feb 27 16:08:12.666: INFO: Got endpoints: latency-svc-wn857 [749.698181ms]
Feb 27 16:08:12.677: INFO: Created: latency-svc-bbnd6
Feb 27 16:08:12.717: INFO: Got endpoints: latency-svc-5wckr [748.744958ms]
Feb 27 16:08:12.729: INFO: Created: latency-svc-jr6jb
Feb 27 16:08:12.764: INFO: Got endpoints: latency-svc-f4dr2 [746.470684ms]
Feb 27 16:08:12.774: INFO: Created: latency-svc-hvw4d
Feb 27 16:08:12.816: INFO: Got endpoints: latency-svc-fw8wp [750.799456ms]
Feb 27 16:08:12.833: INFO: Created: latency-svc-5nrvw
Feb 27 16:08:12.868: INFO: Got endpoints: latency-svc-7mjtl [752.694954ms]
Feb 27 16:08:12.880: INFO: Created: latency-svc-s6chf
Feb 27 16:08:12.914: INFO: Got endpoints: latency-svc-vfqr9 [746.901769ms]
Feb 27 16:08:12.925: INFO: Created: latency-svc-55bh9
Feb 27 16:08:12.968: INFO: Got endpoints: latency-svc-z4lrq [752.314733ms]
Feb 27 16:08:12.982: INFO: Created: latency-svc-wfkm6
Feb 27 16:08:13.017: INFO: Got endpoints: latency-svc-srvj4 [749.435326ms]
Feb 27 16:08:13.031: INFO: Created: latency-svc-7rzvk
Feb 27 16:08:13.066: INFO: Got endpoints: latency-svc-qsjwd [750.320639ms]
Feb 27 16:08:13.076: INFO: Created: latency-svc-87sxf
Feb 27 16:08:13.116: INFO: Got endpoints: latency-svc-wl248 [750.910311ms]
Feb 27 16:08:13.128: INFO: Created: latency-svc-ksnhn
Feb 27 16:08:13.165: INFO: Got endpoints: latency-svc-pd52w [747.878583ms]
Feb 27 16:08:13.178: INFO: Created: latency-svc-hxqgt
Feb 27 16:08:13.214: INFO: Got endpoints: latency-svc-djq78 [749.276981ms]
Feb 27 16:08:13.224: INFO: Created: latency-svc-vbz7h
Feb 27 16:08:13.268: INFO: Got endpoints: latency-svc-gms4t [751.052253ms]
Feb 27 16:08:13.288: INFO: Created: latency-svc-zjdlh
Feb 27 16:08:13.316: INFO: Got endpoints: latency-svc-hsfgv [748.910407ms]
Feb 27 16:08:13.326: INFO: Created: latency-svc-lzf7r
Feb 27 16:08:13.365: INFO: Got endpoints: latency-svc-9l7vl [750.030249ms]
Feb 27 16:08:13.375: INFO: Created: latency-svc-7sdwk
Feb 27 16:08:13.416: INFO: Got endpoints: latency-svc-bbnd6 [750.623894ms]
Feb 27 16:08:13.431: INFO: Created: latency-svc-qf5gc
Feb 27 16:08:13.468: INFO: Got endpoints: latency-svc-jr6jb [751.564757ms]
Feb 27 16:08:13.482: INFO: Created: latency-svc-m5zdh
Feb 27 16:08:13.514: INFO: Got endpoints: latency-svc-hvw4d [750.245815ms]
Feb 27 16:08:13.524: INFO: Created: latency-svc-94qck
Feb 27 16:08:13.567: INFO: Got endpoints: latency-svc-5nrvw [750.583564ms]
Feb 27 16:08:13.580: INFO: Created: latency-svc-sbrhr
Feb 27 16:08:13.615: INFO: Got endpoints: latency-svc-s6chf [747.574153ms]
Feb 27 16:08:13.626: INFO: Created: latency-svc-jg8qf
Feb 27 16:08:13.665: INFO: Got endpoints: latency-svc-55bh9 [750.168126ms]
Feb 27 16:08:13.675: INFO: Created: latency-svc-d54dw
Feb 27 16:08:13.715: INFO: Got endpoints: latency-svc-wfkm6 [747.345509ms]
Feb 27 16:08:13.727: INFO: Created: latency-svc-2rkhd
Feb 27 16:08:13.766: INFO: Got endpoints: latency-svc-7rzvk [749.504725ms]
Feb 27 16:08:13.781: INFO: Created: latency-svc-82jvx
Feb 27 16:08:13.816: INFO: Got endpoints: latency-svc-87sxf [749.676261ms]
Feb 27 16:08:13.825: INFO: Created: latency-svc-8sv9v
Feb 27 16:08:13.867: INFO: Got endpoints: latency-svc-ksnhn [750.391869ms]
Feb 27 16:08:13.877: INFO: Created: latency-svc-97bs2
Feb 27 16:08:13.915: INFO: Got endpoints: latency-svc-hxqgt [750.181611ms]
Feb 27 16:08:13.928: INFO: Created: latency-svc-8526v
Feb 27 16:08:13.965: INFO: Got endpoints: latency-svc-vbz7h [750.162237ms]
Feb 27 16:08:13.974: INFO: Created: latency-svc-lctvq
Feb 27 16:08:14.016: INFO: Got endpoints: latency-svc-zjdlh [748.591454ms]
Feb 27 16:08:14.029: INFO: Created: latency-svc-6bsnf
Feb 27 16:08:14.066: INFO: Got endpoints: latency-svc-lzf7r [750.026309ms]
Feb 27 16:08:14.076: INFO: Created: latency-svc-xllg7
Feb 27 16:08:14.115: INFO: Got endpoints: latency-svc-7sdwk [749.832773ms]
Feb 27 16:08:14.126: INFO: Created: latency-svc-tt4qg
Feb 27 16:08:14.166: INFO: Got endpoints: latency-svc-qf5gc [749.661024ms]
Feb 27 16:08:14.179: INFO: Created: latency-svc-9c9mq
Feb 27 16:08:14.215: INFO: Got endpoints: latency-svc-m5zdh [746.256766ms]
Feb 27 16:08:14.226: INFO: Created: latency-svc-qnzrn
Feb 27 16:08:14.265: INFO: Got endpoints: latency-svc-94qck [750.214759ms]
Feb 27 16:08:14.274: INFO: Created: latency-svc-m7hw9
Feb 27 16:08:14.316: INFO: Got endpoints: latency-svc-sbrhr [748.869352ms]
Feb 27 16:08:14.328: INFO: Created: latency-svc-n85lv
Feb 27 16:08:14.365: INFO: Got endpoints: latency-svc-jg8qf [749.43553ms]
Feb 27 16:08:14.375: INFO: Created: latency-svc-pbwsj
Feb 27 16:08:14.414: INFO: Got endpoints: latency-svc-d54dw [749.711912ms]
Feb 27 16:08:14.424: INFO: Created: latency-svc-47slq
Feb 27 16:08:14.465: INFO: Got endpoints: latency-svc-2rkhd [749.29711ms]
Feb 27 16:08:14.479: INFO: Created: latency-svc-ssvkx
Feb 27 16:08:14.518: INFO: Got endpoints: latency-svc-82jvx [751.48708ms]
Feb 27 16:08:14.528: INFO: Created: latency-svc-cpcvm
Feb 27 16:08:14.565: INFO: Got endpoints: latency-svc-8sv9v [749.435006ms]
Feb 27 16:08:14.576: INFO: Created: latency-svc-2db6g
Feb 27 16:08:14.616: INFO: Got endpoints: latency-svc-97bs2 [749.248721ms]
Feb 27 16:08:14.629: INFO: Created: latency-svc-s8sqv
Feb 27 16:08:14.665: INFO: Got endpoints: latency-svc-8526v [749.623669ms]
Feb 27 16:08:14.675: INFO: Created: latency-svc-jhnnk
Feb 27 16:08:14.715: INFO: Got endpoints: latency-svc-lctvq [749.925836ms]
Feb 27 16:08:14.724: INFO: Created: latency-svc-4lvnr
Feb 27 16:08:14.766: INFO: Got endpoints: latency-svc-6bsnf [749.795271ms]
Feb 27 16:08:14.780: INFO: Created: latency-svc-vjnqg
Feb 27 16:08:14.818: INFO: Got endpoints: latency-svc-xllg7 [752.255945ms]
Feb 27 16:08:14.829: INFO: Created: latency-svc-7qp2f
Feb 27 16:08:14.876: INFO: Got endpoints: latency-svc-tt4qg [760.592521ms]
Feb 27 16:08:14.893: INFO: Created: latency-svc-rrnfb
Feb 27 16:08:14.918: INFO: Got endpoints: latency-svc-9c9mq [751.540938ms]
Feb 27 16:08:14.933: INFO: Created: latency-svc-jsgpd
Feb 27 16:08:14.965: INFO: Got endpoints: latency-svc-qnzrn [750.454679ms]
Feb 27 16:08:14.976: INFO: Created: latency-svc-7v8xb
Feb 27 16:08:15.014: INFO: Got endpoints: latency-svc-m7hw9 [749.481309ms]
Feb 27 16:08:15.024: INFO: Created: latency-svc-bcfwz
Feb 27 16:08:15.067: INFO: Got endpoints: latency-svc-n85lv [751.218223ms]
Feb 27 16:08:15.079: INFO: Created: latency-svc-6mhvr
Feb 27 16:08:15.116: INFO: Got endpoints: latency-svc-pbwsj [751.075727ms]
Feb 27 16:08:15.127: INFO: Created: latency-svc-ppdgd
Feb 27 16:08:15.164: INFO: Got endpoints: latency-svc-47slq [749.774904ms]
Feb 27 16:08:15.174: INFO: Created: latency-svc-rmlqv
Feb 27 16:08:15.217: INFO: Got endpoints: latency-svc-ssvkx [751.55316ms]
Feb 27 16:08:15.229: INFO: Created: latency-svc-5pwz4
Feb 27 16:08:15.265: INFO: Got endpoints: latency-svc-cpcvm [746.903048ms]
Feb 27 16:08:15.278: INFO: Created: latency-svc-5rs2g
Feb 27 16:08:15.316: INFO: Got endpoints: latency-svc-2db6g [750.302886ms]
Feb 27 16:08:15.366: INFO: Got endpoints: latency-svc-s8sqv [750.244368ms]
Feb 27 16:08:15.416: INFO: Got endpoints: latency-svc-jhnnk [750.875588ms]
Feb 27 16:08:15.465: INFO: Got endpoints: latency-svc-4lvnr [750.239741ms]
Feb 27 16:08:15.521: INFO: Got endpoints: latency-svc-vjnqg [754.139624ms]
Feb 27 16:08:15.566: INFO: Got endpoints: latency-svc-7qp2f [747.20137ms]
Feb 27 16:08:15.615: INFO: Got endpoints: latency-svc-rrnfb [739.198926ms]
Feb 27 16:08:15.664: INFO: Got endpoints: latency-svc-jsgpd [746.530122ms]
Feb 27 16:08:15.715: INFO: Got endpoints: latency-svc-7v8xb [749.637876ms]
Feb 27 16:08:15.765: INFO: Got endpoints: latency-svc-bcfwz [750.673273ms]
Feb 27 16:08:15.815: INFO: Got endpoints: latency-svc-6mhvr [747.434279ms]
Feb 27 16:08:15.865: INFO: Got endpoints: latency-svc-ppdgd [748.925257ms]
Feb 27 16:08:15.914: INFO: Got endpoints: latency-svc-rmlqv [749.371352ms]
Feb 27 16:08:15.966: INFO: Got endpoints: latency-svc-5pwz4 [749.070745ms]
Feb 27 16:08:16.016: INFO: Got endpoints: latency-svc-5rs2g [751.395235ms]
Feb 27 16:08:16.017: INFO: Latencies: [23.716938ms 31.189919ms 39.653362ms 45.081053ms 55.579589ms 67.148481ms 74.624983ms 83.513675ms 93.30419ms 102.043683ms 110.351143ms 117.23641ms 117.769173ms 125.242637ms 125.524559ms 125.540219ms 126.005442ms 126.482942ms 126.774816ms 126.952232ms 127.035886ms 127.643295ms 127.734315ms 128.307689ms 128.462246ms 129.130718ms 129.219386ms 129.822485ms 131.432105ms 131.608893ms 132.411872ms 132.443709ms 135.152836ms 135.386327ms 144.067556ms 147.009489ms 191.238209ms 229.275942ms 272.13657ms 302.741186ms 344.877787ms 388.894228ms 429.103785ms 468.579479ms 509.339211ms 549.230289ms 591.369015ms 635.775074ms 674.882001ms 739.198926ms 744.246997ms 745.738127ms 745.744165ms 746.256766ms 746.470684ms 746.530122ms 746.901769ms 746.903048ms 747.086116ms 747.20137ms 747.345509ms 747.407665ms 747.434279ms 747.49943ms 747.574153ms 747.738868ms 747.752113ms 747.878583ms 748.084258ms 748.142455ms 748.26897ms 748.328131ms 748.402618ms 748.586898ms 748.591454ms 748.64882ms 748.676164ms 748.74472ms 748.744958ms 748.869352ms 748.910407ms 748.923397ms 748.925257ms 748.974926ms 749.015565ms 749.044625ms 749.070745ms 749.248721ms 749.266975ms 749.276981ms 749.29711ms 749.371352ms 749.406057ms 749.435006ms 749.435326ms 749.43553ms 749.450417ms 749.478743ms 749.481309ms 749.504725ms 749.544729ms 749.576346ms 749.623669ms 749.637876ms 749.661024ms 749.661114ms 749.676261ms 749.68779ms 749.698181ms 749.703134ms 749.711912ms 749.774904ms 749.795271ms 749.821953ms 749.832773ms 749.855268ms 749.863816ms 749.925836ms 749.948788ms 749.958443ms 749.964816ms 750.026309ms 750.030249ms 750.048085ms 750.080776ms 750.118667ms 750.151644ms 750.162237ms 750.168126ms 750.181611ms 750.214759ms 750.239741ms 750.244368ms 750.245815ms 750.262657ms 750.262789ms 750.295974ms 750.302886ms 750.320639ms 750.322066ms 750.359591ms 750.37299ms 750.375157ms 750.3787ms 750.391869ms 750.421804ms 750.425389ms 750.454679ms 750.503381ms 750.583564ms 750.623894ms 750.635992ms 750.673273ms 750.752034ms 750.764724ms 750.799456ms 750.822627ms 750.875588ms 750.878407ms 750.878575ms 750.910311ms 750.954917ms 750.963973ms 750.971815ms 751.044891ms 751.052253ms 751.068309ms 751.075727ms 751.097989ms 751.135664ms 751.184226ms 751.206791ms 751.218223ms 751.245357ms 751.268107ms 751.270101ms 751.389833ms 751.395235ms 751.435691ms 751.458319ms 751.48708ms 751.540938ms 751.55316ms 751.564757ms 751.690924ms 752.059766ms 752.255945ms 752.314733ms 752.515378ms 752.608516ms 752.694954ms 752.775391ms 752.784973ms 752.927173ms 753.060658ms 754.139624ms 754.270138ms 756.668324ms 760.592521ms 819.994614ms]
Feb 27 16:08:16.017: INFO: 50 %ile: 749.544729ms
Feb 27 16:08:16.017: INFO: 90 %ile: 751.48708ms
Feb 27 16:08:16.017: INFO: 99 %ile: 760.592521ms
Feb 27 16:08:16.017: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:16.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-519" for this suite. 02/27/23 16:08:16.022
------------------------------
• [SLOW TEST] [10.760 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:05.27
    Feb 27 16:08:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svc-latency 02/27/23 16:08:05.27
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:05.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:05.289
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb 27 16:08:05.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-519 02/27/23 16:08:05.292
    I0227 16:08:05.298913      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-519, replica count: 1
    I0227 16:08:06.349753      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0227 16:08:07.349881      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 16:08:07.461: INFO: Created: latency-svc-4gczl
    Feb 27 16:08:07.473: INFO: Got endpoints: latency-svc-4gczl [22.884914ms]
    Feb 27 16:08:07.488: INFO: Created: latency-svc-5k5pr
    Feb 27 16:08:07.497: INFO: Got endpoints: latency-svc-5k5pr [23.716938ms]
    Feb 27 16:08:07.498: INFO: Created: latency-svc-qkt8q
    Feb 27 16:08:07.503: INFO: Created: latency-svc-pqnhm
    Feb 27 16:08:07.505: INFO: Got endpoints: latency-svc-qkt8q [31.189919ms]
    Feb 27 16:08:07.513: INFO: Got endpoints: latency-svc-pqnhm [39.653362ms]
    Feb 27 16:08:07.514: INFO: Created: latency-svc-pkhtl
    Feb 27 16:08:07.519: INFO: Got endpoints: latency-svc-pkhtl [45.081053ms]
    Feb 27 16:08:07.526: INFO: Created: latency-svc-d7jbd
    Feb 27 16:08:07.529: INFO: Got endpoints: latency-svc-d7jbd [55.579589ms]
    Feb 27 16:08:07.538: INFO: Created: latency-svc-v6lfm
    Feb 27 16:08:07.541: INFO: Got endpoints: latency-svc-v6lfm [67.148481ms]
    Feb 27 16:08:07.545: INFO: Created: latency-svc-wckst
    Feb 27 16:08:07.549: INFO: Got endpoints: latency-svc-wckst [74.624983ms]
    Feb 27 16:08:07.553: INFO: Created: latency-svc-dkktj
    Feb 27 16:08:07.558: INFO: Got endpoints: latency-svc-dkktj [83.513675ms]
    Feb 27 16:08:07.564: INFO: Created: latency-svc-fk7pb
    Feb 27 16:08:07.567: INFO: Got endpoints: latency-svc-fk7pb [93.30419ms]
    Feb 27 16:08:07.572: INFO: Created: latency-svc-t9j78
    Feb 27 16:08:07.576: INFO: Got endpoints: latency-svc-t9j78 [102.043683ms]
    Feb 27 16:08:07.579: INFO: Created: latency-svc-psrkb
    Feb 27 16:08:07.584: INFO: Got endpoints: latency-svc-psrkb [110.351143ms]
    Feb 27 16:08:07.588: INFO: Created: latency-svc-svmzw
    Feb 27 16:08:07.591: INFO: Got endpoints: latency-svc-svmzw [117.23641ms]
    Feb 27 16:08:07.592: INFO: Created: latency-svc-f669h
    Feb 27 16:08:07.600: INFO: Got endpoints: latency-svc-f669h [125.540219ms]
    Feb 27 16:08:07.604: INFO: Created: latency-svc-r6xk6
    Feb 27 16:08:07.609: INFO: Got endpoints: latency-svc-r6xk6 [135.152836ms]
    Feb 27 16:08:07.614: INFO: Created: latency-svc-2n9ql
    Feb 27 16:08:07.618: INFO: Got endpoints: latency-svc-2n9ql [144.067556ms]
    Feb 27 16:08:07.621: INFO: Created: latency-svc-76xlp
    Feb 27 16:08:07.625: INFO: Got endpoints: latency-svc-76xlp [127.643295ms]
    Feb 27 16:08:07.629: INFO: Created: latency-svc-4nvc5
    Feb 27 16:08:07.632: INFO: Got endpoints: latency-svc-4nvc5 [126.952232ms]
    Feb 27 16:08:07.638: INFO: Created: latency-svc-pjxtz
    Feb 27 16:08:07.646: INFO: Got endpoints: latency-svc-pjxtz [132.443709ms]
    Feb 27 16:08:07.647: INFO: Created: latency-svc-7bgn8
    Feb 27 16:08:07.651: INFO: Got endpoints: latency-svc-7bgn8 [132.411872ms]
    Feb 27 16:08:07.656: INFO: Created: latency-svc-spsk5
    Feb 27 16:08:07.659: INFO: Got endpoints: latency-svc-spsk5 [129.219386ms]
    Feb 27 16:08:07.666: INFO: Created: latency-svc-6nq4w
    Feb 27 16:08:07.669: INFO: Got endpoints: latency-svc-6nq4w [128.307689ms]
    Feb 27 16:08:07.671: INFO: Created: latency-svc-57lp4
    Feb 27 16:08:07.674: INFO: Got endpoints: latency-svc-57lp4 [125.242637ms]
    Feb 27 16:08:07.680: INFO: Created: latency-svc-4zm4l
    Feb 27 16:08:07.685: INFO: Got endpoints: latency-svc-4zm4l [127.035886ms]
    Feb 27 16:08:07.685: INFO: Created: latency-svc-8xsd5
    Feb 27 16:08:07.690: INFO: Created: latency-svc-b94nv
    Feb 27 16:08:07.693: INFO: Got endpoints: latency-svc-8xsd5 [125.524559ms]
    Feb 27 16:08:07.694: INFO: Got endpoints: latency-svc-b94nv [117.769173ms]
    Feb 27 16:08:07.710: INFO: Created: latency-svc-f8mtq
    Feb 27 16:08:07.711: INFO: Got endpoints: latency-svc-f8mtq [126.774816ms]
    Feb 27 16:08:07.718: INFO: Created: latency-svc-bmqhn
    Feb 27 16:08:07.720: INFO: Got endpoints: latency-svc-bmqhn [128.462246ms]
    Feb 27 16:08:07.722: INFO: Created: latency-svc-fhksg
    Feb 27 16:08:07.726: INFO: Got endpoints: latency-svc-fhksg [126.482942ms]
    Feb 27 16:08:07.732: INFO: Created: latency-svc-vbprn
    Feb 27 16:08:07.735: INFO: Got endpoints: latency-svc-vbprn [126.005442ms]
    Feb 27 16:08:07.742: INFO: Created: latency-svc-8m8hk
    Feb 27 16:08:07.748: INFO: Got endpoints: latency-svc-8m8hk [129.822485ms]
    Feb 27 16:08:07.752: INFO: Created: latency-svc-dcgj8
    Feb 27 16:08:07.756: INFO: Got endpoints: latency-svc-dcgj8 [131.432105ms]
    Feb 27 16:08:07.762: INFO: Created: latency-svc-ffbmp
    Feb 27 16:08:07.768: INFO: Got endpoints: latency-svc-ffbmp [135.386327ms]
    Feb 27 16:08:07.770: INFO: Created: latency-svc-c97pt
    Feb 27 16:08:07.774: INFO: Got endpoints: latency-svc-c97pt [127.734315ms]
    Feb 27 16:08:07.777: INFO: Created: latency-svc-gcswx
    Feb 27 16:08:07.781: INFO: Got endpoints: latency-svc-gcswx [129.130718ms]
    Feb 27 16:08:07.785: INFO: Created: latency-svc-g9f4w
    Feb 27 16:08:07.790: INFO: Got endpoints: latency-svc-g9f4w [131.608893ms]
    Feb 27 16:08:07.792: INFO: Created: latency-svc-z7jwr
    Feb 27 16:08:07.797: INFO: Created: latency-svc-bzk9k
    Feb 27 16:08:07.804: INFO: Created: latency-svc-bvgrb
    Feb 27 16:08:07.810: INFO: Created: latency-svc-nqzhw
    Feb 27 16:08:07.817: INFO: Got endpoints: latency-svc-z7jwr [147.009489ms]
    Feb 27 16:08:07.822: INFO: Created: latency-svc-bmzvl
    Feb 27 16:08:07.829: INFO: Created: latency-svc-xg6mt
    Feb 27 16:08:07.836: INFO: Created: latency-svc-kt65z
    Feb 27 16:08:07.844: INFO: Created: latency-svc-xg9g2
    Feb 27 16:08:07.850: INFO: Created: latency-svc-wppjn
    Feb 27 16:08:07.857: INFO: Created: latency-svc-5zgs8
    Feb 27 16:08:07.862: INFO: Created: latency-svc-hbdwn
    Feb 27 16:08:07.865: INFO: Got endpoints: latency-svc-bzk9k [191.238209ms]
    Feb 27 16:08:07.869: INFO: Created: latency-svc-48flf
    Feb 27 16:08:07.877: INFO: Created: latency-svc-pqm62
    Feb 27 16:08:07.882: INFO: Created: latency-svc-vwxw8
    Feb 27 16:08:07.884: INFO: Created: latency-svc-l2rn7
    Feb 27 16:08:07.891: INFO: Created: latency-svc-lrhnr
    Feb 27 16:08:07.897: INFO: Created: latency-svc-9fstj
    Feb 27 16:08:07.914: INFO: Got endpoints: latency-svc-bvgrb [229.275942ms]
    Feb 27 16:08:07.926: INFO: Created: latency-svc-wlrr9
    Feb 27 16:08:07.965: INFO: Got endpoints: latency-svc-nqzhw [272.13657ms]
    Feb 27 16:08:07.977: INFO: Created: latency-svc-wqwtx
    Feb 27 16:08:08.014: INFO: Got endpoints: latency-svc-bmzvl [302.741186ms]
    Feb 27 16:08:08.024: INFO: Created: latency-svc-cm6jd
    Feb 27 16:08:08.065: INFO: Got endpoints: latency-svc-xg6mt [344.877787ms]
    Feb 27 16:08:08.077: INFO: Created: latency-svc-ngv6c
    Feb 27 16:08:08.115: INFO: Got endpoints: latency-svc-kt65z [388.894228ms]
    Feb 27 16:08:08.134: INFO: Created: latency-svc-k6gbs
    Feb 27 16:08:08.165: INFO: Got endpoints: latency-svc-xg9g2 [429.103785ms]
    Feb 27 16:08:08.175: INFO: Created: latency-svc-wv4s9
    Feb 27 16:08:08.217: INFO: Got endpoints: latency-svc-wppjn [468.579479ms]
    Feb 27 16:08:08.229: INFO: Created: latency-svc-h95zn
    Feb 27 16:08:08.266: INFO: Got endpoints: latency-svc-5zgs8 [509.339211ms]
    Feb 27 16:08:08.277: INFO: Created: latency-svc-mgzkx
    Feb 27 16:08:08.317: INFO: Got endpoints: latency-svc-hbdwn [549.230289ms]
    Feb 27 16:08:08.327: INFO: Created: latency-svc-z6ptf
    Feb 27 16:08:08.365: INFO: Got endpoints: latency-svc-48flf [591.369015ms]
    Feb 27 16:08:08.379: INFO: Created: latency-svc-p5dnl
    Feb 27 16:08:08.416: INFO: Got endpoints: latency-svc-pqm62 [635.775074ms]
    Feb 27 16:08:08.427: INFO: Created: latency-svc-jh8vz
    Feb 27 16:08:08.465: INFO: Got endpoints: latency-svc-vwxw8 [674.882001ms]
    Feb 27 16:08:08.480: INFO: Created: latency-svc-wbtpl
    Feb 27 16:08:08.514: INFO: Got endpoints: latency-svc-l2rn7 [819.994614ms]
    Feb 27 16:08:08.525: INFO: Created: latency-svc-vsmxj
    Feb 27 16:08:08.565: INFO: Got endpoints: latency-svc-lrhnr [748.084258ms]
    Feb 27 16:08:08.574: INFO: Created: latency-svc-znsj9
    Feb 27 16:08:08.616: INFO: Got endpoints: latency-svc-9fstj [750.963973ms]
    Feb 27 16:08:08.627: INFO: Created: latency-svc-c6b92
    Feb 27 16:08:08.666: INFO: Got endpoints: latency-svc-wlrr9 [751.690924ms]
    Feb 27 16:08:08.677: INFO: Created: latency-svc-6fkbh
    Feb 27 16:08:08.717: INFO: Got endpoints: latency-svc-wqwtx [751.389833ms]
    Feb 27 16:08:08.729: INFO: Created: latency-svc-2kplj
    Feb 27 16:08:08.765: INFO: Got endpoints: latency-svc-cm6jd [751.097989ms]
    Feb 27 16:08:08.815: INFO: Got endpoints: latency-svc-ngv6c [749.68779ms]
    Feb 27 16:08:08.848: INFO: Created: latency-svc-zqsdw
    Feb 27 16:08:08.855: INFO: Created: latency-svc-lmv6x
    Feb 27 16:08:08.866: INFO: Got endpoints: latency-svc-k6gbs [750.954917ms]
    Feb 27 16:08:08.876: INFO: Created: latency-svc-m5w4j
    Feb 27 16:08:08.914: INFO: Got endpoints: latency-svc-wv4s9 [749.544729ms]
    Feb 27 16:08:08.926: INFO: Created: latency-svc-zswnl
    Feb 27 16:08:08.965: INFO: Got endpoints: latency-svc-h95zn [748.142455ms]
    Feb 27 16:08:08.978: INFO: Created: latency-svc-48vzg
    Feb 27 16:08:09.016: INFO: Got endpoints: latency-svc-mgzkx [749.855268ms]
    Feb 27 16:08:09.025: INFO: Created: latency-svc-xclhr
    Feb 27 16:08:09.067: INFO: Got endpoints: latency-svc-z6ptf [750.503381ms]
    Feb 27 16:08:09.084: INFO: Created: latency-svc-kl2tl
    Feb 27 16:08:09.116: INFO: Got endpoints: latency-svc-p5dnl [750.635992ms]
    Feb 27 16:08:09.129: INFO: Created: latency-svc-ct5mq
    Feb 27 16:08:09.165: INFO: Got endpoints: latency-svc-jh8vz [748.74472ms]
    Feb 27 16:08:09.177: INFO: Created: latency-svc-7dw4d
    Feb 27 16:08:09.215: INFO: Got endpoints: latency-svc-wbtpl [749.478743ms]
    Feb 27 16:08:09.227: INFO: Created: latency-svc-2t9xz
    Feb 27 16:08:09.264: INFO: Got endpoints: latency-svc-vsmxj [750.37299ms]
    Feb 27 16:08:09.277: INFO: Created: latency-svc-zbvmp
    Feb 27 16:08:09.316: INFO: Got endpoints: latency-svc-znsj9 [751.245357ms]
    Feb 27 16:08:09.327: INFO: Created: latency-svc-jr7gw
    Feb 27 16:08:09.365: INFO: Got endpoints: latency-svc-c6b92 [748.328131ms]
    Feb 27 16:08:09.375: INFO: Created: latency-svc-lpbpz
    Feb 27 16:08:09.415: INFO: Got endpoints: latency-svc-6fkbh [748.923397ms]
    Feb 27 16:08:09.429: INFO: Created: latency-svc-kpmp8
    Feb 27 16:08:09.465: INFO: Got endpoints: latency-svc-2kplj [748.586898ms]
    Feb 27 16:08:09.477: INFO: Created: latency-svc-d542m
    Feb 27 16:08:09.518: INFO: Got endpoints: latency-svc-zqsdw [752.515378ms]
    Feb 27 16:08:09.528: INFO: Created: latency-svc-mrz7b
    Feb 27 16:08:09.565: INFO: Got endpoints: latency-svc-lmv6x [750.151644ms]
    Feb 27 16:08:09.576: INFO: Created: latency-svc-x75tw
    Feb 27 16:08:09.616: INFO: Got endpoints: latency-svc-m5w4j [749.406057ms]
    Feb 27 16:08:09.626: INFO: Created: latency-svc-xbk7v
    Feb 27 16:08:09.665: INFO: Got endpoints: latency-svc-zswnl [750.421804ms]
    Feb 27 16:08:09.678: INFO: Created: latency-svc-76bpf
    Feb 27 16:08:09.715: INFO: Got endpoints: latency-svc-48vzg [749.964816ms]
    Feb 27 16:08:09.738: INFO: Created: latency-svc-4pt6g
    Feb 27 16:08:09.767: INFO: Got endpoints: latency-svc-xclhr [751.135664ms]
    Feb 27 16:08:09.776: INFO: Created: latency-svc-ndmqh
    Feb 27 16:08:09.816: INFO: Got endpoints: latency-svc-kl2tl [748.974926ms]
    Feb 27 16:08:09.826: INFO: Created: latency-svc-xq2r7
    Feb 27 16:08:09.865: INFO: Got endpoints: latency-svc-ct5mq [749.576346ms]
    Feb 27 16:08:09.877: INFO: Created: latency-svc-9276j
    Feb 27 16:08:09.915: INFO: Got endpoints: latency-svc-7dw4d [750.080776ms]
    Feb 27 16:08:09.927: INFO: Created: latency-svc-s2d6m
    Feb 27 16:08:09.966: INFO: Got endpoints: latency-svc-2t9xz [750.971815ms]
    Feb 27 16:08:09.977: INFO: Created: latency-svc-265b5
    Feb 27 16:08:10.014: INFO: Got endpoints: latency-svc-zbvmp [749.948788ms]
    Feb 27 16:08:10.025: INFO: Created: latency-svc-klbb8
    Feb 27 16:08:10.067: INFO: Got endpoints: latency-svc-jr7gw [750.878575ms]
    Feb 27 16:08:10.078: INFO: Created: latency-svc-xzfb9
    Feb 27 16:08:10.115: INFO: Got endpoints: latency-svc-lpbpz [750.375157ms]
    Feb 27 16:08:10.127: INFO: Created: latency-svc-q9rlx
    Feb 27 16:08:10.165: INFO: Got endpoints: latency-svc-kpmp8 [750.764724ms]
    Feb 27 16:08:10.178: INFO: Created: latency-svc-v98j4
    Feb 27 16:08:10.217: INFO: Got endpoints: latency-svc-d542m [751.068309ms]
    Feb 27 16:08:10.228: INFO: Created: latency-svc-d9s7m
    Feb 27 16:08:10.269: INFO: Got endpoints: latency-svc-mrz7b [751.268107ms]
    Feb 27 16:08:10.279: INFO: Created: latency-svc-r28sr
    Feb 27 16:08:10.315: INFO: Got endpoints: latency-svc-x75tw [749.958443ms]
    Feb 27 16:08:10.327: INFO: Created: latency-svc-4mxlk
    Feb 27 16:08:10.364: INFO: Got endpoints: latency-svc-xbk7v [748.26897ms]
    Feb 27 16:08:10.374: INFO: Created: latency-svc-9k27d
    Feb 27 16:08:10.418: INFO: Got endpoints: latency-svc-76bpf [752.927173ms]
    Feb 27 16:08:10.430: INFO: Created: latency-svc-kh6n7
    Feb 27 16:08:10.469: INFO: Got endpoints: latency-svc-4pt6g [754.270138ms]
    Feb 27 16:08:10.485: INFO: Created: latency-svc-zr6dx
    Feb 27 16:08:10.520: INFO: Got endpoints: latency-svc-ndmqh [753.060658ms]
    Feb 27 16:08:10.530: INFO: Created: latency-svc-tmdkb
    Feb 27 16:08:10.565: INFO: Got endpoints: latency-svc-xq2r7 [748.64882ms]
    Feb 27 16:08:10.575: INFO: Created: latency-svc-5xdn7
    Feb 27 16:08:10.615: INFO: Got endpoints: latency-svc-9276j [749.863816ms]
    Feb 27 16:08:10.628: INFO: Created: latency-svc-29rvs
    Feb 27 16:08:10.664: INFO: Got endpoints: latency-svc-s2d6m [748.676164ms]
    Feb 27 16:08:10.674: INFO: Created: latency-svc-j265w
    Feb 27 16:08:10.716: INFO: Got endpoints: latency-svc-265b5 [750.322066ms]
    Feb 27 16:08:10.726: INFO: Created: latency-svc-gfvg2
    Feb 27 16:08:10.765: INFO: Got endpoints: latency-svc-klbb8 [750.752034ms]
    Feb 27 16:08:10.776: INFO: Created: latency-svc-f6brp
    Feb 27 16:08:10.817: INFO: Got endpoints: latency-svc-xzfb9 [749.661114ms]
    Feb 27 16:08:10.828: INFO: Created: latency-svc-zkrsz
    Feb 27 16:08:10.866: INFO: Got endpoints: latency-svc-q9rlx [750.048085ms]
    Feb 27 16:08:10.876: INFO: Created: latency-svc-vzk4c
    Feb 27 16:08:10.918: INFO: Got endpoints: latency-svc-v98j4 [752.784973ms]
    Feb 27 16:08:10.931: INFO: Created: latency-svc-9v759
    Feb 27 16:08:10.964: INFO: Got endpoints: latency-svc-d9s7m [747.49943ms]
    Feb 27 16:08:10.980: INFO: Created: latency-svc-6xzkv
    Feb 27 16:08:11.015: INFO: Got endpoints: latency-svc-r28sr [745.738127ms]
    Feb 27 16:08:11.027: INFO: Created: latency-svc-mv9k9
    Feb 27 16:08:11.072: INFO: Got endpoints: latency-svc-4mxlk [756.668324ms]
    Feb 27 16:08:11.085: INFO: Created: latency-svc-zq9wm
    Feb 27 16:08:11.114: INFO: Got endpoints: latency-svc-9k27d [750.3787ms]
    Feb 27 16:08:11.126: INFO: Created: latency-svc-86kfl
    Feb 27 16:08:11.165: INFO: Got endpoints: latency-svc-kh6n7 [747.086116ms]
    Feb 27 16:08:11.175: INFO: Created: latency-svc-92z58
    Feb 27 16:08:11.217: INFO: Got endpoints: latency-svc-zr6dx [747.407665ms]
    Feb 27 16:08:11.227: INFO: Created: latency-svc-t4htm
    Feb 27 16:08:11.266: INFO: Got endpoints: latency-svc-tmdkb [745.744165ms]
    Feb 27 16:08:11.277: INFO: Created: latency-svc-2nrq9
    Feb 27 16:08:11.315: INFO: Got endpoints: latency-svc-5xdn7 [749.703134ms]
    Feb 27 16:08:11.327: INFO: Created: latency-svc-5g46z
    Feb 27 16:08:11.366: INFO: Got endpoints: latency-svc-29rvs [750.425389ms]
    Feb 27 16:08:11.377: INFO: Created: latency-svc-hbw5v
    Feb 27 16:08:11.417: INFO: Got endpoints: latency-svc-j265w [752.608516ms]
    Feb 27 16:08:11.427: INFO: Created: latency-svc-qpkgj
    Feb 27 16:08:11.465: INFO: Got endpoints: latency-svc-gfvg2 [748.402618ms]
    Feb 27 16:08:11.478: INFO: Created: latency-svc-4h46s
    Feb 27 16:08:11.517: INFO: Got endpoints: latency-svc-f6brp [751.458319ms]
    Feb 27 16:08:11.527: INFO: Created: latency-svc-plxl2
    Feb 27 16:08:11.566: INFO: Got endpoints: latency-svc-zkrsz [749.015565ms]
    Feb 27 16:08:11.575: INFO: Created: latency-svc-c4tsj
    Feb 27 16:08:11.616: INFO: Got endpoints: latency-svc-vzk4c [749.821953ms]
    Feb 27 16:08:11.627: INFO: Created: latency-svc-kjhdc
    Feb 27 16:08:11.666: INFO: Got endpoints: latency-svc-9v759 [747.752113ms]
    Feb 27 16:08:11.677: INFO: Created: latency-svc-4gl4w
    Feb 27 16:08:11.717: INFO: Got endpoints: latency-svc-6xzkv [752.775391ms]
    Feb 27 16:08:11.727: INFO: Created: latency-svc-d66b8
    Feb 27 16:08:11.765: INFO: Got endpoints: latency-svc-mv9k9 [750.262657ms]
    Feb 27 16:08:11.777: INFO: Created: latency-svc-plz8j
    Feb 27 16:08:11.816: INFO: Got endpoints: latency-svc-zq9wm [744.246997ms]
    Feb 27 16:08:11.829: INFO: Created: latency-svc-xvfbt
    Feb 27 16:08:11.866: INFO: Got endpoints: latency-svc-86kfl [751.270101ms]
    Feb 27 16:08:11.877: INFO: Created: latency-svc-6r6pr
    Feb 27 16:08:11.916: INFO: Got endpoints: latency-svc-92z58 [751.206791ms]
    Feb 27 16:08:11.927: INFO: Created: latency-svc-wn857
    Feb 27 16:08:11.968: INFO: Got endpoints: latency-svc-t4htm [750.822627ms]
    Feb 27 16:08:11.979: INFO: Created: latency-svc-5wckr
    Feb 27 16:08:12.018: INFO: Got endpoints: latency-svc-2nrq9 [752.059766ms]
    Feb 27 16:08:12.027: INFO: Created: latency-svc-f4dr2
    Feb 27 16:08:12.065: INFO: Got endpoints: latency-svc-5g46z [750.359591ms]
    Feb 27 16:08:12.078: INFO: Created: latency-svc-fw8wp
    Feb 27 16:08:12.115: INFO: Got endpoints: latency-svc-hbw5v [749.450417ms]
    Feb 27 16:08:12.130: INFO: Created: latency-svc-7mjtl
    Feb 27 16:08:12.167: INFO: Got endpoints: latency-svc-qpkgj [750.262789ms]
    Feb 27 16:08:12.177: INFO: Created: latency-svc-vfqr9
    Feb 27 16:08:12.216: INFO: Got endpoints: latency-svc-4h46s [750.878407ms]
    Feb 27 16:08:12.232: INFO: Created: latency-svc-z4lrq
    Feb 27 16:08:12.267: INFO: Got endpoints: latency-svc-plxl2 [750.295974ms]
    Feb 27 16:08:12.281: INFO: Created: latency-svc-srvj4
    Feb 27 16:08:12.316: INFO: Got endpoints: latency-svc-c4tsj [750.118667ms]
    Feb 27 16:08:12.326: INFO: Created: latency-svc-qsjwd
    Feb 27 16:08:12.365: INFO: Got endpoints: latency-svc-kjhdc [749.266975ms]
    Feb 27 16:08:12.378: INFO: Created: latency-svc-wl248
    Feb 27 16:08:12.417: INFO: Got endpoints: latency-svc-4gl4w [751.184226ms]
    Feb 27 16:08:12.428: INFO: Created: latency-svc-pd52w
    Feb 27 16:08:12.465: INFO: Got endpoints: latency-svc-d66b8 [747.738868ms]
    Feb 27 16:08:12.477: INFO: Created: latency-svc-djq78
    Feb 27 16:08:12.517: INFO: Got endpoints: latency-svc-plz8j [751.435691ms]
    Feb 27 16:08:12.530: INFO: Created: latency-svc-gms4t
    Feb 27 16:08:12.567: INFO: Got endpoints: latency-svc-xvfbt [751.044891ms]
    Feb 27 16:08:12.578: INFO: Created: latency-svc-hsfgv
    Feb 27 16:08:12.615: INFO: Got endpoints: latency-svc-6r6pr [749.044625ms]
    Feb 27 16:08:12.625: INFO: Created: latency-svc-9l7vl
    Feb 27 16:08:12.666: INFO: Got endpoints: latency-svc-wn857 [749.698181ms]
    Feb 27 16:08:12.677: INFO: Created: latency-svc-bbnd6
    Feb 27 16:08:12.717: INFO: Got endpoints: latency-svc-5wckr [748.744958ms]
    Feb 27 16:08:12.729: INFO: Created: latency-svc-jr6jb
    Feb 27 16:08:12.764: INFO: Got endpoints: latency-svc-f4dr2 [746.470684ms]
    Feb 27 16:08:12.774: INFO: Created: latency-svc-hvw4d
    Feb 27 16:08:12.816: INFO: Got endpoints: latency-svc-fw8wp [750.799456ms]
    Feb 27 16:08:12.833: INFO: Created: latency-svc-5nrvw
    Feb 27 16:08:12.868: INFO: Got endpoints: latency-svc-7mjtl [752.694954ms]
    Feb 27 16:08:12.880: INFO: Created: latency-svc-s6chf
    Feb 27 16:08:12.914: INFO: Got endpoints: latency-svc-vfqr9 [746.901769ms]
    Feb 27 16:08:12.925: INFO: Created: latency-svc-55bh9
    Feb 27 16:08:12.968: INFO: Got endpoints: latency-svc-z4lrq [752.314733ms]
    Feb 27 16:08:12.982: INFO: Created: latency-svc-wfkm6
    Feb 27 16:08:13.017: INFO: Got endpoints: latency-svc-srvj4 [749.435326ms]
    Feb 27 16:08:13.031: INFO: Created: latency-svc-7rzvk
    Feb 27 16:08:13.066: INFO: Got endpoints: latency-svc-qsjwd [750.320639ms]
    Feb 27 16:08:13.076: INFO: Created: latency-svc-87sxf
    Feb 27 16:08:13.116: INFO: Got endpoints: latency-svc-wl248 [750.910311ms]
    Feb 27 16:08:13.128: INFO: Created: latency-svc-ksnhn
    Feb 27 16:08:13.165: INFO: Got endpoints: latency-svc-pd52w [747.878583ms]
    Feb 27 16:08:13.178: INFO: Created: latency-svc-hxqgt
    Feb 27 16:08:13.214: INFO: Got endpoints: latency-svc-djq78 [749.276981ms]
    Feb 27 16:08:13.224: INFO: Created: latency-svc-vbz7h
    Feb 27 16:08:13.268: INFO: Got endpoints: latency-svc-gms4t [751.052253ms]
    Feb 27 16:08:13.288: INFO: Created: latency-svc-zjdlh
    Feb 27 16:08:13.316: INFO: Got endpoints: latency-svc-hsfgv [748.910407ms]
    Feb 27 16:08:13.326: INFO: Created: latency-svc-lzf7r
    Feb 27 16:08:13.365: INFO: Got endpoints: latency-svc-9l7vl [750.030249ms]
    Feb 27 16:08:13.375: INFO: Created: latency-svc-7sdwk
    Feb 27 16:08:13.416: INFO: Got endpoints: latency-svc-bbnd6 [750.623894ms]
    Feb 27 16:08:13.431: INFO: Created: latency-svc-qf5gc
    Feb 27 16:08:13.468: INFO: Got endpoints: latency-svc-jr6jb [751.564757ms]
    Feb 27 16:08:13.482: INFO: Created: latency-svc-m5zdh
    Feb 27 16:08:13.514: INFO: Got endpoints: latency-svc-hvw4d [750.245815ms]
    Feb 27 16:08:13.524: INFO: Created: latency-svc-94qck
    Feb 27 16:08:13.567: INFO: Got endpoints: latency-svc-5nrvw [750.583564ms]
    Feb 27 16:08:13.580: INFO: Created: latency-svc-sbrhr
    Feb 27 16:08:13.615: INFO: Got endpoints: latency-svc-s6chf [747.574153ms]
    Feb 27 16:08:13.626: INFO: Created: latency-svc-jg8qf
    Feb 27 16:08:13.665: INFO: Got endpoints: latency-svc-55bh9 [750.168126ms]
    Feb 27 16:08:13.675: INFO: Created: latency-svc-d54dw
    Feb 27 16:08:13.715: INFO: Got endpoints: latency-svc-wfkm6 [747.345509ms]
    Feb 27 16:08:13.727: INFO: Created: latency-svc-2rkhd
    Feb 27 16:08:13.766: INFO: Got endpoints: latency-svc-7rzvk [749.504725ms]
    Feb 27 16:08:13.781: INFO: Created: latency-svc-82jvx
    Feb 27 16:08:13.816: INFO: Got endpoints: latency-svc-87sxf [749.676261ms]
    Feb 27 16:08:13.825: INFO: Created: latency-svc-8sv9v
    Feb 27 16:08:13.867: INFO: Got endpoints: latency-svc-ksnhn [750.391869ms]
    Feb 27 16:08:13.877: INFO: Created: latency-svc-97bs2
    Feb 27 16:08:13.915: INFO: Got endpoints: latency-svc-hxqgt [750.181611ms]
    Feb 27 16:08:13.928: INFO: Created: latency-svc-8526v
    Feb 27 16:08:13.965: INFO: Got endpoints: latency-svc-vbz7h [750.162237ms]
    Feb 27 16:08:13.974: INFO: Created: latency-svc-lctvq
    Feb 27 16:08:14.016: INFO: Got endpoints: latency-svc-zjdlh [748.591454ms]
    Feb 27 16:08:14.029: INFO: Created: latency-svc-6bsnf
    Feb 27 16:08:14.066: INFO: Got endpoints: latency-svc-lzf7r [750.026309ms]
    Feb 27 16:08:14.076: INFO: Created: latency-svc-xllg7
    Feb 27 16:08:14.115: INFO: Got endpoints: latency-svc-7sdwk [749.832773ms]
    Feb 27 16:08:14.126: INFO: Created: latency-svc-tt4qg
    Feb 27 16:08:14.166: INFO: Got endpoints: latency-svc-qf5gc [749.661024ms]
    Feb 27 16:08:14.179: INFO: Created: latency-svc-9c9mq
    Feb 27 16:08:14.215: INFO: Got endpoints: latency-svc-m5zdh [746.256766ms]
    Feb 27 16:08:14.226: INFO: Created: latency-svc-qnzrn
    Feb 27 16:08:14.265: INFO: Got endpoints: latency-svc-94qck [750.214759ms]
    Feb 27 16:08:14.274: INFO: Created: latency-svc-m7hw9
    Feb 27 16:08:14.316: INFO: Got endpoints: latency-svc-sbrhr [748.869352ms]
    Feb 27 16:08:14.328: INFO: Created: latency-svc-n85lv
    Feb 27 16:08:14.365: INFO: Got endpoints: latency-svc-jg8qf [749.43553ms]
    Feb 27 16:08:14.375: INFO: Created: latency-svc-pbwsj
    Feb 27 16:08:14.414: INFO: Got endpoints: latency-svc-d54dw [749.711912ms]
    Feb 27 16:08:14.424: INFO: Created: latency-svc-47slq
    Feb 27 16:08:14.465: INFO: Got endpoints: latency-svc-2rkhd [749.29711ms]
    Feb 27 16:08:14.479: INFO: Created: latency-svc-ssvkx
    Feb 27 16:08:14.518: INFO: Got endpoints: latency-svc-82jvx [751.48708ms]
    Feb 27 16:08:14.528: INFO: Created: latency-svc-cpcvm
    Feb 27 16:08:14.565: INFO: Got endpoints: latency-svc-8sv9v [749.435006ms]
    Feb 27 16:08:14.576: INFO: Created: latency-svc-2db6g
    Feb 27 16:08:14.616: INFO: Got endpoints: latency-svc-97bs2 [749.248721ms]
    Feb 27 16:08:14.629: INFO: Created: latency-svc-s8sqv
    Feb 27 16:08:14.665: INFO: Got endpoints: latency-svc-8526v [749.623669ms]
    Feb 27 16:08:14.675: INFO: Created: latency-svc-jhnnk
    Feb 27 16:08:14.715: INFO: Got endpoints: latency-svc-lctvq [749.925836ms]
    Feb 27 16:08:14.724: INFO: Created: latency-svc-4lvnr
    Feb 27 16:08:14.766: INFO: Got endpoints: latency-svc-6bsnf [749.795271ms]
    Feb 27 16:08:14.780: INFO: Created: latency-svc-vjnqg
    Feb 27 16:08:14.818: INFO: Got endpoints: latency-svc-xllg7 [752.255945ms]
    Feb 27 16:08:14.829: INFO: Created: latency-svc-7qp2f
    Feb 27 16:08:14.876: INFO: Got endpoints: latency-svc-tt4qg [760.592521ms]
    Feb 27 16:08:14.893: INFO: Created: latency-svc-rrnfb
    Feb 27 16:08:14.918: INFO: Got endpoints: latency-svc-9c9mq [751.540938ms]
    Feb 27 16:08:14.933: INFO: Created: latency-svc-jsgpd
    Feb 27 16:08:14.965: INFO: Got endpoints: latency-svc-qnzrn [750.454679ms]
    Feb 27 16:08:14.976: INFO: Created: latency-svc-7v8xb
    Feb 27 16:08:15.014: INFO: Got endpoints: latency-svc-m7hw9 [749.481309ms]
    Feb 27 16:08:15.024: INFO: Created: latency-svc-bcfwz
    Feb 27 16:08:15.067: INFO: Got endpoints: latency-svc-n85lv [751.218223ms]
    Feb 27 16:08:15.079: INFO: Created: latency-svc-6mhvr
    Feb 27 16:08:15.116: INFO: Got endpoints: latency-svc-pbwsj [751.075727ms]
    Feb 27 16:08:15.127: INFO: Created: latency-svc-ppdgd
    Feb 27 16:08:15.164: INFO: Got endpoints: latency-svc-47slq [749.774904ms]
    Feb 27 16:08:15.174: INFO: Created: latency-svc-rmlqv
    Feb 27 16:08:15.217: INFO: Got endpoints: latency-svc-ssvkx [751.55316ms]
    Feb 27 16:08:15.229: INFO: Created: latency-svc-5pwz4
    Feb 27 16:08:15.265: INFO: Got endpoints: latency-svc-cpcvm [746.903048ms]
    Feb 27 16:08:15.278: INFO: Created: latency-svc-5rs2g
    Feb 27 16:08:15.316: INFO: Got endpoints: latency-svc-2db6g [750.302886ms]
    Feb 27 16:08:15.366: INFO: Got endpoints: latency-svc-s8sqv [750.244368ms]
    Feb 27 16:08:15.416: INFO: Got endpoints: latency-svc-jhnnk [750.875588ms]
    Feb 27 16:08:15.465: INFO: Got endpoints: latency-svc-4lvnr [750.239741ms]
    Feb 27 16:08:15.521: INFO: Got endpoints: latency-svc-vjnqg [754.139624ms]
    Feb 27 16:08:15.566: INFO: Got endpoints: latency-svc-7qp2f [747.20137ms]
    Feb 27 16:08:15.615: INFO: Got endpoints: latency-svc-rrnfb [739.198926ms]
    Feb 27 16:08:15.664: INFO: Got endpoints: latency-svc-jsgpd [746.530122ms]
    Feb 27 16:08:15.715: INFO: Got endpoints: latency-svc-7v8xb [749.637876ms]
    Feb 27 16:08:15.765: INFO: Got endpoints: latency-svc-bcfwz [750.673273ms]
    Feb 27 16:08:15.815: INFO: Got endpoints: latency-svc-6mhvr [747.434279ms]
    Feb 27 16:08:15.865: INFO: Got endpoints: latency-svc-ppdgd [748.925257ms]
    Feb 27 16:08:15.914: INFO: Got endpoints: latency-svc-rmlqv [749.371352ms]
    Feb 27 16:08:15.966: INFO: Got endpoints: latency-svc-5pwz4 [749.070745ms]
    Feb 27 16:08:16.016: INFO: Got endpoints: latency-svc-5rs2g [751.395235ms]
    Feb 27 16:08:16.017: INFO: Latencies: [23.716938ms 31.189919ms 39.653362ms 45.081053ms 55.579589ms 67.148481ms 74.624983ms 83.513675ms 93.30419ms 102.043683ms 110.351143ms 117.23641ms 117.769173ms 125.242637ms 125.524559ms 125.540219ms 126.005442ms 126.482942ms 126.774816ms 126.952232ms 127.035886ms 127.643295ms 127.734315ms 128.307689ms 128.462246ms 129.130718ms 129.219386ms 129.822485ms 131.432105ms 131.608893ms 132.411872ms 132.443709ms 135.152836ms 135.386327ms 144.067556ms 147.009489ms 191.238209ms 229.275942ms 272.13657ms 302.741186ms 344.877787ms 388.894228ms 429.103785ms 468.579479ms 509.339211ms 549.230289ms 591.369015ms 635.775074ms 674.882001ms 739.198926ms 744.246997ms 745.738127ms 745.744165ms 746.256766ms 746.470684ms 746.530122ms 746.901769ms 746.903048ms 747.086116ms 747.20137ms 747.345509ms 747.407665ms 747.434279ms 747.49943ms 747.574153ms 747.738868ms 747.752113ms 747.878583ms 748.084258ms 748.142455ms 748.26897ms 748.328131ms 748.402618ms 748.586898ms 748.591454ms 748.64882ms 748.676164ms 748.74472ms 748.744958ms 748.869352ms 748.910407ms 748.923397ms 748.925257ms 748.974926ms 749.015565ms 749.044625ms 749.070745ms 749.248721ms 749.266975ms 749.276981ms 749.29711ms 749.371352ms 749.406057ms 749.435006ms 749.435326ms 749.43553ms 749.450417ms 749.478743ms 749.481309ms 749.504725ms 749.544729ms 749.576346ms 749.623669ms 749.637876ms 749.661024ms 749.661114ms 749.676261ms 749.68779ms 749.698181ms 749.703134ms 749.711912ms 749.774904ms 749.795271ms 749.821953ms 749.832773ms 749.855268ms 749.863816ms 749.925836ms 749.948788ms 749.958443ms 749.964816ms 750.026309ms 750.030249ms 750.048085ms 750.080776ms 750.118667ms 750.151644ms 750.162237ms 750.168126ms 750.181611ms 750.214759ms 750.239741ms 750.244368ms 750.245815ms 750.262657ms 750.262789ms 750.295974ms 750.302886ms 750.320639ms 750.322066ms 750.359591ms 750.37299ms 750.375157ms 750.3787ms 750.391869ms 750.421804ms 750.425389ms 750.454679ms 750.503381ms 750.583564ms 750.623894ms 750.635992ms 750.673273ms 750.752034ms 750.764724ms 750.799456ms 750.822627ms 750.875588ms 750.878407ms 750.878575ms 750.910311ms 750.954917ms 750.963973ms 750.971815ms 751.044891ms 751.052253ms 751.068309ms 751.075727ms 751.097989ms 751.135664ms 751.184226ms 751.206791ms 751.218223ms 751.245357ms 751.268107ms 751.270101ms 751.389833ms 751.395235ms 751.435691ms 751.458319ms 751.48708ms 751.540938ms 751.55316ms 751.564757ms 751.690924ms 752.059766ms 752.255945ms 752.314733ms 752.515378ms 752.608516ms 752.694954ms 752.775391ms 752.784973ms 752.927173ms 753.060658ms 754.139624ms 754.270138ms 756.668324ms 760.592521ms 819.994614ms]
    Feb 27 16:08:16.017: INFO: 50 %ile: 749.544729ms
    Feb 27 16:08:16.017: INFO: 90 %ile: 751.48708ms
    Feb 27 16:08:16.017: INFO: 99 %ile: 760.592521ms
    Feb 27 16:08:16.017: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:16.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-519" for this suite. 02/27/23 16:08:16.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:16.03
Feb 27 16:08:16.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename subpath 02/27/23 16:08:16.031
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:16.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:16.055
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 16:08:16.059
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-9mr9 02/27/23 16:08:16.069
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 16:08:16.069
Feb 27 16:08:16.079: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9mr9" in namespace "subpath-9182" to be "Succeeded or Failed"
Feb 27 16:08:16.087: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059822ms
Feb 27 16:08:18.090: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010950599s
Feb 27 16:08:20.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 4.01248638s
Feb 27 16:08:22.090: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 6.010613915s
Feb 27 16:08:24.093: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 8.013151175s
Feb 27 16:08:26.091: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 10.011879089s
Feb 27 16:08:28.090: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 12.010579115s
Feb 27 16:08:30.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 14.012114683s
Feb 27 16:08:32.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 16.012526579s
Feb 27 16:08:34.091: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 18.011218255s
Feb 27 16:08:36.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 20.012610425s
Feb 27 16:08:38.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=false. Elapsed: 22.012081494s
Feb 27 16:08:40.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012444262s
STEP: Saw pod success 02/27/23 16:08:40.092
Feb 27 16:08:40.092: INFO: Pod "pod-subpath-test-configmap-9mr9" satisfied condition "Succeeded or Failed"
Feb 27 16:08:40.096: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-configmap-9mr9 container test-container-subpath-configmap-9mr9: <nil>
STEP: delete the pod 02/27/23 16:08:40.101
Feb 27 16:08:40.113: INFO: Waiting for pod pod-subpath-test-configmap-9mr9 to disappear
Feb 27 16:08:40.117: INFO: Pod pod-subpath-test-configmap-9mr9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9mr9 02/27/23 16:08:40.117
Feb 27 16:08:40.117: INFO: Deleting pod "pod-subpath-test-configmap-9mr9" in namespace "subpath-9182"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:40.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9182" for this suite. 02/27/23 16:08:40.124
------------------------------
• [SLOW TEST] [24.100 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:16.03
    Feb 27 16:08:16.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename subpath 02/27/23 16:08:16.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:16.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:16.055
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 16:08:16.059
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-9mr9 02/27/23 16:08:16.069
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 16:08:16.069
    Feb 27 16:08:16.079: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9mr9" in namespace "subpath-9182" to be "Succeeded or Failed"
    Feb 27 16:08:16.087: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059822ms
    Feb 27 16:08:18.090: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010950599s
    Feb 27 16:08:20.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 4.01248638s
    Feb 27 16:08:22.090: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 6.010613915s
    Feb 27 16:08:24.093: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 8.013151175s
    Feb 27 16:08:26.091: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 10.011879089s
    Feb 27 16:08:28.090: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 12.010579115s
    Feb 27 16:08:30.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 14.012114683s
    Feb 27 16:08:32.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 16.012526579s
    Feb 27 16:08:34.091: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 18.011218255s
    Feb 27 16:08:36.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=true. Elapsed: 20.012610425s
    Feb 27 16:08:38.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Running", Reason="", readiness=false. Elapsed: 22.012081494s
    Feb 27 16:08:40.092: INFO: Pod "pod-subpath-test-configmap-9mr9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012444262s
    STEP: Saw pod success 02/27/23 16:08:40.092
    Feb 27 16:08:40.092: INFO: Pod "pod-subpath-test-configmap-9mr9" satisfied condition "Succeeded or Failed"
    Feb 27 16:08:40.096: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-configmap-9mr9 container test-container-subpath-configmap-9mr9: <nil>
    STEP: delete the pod 02/27/23 16:08:40.101
    Feb 27 16:08:40.113: INFO: Waiting for pod pod-subpath-test-configmap-9mr9 to disappear
    Feb 27 16:08:40.117: INFO: Pod pod-subpath-test-configmap-9mr9 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-9mr9 02/27/23 16:08:40.117
    Feb 27 16:08:40.117: INFO: Deleting pod "pod-subpath-test-configmap-9mr9" in namespace "subpath-9182"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:40.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9182" for this suite. 02/27/23 16:08:40.124
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:40.13
Feb 27 16:08:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 16:08:40.131
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:40.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:40.15
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Feb 27 16:08:40.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: creating the pod 02/27/23 16:08:40.153
STEP: submitting the pod to kubernetes 02/27/23 16:08:40.154
Feb 27 16:08:40.161: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594" in namespace "pods-2608" to be "running and ready"
Feb 27 16:08:40.164: INFO: Pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752404ms
Feb 27 16:08:40.164: INFO: The phase of Pod pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:08:42.169: INFO: Pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594": Phase="Running", Reason="", readiness=true. Elapsed: 2.00824893s
Feb 27 16:08:42.169: INFO: The phase of Pod pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594 is Running (Ready = true)
Feb 27 16:08:42.169: INFO: Pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:42.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2608" for this suite. 02/27/23 16:08:42.186
------------------------------
• [2.065 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:40.13
    Feb 27 16:08:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 16:08:40.131
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:40.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:40.15
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Feb 27 16:08:40.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: creating the pod 02/27/23 16:08:40.153
    STEP: submitting the pod to kubernetes 02/27/23 16:08:40.154
    Feb 27 16:08:40.161: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594" in namespace "pods-2608" to be "running and ready"
    Feb 27 16:08:40.164: INFO: Pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594": Phase="Pending", Reason="", readiness=false. Elapsed: 3.752404ms
    Feb 27 16:08:40.164: INFO: The phase of Pod pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:08:42.169: INFO: Pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594": Phase="Running", Reason="", readiness=true. Elapsed: 2.00824893s
    Feb 27 16:08:42.169: INFO: The phase of Pod pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594 is Running (Ready = true)
    Feb 27 16:08:42.169: INFO: Pod "pod-logs-websocket-e46fb8a9-d99e-4fb6-8abd-74cb165e7594" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:42.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2608" for this suite. 02/27/23 16:08:42.186
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:42.196
Feb 27 16:08:42.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename security-context-test 02/27/23 16:08:42.196
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:42.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:42.217
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Feb 27 16:08:42.228: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0" in namespace "security-context-test-3176" to be "Succeeded or Failed"
Feb 27 16:08:42.230: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665255ms
Feb 27 16:08:44.235: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007489475s
Feb 27 16:08:46.236: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008010794s
Feb 27 16:08:46.236: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0" satisfied condition "Succeeded or Failed"
Feb 27 16:08:46.242: INFO: Got logs for pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:46.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3176" for this suite. 02/27/23 16:08:46.245
------------------------------
• [4.060 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:42.196
    Feb 27 16:08:42.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename security-context-test 02/27/23 16:08:42.196
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:42.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:42.217
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Feb 27 16:08:42.228: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0" in namespace "security-context-test-3176" to be "Succeeded or Failed"
    Feb 27 16:08:42.230: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665255ms
    Feb 27 16:08:44.235: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007489475s
    Feb 27 16:08:46.236: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008010794s
    Feb 27 16:08:46.236: INFO: Pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0" satisfied condition "Succeeded or Failed"
    Feb 27 16:08:46.242: INFO: Got logs for pod "busybox-privileged-false-0137015c-7a9d-49df-a1c2-97b2f396bff0": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:46.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3176" for this suite. 02/27/23 16:08:46.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:46.257
Feb 27 16:08:46.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:08:46.258
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:46.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:46.274
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-be4b6bf3-8e76-48b3-bb46-13ac9d0de076 02/27/23 16:08:46.277
STEP: Creating a pod to test consume configMaps 02/27/23 16:08:46.282
Feb 27 16:08:46.289: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8" in namespace "projected-5964" to be "Succeeded or Failed"
Feb 27 16:08:46.295: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899996ms
Feb 27 16:08:48.300: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010356737s
Feb 27 16:08:50.299: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009985328s
STEP: Saw pod success 02/27/23 16:08:50.299
Feb 27 16:08:50.299: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8" satisfied condition "Succeeded or Failed"
Feb 27 16:08:50.303: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 16:08:50.314
Feb 27 16:08:50.327: INFO: Waiting for pod pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8 to disappear
Feb 27 16:08:50.331: INFO: Pod pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:08:50.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5964" for this suite. 02/27/23 16:08:50.334
------------------------------
• [4.083 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:46.257
    Feb 27 16:08:46.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:08:46.258
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:46.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:46.274
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-be4b6bf3-8e76-48b3-bb46-13ac9d0de076 02/27/23 16:08:46.277
    STEP: Creating a pod to test consume configMaps 02/27/23 16:08:46.282
    Feb 27 16:08:46.289: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8" in namespace "projected-5964" to be "Succeeded or Failed"
    Feb 27 16:08:46.295: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899996ms
    Feb 27 16:08:48.300: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010356737s
    Feb 27 16:08:50.299: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009985328s
    STEP: Saw pod success 02/27/23 16:08:50.299
    Feb 27 16:08:50.299: INFO: Pod "pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8" satisfied condition "Succeeded or Failed"
    Feb 27 16:08:50.303: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 16:08:50.314
    Feb 27 16:08:50.327: INFO: Waiting for pod pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8 to disappear
    Feb 27 16:08:50.331: INFO: Pod pod-projected-configmaps-20c210c6-53d8-474f-82d4-560cf0f906b8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:08:50.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5964" for this suite. 02/27/23 16:08:50.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:08:50.341
Feb 27 16:08:50.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:08:50.342
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:50.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:50.363
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 02/27/23 16:08:50.366
STEP: Creating a ResourceQuota 02/27/23 16:08:55.371
STEP: Ensuring resource quota status is calculated 02/27/23 16:08:55.376
STEP: Creating a Pod that fits quota 02/27/23 16:08:57.381
STEP: Ensuring ResourceQuota status captures the pod usage 02/27/23 16:08:57.396
STEP: Not allowing a pod to be created that exceeds remaining quota 02/27/23 16:08:59.401
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/27/23 16:08:59.403
STEP: Ensuring a pod cannot update its resource requirements 02/27/23 16:08:59.405
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/27/23 16:08:59.41
STEP: Deleting the pod 02/27/23 16:09:01.414
STEP: Ensuring resource quota status released the pod usage 02/27/23 16:09:01.43
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:03.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2909" for this suite. 02/27/23 16:09:03.439
------------------------------
• [SLOW TEST] [13.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:08:50.341
    Feb 27 16:08:50.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:08:50.342
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:08:50.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:08:50.363
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 02/27/23 16:08:50.366
    STEP: Creating a ResourceQuota 02/27/23 16:08:55.371
    STEP: Ensuring resource quota status is calculated 02/27/23 16:08:55.376
    STEP: Creating a Pod that fits quota 02/27/23 16:08:57.381
    STEP: Ensuring ResourceQuota status captures the pod usage 02/27/23 16:08:57.396
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/27/23 16:08:59.401
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/27/23 16:08:59.403
    STEP: Ensuring a pod cannot update its resource requirements 02/27/23 16:08:59.405
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/27/23 16:08:59.41
    STEP: Deleting the pod 02/27/23 16:09:01.414
    STEP: Ensuring resource quota status released the pod usage 02/27/23 16:09:01.43
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:03.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2909" for this suite. 02/27/23 16:09:03.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:03.447
Feb 27 16:09:03.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 16:09:03.448
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:03.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:03.474
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/27/23 16:09:03.477
Feb 27 16:09:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 16:09:05.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:11.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8192" for this suite. 02/27/23 16:09:11.129
------------------------------
• [SLOW TEST] [7.689 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:03.447
    Feb 27 16:09:03.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 16:09:03.448
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:03.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:03.474
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/27/23 16:09:03.477
    Feb 27 16:09:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 16:09:05.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:11.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8192" for this suite. 02/27/23 16:09:11.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:11.137
Feb 27 16:09:11.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:09:11.138
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:11.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:11.156
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:09:11.159
Feb 27 16:09:11.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f" in namespace "downward-api-734" to be "Succeeded or Failed"
Feb 27 16:09:11.173: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.962243ms
Feb 27 16:09:13.177: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008143559s
Feb 27 16:09:15.178: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008898819s
STEP: Saw pod success 02/27/23 16:09:15.178
Feb 27 16:09:15.178: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f" satisfied condition "Succeeded or Failed"
Feb 27 16:09:15.182: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f container client-container: <nil>
STEP: delete the pod 02/27/23 16:09:15.188
Feb 27 16:09:15.208: INFO: Waiting for pod downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f to disappear
Feb 27 16:09:15.211: INFO: Pod downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:15.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-734" for this suite. 02/27/23 16:09:15.215
------------------------------
• [4.088 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:11.137
    Feb 27 16:09:11.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:09:11.138
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:11.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:11.156
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:09:11.159
    Feb 27 16:09:11.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f" in namespace "downward-api-734" to be "Succeeded or Failed"
    Feb 27 16:09:11.173: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.962243ms
    Feb 27 16:09:13.177: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008143559s
    Feb 27 16:09:15.178: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008898819s
    STEP: Saw pod success 02/27/23 16:09:15.178
    Feb 27 16:09:15.178: INFO: Pod "downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f" satisfied condition "Succeeded or Failed"
    Feb 27 16:09:15.182: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f container client-container: <nil>
    STEP: delete the pod 02/27/23 16:09:15.188
    Feb 27 16:09:15.208: INFO: Waiting for pod downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f to disappear
    Feb 27 16:09:15.211: INFO: Pod downwardapi-volume-4025f692-5810-42f8-befd-36199ac8cc7f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:15.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-734" for this suite. 02/27/23 16:09:15.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:15.226
Feb 27 16:09:15.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:09:15.227
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:15.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:15.247
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 02/27/23 16:09:15.25
Feb 27 16:09:15.259: INFO: Waiting up to 5m0s for pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a" in namespace "downward-api-2101" to be "Succeeded or Failed"
Feb 27 16:09:15.266: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.114619ms
Feb 27 16:09:17.272: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012727369s
Feb 27 16:09:19.271: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012341989s
STEP: Saw pod success 02/27/23 16:09:19.271
Feb 27 16:09:19.271: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a" satisfied condition "Succeeded or Failed"
Feb 27 16:09:19.275: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a container dapi-container: <nil>
STEP: delete the pod 02/27/23 16:09:19.282
Feb 27 16:09:19.295: INFO: Waiting for pod downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a to disappear
Feb 27 16:09:19.298: INFO: Pod downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2101" for this suite. 02/27/23 16:09:19.302
------------------------------
• [4.083 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:15.226
    Feb 27 16:09:15.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:09:15.227
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:15.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:15.247
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 02/27/23 16:09:15.25
    Feb 27 16:09:15.259: INFO: Waiting up to 5m0s for pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a" in namespace "downward-api-2101" to be "Succeeded or Failed"
    Feb 27 16:09:15.266: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.114619ms
    Feb 27 16:09:17.272: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012727369s
    Feb 27 16:09:19.271: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012341989s
    STEP: Saw pod success 02/27/23 16:09:19.271
    Feb 27 16:09:19.271: INFO: Pod "downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a" satisfied condition "Succeeded or Failed"
    Feb 27 16:09:19.275: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a container dapi-container: <nil>
    STEP: delete the pod 02/27/23 16:09:19.282
    Feb 27 16:09:19.295: INFO: Waiting for pod downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a to disappear
    Feb 27 16:09:19.298: INFO: Pod downward-api-fe6a5628-775e-4aaa-81ed-33e8d4833e1a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:19.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2101" for this suite. 02/27/23 16:09:19.302
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:19.309
Feb 27 16:09:19.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename namespaces 02/27/23 16:09:19.31
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:19.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:19.33
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-zlwx6" 02/27/23 16:09:19.333
Feb 27 16:09:19.348: INFO: Namespace "e2e-ns-zlwx6-8555" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-zlwx6-8555" 02/27/23 16:09:19.348
Feb 27 16:09:19.357: INFO: Namespace "e2e-ns-zlwx6-8555" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-zlwx6-8555" 02/27/23 16:09:19.357
Feb 27 16:09:19.368: INFO: Namespace "e2e-ns-zlwx6-8555" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:19.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5797" for this suite. 02/27/23 16:09:19.371
STEP: Destroying namespace "e2e-ns-zlwx6-8555" for this suite. 02/27/23 16:09:19.377
------------------------------
• [0.078 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:19.309
    Feb 27 16:09:19.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename namespaces 02/27/23 16:09:19.31
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:19.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:19.33
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-zlwx6" 02/27/23 16:09:19.333
    Feb 27 16:09:19.348: INFO: Namespace "e2e-ns-zlwx6-8555" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-zlwx6-8555" 02/27/23 16:09:19.348
    Feb 27 16:09:19.357: INFO: Namespace "e2e-ns-zlwx6-8555" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-zlwx6-8555" 02/27/23 16:09:19.357
    Feb 27 16:09:19.368: INFO: Namespace "e2e-ns-zlwx6-8555" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:19.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5797" for this suite. 02/27/23 16:09:19.371
    STEP: Destroying namespace "e2e-ns-zlwx6-8555" for this suite. 02/27/23 16:09:19.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:19.387
Feb 27 16:09:19.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 16:09:19.388
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:19.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:19.408
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-77971798-e6af-456e-891b-ff948b5fc9cd 02/27/23 16:09:19.412
STEP: Creating a pod to test consume secrets 02/27/23 16:09:19.417
Feb 27 16:09:19.424: INFO: Waiting up to 5m0s for pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0" in namespace "secrets-7063" to be "Succeeded or Failed"
Feb 27 16:09:19.428: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771695ms
Feb 27 16:09:21.432: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008473134s
Feb 27 16:09:23.433: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009194833s
STEP: Saw pod success 02/27/23 16:09:23.433
Feb 27 16:09:23.433: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0" satisfied condition "Succeeded or Failed"
Feb 27 16:09:23.437: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0 container secret-env-test: <nil>
STEP: delete the pod 02/27/23 16:09:23.443
Feb 27 16:09:23.454: INFO: Waiting for pod pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0 to disappear
Feb 27 16:09:23.458: INFO: Pod pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:23.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7063" for this suite. 02/27/23 16:09:23.461
------------------------------
• [4.081 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:19.387
    Feb 27 16:09:19.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 16:09:19.388
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:19.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:19.408
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-77971798-e6af-456e-891b-ff948b5fc9cd 02/27/23 16:09:19.412
    STEP: Creating a pod to test consume secrets 02/27/23 16:09:19.417
    Feb 27 16:09:19.424: INFO: Waiting up to 5m0s for pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0" in namespace "secrets-7063" to be "Succeeded or Failed"
    Feb 27 16:09:19.428: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771695ms
    Feb 27 16:09:21.432: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008473134s
    Feb 27 16:09:23.433: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009194833s
    STEP: Saw pod success 02/27/23 16:09:23.433
    Feb 27 16:09:23.433: INFO: Pod "pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0" satisfied condition "Succeeded or Failed"
    Feb 27 16:09:23.437: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0 container secret-env-test: <nil>
    STEP: delete the pod 02/27/23 16:09:23.443
    Feb 27 16:09:23.454: INFO: Waiting for pod pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0 to disappear
    Feb 27 16:09:23.458: INFO: Pod pod-secrets-d6eecf9f-2489-4251-aefd-f9e85080e8f0 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:23.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7063" for this suite. 02/27/23 16:09:23.461
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:23.469
Feb 27 16:09:23.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename disruption 02/27/23 16:09:23.469
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:23.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:23.489
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:23.493
Feb 27 16:09:23.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename disruption-2 02/27/23 16:09:23.493
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:23.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:23.513
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 02/27/23 16:09:23.521
STEP: Waiting for the pdb to be processed 02/27/23 16:09:25.536
STEP: Waiting for the pdb to be processed 02/27/23 16:09:25.548
STEP: listing a collection of PDBs across all namespaces 02/27/23 16:09:27.555
STEP: listing a collection of PDBs in namespace disruption-19 02/27/23 16:09:27.558
STEP: deleting a collection of PDBs 02/27/23 16:09:27.562
STEP: Waiting for the PDB collection to be deleted 02/27/23 16:09:27.575
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:27.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 16:09:27.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-1241" for this suite. 02/27/23 16:09:27.587
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-19" for this suite. 02/27/23 16:09:27.594
------------------------------
• [4.132 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:23.469
    Feb 27 16:09:23.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename disruption 02/27/23 16:09:23.469
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:23.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:23.489
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:23.493
    Feb 27 16:09:23.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename disruption-2 02/27/23 16:09:23.493
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:23.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:23.513
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 02/27/23 16:09:23.521
    STEP: Waiting for the pdb to be processed 02/27/23 16:09:25.536
    STEP: Waiting for the pdb to be processed 02/27/23 16:09:25.548
    STEP: listing a collection of PDBs across all namespaces 02/27/23 16:09:27.555
    STEP: listing a collection of PDBs in namespace disruption-19 02/27/23 16:09:27.558
    STEP: deleting a collection of PDBs 02/27/23 16:09:27.562
    STEP: Waiting for the PDB collection to be deleted 02/27/23 16:09:27.575
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:27.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:09:27.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-1241" for this suite. 02/27/23 16:09:27.587
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-19" for this suite. 02/27/23 16:09:27.594
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:09:27.601
Feb 27 16:09:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 16:09:27.601
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:27.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:27.623
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-863 02/27/23 16:09:27.626
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 02/27/23 16:09:27.631
Feb 27 16:09:27.648: INFO: Found 0 stateful pods, waiting for 3
Feb 27 16:09:37.655: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:09:37.655: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:09:37.655: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/27/23 16:09:37.664
Feb 27 16:09:37.684: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/27/23 16:09:37.684
STEP: Not applying an update when the partition is greater than the number of replicas 02/27/23 16:09:47.7
STEP: Performing a canary update 02/27/23 16:09:47.7
Feb 27 16:09:47.720: INFO: Updating stateful set ss2
Feb 27 16:09:47.728: INFO: Waiting for Pod statefulset-863/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 02/27/23 16:09:57.736
Feb 27 16:09:57.807: INFO: Found 2 stateful pods, waiting for 3
Feb 27 16:10:07.812: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:10:07.812: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:10:07.812: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/27/23 16:10:07.819
Feb 27 16:10:07.839: INFO: Updating stateful set ss2
Feb 27 16:10:07.845: INFO: Waiting for Pod statefulset-863/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Feb 27 16:10:17.874: INFO: Updating stateful set ss2
Feb 27 16:10:17.884: INFO: Waiting for StatefulSet statefulset-863/ss2 to complete update
Feb 27 16:10:17.884: INFO: Waiting for Pod statefulset-863/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 16:10:27.893: INFO: Deleting all statefulset in ns statefulset-863
Feb 27 16:10:27.896: INFO: Scaling statefulset ss2 to 0
Feb 27 16:10:37.913: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 16:10:37.916: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:37.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-863" for this suite. 02/27/23 16:10:37.932
------------------------------
• [SLOW TEST] [70.339 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:09:27.601
    Feb 27 16:09:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 16:09:27.601
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:09:27.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:09:27.623
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-863 02/27/23 16:09:27.626
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 02/27/23 16:09:27.631
    Feb 27 16:09:27.648: INFO: Found 0 stateful pods, waiting for 3
    Feb 27 16:09:37.655: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:09:37.655: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:09:37.655: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/27/23 16:09:37.664
    Feb 27 16:09:37.684: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/27/23 16:09:37.684
    STEP: Not applying an update when the partition is greater than the number of replicas 02/27/23 16:09:47.7
    STEP: Performing a canary update 02/27/23 16:09:47.7
    Feb 27 16:09:47.720: INFO: Updating stateful set ss2
    Feb 27 16:09:47.728: INFO: Waiting for Pod statefulset-863/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 02/27/23 16:09:57.736
    Feb 27 16:09:57.807: INFO: Found 2 stateful pods, waiting for 3
    Feb 27 16:10:07.812: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:10:07.812: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:10:07.812: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/27/23 16:10:07.819
    Feb 27 16:10:07.839: INFO: Updating stateful set ss2
    Feb 27 16:10:07.845: INFO: Waiting for Pod statefulset-863/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Feb 27 16:10:17.874: INFO: Updating stateful set ss2
    Feb 27 16:10:17.884: INFO: Waiting for StatefulSet statefulset-863/ss2 to complete update
    Feb 27 16:10:17.884: INFO: Waiting for Pod statefulset-863/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 16:10:27.893: INFO: Deleting all statefulset in ns statefulset-863
    Feb 27 16:10:27.896: INFO: Scaling statefulset ss2 to 0
    Feb 27 16:10:37.913: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 16:10:37.916: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:37.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-863" for this suite. 02/27/23 16:10:37.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:37.942
Feb 27 16:10:37.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename events 02/27/23 16:10:37.943
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:37.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:37.961
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/27/23 16:10:37.964
STEP: get a list of Events with a label in the current namespace 02/27/23 16:10:37.981
STEP: delete a list of events 02/27/23 16:10:37.985
Feb 27 16:10:37.985: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/27/23 16:10:38.006
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:38.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1645" for this suite. 02/27/23 16:10:38.013
------------------------------
• [0.079 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:37.942
    Feb 27 16:10:37.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename events 02/27/23 16:10:37.943
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:37.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:37.961
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/27/23 16:10:37.964
    STEP: get a list of Events with a label in the current namespace 02/27/23 16:10:37.981
    STEP: delete a list of events 02/27/23 16:10:37.985
    Feb 27 16:10:37.985: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/27/23 16:10:38.006
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:38.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1645" for this suite. 02/27/23 16:10:38.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:38.022
Feb 27 16:10:38.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:10:38.023
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:38.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:38.042
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 02/27/23 16:10:38.044
STEP: fetching the ConfigMap 02/27/23 16:10:38.049
STEP: patching the ConfigMap 02/27/23 16:10:38.051
STEP: listing all ConfigMaps in all namespaces with a label selector 02/27/23 16:10:38.056
STEP: deleting the ConfigMap by collection with a label selector 02/27/23 16:10:38.06
STEP: listing all ConfigMaps in test namespace 02/27/23 16:10:38.068
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:38.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4860" for this suite. 02/27/23 16:10:38.073
------------------------------
• [0.058 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:38.022
    Feb 27 16:10:38.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:10:38.023
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:38.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:38.042
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 02/27/23 16:10:38.044
    STEP: fetching the ConfigMap 02/27/23 16:10:38.049
    STEP: patching the ConfigMap 02/27/23 16:10:38.051
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/27/23 16:10:38.056
    STEP: deleting the ConfigMap by collection with a label selector 02/27/23 16:10:38.06
    STEP: listing all ConfigMaps in test namespace 02/27/23 16:10:38.068
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:38.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4860" for this suite. 02/27/23 16:10:38.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:38.082
Feb 27 16:10:38.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:10:38.083
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:38.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:38.1
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:10:38.103
Feb 27 16:10:38.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24" in namespace "downward-api-4789" to be "Succeeded or Failed"
Feb 27 16:10:38.119: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.904604ms
Feb 27 16:10:40.127: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24": Phase="Running", Reason="", readiness=false. Elapsed: 2.013709758s
Feb 27 16:10:42.125: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01171401s
STEP: Saw pod success 02/27/23 16:10:42.125
Feb 27 16:10:42.125: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24" satisfied condition "Succeeded or Failed"
Feb 27 16:10:42.128: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24 container client-container: <nil>
STEP: delete the pod 02/27/23 16:10:42.134
Feb 27 16:10:42.149: INFO: Waiting for pod downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24 to disappear
Feb 27 16:10:42.151: INFO: Pod downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:42.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4789" for this suite. 02/27/23 16:10:42.155
------------------------------
• [4.080 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:38.082
    Feb 27 16:10:38.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:10:38.083
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:38.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:38.1
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:10:38.103
    Feb 27 16:10:38.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24" in namespace "downward-api-4789" to be "Succeeded or Failed"
    Feb 27 16:10:38.119: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.904604ms
    Feb 27 16:10:40.127: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24": Phase="Running", Reason="", readiness=false. Elapsed: 2.013709758s
    Feb 27 16:10:42.125: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01171401s
    STEP: Saw pod success 02/27/23 16:10:42.125
    Feb 27 16:10:42.125: INFO: Pod "downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24" satisfied condition "Succeeded or Failed"
    Feb 27 16:10:42.128: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24 container client-container: <nil>
    STEP: delete the pod 02/27/23 16:10:42.134
    Feb 27 16:10:42.149: INFO: Waiting for pod downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24 to disappear
    Feb 27 16:10:42.151: INFO: Pod downwardapi-volume-a6025809-8540-4b7c-a69d-986b12403d24 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:42.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4789" for this suite. 02/27/23 16:10:42.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:42.163
Feb 27 16:10:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename events 02/27/23 16:10:42.164
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:42.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:42.184
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/27/23 16:10:42.187
Feb 27 16:10:42.191: INFO: created test-event-1
Feb 27 16:10:42.196: INFO: created test-event-2
Feb 27 16:10:42.205: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/27/23 16:10:42.205
STEP: delete collection of events 02/27/23 16:10:42.209
Feb 27 16:10:42.209: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/27/23 16:10:42.234
Feb 27 16:10:42.234: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:42.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4732" for this suite. 02/27/23 16:10:42.241
------------------------------
• [0.084 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:42.163
    Feb 27 16:10:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename events 02/27/23 16:10:42.164
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:42.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:42.184
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/27/23 16:10:42.187
    Feb 27 16:10:42.191: INFO: created test-event-1
    Feb 27 16:10:42.196: INFO: created test-event-2
    Feb 27 16:10:42.205: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/27/23 16:10:42.205
    STEP: delete collection of events 02/27/23 16:10:42.209
    Feb 27 16:10:42.209: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/27/23 16:10:42.234
    Feb 27 16:10:42.234: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:42.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4732" for this suite. 02/27/23 16:10:42.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:42.248
Feb 27 16:10:42.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 16:10:42.249
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:42.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:42.268
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 02/27/23 16:10:42.275
STEP: watching for Pod to be ready 02/27/23 16:10:42.284
Feb 27 16:10:42.285: INFO: observed Pod pod-test in namespace pods-1033 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 27 16:10:42.287: INFO: observed Pod pod-test in namespace pods-1033 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  }]
Feb 27 16:10:42.302: INFO: observed Pod pod-test in namespace pods-1033 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  }]
Feb 27 16:10:43.827: INFO: Found Pod pod-test in namespace pods-1033 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/27/23 16:10:43.831
STEP: getting the Pod and ensuring that it's patched 02/27/23 16:10:43.84
STEP: replacing the Pod's status Ready condition to False 02/27/23 16:10:43.844
STEP: check the Pod again to ensure its Ready conditions are False 02/27/23 16:10:43.854
STEP: deleting the Pod via a Collection with a LabelSelector 02/27/23 16:10:43.854
STEP: watching for the Pod to be deleted 02/27/23 16:10:43.865
Feb 27 16:10:43.866: INFO: observed event type MODIFIED
Feb 27 16:10:45.836: INFO: observed event type MODIFIED
Feb 27 16:10:46.834: INFO: observed event type MODIFIED
Feb 27 16:10:46.842: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:46.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1033" for this suite. 02/27/23 16:10:46.853
------------------------------
• [4.612 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:42.248
    Feb 27 16:10:42.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 16:10:42.249
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:42.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:42.268
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 02/27/23 16:10:42.275
    STEP: watching for Pod to be ready 02/27/23 16:10:42.284
    Feb 27 16:10:42.285: INFO: observed Pod pod-test in namespace pods-1033 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb 27 16:10:42.287: INFO: observed Pod pod-test in namespace pods-1033 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  }]
    Feb 27 16:10:42.302: INFO: observed Pod pod-test in namespace pods-1033 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  }]
    Feb 27 16:10:43.827: INFO: Found Pod pod-test in namespace pods-1033 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:10:42 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/27/23 16:10:43.831
    STEP: getting the Pod and ensuring that it's patched 02/27/23 16:10:43.84
    STEP: replacing the Pod's status Ready condition to False 02/27/23 16:10:43.844
    STEP: check the Pod again to ensure its Ready conditions are False 02/27/23 16:10:43.854
    STEP: deleting the Pod via a Collection with a LabelSelector 02/27/23 16:10:43.854
    STEP: watching for the Pod to be deleted 02/27/23 16:10:43.865
    Feb 27 16:10:43.866: INFO: observed event type MODIFIED
    Feb 27 16:10:45.836: INFO: observed event type MODIFIED
    Feb 27 16:10:46.834: INFO: observed event type MODIFIED
    Feb 27 16:10:46.842: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:46.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1033" for this suite. 02/27/23 16:10:46.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:46.862
Feb 27 16:10:46.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-webhook 02/27/23 16:10:46.862
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:46.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:46.88
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/27/23 16:10:46.883
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 16:10:47.229
STEP: Deploying the custom resource conversion webhook pod 02/27/23 16:10:47.236
STEP: Wait for the deployment to be ready 02/27/23 16:10:47.25
Feb 27 16:10:47.258: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 16:10:49.269
STEP: Verifying the service has paired with the endpoint 02/27/23 16:10:49.28
Feb 27 16:10:50.280: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb 27 16:10:50.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Creating a v1 custom resource 02/27/23 16:10:52.852
STEP: v2 custom resource should be converted 02/27/23 16:10:52.858
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:53.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8714" for this suite. 02/27/23 16:10:53.427
------------------------------
• [SLOW TEST] [6.574 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:46.862
    Feb 27 16:10:46.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-webhook 02/27/23 16:10:46.862
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:46.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:46.88
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/27/23 16:10:46.883
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 16:10:47.229
    STEP: Deploying the custom resource conversion webhook pod 02/27/23 16:10:47.236
    STEP: Wait for the deployment to be ready 02/27/23 16:10:47.25
    Feb 27 16:10:47.258: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 16:10:49.269
    STEP: Verifying the service has paired with the endpoint 02/27/23 16:10:49.28
    Feb 27 16:10:50.280: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb 27 16:10:50.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Creating a v1 custom resource 02/27/23 16:10:52.852
    STEP: v2 custom resource should be converted 02/27/23 16:10:52.858
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:53.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8714" for this suite. 02/27/23 16:10:53.427
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:53.437
Feb 27 16:10:53.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 16:10:53.438
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:53.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:53.455
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 02/27/23 16:10:53.458
STEP: setting up watch 02/27/23 16:10:53.458
STEP: submitting the pod to kubernetes 02/27/23 16:10:53.562
STEP: verifying the pod is in kubernetes 02/27/23 16:10:53.569
STEP: verifying pod creation was observed 02/27/23 16:10:53.576
Feb 27 16:10:53.576: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284" in namespace "pods-1131" to be "running"
Feb 27 16:10:53.580: INFO: Pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610688ms
Feb 27 16:10:55.585: INFO: Pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284": Phase="Running", Reason="", readiness=true. Elapsed: 2.008777577s
Feb 27 16:10:55.585: INFO: Pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 16:10:55.588
STEP: verifying pod deletion was observed 02/27/23 16:10:55.596
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 16:10:57.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1131" for this suite. 02/27/23 16:10:57.877
------------------------------
• [4.447 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:53.437
    Feb 27 16:10:53.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 16:10:53.438
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:53.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:53.455
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 02/27/23 16:10:53.458
    STEP: setting up watch 02/27/23 16:10:53.458
    STEP: submitting the pod to kubernetes 02/27/23 16:10:53.562
    STEP: verifying the pod is in kubernetes 02/27/23 16:10:53.569
    STEP: verifying pod creation was observed 02/27/23 16:10:53.576
    Feb 27 16:10:53.576: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284" in namespace "pods-1131" to be "running"
    Feb 27 16:10:53.580: INFO: Pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610688ms
    Feb 27 16:10:55.585: INFO: Pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284": Phase="Running", Reason="", readiness=true. Elapsed: 2.008777577s
    Feb 27 16:10:55.585: INFO: Pod "pod-submit-remove-fb527922-6d98-428f-b006-971ed0be2284" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 16:10:55.588
    STEP: verifying pod deletion was observed 02/27/23 16:10:55.596
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:10:57.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1131" for this suite. 02/27/23 16:10:57.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:10:57.888
Feb 27 16:10:57.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 16:10:57.888
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:57.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:57.906
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 16:10:57.923
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:10:58.133
STEP: Deploying the webhook pod 02/27/23 16:10:58.138
STEP: Wait for the deployment to be ready 02/27/23 16:10:58.152
Feb 27 16:10:58.165: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 16:11:00.176
STEP: Verifying the service has paired with the endpoint 02/27/23 16:11:00.187
Feb 27 16:11:01.187: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/27/23 16:11:01.191
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:01.191
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/27/23 16:11:01.204
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/27/23 16:11:02.215
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:02.215
STEP: Having no error when timeout is longer than webhook latency 02/27/23 16:11:03.243
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:03.243
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/27/23 16:11:08.275
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:08.275
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:11:13.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3234" for this suite. 02/27/23 16:11:13.356
STEP: Destroying namespace "webhook-3234-markers" for this suite. 02/27/23 16:11:13.366
------------------------------
• [SLOW TEST] [15.489 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:10:57.888
    Feb 27 16:10:57.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 16:10:57.888
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:10:57.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:10:57.906
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 16:10:57.923
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:10:58.133
    STEP: Deploying the webhook pod 02/27/23 16:10:58.138
    STEP: Wait for the deployment to be ready 02/27/23 16:10:58.152
    Feb 27 16:10:58.165: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 16:11:00.176
    STEP: Verifying the service has paired with the endpoint 02/27/23 16:11:00.187
    Feb 27 16:11:01.187: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/27/23 16:11:01.191
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:01.191
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/27/23 16:11:01.204
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/27/23 16:11:02.215
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:02.215
    STEP: Having no error when timeout is longer than webhook latency 02/27/23 16:11:03.243
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:03.243
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/27/23 16:11:08.275
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 16:11:08.275
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:11:13.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3234" for this suite. 02/27/23 16:11:13.356
    STEP: Destroying namespace "webhook-3234-markers" for this suite. 02/27/23 16:11:13.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:11:13.377
Feb 27 16:11:13.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 16:11:13.378
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:11:13.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:11:13.402
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb 27 16:11:13.405: INFO: Creating simple deployment test-new-deployment
Feb 27 16:11:13.421: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 02/27/23 16:11:15.435
STEP: updating a scale subresource 02/27/23 16:11:15.438
STEP: verifying the deployment Spec.Replicas was modified 02/27/23 16:11:15.445
STEP: Patch a scale subresource 02/27/23 16:11:15.448
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 16:11:15.465: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1635  c75a0fa0-0d2c-49e3-9c4b-b70f364a5629 29492 3 2023-02-27 16:11:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-27 16:11:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:11:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012d5718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 16:11:14 +0000 UTC,LastTransitionTime:2023-02-27 16:11:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-02-27 16:11:14 +0000 UTC,LastTransitionTime:2023-02-27 16:11:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 16:11:15.470: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1635  32875057-4667-4373-8e40-d51f01f8f6e0 29491 2 2023-02-27 16:11:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c75a0fa0-0d2c-49e3-9c4b-b70f364a5629 0xc0012d5b47 0xc0012d5b48}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:11:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-27 16:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c75a0fa0-0d2c-49e3-9c4b-b70f364a5629\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012d5bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 16:11:15.476: INFO: Pod "test-new-deployment-7f5969cbc7-kjqc4" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kjqc4 test-new-deployment-7f5969cbc7- deployment-1635  b8459421-d979-4ac5-b4e5-c2be6ae14580 29486 0 2023-02-27 16:11:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 32875057-4667-4373-8e40-d51f01f8f6e0 0xc0012d5f67 0xc0012d5f68}] [] [{kube-controller-manager Update v1 2023-02-27 16:11:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32875057-4667-4373-8e40-d51f01f8f6e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:11:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dhb6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dhb6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.184,StartTime:2023-02-27 16:11:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:11:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0a359709718024b0cba23d3b1bf645114d8110f8dcf2e5f72392fc02dc24b5cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:11:15.476: INFO: Pod "test-new-deployment-7f5969cbc7-tj6dx" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tj6dx test-new-deployment-7f5969cbc7- deployment-1635  d08c7162-ff4e-49d4-8664-83bb12f0f920 29496 0 2023-02-27 16:11:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 32875057-4667-4373-8e40-d51f01f8f6e0 0xc004a78157 0xc004a78158}] [] [{kube-controller-manager Update v1 2023-02-27 16:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32875057-4667-4373-8e40-d51f01f8f6e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79v4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79v4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 16:11:15.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1635" for this suite. 02/27/23 16:11:15.49
------------------------------
• [2.133 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:11:13.377
    Feb 27 16:11:13.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 16:11:13.378
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:11:13.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:11:13.402
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb 27 16:11:13.405: INFO: Creating simple deployment test-new-deployment
    Feb 27 16:11:13.421: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 02/27/23 16:11:15.435
    STEP: updating a scale subresource 02/27/23 16:11:15.438
    STEP: verifying the deployment Spec.Replicas was modified 02/27/23 16:11:15.445
    STEP: Patch a scale subresource 02/27/23 16:11:15.448
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 16:11:15.465: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1635  c75a0fa0-0d2c-49e3-9c4b-b70f364a5629 29492 3 2023-02-27 16:11:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-27 16:11:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:11:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012d5718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 16:11:14 +0000 UTC,LastTransitionTime:2023-02-27 16:11:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-02-27 16:11:14 +0000 UTC,LastTransitionTime:2023-02-27 16:11:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 16:11:15.470: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1635  32875057-4667-4373-8e40-d51f01f8f6e0 29491 2 2023-02-27 16:11:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c75a0fa0-0d2c-49e3-9c4b-b70f364a5629 0xc0012d5b47 0xc0012d5b48}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:11:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-27 16:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c75a0fa0-0d2c-49e3-9c4b-b70f364a5629\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012d5bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 16:11:15.476: INFO: Pod "test-new-deployment-7f5969cbc7-kjqc4" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kjqc4 test-new-deployment-7f5969cbc7- deployment-1635  b8459421-d979-4ac5-b4e5-c2be6ae14580 29486 0 2023-02-27 16:11:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 32875057-4667-4373-8e40-d51f01f8f6e0 0xc0012d5f67 0xc0012d5f68}] [] [{kube-controller-manager Update v1 2023-02-27 16:11:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32875057-4667-4373-8e40-d51f01f8f6e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:11:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dhb6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dhb6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.184,StartTime:2023-02-27 16:11:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:11:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0a359709718024b0cba23d3b1bf645114d8110f8dcf2e5f72392fc02dc24b5cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:11:15.476: INFO: Pod "test-new-deployment-7f5969cbc7-tj6dx" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-tj6dx test-new-deployment-7f5969cbc7- deployment-1635  d08c7162-ff4e-49d4-8664-83bb12f0f920 29496 0 2023-02-27 16:11:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 32875057-4667-4373-8e40-d51f01f8f6e0 0xc004a78157 0xc004a78158}] [] [{kube-controller-manager Update v1 2023-02-27 16:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32875057-4667-4373-8e40-d51f01f8f6e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79v4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79v4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:11:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:11:15.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1635" for this suite. 02/27/23 16:11:15.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:11:15.511
Feb 27 16:11:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:11:15.512
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:11:15.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:11:15.543
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-ba668976-768f-4107-8c3a-407dcaec1ca5 02/27/23 16:11:15.546
STEP: Creating a pod to test consume configMaps 02/27/23 16:11:15.551
Feb 27 16:11:15.560: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8" in namespace "configmap-5184" to be "Succeeded or Failed"
Feb 27 16:11:15.563: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.522385ms
Feb 27 16:11:17.568: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007855626s
Feb 27 16:11:19.568: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008240062s
STEP: Saw pod success 02/27/23 16:11:19.568
Feb 27 16:11:19.568: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8" satisfied condition "Succeeded or Failed"
Feb 27 16:11:19.572: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8 container configmap-volume-test: <nil>
STEP: delete the pod 02/27/23 16:11:19.578
Feb 27 16:11:19.594: INFO: Waiting for pod pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8 to disappear
Feb 27 16:11:19.597: INFO: Pod pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:11:19.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5184" for this suite. 02/27/23 16:11:19.601
------------------------------
• [4.097 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:11:15.511
    Feb 27 16:11:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:11:15.512
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:11:15.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:11:15.543
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-ba668976-768f-4107-8c3a-407dcaec1ca5 02/27/23 16:11:15.546
    STEP: Creating a pod to test consume configMaps 02/27/23 16:11:15.551
    Feb 27 16:11:15.560: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8" in namespace "configmap-5184" to be "Succeeded or Failed"
    Feb 27 16:11:15.563: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.522385ms
    Feb 27 16:11:17.568: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007855626s
    Feb 27 16:11:19.568: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008240062s
    STEP: Saw pod success 02/27/23 16:11:19.568
    Feb 27 16:11:19.568: INFO: Pod "pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8" satisfied condition "Succeeded or Failed"
    Feb 27 16:11:19.572: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8 container configmap-volume-test: <nil>
    STEP: delete the pod 02/27/23 16:11:19.578
    Feb 27 16:11:19.594: INFO: Waiting for pod pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8 to disappear
    Feb 27 16:11:19.597: INFO: Pod pod-configmaps-a9c619ce-bf1a-4b49-8fc9-21d435a805c8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:11:19.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5184" for this suite. 02/27/23 16:11:19.601
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:11:19.608
Feb 27 16:11:19.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 16:11:19.609
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:11:19.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:11:19.639
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b in namespace container-probe-7134 02/27/23 16:11:19.641
Feb 27 16:11:19.650: INFO: Waiting up to 5m0s for pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b" in namespace "container-probe-7134" to be "not pending"
Feb 27 16:11:19.657: INFO: Pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.682471ms
Feb 27 16:11:21.661: INFO: Pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.01107072s
Feb 27 16:11:21.661: INFO: Pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b" satisfied condition "not pending"
Feb 27 16:11:21.661: INFO: Started pod liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b in namespace container-probe-7134
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 16:11:21.661
Feb 27 16:11:21.665: INFO: Initial restart count of pod liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is 0
Feb 27 16:11:41.714: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 1 (20.049142211s elapsed)
Feb 27 16:12:01.761: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 2 (40.095797411s elapsed)
Feb 27 16:12:21.805: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 3 (1m0.140305925s elapsed)
Feb 27 16:12:41.849: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 4 (1m20.183615485s elapsed)
Feb 27 16:13:52.003: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 5 (2m30.338233179s elapsed)
STEP: deleting the pod 02/27/23 16:13:52.003
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 16:13:52.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7134" for this suite. 02/27/23 16:13:52.023
------------------------------
• [SLOW TEST] [152.420 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:11:19.608
    Feb 27 16:11:19.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 16:11:19.609
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:11:19.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:11:19.639
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b in namespace container-probe-7134 02/27/23 16:11:19.641
    Feb 27 16:11:19.650: INFO: Waiting up to 5m0s for pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b" in namespace "container-probe-7134" to be "not pending"
    Feb 27 16:11:19.657: INFO: Pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.682471ms
    Feb 27 16:11:21.661: INFO: Pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.01107072s
    Feb 27 16:11:21.661: INFO: Pod "liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b" satisfied condition "not pending"
    Feb 27 16:11:21.661: INFO: Started pod liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b in namespace container-probe-7134
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 16:11:21.661
    Feb 27 16:11:21.665: INFO: Initial restart count of pod liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is 0
    Feb 27 16:11:41.714: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 1 (20.049142211s elapsed)
    Feb 27 16:12:01.761: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 2 (40.095797411s elapsed)
    Feb 27 16:12:21.805: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 3 (1m0.140305925s elapsed)
    Feb 27 16:12:41.849: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 4 (1m20.183615485s elapsed)
    Feb 27 16:13:52.003: INFO: Restart count of pod container-probe-7134/liveness-e210bace-820e-4f6a-8cfe-f2ac30f99c5b is now 5 (2m30.338233179s elapsed)
    STEP: deleting the pod 02/27/23 16:13:52.003
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:13:52.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7134" for this suite. 02/27/23 16:13:52.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:13:52.029
Feb 27 16:13:52.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replicaset 02/27/23 16:13:52.03
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:13:52.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:13:52.05
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/27/23 16:13:52.052
Feb 27 16:13:52.060: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 16:13:57.064: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 16:13:57.064
STEP: getting scale subresource 02/27/23 16:13:57.064
STEP: updating a scale subresource 02/27/23 16:13:57.069
STEP: verifying the replicaset Spec.Replicas was modified 02/27/23 16:13:57.076
STEP: Patch a scale subresource 02/27/23 16:13:57.079
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 16:13:57.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5899" for this suite. 02/27/23 16:13:57.094
------------------------------
• [SLOW TEST] [5.074 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:13:52.029
    Feb 27 16:13:52.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replicaset 02/27/23 16:13:52.03
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:13:52.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:13:52.05
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/27/23 16:13:52.052
    Feb 27 16:13:52.060: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 16:13:57.064: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 16:13:57.064
    STEP: getting scale subresource 02/27/23 16:13:57.064
    STEP: updating a scale subresource 02/27/23 16:13:57.069
    STEP: verifying the replicaset Spec.Replicas was modified 02/27/23 16:13:57.076
    STEP: Patch a scale subresource 02/27/23 16:13:57.079
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:13:57.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5899" for this suite. 02/27/23 16:13:57.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:13:57.104
Feb 27 16:13:57.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 16:13:57.105
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:13:57.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:13:57.129
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-bb9c000c-5911-480b-bf21-1a8ef6e503c3 02/27/23 16:13:57.209
STEP: Creating a pod to test consume secrets 02/27/23 16:13:57.213
Feb 27 16:13:57.222: INFO: Waiting up to 5m0s for pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b" in namespace "secrets-8675" to be "Succeeded or Failed"
Feb 27 16:13:57.224: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.660974ms
Feb 27 16:13:59.229: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007264149s
Feb 27 16:14:01.229: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007415094s
STEP: Saw pod success 02/27/23 16:14:01.229
Feb 27 16:14:01.229: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b" satisfied condition "Succeeded or Failed"
Feb 27 16:14:01.232: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 16:14:01.246
Feb 27 16:14:01.260: INFO: Waiting for pod pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b to disappear
Feb 27 16:14:01.262: INFO: Pod pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 16:14:01.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8675" for this suite. 02/27/23 16:14:01.266
STEP: Destroying namespace "secret-namespace-4026" for this suite. 02/27/23 16:14:01.272
------------------------------
• [4.174 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:13:57.104
    Feb 27 16:13:57.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 16:13:57.105
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:13:57.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:13:57.129
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-bb9c000c-5911-480b-bf21-1a8ef6e503c3 02/27/23 16:13:57.209
    STEP: Creating a pod to test consume secrets 02/27/23 16:13:57.213
    Feb 27 16:13:57.222: INFO: Waiting up to 5m0s for pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b" in namespace "secrets-8675" to be "Succeeded or Failed"
    Feb 27 16:13:57.224: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.660974ms
    Feb 27 16:13:59.229: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007264149s
    Feb 27 16:14:01.229: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007415094s
    STEP: Saw pod success 02/27/23 16:14:01.229
    Feb 27 16:14:01.229: INFO: Pod "pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b" satisfied condition "Succeeded or Failed"
    Feb 27 16:14:01.232: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 16:14:01.246
    Feb 27 16:14:01.260: INFO: Waiting for pod pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b to disappear
    Feb 27 16:14:01.262: INFO: Pod pod-secrets-61d4c6ca-9cee-41e2-b8c3-7c9be3b2b90b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:14:01.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8675" for this suite. 02/27/23 16:14:01.266
    STEP: Destroying namespace "secret-namespace-4026" for this suite. 02/27/23 16:14:01.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:14:01.278
Feb 27 16:14:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 16:14:01.279
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:01.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:01.298
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 02/27/23 16:14:01.3
Feb 27 16:14:01.309: INFO: Waiting up to 5m0s for pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f" in namespace "var-expansion-6358" to be "Succeeded or Failed"
Feb 27 16:14:01.313: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208361ms
Feb 27 16:14:03.318: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008657253s
Feb 27 16:14:05.317: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007552137s
STEP: Saw pod success 02/27/23 16:14:05.317
Feb 27 16:14:05.317: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f" satisfied condition "Succeeded or Failed"
Feb 27 16:14:05.321: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f container dapi-container: <nil>
STEP: delete the pod 02/27/23 16:14:05.326
Feb 27 16:14:05.343: INFO: Waiting for pod var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f to disappear
Feb 27 16:14:05.346: INFO: Pod var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 16:14:05.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6358" for this suite. 02/27/23 16:14:05.35
------------------------------
• [4.078 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:14:01.278
    Feb 27 16:14:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 16:14:01.279
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:01.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:01.298
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 02/27/23 16:14:01.3
    Feb 27 16:14:01.309: INFO: Waiting up to 5m0s for pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f" in namespace "var-expansion-6358" to be "Succeeded or Failed"
    Feb 27 16:14:01.313: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208361ms
    Feb 27 16:14:03.318: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008657253s
    Feb 27 16:14:05.317: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007552137s
    STEP: Saw pod success 02/27/23 16:14:05.317
    Feb 27 16:14:05.317: INFO: Pod "var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f" satisfied condition "Succeeded or Failed"
    Feb 27 16:14:05.321: INFO: Trying to get logs from node ip-172-31-42-40 pod var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f container dapi-container: <nil>
    STEP: delete the pod 02/27/23 16:14:05.326
    Feb 27 16:14:05.343: INFO: Waiting for pod var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f to disappear
    Feb 27 16:14:05.346: INFO: Pod var-expansion-da91498d-53bd-45cc-be82-485e779c9b4f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:14:05.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6358" for this suite. 02/27/23 16:14:05.35
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:14:05.357
Feb 27 16:14:05.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 16:14:05.357
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:05.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:05.381
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-0a90d3e7-2b40-4870-abb2-a5f5977c9290 02/27/23 16:14:05.384
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 16:14:05.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2048" for this suite. 02/27/23 16:14:05.389
------------------------------
• [0.039 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:14:05.357
    Feb 27 16:14:05.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 16:14:05.357
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:05.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:05.381
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-0a90d3e7-2b40-4870-abb2-a5f5977c9290 02/27/23 16:14:05.384
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:14:05.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2048" for this suite. 02/27/23 16:14:05.389
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:14:05.396
Feb 27 16:14:05.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename certificates 02/27/23 16:14:05.396
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:05.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:05.416
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/27/23 16:14:05.911
STEP: getting /apis/certificates.k8s.io 02/27/23 16:14:05.914
STEP: getting /apis/certificates.k8s.io/v1 02/27/23 16:14:05.915
STEP: creating 02/27/23 16:14:05.916
STEP: getting 02/27/23 16:14:05.931
STEP: listing 02/27/23 16:14:05.934
STEP: watching 02/27/23 16:14:05.938
Feb 27 16:14:05.938: INFO: starting watch
STEP: patching 02/27/23 16:14:05.939
STEP: updating 02/27/23 16:14:05.945
Feb 27 16:14:05.950: INFO: waiting for watch events with expected annotations
Feb 27 16:14:05.950: INFO: saw patched and updated annotations
STEP: getting /approval 02/27/23 16:14:05.95
STEP: patching /approval 02/27/23 16:14:05.954
STEP: updating /approval 02/27/23 16:14:05.962
STEP: getting /status 02/27/23 16:14:05.968
STEP: patching /status 02/27/23 16:14:05.972
STEP: updating /status 02/27/23 16:14:05.98
STEP: deleting 02/27/23 16:14:05.986
STEP: deleting a collection 02/27/23 16:14:05.999
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:14:06.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-1187" for this suite. 02/27/23 16:14:06.021
------------------------------
• [0.633 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:14:05.396
    Feb 27 16:14:05.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename certificates 02/27/23 16:14:05.396
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:05.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:05.416
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/27/23 16:14:05.911
    STEP: getting /apis/certificates.k8s.io 02/27/23 16:14:05.914
    STEP: getting /apis/certificates.k8s.io/v1 02/27/23 16:14:05.915
    STEP: creating 02/27/23 16:14:05.916
    STEP: getting 02/27/23 16:14:05.931
    STEP: listing 02/27/23 16:14:05.934
    STEP: watching 02/27/23 16:14:05.938
    Feb 27 16:14:05.938: INFO: starting watch
    STEP: patching 02/27/23 16:14:05.939
    STEP: updating 02/27/23 16:14:05.945
    Feb 27 16:14:05.950: INFO: waiting for watch events with expected annotations
    Feb 27 16:14:05.950: INFO: saw patched and updated annotations
    STEP: getting /approval 02/27/23 16:14:05.95
    STEP: patching /approval 02/27/23 16:14:05.954
    STEP: updating /approval 02/27/23 16:14:05.962
    STEP: getting /status 02/27/23 16:14:05.968
    STEP: patching /status 02/27/23 16:14:05.972
    STEP: updating /status 02/27/23 16:14:05.98
    STEP: deleting 02/27/23 16:14:05.986
    STEP: deleting a collection 02/27/23 16:14:05.999
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:14:06.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-1187" for this suite. 02/27/23 16:14:06.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:14:06.03
Feb 27 16:14:06.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:14:06.031
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:06.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:06.05
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Feb 27 16:14:06.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9382 version'
Feb 27 16:14:06.095: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb 27 16:14:06.095: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-19T02:09:07Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:14:06.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9382" for this suite. 02/27/23 16:14:06.1
------------------------------
• [0.077 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:14:06.03
    Feb 27 16:14:06.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:14:06.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:06.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:06.05
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Feb 27 16:14:06.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9382 version'
    Feb 27 16:14:06.095: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb 27 16:14:06.095: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-19T02:09:07Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:14:06.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9382" for this suite. 02/27/23 16:14:06.1
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:14:06.107
Feb 27 16:14:06.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 16:14:06.108
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:06.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:06.136
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-629 02/27/23 16:14:06.14
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 02/27/23 16:14:06.145
Feb 27 16:14:06.268: INFO: Found 0 stateful pods, waiting for 3
Feb 27 16:14:16.273: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:14:16.274: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:14:16.274: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:14:16.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 16:14:16.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 16:14:16.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 16:14:16.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/27/23 16:14:26.429
Feb 27 16:14:26.451: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/27/23 16:14:26.451
STEP: Updating Pods in reverse ordinal order 02/27/23 16:14:36.468
Feb 27 16:14:36.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 16:14:36.589: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 16:14:36.589: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 16:14:36.589: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 02/27/23 16:14:46.613
Feb 27 16:14:46.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 16:14:46.742: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 16:14:46.742: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 16:14:46.742: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 16:14:56.776: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/27/23 16:15:06.792
Feb 27 16:15:06.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 16:15:06.904: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 16:15:06.904: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 16:15:06.904: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 16:15:16.925: INFO: Deleting all statefulset in ns statefulset-629
Feb 27 16:15:16.929: INFO: Scaling statefulset ss2 to 0
Feb 27 16:15:26.947: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 16:15:26.951: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 16:15:26.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-629" for this suite. 02/27/23 16:15:26.965
------------------------------
• [SLOW TEST] [80.866 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:14:06.107
    Feb 27 16:14:06.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 16:14:06.108
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:14:06.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:14:06.136
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-629 02/27/23 16:14:06.14
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 02/27/23 16:14:06.145
    Feb 27 16:14:06.268: INFO: Found 0 stateful pods, waiting for 3
    Feb 27 16:14:16.273: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:14:16.274: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:14:16.274: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:14:16.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 16:14:16.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 16:14:16.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 16:14:16.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 02/27/23 16:14:26.429
    Feb 27 16:14:26.451: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/27/23 16:14:26.451
    STEP: Updating Pods in reverse ordinal order 02/27/23 16:14:36.468
    Feb 27 16:14:36.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 16:14:36.589: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 16:14:36.589: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 16:14:36.589: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 02/27/23 16:14:46.613
    Feb 27 16:14:46.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 16:14:46.742: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 16:14:46.742: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 16:14:46.742: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 16:14:56.776: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/27/23 16:15:06.792
    Feb 27 16:15:06.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-629 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 16:15:06.904: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 16:15:06.904: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 16:15:06.904: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 16:15:16.925: INFO: Deleting all statefulset in ns statefulset-629
    Feb 27 16:15:16.929: INFO: Scaling statefulset ss2 to 0
    Feb 27 16:15:26.947: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 16:15:26.951: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:15:26.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-629" for this suite. 02/27/23 16:15:26.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:15:26.975
Feb 27 16:15:26.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 16:15:26.976
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:26.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:26.994
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 16:15:27.012
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:15:27.475
STEP: Deploying the webhook pod 02/27/23 16:15:27.484
STEP: Wait for the deployment to be ready 02/27/23 16:15:27.498
Feb 27 16:15:27.506: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 16:15:29.518
STEP: Verifying the service has paired with the endpoint 02/27/23 16:15:29.529
Feb 27 16:15:30.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Feb 27 16:15:30.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-49-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 16:15:31.046
STEP: Creating a custom resource while v1 is storage version 02/27/23 16:15:31.06
STEP: Patching Custom Resource Definition to set v2 as storage 02/27/23 16:15:33.106
STEP: Patching the custom resource while v2 is storage version 02/27/23 16:15:33.123
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:15:33.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4483" for this suite. 02/27/23 16:15:33.697
STEP: Destroying namespace "webhook-4483-markers" for this suite. 02/27/23 16:15:33.705
------------------------------
• [SLOW TEST] [6.737 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:15:26.975
    Feb 27 16:15:26.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 16:15:26.976
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:26.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:26.994
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 16:15:27.012
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:15:27.475
    STEP: Deploying the webhook pod 02/27/23 16:15:27.484
    STEP: Wait for the deployment to be ready 02/27/23 16:15:27.498
    Feb 27 16:15:27.506: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 16:15:29.518
    STEP: Verifying the service has paired with the endpoint 02/27/23 16:15:29.529
    Feb 27 16:15:30.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Feb 27 16:15:30.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-49-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 16:15:31.046
    STEP: Creating a custom resource while v1 is storage version 02/27/23 16:15:31.06
    STEP: Patching Custom Resource Definition to set v2 as storage 02/27/23 16:15:33.106
    STEP: Patching the custom resource while v2 is storage version 02/27/23 16:15:33.123
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:15:33.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4483" for this suite. 02/27/23 16:15:33.697
    STEP: Destroying namespace "webhook-4483-markers" for this suite. 02/27/23 16:15:33.705
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:15:33.713
Feb 27 16:15:33.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename containers 02/27/23 16:15:33.714
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:33.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:33.73
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 02/27/23 16:15:33.734
Feb 27 16:15:33.743: INFO: Waiting up to 5m0s for pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832" in namespace "containers-9280" to be "Succeeded or Failed"
Feb 27 16:15:33.748: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832": Phase="Pending", Reason="", readiness=false. Elapsed: 4.448716ms
Feb 27 16:15:35.752: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008893492s
Feb 27 16:15:37.752: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008566458s
STEP: Saw pod success 02/27/23 16:15:37.752
Feb 27 16:15:37.752: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832" satisfied condition "Succeeded or Failed"
Feb 27 16:15:37.755: INFO: Trying to get logs from node ip-172-31-42-40 pod client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 16:15:37.766
Feb 27 16:15:37.779: INFO: Waiting for pod client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832 to disappear
Feb 27 16:15:37.781: INFO: Pod client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 16:15:37.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9280" for this suite. 02/27/23 16:15:37.785
------------------------------
• [4.080 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:15:33.713
    Feb 27 16:15:33.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename containers 02/27/23 16:15:33.714
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:33.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:33.73
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 02/27/23 16:15:33.734
    Feb 27 16:15:33.743: INFO: Waiting up to 5m0s for pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832" in namespace "containers-9280" to be "Succeeded or Failed"
    Feb 27 16:15:33.748: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832": Phase="Pending", Reason="", readiness=false. Elapsed: 4.448716ms
    Feb 27 16:15:35.752: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008893492s
    Feb 27 16:15:37.752: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008566458s
    STEP: Saw pod success 02/27/23 16:15:37.752
    Feb 27 16:15:37.752: INFO: Pod "client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832" satisfied condition "Succeeded or Failed"
    Feb 27 16:15:37.755: INFO: Trying to get logs from node ip-172-31-42-40 pod client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 16:15:37.766
    Feb 27 16:15:37.779: INFO: Waiting for pod client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832 to disappear
    Feb 27 16:15:37.781: INFO: Pod client-containers-71eb8457-3f7a-43b4-a6fc-04f9b59fc832 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:15:37.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9280" for this suite. 02/27/23 16:15:37.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:15:37.794
Feb 27 16:15:37.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename daemonsets 02/27/23 16:15:37.794
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:37.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:37.816
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 16:15:37.836
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:15:37.842
Feb 27 16:15:37.848: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:37.848: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:37.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:15:37.852: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:15:38.855: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:38.855: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:38.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:15:38.860: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:15:39.857: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:39.857: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:39.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 16:15:39.860: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/27/23 16:15:39.863
Feb 27 16:15:39.878: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:39.878: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:39.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:15:39.882: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
Feb 27 16:15:40.886: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:40.886: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:40.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:15:40.889: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
Feb 27 16:15:41.887: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:41.887: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:41.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:15:41.890: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
Feb 27 16:15:42.886: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:42.886: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:42.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:15:42.889: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
Feb 27 16:15:43.887: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:43.887: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:15:43.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 16:15:43.890: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:15:43.893
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3334, will wait for the garbage collector to delete the pods 02/27/23 16:15:43.893
Feb 27 16:15:43.954: INFO: Deleting DaemonSet.extensions daemon-set took: 5.99043ms
Feb 27 16:15:44.054: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.685042ms
Feb 27 16:15:46.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:15:46.158: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 16:15:46.162: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31075"},"items":null}

Feb 27 16:15:46.165: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31075"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:15:46.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3334" for this suite. 02/27/23 16:15:46.181
------------------------------
• [SLOW TEST] [8.394 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:15:37.794
    Feb 27 16:15:37.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename daemonsets 02/27/23 16:15:37.794
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:37.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:37.816
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 16:15:37.836
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:15:37.842
    Feb 27 16:15:37.848: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:37.848: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:37.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:15:37.852: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:15:38.855: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:38.855: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:38.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:15:38.860: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:15:39.857: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:39.857: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:39.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 16:15:39.860: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/27/23 16:15:39.863
    Feb 27 16:15:39.878: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:39.878: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:39.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:15:39.882: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
    Feb 27 16:15:40.886: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:40.886: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:40.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:15:40.889: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
    Feb 27 16:15:41.887: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:41.887: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:41.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:15:41.890: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
    Feb 27 16:15:42.886: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:42.886: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:42.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:15:42.889: INFO: Node ip-172-31-84-171 is running 0 daemon pod, expected 1
    Feb 27 16:15:43.887: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:43.887: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:15:43.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 16:15:43.890: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:15:43.893
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3334, will wait for the garbage collector to delete the pods 02/27/23 16:15:43.893
    Feb 27 16:15:43.954: INFO: Deleting DaemonSet.extensions daemon-set took: 5.99043ms
    Feb 27 16:15:44.054: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.685042ms
    Feb 27 16:15:46.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:15:46.158: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 16:15:46.162: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31075"},"items":null}

    Feb 27 16:15:46.165: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31075"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:15:46.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3334" for this suite. 02/27/23 16:15:46.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:15:46.188
Feb 27 16:15:46.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename gc 02/27/23 16:15:46.189
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:46.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:46.21
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/27/23 16:15:46.213
STEP: delete the rc 02/27/23 16:15:51.224
STEP: wait for all pods to be garbage collected 02/27/23 16:15:51.231
STEP: Gathering metrics 02/27/23 16:15:56.239
W0227 16:15:56.243043      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 16:15:56.243: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 16:15:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-820" for this suite. 02/27/23 16:15:56.247
------------------------------
• [SLOW TEST] [10.066 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:15:46.188
    Feb 27 16:15:46.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename gc 02/27/23 16:15:46.189
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:46.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:46.21
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/27/23 16:15:46.213
    STEP: delete the rc 02/27/23 16:15:51.224
    STEP: wait for all pods to be garbage collected 02/27/23 16:15:51.231
    STEP: Gathering metrics 02/27/23 16:15:56.239
    W0227 16:15:56.243043      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 16:15:56.243: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:15:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-820" for this suite. 02/27/23 16:15:56.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:15:56.255
Feb 27 16:15:56.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 16:15:56.256
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:56.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:56.278
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 02/27/23 16:15:56.281
Feb 27 16:15:56.288: INFO: Waiting up to 2m0s for pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" in namespace "var-expansion-9939" to be "running"
Feb 27 16:15:56.294: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918406ms
Feb 27 16:15:58.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009461765s
Feb 27 16:16:00.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009827255s
Feb 27 16:16:02.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00971545s
Feb 27 16:16:04.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009967633s
Feb 27 16:16:06.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01081062s
Feb 27 16:16:08.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011005869s
Feb 27 16:16:10.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010197414s
Feb 27 16:16:12.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009689206s
Feb 27 16:16:14.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011634737s
Feb 27 16:16:16.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009560817s
Feb 27 16:16:18.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01180596s
Feb 27 16:16:20.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010612414s
Feb 27 16:16:22.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011103523s
Feb 27 16:16:24.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01250572s
Feb 27 16:16:26.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009661672s
Feb 27 16:16:28.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010634624s
Feb 27 16:16:30.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010083924s
Feb 27 16:16:32.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010301815s
Feb 27 16:16:34.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010607735s
Feb 27 16:16:36.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010153007s
Feb 27 16:16:38.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010309782s
Feb 27 16:16:40.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010763978s
Feb 27 16:16:42.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009542178s
Feb 27 16:16:44.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010436033s
Feb 27 16:16:46.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011254789s
Feb 27 16:16:48.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012066583s
Feb 27 16:16:50.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010451526s
Feb 27 16:16:52.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010435724s
Feb 27 16:16:54.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009844708s
Feb 27 16:16:56.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010417516s
Feb 27 16:16:58.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010700145s
Feb 27 16:17:00.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010512884s
Feb 27 16:17:02.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010759074s
Feb 27 16:17:04.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009618867s
Feb 27 16:17:06.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010464178s
Feb 27 16:17:08.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010988281s
Feb 27 16:17:10.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009745975s
Feb 27 16:17:12.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010721058s
Feb 27 16:17:14.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010236052s
Feb 27 16:17:16.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012073171s
Feb 27 16:17:18.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00949775s
Feb 27 16:17:20.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010857515s
Feb 27 16:17:22.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011561733s
Feb 27 16:17:24.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009678718s
Feb 27 16:17:26.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01079646s
Feb 27 16:17:28.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01164007s
Feb 27 16:17:30.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009970502s
Feb 27 16:17:32.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012482079s
Feb 27 16:17:34.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011786654s
Feb 27 16:17:36.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010406509s
Feb 27 16:17:38.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010606348s
Feb 27 16:17:40.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010872745s
Feb 27 16:17:42.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010377143s
Feb 27 16:17:44.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010245133s
Feb 27 16:17:46.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010730357s
Feb 27 16:17:48.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010931449s
Feb 27 16:17:50.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011465079s
Feb 27 16:17:52.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009090785s
Feb 27 16:17:54.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009552486s
Feb 27 16:17:56.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011715258s
Feb 27 16:17:56.303: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014712822s
STEP: updating the pod 02/27/23 16:17:56.303
Feb 27 16:17:56.816: INFO: Successfully updated pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e"
STEP: waiting for pod running 02/27/23 16:17:56.816
Feb 27 16:17:56.816: INFO: Waiting up to 2m0s for pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" in namespace "var-expansion-9939" to be "running"
Feb 27 16:17:56.819: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843523ms
Feb 27 16:17:58.824: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007503935s
Feb 27 16:17:58.824: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 16:17:58.824
Feb 27 16:17:58.824: INFO: Deleting pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" in namespace "var-expansion-9939"
Feb 27 16:17:58.833: INFO: Wait up to 5m0s for pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 16:18:30.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9939" for this suite. 02/27/23 16:18:30.845
------------------------------
• [SLOW TEST] [154.596 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:15:56.255
    Feb 27 16:15:56.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 16:15:56.256
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:15:56.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:15:56.278
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 02/27/23 16:15:56.281
    Feb 27 16:15:56.288: INFO: Waiting up to 2m0s for pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" in namespace "var-expansion-9939" to be "running"
    Feb 27 16:15:56.294: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918406ms
    Feb 27 16:15:58.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009461765s
    Feb 27 16:16:00.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009827255s
    Feb 27 16:16:02.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00971545s
    Feb 27 16:16:04.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009967633s
    Feb 27 16:16:06.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01081062s
    Feb 27 16:16:08.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011005869s
    Feb 27 16:16:10.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010197414s
    Feb 27 16:16:12.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009689206s
    Feb 27 16:16:14.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011634737s
    Feb 27 16:16:16.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009560817s
    Feb 27 16:16:18.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01180596s
    Feb 27 16:16:20.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010612414s
    Feb 27 16:16:22.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011103523s
    Feb 27 16:16:24.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01250572s
    Feb 27 16:16:26.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009661672s
    Feb 27 16:16:28.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010634624s
    Feb 27 16:16:30.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.010083924s
    Feb 27 16:16:32.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010301815s
    Feb 27 16:16:34.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010607735s
    Feb 27 16:16:36.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010153007s
    Feb 27 16:16:38.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010309782s
    Feb 27 16:16:40.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010763978s
    Feb 27 16:16:42.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009542178s
    Feb 27 16:16:44.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010436033s
    Feb 27 16:16:46.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011254789s
    Feb 27 16:16:48.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012066583s
    Feb 27 16:16:50.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010451526s
    Feb 27 16:16:52.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010435724s
    Feb 27 16:16:54.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009844708s
    Feb 27 16:16:56.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010417516s
    Feb 27 16:16:58.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010700145s
    Feb 27 16:17:00.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010512884s
    Feb 27 16:17:02.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010759074s
    Feb 27 16:17:04.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009618867s
    Feb 27 16:17:06.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010464178s
    Feb 27 16:17:08.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010988281s
    Feb 27 16:17:10.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009745975s
    Feb 27 16:17:12.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010721058s
    Feb 27 16:17:14.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010236052s
    Feb 27 16:17:16.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012073171s
    Feb 27 16:17:18.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00949775s
    Feb 27 16:17:20.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010857515s
    Feb 27 16:17:22.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011561733s
    Feb 27 16:17:24.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009678718s
    Feb 27 16:17:26.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01079646s
    Feb 27 16:17:28.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01164007s
    Feb 27 16:17:30.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009970502s
    Feb 27 16:17:32.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012482079s
    Feb 27 16:17:34.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011786654s
    Feb 27 16:17:36.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010406509s
    Feb 27 16:17:38.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010606348s
    Feb 27 16:17:40.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010872745s
    Feb 27 16:17:42.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010377143s
    Feb 27 16:17:44.298: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010245133s
    Feb 27 16:17:46.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010730357s
    Feb 27 16:17:48.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010931449s
    Feb 27 16:17:50.299: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011465079s
    Feb 27 16:17:52.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009090785s
    Feb 27 16:17:54.297: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009552486s
    Feb 27 16:17:56.300: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011715258s
    Feb 27 16:17:56.303: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014712822s
    STEP: updating the pod 02/27/23 16:17:56.303
    Feb 27 16:17:56.816: INFO: Successfully updated pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e"
    STEP: waiting for pod running 02/27/23 16:17:56.816
    Feb 27 16:17:56.816: INFO: Waiting up to 2m0s for pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" in namespace "var-expansion-9939" to be "running"
    Feb 27 16:17:56.819: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843523ms
    Feb 27 16:17:58.824: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e": Phase="Running", Reason="", readiness=true. Elapsed: 2.007503935s
    Feb 27 16:17:58.824: INFO: Pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 16:17:58.824
    Feb 27 16:17:58.824: INFO: Deleting pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" in namespace "var-expansion-9939"
    Feb 27 16:17:58.833: INFO: Wait up to 5m0s for pod "var-expansion-50bd1b17-7f0f-4b2a-8425-a379663bd78e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:18:30.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9939" for this suite. 02/27/23 16:18:30.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:18:30.852
Feb 27 16:18:30.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename namespaces 02/27/23 16:18:30.853
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:30.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:18:30.874
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 02/27/23 16:18:30.876
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:30.89
STEP: Creating a service in the namespace 02/27/23 16:18:30.893
STEP: Deleting the namespace 02/27/23 16:18:30.904
STEP: Waiting for the namespace to be removed. 02/27/23 16:18:30.914
STEP: Recreating the namespace 02/27/23 16:18:36.918
STEP: Verifying there is no service in the namespace 02/27/23 16:18:36.934
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:18:36.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5078" for this suite. 02/27/23 16:18:36.94
STEP: Destroying namespace "nsdeletetest-2488" for this suite. 02/27/23 16:18:36.949
Feb 27 16:18:36.951: INFO: Namespace nsdeletetest-2488 was already deleted
STEP: Destroying namespace "nsdeletetest-1147" for this suite. 02/27/23 16:18:36.951
------------------------------
• [SLOW TEST] [6.106 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:18:30.852
    Feb 27 16:18:30.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename namespaces 02/27/23 16:18:30.853
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:30.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:18:30.874
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 02/27/23 16:18:30.876
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:30.89
    STEP: Creating a service in the namespace 02/27/23 16:18:30.893
    STEP: Deleting the namespace 02/27/23 16:18:30.904
    STEP: Waiting for the namespace to be removed. 02/27/23 16:18:30.914
    STEP: Recreating the namespace 02/27/23 16:18:36.918
    STEP: Verifying there is no service in the namespace 02/27/23 16:18:36.934
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:18:36.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5078" for this suite. 02/27/23 16:18:36.94
    STEP: Destroying namespace "nsdeletetest-2488" for this suite. 02/27/23 16:18:36.949
    Feb 27 16:18:36.951: INFO: Namespace nsdeletetest-2488 was already deleted
    STEP: Destroying namespace "nsdeletetest-1147" for this suite. 02/27/23 16:18:36.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:18:36.959
Feb 27 16:18:36.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename webhook 02/27/23 16:18:36.959
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:36.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:18:36.978
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 16:18:36.992
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:18:37.29
STEP: Deploying the webhook pod 02/27/23 16:18:37.299
STEP: Wait for the deployment to be ready 02/27/23 16:18:37.312
Feb 27 16:18:37.318: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 16:18:39.33
STEP: Verifying the service has paired with the endpoint 02/27/23 16:18:39.343
Feb 27 16:18:40.343: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 16:18:40.348
STEP: create a pod 02/27/23 16:18:40.359
Feb 27 16:18:40.369: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9800" to be "running"
Feb 27 16:18:40.373: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63466ms
Feb 27 16:18:42.378: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008164751s
Feb 27 16:18:42.378: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/27/23 16:18:42.378
Feb 27 16:18:42.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=webhook-9800 attach --namespace=webhook-9800 to-be-attached-pod -i -c=container1'
Feb 27 16:18:42.449: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:18:42.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9800" for this suite. 02/27/23 16:18:42.508
STEP: Destroying namespace "webhook-9800-markers" for this suite. 02/27/23 16:18:42.518
------------------------------
• [SLOW TEST] [5.567 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:18:36.959
    Feb 27 16:18:36.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename webhook 02/27/23 16:18:36.959
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:36.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:18:36.978
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 16:18:36.992
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 16:18:37.29
    STEP: Deploying the webhook pod 02/27/23 16:18:37.299
    STEP: Wait for the deployment to be ready 02/27/23 16:18:37.312
    Feb 27 16:18:37.318: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 16:18:39.33
    STEP: Verifying the service has paired with the endpoint 02/27/23 16:18:39.343
    Feb 27 16:18:40.343: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 16:18:40.348
    STEP: create a pod 02/27/23 16:18:40.359
    Feb 27 16:18:40.369: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9800" to be "running"
    Feb 27 16:18:40.373: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63466ms
    Feb 27 16:18:42.378: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008164751s
    Feb 27 16:18:42.378: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/27/23 16:18:42.378
    Feb 27 16:18:42.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=webhook-9800 attach --namespace=webhook-9800 to-be-attached-pod -i -c=container1'
    Feb 27 16:18:42.449: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:18:42.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9800" for this suite. 02/27/23 16:18:42.508
    STEP: Destroying namespace "webhook-9800-markers" for this suite. 02/27/23 16:18:42.518
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:18:42.526
Feb 27 16:18:42.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename cronjob 02/27/23 16:18:42.527
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:42.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:18:42.546
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/27/23 16:18:42.549
STEP: Ensuring more than one job is running at a time 02/27/23 16:18:42.558
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/27/23 16:20:00.563
STEP: Removing cronjob 02/27/23 16:20:00.567
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:00.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4917" for this suite. 02/27/23 16:20:00.582
------------------------------
• [SLOW TEST] [78.063 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:18:42.526
    Feb 27 16:18:42.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename cronjob 02/27/23 16:18:42.527
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:18:42.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:18:42.546
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/27/23 16:18:42.549
    STEP: Ensuring more than one job is running at a time 02/27/23 16:18:42.558
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/27/23 16:20:00.563
    STEP: Removing cronjob 02/27/23 16:20:00.567
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:00.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4917" for this suite. 02/27/23 16:20:00.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:00.589
Feb 27 16:20:00.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename daemonsets 02/27/23 16:20:00.59
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:00.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:00.622
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 02/27/23 16:20:00.646
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:20:00.653
Feb 27 16:20:00.659: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:00.659: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:00.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:20:00.775: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:20:01.780: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:01.780: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:01.784: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:20:01.784: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:20:02.780: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:02.780: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:02.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:20:02.783: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:20:03.780: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:03.780: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:03.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 16:20:03.783: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/27/23 16:20:03.787
Feb 27 16:20:03.804: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:03.804: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:03.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:20:03.809: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:20:04.813: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:04.813: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:04.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 16:20:04.816: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:20:05.814: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:05.814: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:20:05.817: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 16:20:05.817: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/27/23 16:20:05.817
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:20:05.824
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5152, will wait for the garbage collector to delete the pods 02/27/23 16:20:05.824
Feb 27 16:20:05.885: INFO: Deleting DaemonSet.extensions daemon-set took: 7.727383ms
Feb 27 16:20:05.986: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022934ms
Feb 27 16:20:08.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:20:08.390: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 16:20:08.392: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32070"},"items":null}

Feb 27 16:20:08.396: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32070"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:08.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5152" for this suite. 02/27/23 16:20:08.412
------------------------------
• [SLOW TEST] [7.830 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:00.589
    Feb 27 16:20:00.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename daemonsets 02/27/23 16:20:00.59
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:00.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:00.622
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 02/27/23 16:20:00.646
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:20:00.653
    Feb 27 16:20:00.659: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:00.659: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:00.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:20:00.775: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:20:01.780: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:01.780: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:01.784: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:20:01.784: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:20:02.780: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:02.780: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:02.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:20:02.783: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:20:03.780: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:03.780: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:03.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 16:20:03.783: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/27/23 16:20:03.787
    Feb 27 16:20:03.804: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:03.804: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:03.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:20:03.809: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:20:04.813: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:04.813: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:04.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 16:20:04.816: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:20:05.814: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:05.814: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:20:05.817: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 16:20:05.817: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/27/23 16:20:05.817
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:20:05.824
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5152, will wait for the garbage collector to delete the pods 02/27/23 16:20:05.824
    Feb 27 16:20:05.885: INFO: Deleting DaemonSet.extensions daemon-set took: 7.727383ms
    Feb 27 16:20:05.986: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022934ms
    Feb 27 16:20:08.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:20:08.390: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 16:20:08.392: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32070"},"items":null}

    Feb 27 16:20:08.396: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32070"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:08.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5152" for this suite. 02/27/23 16:20:08.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:08.419
Feb 27 16:20:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename ingress 02/27/23 16:20:08.419
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:08.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:08.435
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/27/23 16:20:08.438
STEP: getting /apis/networking.k8s.io 02/27/23 16:20:08.441
STEP: getting /apis/networking.k8s.iov1 02/27/23 16:20:08.442
STEP: creating 02/27/23 16:20:08.443
STEP: getting 02/27/23 16:20:08.461
STEP: listing 02/27/23 16:20:08.469
STEP: watching 02/27/23 16:20:08.472
Feb 27 16:20:08.472: INFO: starting watch
STEP: cluster-wide listing 02/27/23 16:20:08.474
STEP: cluster-wide watching 02/27/23 16:20:08.477
Feb 27 16:20:08.478: INFO: starting watch
STEP: patching 02/27/23 16:20:08.479
STEP: updating 02/27/23 16:20:08.485
Feb 27 16:20:08.495: INFO: waiting for watch events with expected annotations
Feb 27 16:20:08.495: INFO: saw patched and updated annotations
STEP: patching /status 02/27/23 16:20:08.495
STEP: updating /status 02/27/23 16:20:08.503
STEP: get /status 02/27/23 16:20:08.518
STEP: deleting 02/27/23 16:20:08.524
STEP: deleting a collection 02/27/23 16:20:08.543
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:08.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5005" for this suite. 02/27/23 16:20:08.573
------------------------------
• [0.162 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:08.419
    Feb 27 16:20:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename ingress 02/27/23 16:20:08.419
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:08.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:08.435
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/27/23 16:20:08.438
    STEP: getting /apis/networking.k8s.io 02/27/23 16:20:08.441
    STEP: getting /apis/networking.k8s.iov1 02/27/23 16:20:08.442
    STEP: creating 02/27/23 16:20:08.443
    STEP: getting 02/27/23 16:20:08.461
    STEP: listing 02/27/23 16:20:08.469
    STEP: watching 02/27/23 16:20:08.472
    Feb 27 16:20:08.472: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 16:20:08.474
    STEP: cluster-wide watching 02/27/23 16:20:08.477
    Feb 27 16:20:08.478: INFO: starting watch
    STEP: patching 02/27/23 16:20:08.479
    STEP: updating 02/27/23 16:20:08.485
    Feb 27 16:20:08.495: INFO: waiting for watch events with expected annotations
    Feb 27 16:20:08.495: INFO: saw patched and updated annotations
    STEP: patching /status 02/27/23 16:20:08.495
    STEP: updating /status 02/27/23 16:20:08.503
    STEP: get /status 02/27/23 16:20:08.518
    STEP: deleting 02/27/23 16:20:08.524
    STEP: deleting a collection 02/27/23 16:20:08.543
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:08.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5005" for this suite. 02/27/23 16:20:08.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:08.583
Feb 27 16:20:08.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replication-controller 02/27/23 16:20:08.585
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:08.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:08.603
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Feb 27 16:20:08.606: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/27/23 16:20:08.691
STEP: Checking rc "condition-test" has the desired failure condition set 02/27/23 16:20:08.696
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/27/23 16:20:09.705
Feb 27 16:20:09.717: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/27/23 16:20:09.717
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:10.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9556" for this suite. 02/27/23 16:20:10.728
------------------------------
• [2.151 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:08.583
    Feb 27 16:20:08.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replication-controller 02/27/23 16:20:08.585
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:08.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:08.603
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Feb 27 16:20:08.606: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/27/23 16:20:08.691
    STEP: Checking rc "condition-test" has the desired failure condition set 02/27/23 16:20:08.696
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/27/23 16:20:09.705
    Feb 27 16:20:09.717: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/27/23 16:20:09.717
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:10.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9556" for this suite. 02/27/23 16:20:10.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:10.736
Feb 27 16:20:10.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename watch 02/27/23 16:20:10.736
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:10.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:10.754
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/27/23 16:20:10.757
STEP: creating a new configmap 02/27/23 16:20:10.759
STEP: modifying the configmap once 02/27/23 16:20:10.763
STEP: closing the watch once it receives two notifications 02/27/23 16:20:10.77
Feb 27 16:20:10.770: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32170 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 16:20:10.771: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32171 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/27/23 16:20:10.771
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/27/23 16:20:10.777
STEP: deleting the configmap 02/27/23 16:20:10.779
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/27/23 16:20:10.785
Feb 27 16:20:10.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32172 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 16:20:10.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32173 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:10.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1227" for this suite. 02/27/23 16:20:10.788
------------------------------
• [0.059 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:10.736
    Feb 27 16:20:10.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename watch 02/27/23 16:20:10.736
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:10.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:10.754
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/27/23 16:20:10.757
    STEP: creating a new configmap 02/27/23 16:20:10.759
    STEP: modifying the configmap once 02/27/23 16:20:10.763
    STEP: closing the watch once it receives two notifications 02/27/23 16:20:10.77
    Feb 27 16:20:10.770: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32170 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 16:20:10.771: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32171 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/27/23 16:20:10.771
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/27/23 16:20:10.777
    STEP: deleting the configmap 02/27/23 16:20:10.779
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/27/23 16:20:10.785
    Feb 27 16:20:10.785: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32172 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 16:20:10.785: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1227  d4348538-837c-441a-91fa-7f4de5565f79 32173 0 2023-02-27 16:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 16:20:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:10.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1227" for this suite. 02/27/23 16:20:10.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:10.795
Feb 27 16:20:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:20:10.796
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:10.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:10.817
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  02/27/23 16:20:10.82
Feb 27 16:20:10.830: INFO: Waiting up to 5m0s for pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff" in namespace "svcaccounts-1588" to be "Succeeded or Failed"
Feb 27 16:20:10.835: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.622104ms
Feb 27 16:20:12.840: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010065293s
Feb 27 16:20:14.840: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009992403s
STEP: Saw pod success 02/27/23 16:20:14.84
Feb 27 16:20:14.840: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff" satisfied condition "Succeeded or Failed"
Feb 27 16:20:14.843: INFO: Trying to get logs from node ip-172-31-42-40 pod test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff container agnhost-container: <nil>
STEP: delete the pod 02/27/23 16:20:14.859
Feb 27 16:20:14.875: INFO: Waiting for pod test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff to disappear
Feb 27 16:20:14.878: INFO: Pod test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:14.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1588" for this suite. 02/27/23 16:20:14.882
------------------------------
• [4.094 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:10.795
    Feb 27 16:20:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:20:10.796
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:10.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:10.817
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  02/27/23 16:20:10.82
    Feb 27 16:20:10.830: INFO: Waiting up to 5m0s for pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff" in namespace "svcaccounts-1588" to be "Succeeded or Failed"
    Feb 27 16:20:10.835: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.622104ms
    Feb 27 16:20:12.840: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010065293s
    Feb 27 16:20:14.840: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009992403s
    STEP: Saw pod success 02/27/23 16:20:14.84
    Feb 27 16:20:14.840: INFO: Pod "test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff" satisfied condition "Succeeded or Failed"
    Feb 27 16:20:14.843: INFO: Trying to get logs from node ip-172-31-42-40 pod test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 16:20:14.859
    Feb 27 16:20:14.875: INFO: Waiting for pod test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff to disappear
    Feb 27 16:20:14.878: INFO: Pod test-pod-76cc21c7-f20b-4c97-bb5f-fe72971b1eff no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:14.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1588" for this suite. 02/27/23 16:20:14.882
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:14.889
Feb 27 16:20:14.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:20:14.89
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:14.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:14.909
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-829b028c-11ea-41e2-9b8d-49d5cd6f83f5 02/27/23 16:20:14.912
STEP: Creating a pod to test consume secrets 02/27/23 16:20:14.917
Feb 27 16:20:14.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4" in namespace "projected-4219" to be "Succeeded or Failed"
Feb 27 16:20:14.930: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.951345ms
Feb 27 16:20:16.936: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011159051s
Feb 27 16:20:18.934: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009518537s
STEP: Saw pod success 02/27/23 16:20:18.934
Feb 27 16:20:18.934: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4" satisfied condition "Succeeded or Failed"
Feb 27 16:20:18.938: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 16:20:18.944
Feb 27 16:20:18.955: INFO: Waiting for pod pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4 to disappear
Feb 27 16:20:18.958: INFO: Pod pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:18.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4219" for this suite. 02/27/23 16:20:18.962
------------------------------
• [4.079 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:14.889
    Feb 27 16:20:14.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:20:14.89
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:14.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:14.909
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-829b028c-11ea-41e2-9b8d-49d5cd6f83f5 02/27/23 16:20:14.912
    STEP: Creating a pod to test consume secrets 02/27/23 16:20:14.917
    Feb 27 16:20:14.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4" in namespace "projected-4219" to be "Succeeded or Failed"
    Feb 27 16:20:14.930: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.951345ms
    Feb 27 16:20:16.936: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011159051s
    Feb 27 16:20:18.934: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009518537s
    STEP: Saw pod success 02/27/23 16:20:18.934
    Feb 27 16:20:18.934: INFO: Pod "pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4" satisfied condition "Succeeded or Failed"
    Feb 27 16:20:18.938: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 16:20:18.944
    Feb 27 16:20:18.955: INFO: Waiting for pod pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4 to disappear
    Feb 27 16:20:18.958: INFO: Pod pod-projected-secrets-c8722961-4ac8-41e6-a032-8466262ea4a4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:18.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4219" for this suite. 02/27/23 16:20:18.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:18.969
Feb 27 16:20:18.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 16:20:18.969
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:18.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:18.988
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-9731462a-9c94-4d74-bf2e-91faea659b68 in namespace container-probe-3921 02/27/23 16:20:18.991
Feb 27 16:20:19.000: INFO: Waiting up to 5m0s for pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68" in namespace "container-probe-3921" to be "not pending"
Feb 27 16:20:19.007: INFO: Pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.015035ms
Feb 27 16:20:21.012: INFO: Pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68": Phase="Running", Reason="", readiness=true. Elapsed: 2.011653569s
Feb 27 16:20:21.012: INFO: Pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68" satisfied condition "not pending"
Feb 27 16:20:21.012: INFO: Started pod liveness-9731462a-9c94-4d74-bf2e-91faea659b68 in namespace container-probe-3921
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 16:20:21.012
Feb 27 16:20:21.015: INFO: Initial restart count of pod liveness-9731462a-9c94-4d74-bf2e-91faea659b68 is 0
Feb 27 16:20:41.064: INFO: Restart count of pod container-probe-3921/liveness-9731462a-9c94-4d74-bf2e-91faea659b68 is now 1 (20.048573183s elapsed)
STEP: deleting the pod 02/27/23 16:20:41.064
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:41.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3921" for this suite. 02/27/23 16:20:41.082
------------------------------
• [SLOW TEST] [22.120 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:18.969
    Feb 27 16:20:18.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 16:20:18.969
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:18.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:18.988
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-9731462a-9c94-4d74-bf2e-91faea659b68 in namespace container-probe-3921 02/27/23 16:20:18.991
    Feb 27 16:20:19.000: INFO: Waiting up to 5m0s for pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68" in namespace "container-probe-3921" to be "not pending"
    Feb 27 16:20:19.007: INFO: Pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.015035ms
    Feb 27 16:20:21.012: INFO: Pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68": Phase="Running", Reason="", readiness=true. Elapsed: 2.011653569s
    Feb 27 16:20:21.012: INFO: Pod "liveness-9731462a-9c94-4d74-bf2e-91faea659b68" satisfied condition "not pending"
    Feb 27 16:20:21.012: INFO: Started pod liveness-9731462a-9c94-4d74-bf2e-91faea659b68 in namespace container-probe-3921
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 16:20:21.012
    Feb 27 16:20:21.015: INFO: Initial restart count of pod liveness-9731462a-9c94-4d74-bf2e-91faea659b68 is 0
    Feb 27 16:20:41.064: INFO: Restart count of pod container-probe-3921/liveness-9731462a-9c94-4d74-bf2e-91faea659b68 is now 1 (20.048573183s elapsed)
    STEP: deleting the pod 02/27/23 16:20:41.064
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:41.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3921" for this suite. 02/27/23 16:20:41.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:41.09
Feb 27 16:20:41.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 16:20:41.09
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:41.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:41.11
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Feb 27 16:20:41.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 16:20:42.698
Feb 27 16:20:42.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 create -f -'
Feb 27 16:20:43.367: INFO: stderr: ""
Feb 27 16:20:43.367: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 27 16:20:43.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 delete e2e-test-crd-publish-openapi-98-crds test-cr'
Feb 27 16:20:43.421: INFO: stderr: ""
Feb 27 16:20:43.421: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 27 16:20:43.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 apply -f -'
Feb 27 16:20:43.664: INFO: stderr: ""
Feb 27 16:20:43.664: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 27 16:20:43.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 delete e2e-test-crd-publish-openapi-98-crds test-cr'
Feb 27 16:20:43.715: INFO: stderr: ""
Feb 27 16:20:43.715: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/27/23 16:20:43.715
Feb 27 16:20:43.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 explain e2e-test-crd-publish-openapi-98-crds'
Feb 27 16:20:43.939: INFO: stderr: ""
Feb 27 16:20:43.939: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-98-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9469" for this suite. 02/27/23 16:20:45.474
------------------------------
• [4.392 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:41.09
    Feb 27 16:20:41.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 16:20:41.09
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:41.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:41.11
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Feb 27 16:20:41.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 16:20:42.698
    Feb 27 16:20:42.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 create -f -'
    Feb 27 16:20:43.367: INFO: stderr: ""
    Feb 27 16:20:43.367: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 27 16:20:43.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 delete e2e-test-crd-publish-openapi-98-crds test-cr'
    Feb 27 16:20:43.421: INFO: stderr: ""
    Feb 27 16:20:43.421: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb 27 16:20:43.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 apply -f -'
    Feb 27 16:20:43.664: INFO: stderr: ""
    Feb 27 16:20:43.664: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 27 16:20:43.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 --namespace=crd-publish-openapi-9469 delete e2e-test-crd-publish-openapi-98-crds test-cr'
    Feb 27 16:20:43.715: INFO: stderr: ""
    Feb 27 16:20:43.715: INFO: stdout: "e2e-test-crd-publish-openapi-98-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/27/23 16:20:43.715
    Feb 27 16:20:43.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=crd-publish-openapi-9469 explain e2e-test-crd-publish-openapi-98-crds'
    Feb 27 16:20:43.939: INFO: stderr: ""
    Feb 27 16:20:43.939: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-98-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9469" for this suite. 02/27/23 16:20:45.474
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:45.482
Feb 27 16:20:45.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename job 02/27/23 16:20:45.483
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:45.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:45.504
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 02/27/23 16:20:45.508
STEP: Ensuring active pods == parallelism 02/27/23 16:20:45.513
STEP: Orphaning one of the Job's Pods 02/27/23 16:20:47.517
Feb 27 16:20:48.032: INFO: Successfully updated pod "adopt-release-kw5ft"
STEP: Checking that the Job readopts the Pod 02/27/23 16:20:48.032
Feb 27 16:20:48.032: INFO: Waiting up to 15m0s for pod "adopt-release-kw5ft" in namespace "job-6522" to be "adopted"
Feb 27 16:20:48.036: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 4.075698ms
Feb 27 16:20:50.041: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 2.009039274s
Feb 27 16:20:50.041: INFO: Pod "adopt-release-kw5ft" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/27/23 16:20:50.041
Feb 27 16:20:50.553: INFO: Successfully updated pod "adopt-release-kw5ft"
STEP: Checking that the Job releases the Pod 02/27/23 16:20:50.553
Feb 27 16:20:50.554: INFO: Waiting up to 15m0s for pod "adopt-release-kw5ft" in namespace "job-6522" to be "released"
Feb 27 16:20:50.561: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 7.002049ms
Feb 27 16:20:52.565: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 2.011897573s
Feb 27 16:20:52.566: INFO: Pod "adopt-release-kw5ft" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:52.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6522" for this suite. 02/27/23 16:20:52.569
------------------------------
• [SLOW TEST] [7.094 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:45.482
    Feb 27 16:20:45.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename job 02/27/23 16:20:45.483
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:45.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:45.504
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 02/27/23 16:20:45.508
    STEP: Ensuring active pods == parallelism 02/27/23 16:20:45.513
    STEP: Orphaning one of the Job's Pods 02/27/23 16:20:47.517
    Feb 27 16:20:48.032: INFO: Successfully updated pod "adopt-release-kw5ft"
    STEP: Checking that the Job readopts the Pod 02/27/23 16:20:48.032
    Feb 27 16:20:48.032: INFO: Waiting up to 15m0s for pod "adopt-release-kw5ft" in namespace "job-6522" to be "adopted"
    Feb 27 16:20:48.036: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 4.075698ms
    Feb 27 16:20:50.041: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 2.009039274s
    Feb 27 16:20:50.041: INFO: Pod "adopt-release-kw5ft" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/27/23 16:20:50.041
    Feb 27 16:20:50.553: INFO: Successfully updated pod "adopt-release-kw5ft"
    STEP: Checking that the Job releases the Pod 02/27/23 16:20:50.553
    Feb 27 16:20:50.554: INFO: Waiting up to 15m0s for pod "adopt-release-kw5ft" in namespace "job-6522" to be "released"
    Feb 27 16:20:50.561: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 7.002049ms
    Feb 27 16:20:52.565: INFO: Pod "adopt-release-kw5ft": Phase="Running", Reason="", readiness=true. Elapsed: 2.011897573s
    Feb 27 16:20:52.566: INFO: Pod "adopt-release-kw5ft" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:52.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6522" for this suite. 02/27/23 16:20:52.569
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:52.576
Feb 27 16:20:52.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 16:20:52.577
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.598
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:52.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-947" for this suite. 02/27/23 16:20:52.645
------------------------------
• [0.076 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:52.576
    Feb 27 16:20:52.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 16:20:52.577
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.598
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:52.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-947" for this suite. 02/27/23 16:20:52.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:52.653
Feb 27 16:20:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:20:52.654
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.672
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 02/27/23 16:20:52.675
Feb 27 16:20:52.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1510 api-versions'
Feb 27 16:20:52.731: INFO: stderr: ""
Feb 27 16:20:52.731: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:52.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1510" for this suite. 02/27/23 16:20:52.736
------------------------------
• [0.090 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:52.653
    Feb 27 16:20:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:20:52.654
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.672
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 02/27/23 16:20:52.675
    Feb 27 16:20:52.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-1510 api-versions'
    Feb 27 16:20:52.731: INFO: stderr: ""
    Feb 27 16:20:52.731: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:52.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1510" for this suite. 02/27/23 16:20:52.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:52.744
Feb 27 16:20:52.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:20:52.744
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.763
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-wlvn5"  02/27/23 16:20:52.766
Feb 27 16:20:52.773: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-wlvn5"  02/27/23 16:20:52.773
Feb 27 16:20:52.784: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:52.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4233" for this suite. 02/27/23 16:20:52.788
------------------------------
• [0.051 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:52.744
    Feb 27 16:20:52.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:20:52.744
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.763
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-wlvn5"  02/27/23 16:20:52.766
    Feb 27 16:20:52.773: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-wlvn5"  02/27/23 16:20:52.773
    Feb 27 16:20:52.784: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:52.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4233" for this suite. 02/27/23 16:20:52.788
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:52.796
Feb 27 16:20:52.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:20:52.796
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.814
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1050/configmap-test-444ddf53-abd6-42e3-be75-358154fac5ee 02/27/23 16:20:52.818
STEP: Creating a pod to test consume configMaps 02/27/23 16:20:52.824
Feb 27 16:20:52.832: INFO: Waiting up to 5m0s for pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d" in namespace "configmap-1050" to be "Succeeded or Failed"
Feb 27 16:20:52.838: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.487719ms
Feb 27 16:20:54.843: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010150291s
Feb 27 16:20:56.842: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009501125s
STEP: Saw pod success 02/27/23 16:20:56.842
Feb 27 16:20:56.842: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d" satisfied condition "Succeeded or Failed"
Feb 27 16:20:56.845: INFO: Trying to get logs from node ip-172-31-3-182 pod pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d container env-test: <nil>
STEP: delete the pod 02/27/23 16:20:56.862
Feb 27 16:20:56.877: INFO: Waiting for pod pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d to disappear
Feb 27 16:20:56.880: INFO: Pod pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:20:56.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1050" for this suite. 02/27/23 16:20:56.883
------------------------------
• [4.095 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:52.796
    Feb 27 16:20:52.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:20:52.796
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:52.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:52.814
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1050/configmap-test-444ddf53-abd6-42e3-be75-358154fac5ee 02/27/23 16:20:52.818
    STEP: Creating a pod to test consume configMaps 02/27/23 16:20:52.824
    Feb 27 16:20:52.832: INFO: Waiting up to 5m0s for pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d" in namespace "configmap-1050" to be "Succeeded or Failed"
    Feb 27 16:20:52.838: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.487719ms
    Feb 27 16:20:54.843: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010150291s
    Feb 27 16:20:56.842: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009501125s
    STEP: Saw pod success 02/27/23 16:20:56.842
    Feb 27 16:20:56.842: INFO: Pod "pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d" satisfied condition "Succeeded or Failed"
    Feb 27 16:20:56.845: INFO: Trying to get logs from node ip-172-31-3-182 pod pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d container env-test: <nil>
    STEP: delete the pod 02/27/23 16:20:56.862
    Feb 27 16:20:56.877: INFO: Waiting for pod pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d to disappear
    Feb 27 16:20:56.880: INFO: Pod pod-configmaps-176c4943-a3fa-4f4e-aa87-c682d7d1587d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:20:56.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1050" for this suite. 02/27/23 16:20:56.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:20:56.891
Feb 27 16:20:56.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename cronjob 02/27/23 16:20:56.892
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:56.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:56.912
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/27/23 16:20:56.914
STEP: Ensuring a job is scheduled 02/27/23 16:20:56.92
STEP: Ensuring exactly one is scheduled 02/27/23 16:21:00.924
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 16:21:00.928
STEP: Ensuring the job is replaced with a new one 02/27/23 16:21:00.931
STEP: Removing cronjob 02/27/23 16:22:00.936
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 16:22:00.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5086" for this suite. 02/27/23 16:22:00.946
------------------------------
• [SLOW TEST] [64.063 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:20:56.891
    Feb 27 16:20:56.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename cronjob 02/27/23 16:20:56.892
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:20:56.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:20:56.912
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/27/23 16:20:56.914
    STEP: Ensuring a job is scheduled 02/27/23 16:20:56.92
    STEP: Ensuring exactly one is scheduled 02/27/23 16:21:00.924
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 16:21:00.928
    STEP: Ensuring the job is replaced with a new one 02/27/23 16:21:00.931
    STEP: Removing cronjob 02/27/23 16:22:00.936
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:22:00.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5086" for this suite. 02/27/23 16:22:00.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:22:00.955
Feb 27 16:22:00.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:22:00.956
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:00.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:00.979
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 02/27/23 16:22:00.982
STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:22:00.987
STEP: Creating a ResourceQuota with not best effort scope 02/27/23 16:22:02.991
STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:22:02.996
STEP: Creating a best-effort pod 02/27/23 16:22:05
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/27/23 16:22:05.019
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/27/23 16:22:07.023
STEP: Deleting the pod 02/27/23 16:22:09.029
STEP: Ensuring resource quota status released the pod usage 02/27/23 16:22:09.042
STEP: Creating a not best-effort pod 02/27/23 16:22:11.047
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/27/23 16:22:11.057
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/27/23 16:22:13.062
STEP: Deleting the pod 02/27/23 16:22:15.067
STEP: Ensuring resource quota status released the pod usage 02/27/23 16:22:15.084
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:22:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7541" for this suite. 02/27/23 16:22:17.092
------------------------------
• [SLOW TEST] [16.143 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:22:00.955
    Feb 27 16:22:00.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:22:00.956
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:00.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:00.979
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 02/27/23 16:22:00.982
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:22:00.987
    STEP: Creating a ResourceQuota with not best effort scope 02/27/23 16:22:02.991
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 16:22:02.996
    STEP: Creating a best-effort pod 02/27/23 16:22:05
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/27/23 16:22:05.019
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/27/23 16:22:07.023
    STEP: Deleting the pod 02/27/23 16:22:09.029
    STEP: Ensuring resource quota status released the pod usage 02/27/23 16:22:09.042
    STEP: Creating a not best-effort pod 02/27/23 16:22:11.047
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/27/23 16:22:11.057
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/27/23 16:22:13.062
    STEP: Deleting the pod 02/27/23 16:22:15.067
    STEP: Ensuring resource quota status released the pod usage 02/27/23 16:22:15.084
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:22:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7541" for this suite. 02/27/23 16:22:17.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:22:17.098
Feb 27 16:22:17.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename emptydir 02/27/23 16:22:17.099
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:17.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:17.122
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 16:22:17.125
Feb 27 16:22:17.235: INFO: Waiting up to 5m0s for pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2" in namespace "emptydir-8427" to be "Succeeded or Failed"
Feb 27 16:22:17.240: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.151072ms
Feb 27 16:22:19.245: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2": Phase="Running", Reason="", readiness=false. Elapsed: 2.009814391s
Feb 27 16:22:21.245: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010341922s
STEP: Saw pod success 02/27/23 16:22:21.245
Feb 27 16:22:21.245: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2" satisfied condition "Succeeded or Failed"
Feb 27 16:22:21.250: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2 container test-container: <nil>
STEP: delete the pod 02/27/23 16:22:21.261
Feb 27 16:22:21.273: INFO: Waiting for pod pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2 to disappear
Feb 27 16:22:21.278: INFO: Pod pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 16:22:21.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8427" for this suite. 02/27/23 16:22:21.285
------------------------------
• [4.195 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:22:17.098
    Feb 27 16:22:17.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename emptydir 02/27/23 16:22:17.099
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:17.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:17.122
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 16:22:17.125
    Feb 27 16:22:17.235: INFO: Waiting up to 5m0s for pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2" in namespace "emptydir-8427" to be "Succeeded or Failed"
    Feb 27 16:22:17.240: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.151072ms
    Feb 27 16:22:19.245: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2": Phase="Running", Reason="", readiness=false. Elapsed: 2.009814391s
    Feb 27 16:22:21.245: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010341922s
    STEP: Saw pod success 02/27/23 16:22:21.245
    Feb 27 16:22:21.245: INFO: Pod "pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2" satisfied condition "Succeeded or Failed"
    Feb 27 16:22:21.250: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2 container test-container: <nil>
    STEP: delete the pod 02/27/23 16:22:21.261
    Feb 27 16:22:21.273: INFO: Waiting for pod pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2 to disappear
    Feb 27 16:22:21.278: INFO: Pod pod-7e4114a4-05b9-4fe1-8c74-eb0ec665adf2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:22:21.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8427" for this suite. 02/27/23 16:22:21.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:22:21.295
Feb 27 16:22:21.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename proxy 02/27/23 16:22:21.296
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:21.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:21.319
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb 27 16:22:21.322: INFO: Creating pod...
Feb 27 16:22:21.332: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-85" to be "running"
Feb 27 16:22:21.336: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.364059ms
Feb 27 16:22:23.340: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007888279s
Feb 27 16:22:23.340: INFO: Pod "agnhost" satisfied condition "running"
Feb 27 16:22:23.340: INFO: Creating service...
Feb 27 16:22:23.351: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=DELETE
Feb 27 16:22:23.356: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 16:22:23.356: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=OPTIONS
Feb 27 16:22:23.361: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 16:22:23.361: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=PATCH
Feb 27 16:22:23.366: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 16:22:23.366: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=POST
Feb 27 16:22:23.370: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 16:22:23.370: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=PUT
Feb 27 16:22:23.375: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 16:22:23.375: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=DELETE
Feb 27 16:22:23.389: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 16:22:23.389: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb 27 16:22:23.395: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 16:22:23.395: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=PATCH
Feb 27 16:22:23.402: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 16:22:23.402: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=POST
Feb 27 16:22:23.408: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 16:22:23.408: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=PUT
Feb 27 16:22:23.413: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 16:22:23.413: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=GET
Feb 27 16:22:23.417: INFO: http.Client request:GET StatusCode:301
Feb 27 16:22:23.417: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=GET
Feb 27 16:22:23.422: INFO: http.Client request:GET StatusCode:301
Feb 27 16:22:23.422: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=HEAD
Feb 27 16:22:23.425: INFO: http.Client request:HEAD StatusCode:301
Feb 27 16:22:23.425: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=HEAD
Feb 27 16:22:23.430: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 27 16:22:23.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-85" for this suite. 02/27/23 16:22:23.434
------------------------------
• [2.146 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:22:21.295
    Feb 27 16:22:21.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename proxy 02/27/23 16:22:21.296
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:21.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:21.319
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb 27 16:22:21.322: INFO: Creating pod...
    Feb 27 16:22:21.332: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-85" to be "running"
    Feb 27 16:22:21.336: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.364059ms
    Feb 27 16:22:23.340: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007888279s
    Feb 27 16:22:23.340: INFO: Pod "agnhost" satisfied condition "running"
    Feb 27 16:22:23.340: INFO: Creating service...
    Feb 27 16:22:23.351: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=DELETE
    Feb 27 16:22:23.356: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 16:22:23.356: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=OPTIONS
    Feb 27 16:22:23.361: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 16:22:23.361: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=PATCH
    Feb 27 16:22:23.366: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 16:22:23.366: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=POST
    Feb 27 16:22:23.370: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 16:22:23.370: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=PUT
    Feb 27 16:22:23.375: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 16:22:23.375: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb 27 16:22:23.389: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 16:22:23.389: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb 27 16:22:23.395: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 16:22:23.395: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb 27 16:22:23.402: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 16:22:23.402: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=POST
    Feb 27 16:22:23.408: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 16:22:23.408: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=PUT
    Feb 27 16:22:23.413: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 16:22:23.413: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=GET
    Feb 27 16:22:23.417: INFO: http.Client request:GET StatusCode:301
    Feb 27 16:22:23.417: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=GET
    Feb 27 16:22:23.422: INFO: http.Client request:GET StatusCode:301
    Feb 27 16:22:23.422: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/pods/agnhost/proxy?method=HEAD
    Feb 27 16:22:23.425: INFO: http.Client request:HEAD StatusCode:301
    Feb 27 16:22:23.425: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-85/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb 27 16:22:23.430: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:22:23.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-85" for this suite. 02/27/23 16:22:23.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:22:23.442
Feb 27 16:22:23.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 16:22:23.442
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:23.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:23.461
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2 in namespace container-probe-1862 02/27/23 16:22:23.465
Feb 27 16:22:23.473: INFO: Waiting up to 5m0s for pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2" in namespace "container-probe-1862" to be "not pending"
Feb 27 16:22:23.477: INFO: Pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313681ms
Feb 27 16:22:25.481: INFO: Pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007867798s
Feb 27 16:22:25.481: INFO: Pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2" satisfied condition "not pending"
Feb 27 16:22:25.481: INFO: Started pod busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2 in namespace container-probe-1862
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 16:22:25.481
Feb 27 16:22:25.485: INFO: Initial restart count of pod busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2 is 0
STEP: deleting the pod 02/27/23 16:26:26.029
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:26.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1862" for this suite. 02/27/23 16:26:26.048
------------------------------
• [SLOW TEST] [242.612 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:22:23.442
    Feb 27 16:22:23.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 16:22:23.442
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:22:23.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:22:23.461
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2 in namespace container-probe-1862 02/27/23 16:22:23.465
    Feb 27 16:22:23.473: INFO: Waiting up to 5m0s for pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2" in namespace "container-probe-1862" to be "not pending"
    Feb 27 16:22:23.477: INFO: Pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313681ms
    Feb 27 16:22:25.481: INFO: Pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007867798s
    Feb 27 16:22:25.481: INFO: Pod "busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2" satisfied condition "not pending"
    Feb 27 16:22:25.481: INFO: Started pod busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2 in namespace container-probe-1862
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 16:22:25.481
    Feb 27 16:22:25.485: INFO: Initial restart count of pod busybox-9499a774-a77e-4511-85ba-dc0250a1b4b2 is 0
    STEP: deleting the pod 02/27/23 16:26:26.029
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:26.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1862" for this suite. 02/27/23 16:26:26.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:26.055
Feb 27 16:26:26.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename namespaces 02/27/23 16:26:26.056
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:26.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:26.078
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 02/27/23 16:26:26.081
Feb 27 16:26:26.085: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/27/23 16:26:26.085
Feb 27 16:26:26.091: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/27/23 16:26:26.091
Feb 27 16:26:26.102: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:26.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1012" for this suite. 02/27/23 16:26:26.106
------------------------------
• [0.058 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:26.055
    Feb 27 16:26:26.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename namespaces 02/27/23 16:26:26.056
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:26.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:26.078
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 02/27/23 16:26:26.081
    Feb 27 16:26:26.085: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/27/23 16:26:26.085
    Feb 27 16:26:26.091: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/27/23 16:26:26.091
    Feb 27 16:26:26.102: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:26.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1012" for this suite. 02/27/23 16:26:26.106
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:26.113
Feb 27 16:26:26.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:26:26.114
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:26.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:26.136
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:26:26.14
Feb 27 16:26:26.149: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934" in namespace "projected-4675" to be "Succeeded or Failed"
Feb 27 16:26:26.153: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155927ms
Feb 27 16:26:28.157: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008122s
Feb 27 16:26:30.159: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009752895s
STEP: Saw pod success 02/27/23 16:26:30.159
Feb 27 16:26:30.159: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934" satisfied condition "Succeeded or Failed"
Feb 27 16:26:30.162: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934 container client-container: <nil>
STEP: delete the pod 02/27/23 16:26:30.175
Feb 27 16:26:30.190: INFO: Waiting for pod downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934 to disappear
Feb 27 16:26:30.194: INFO: Pod downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:30.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4675" for this suite. 02/27/23 16:26:30.198
------------------------------
• [4.092 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:26.113
    Feb 27 16:26:26.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:26:26.114
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:26.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:26.136
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:26:26.14
    Feb 27 16:26:26.149: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934" in namespace "projected-4675" to be "Succeeded or Failed"
    Feb 27 16:26:26.153: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155927ms
    Feb 27 16:26:28.157: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008122s
    Feb 27 16:26:30.159: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009752895s
    STEP: Saw pod success 02/27/23 16:26:30.159
    Feb 27 16:26:30.159: INFO: Pod "downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934" satisfied condition "Succeeded or Failed"
    Feb 27 16:26:30.162: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934 container client-container: <nil>
    STEP: delete the pod 02/27/23 16:26:30.175
    Feb 27 16:26:30.190: INFO: Waiting for pod downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934 to disappear
    Feb 27 16:26:30.194: INFO: Pod downwardapi-volume-56897e72-5463-472e-adef-d65acbfa0934 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:30.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4675" for this suite. 02/27/23 16:26:30.198
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:30.206
Feb 27 16:26:30.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename init-container 02/27/23 16:26:30.206
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:30.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:30.223
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 02/27/23 16:26:30.227
Feb 27 16:26:30.227: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:34.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6235" for this suite. 02/27/23 16:26:34.345
------------------------------
• [4.146 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:30.206
    Feb 27 16:26:30.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename init-container 02/27/23 16:26:30.206
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:30.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:30.223
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 02/27/23 16:26:30.227
    Feb 27 16:26:30.227: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:34.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6235" for this suite. 02/27/23 16:26:34.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:34.352
Feb 27 16:26:34.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename daemonsets 02/27/23 16:26:34.353
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:34.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:34.371
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 16:26:34.395
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:26:34.4
Feb 27 16:26:34.405: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:26:34.405: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:26:34.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:26:34.409: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:26:35.414: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:26:35.414: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:26:35.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 16:26:35.418: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:26:36.413: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:26:36.413: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:26:36.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 16:26:36.417: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 02/27/23 16:26:36.42
Feb 27 16:26:36.424: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/27/23 16:26:36.424
Feb 27 16:26:36.434: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/27/23 16:26:36.434
Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: ADDED
Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.436: INFO: Found daemon set daemon-set in namespace daemonsets-4422 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 16:26:36.436: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/27/23 16:26:36.436
STEP: watching for the daemon set status to be patched 02/27/23 16:26:36.445
Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: ADDED
Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.447: INFO: Observed daemon set daemon-set in namespace daemonsets-4422 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 16:26:36.447: INFO: Found daemon set daemon-set in namespace daemonsets-4422 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 27 16:26:36.447: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:26:36.45
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4422, will wait for the garbage collector to delete the pods 02/27/23 16:26:36.45
Feb 27 16:26:36.511: INFO: Deleting DaemonSet.extensions daemon-set took: 6.955621ms
Feb 27 16:26:36.612: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.849351ms
Feb 27 16:26:39.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:26:39.117: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 16:26:39.119: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33765"},"items":null}

Feb 27 16:26:39.125: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33765"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:39.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4422" for this suite. 02/27/23 16:26:39.142
------------------------------
• [4.796 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:34.352
    Feb 27 16:26:34.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename daemonsets 02/27/23 16:26:34.353
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:34.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:34.371
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 16:26:34.395
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 16:26:34.4
    Feb 27 16:26:34.405: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:26:34.405: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:26:34.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:26:34.409: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:26:35.414: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:26:35.414: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:26:35.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 16:26:35.418: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:26:36.413: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:26:36.413: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:26:36.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 16:26:36.417: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 02/27/23 16:26:36.42
    Feb 27 16:26:36.424: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/27/23 16:26:36.424
    Feb 27 16:26:36.434: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/27/23 16:26:36.434
    Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: ADDED
    Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.436: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.436: INFO: Found daemon set daemon-set in namespace daemonsets-4422 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 16:26:36.436: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/27/23 16:26:36.436
    STEP: watching for the daemon set status to be patched 02/27/23 16:26:36.445
    Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: ADDED
    Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.447: INFO: Observed daemon set daemon-set in namespace daemonsets-4422 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 16:26:36.447: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 16:26:36.447: INFO: Found daemon set daemon-set in namespace daemonsets-4422 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb 27 16:26:36.447: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:26:36.45
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4422, will wait for the garbage collector to delete the pods 02/27/23 16:26:36.45
    Feb 27 16:26:36.511: INFO: Deleting DaemonSet.extensions daemon-set took: 6.955621ms
    Feb 27 16:26:36.612: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.849351ms
    Feb 27 16:26:39.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:26:39.117: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 16:26:39.119: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33765"},"items":null}

    Feb 27 16:26:39.125: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33765"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:39.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4422" for this suite. 02/27/23 16:26:39.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:39.149
Feb 27 16:26:39.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:26:39.149
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:39.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:39.166
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-27f6331b-5c37-453e-8363-9875cb429943 02/27/23 16:26:39.169
STEP: Creating a pod to test consume configMaps 02/27/23 16:26:39.174
Feb 27 16:26:39.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b" in namespace "configmap-7436" to be "Succeeded or Failed"
Feb 27 16:26:39.187: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.775656ms
Feb 27 16:26:41.191: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01101776s
Feb 27 16:26:43.192: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011574435s
STEP: Saw pod success 02/27/23 16:26:43.192
Feb 27 16:26:43.192: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b" satisfied condition "Succeeded or Failed"
Feb 27 16:26:43.196: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b container agnhost-container: <nil>
STEP: delete the pod 02/27/23 16:26:43.202
Feb 27 16:26:43.217: INFO: Waiting for pod pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b to disappear
Feb 27 16:26:43.221: INFO: Pod pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:43.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7436" for this suite. 02/27/23 16:26:43.225
------------------------------
• [4.083 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:39.149
    Feb 27 16:26:39.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:26:39.149
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:39.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:39.166
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-27f6331b-5c37-453e-8363-9875cb429943 02/27/23 16:26:39.169
    STEP: Creating a pod to test consume configMaps 02/27/23 16:26:39.174
    Feb 27 16:26:39.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b" in namespace "configmap-7436" to be "Succeeded or Failed"
    Feb 27 16:26:39.187: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.775656ms
    Feb 27 16:26:41.191: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01101776s
    Feb 27 16:26:43.192: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011574435s
    STEP: Saw pod success 02/27/23 16:26:43.192
    Feb 27 16:26:43.192: INFO: Pod "pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b" satisfied condition "Succeeded or Failed"
    Feb 27 16:26:43.196: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 16:26:43.202
    Feb 27 16:26:43.217: INFO: Waiting for pod pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b to disappear
    Feb 27 16:26:43.221: INFO: Pod pod-configmaps-e33e4292-da72-4039-99a5-dff23920b19b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:43.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7436" for this suite. 02/27/23 16:26:43.225
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:43.232
Feb 27 16:26:43.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 16:26:43.233
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:43.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:43.248
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb 27 16:26:43.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:46.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1741" for this suite. 02/27/23 16:26:46.402
------------------------------
• [3.177 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:43.232
    Feb 27 16:26:43.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 16:26:43.233
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:43.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:43.248
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb 27 16:26:43.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:46.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1741" for this suite. 02/27/23 16:26:46.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:46.41
Feb 27 16:26:46.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename job 02/27/23 16:26:46.41
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:46.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:46.433
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 02/27/23 16:26:46.436
STEP: Ensure pods equal to parallelism count is attached to the job 02/27/23 16:26:46.446
STEP: patching /status 02/27/23 16:26:48.451
STEP: updating /status 02/27/23 16:26:48.459
STEP: get /status 02/27/23 16:26:48.467
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:48.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8509" for this suite. 02/27/23 16:26:48.474
------------------------------
• [2.073 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:46.41
    Feb 27 16:26:46.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename job 02/27/23 16:26:46.41
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:46.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:46.433
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 02/27/23 16:26:46.436
    STEP: Ensure pods equal to parallelism count is attached to the job 02/27/23 16:26:46.446
    STEP: patching /status 02/27/23 16:26:48.451
    STEP: updating /status 02/27/23 16:26:48.459
    STEP: get /status 02/27/23 16:26:48.467
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:48.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8509" for this suite. 02/27/23 16:26:48.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:48.483
Feb 27 16:26:48.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:26:48.484
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:48.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:48.499
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 02/27/23 16:26:48.502
Feb 27 16:26:48.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9529 cluster-info'
Feb 27 16:26:48.551: INFO: stderr: ""
Feb 27 16:26:48.551: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:26:48.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9529" for this suite. 02/27/23 16:26:48.554
------------------------------
• [0.078 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:48.483
    Feb 27 16:26:48.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:26:48.484
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:48.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:48.499
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 02/27/23 16:26:48.502
    Feb 27 16:26:48.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-9529 cluster-info'
    Feb 27 16:26:48.551: INFO: stderr: ""
    Feb 27 16:26:48.551: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:26:48.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9529" for this suite. 02/27/23 16:26:48.554
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:26:48.561
Feb 27 16:26:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename gc 02/27/23 16:26:48.562
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:48.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:48.58
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/27/23 16:26:48.588
STEP: delete the rc 02/27/23 16:26:53.597
STEP: wait for the rc to be deleted 02/27/23 16:26:53.656
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/27/23 16:26:58.664
STEP: Gathering metrics 02/27/23 16:27:28.674
W0227 16:27:28.679371      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 16:27:28.679: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 27 16:27:28.679: INFO: Deleting pod "simpletest.rc-24l28" in namespace "gc-6355"
Feb 27 16:27:28.694: INFO: Deleting pod "simpletest.rc-2cn9r" in namespace "gc-6355"
Feb 27 16:27:28.713: INFO: Deleting pod "simpletest.rc-2kdxn" in namespace "gc-6355"
Feb 27 16:27:28.726: INFO: Deleting pod "simpletest.rc-2qwgf" in namespace "gc-6355"
Feb 27 16:27:28.740: INFO: Deleting pod "simpletest.rc-2vj8z" in namespace "gc-6355"
Feb 27 16:27:28.750: INFO: Deleting pod "simpletest.rc-2x7nm" in namespace "gc-6355"
Feb 27 16:27:28.763: INFO: Deleting pod "simpletest.rc-44twr" in namespace "gc-6355"
Feb 27 16:27:28.773: INFO: Deleting pod "simpletest.rc-4gqhh" in namespace "gc-6355"
Feb 27 16:27:28.788: INFO: Deleting pod "simpletest.rc-4qdv2" in namespace "gc-6355"
Feb 27 16:27:28.803: INFO: Deleting pod "simpletest.rc-4rlb4" in namespace "gc-6355"
Feb 27 16:27:28.816: INFO: Deleting pod "simpletest.rc-4v78w" in namespace "gc-6355"
Feb 27 16:27:28.830: INFO: Deleting pod "simpletest.rc-576bt" in namespace "gc-6355"
Feb 27 16:27:28.842: INFO: Deleting pod "simpletest.rc-57sln" in namespace "gc-6355"
Feb 27 16:27:28.854: INFO: Deleting pod "simpletest.rc-5bvwg" in namespace "gc-6355"
Feb 27 16:27:28.869: INFO: Deleting pod "simpletest.rc-5dwcq" in namespace "gc-6355"
Feb 27 16:27:28.885: INFO: Deleting pod "simpletest.rc-5hhqd" in namespace "gc-6355"
Feb 27 16:27:28.898: INFO: Deleting pod "simpletest.rc-5kl9b" in namespace "gc-6355"
Feb 27 16:27:28.913: INFO: Deleting pod "simpletest.rc-5klvw" in namespace "gc-6355"
Feb 27 16:27:28.925: INFO: Deleting pod "simpletest.rc-5m5km" in namespace "gc-6355"
Feb 27 16:27:28.939: INFO: Deleting pod "simpletest.rc-5zl5l" in namespace "gc-6355"
Feb 27 16:27:28.952: INFO: Deleting pod "simpletest.rc-6gsnp" in namespace "gc-6355"
Feb 27 16:27:28.966: INFO: Deleting pod "simpletest.rc-7qfbl" in namespace "gc-6355"
Feb 27 16:27:28.979: INFO: Deleting pod "simpletest.rc-7r2d8" in namespace "gc-6355"
Feb 27 16:27:28.992: INFO: Deleting pod "simpletest.rc-82p6f" in namespace "gc-6355"
Feb 27 16:27:29.011: INFO: Deleting pod "simpletest.rc-88697" in namespace "gc-6355"
Feb 27 16:27:29.023: INFO: Deleting pod "simpletest.rc-8cfc2" in namespace "gc-6355"
Feb 27 16:27:29.033: INFO: Deleting pod "simpletest.rc-8jwd4" in namespace "gc-6355"
Feb 27 16:27:29.048: INFO: Deleting pod "simpletest.rc-8lssp" in namespace "gc-6355"
Feb 27 16:27:29.064: INFO: Deleting pod "simpletest.rc-8wls8" in namespace "gc-6355"
Feb 27 16:27:29.081: INFO: Deleting pod "simpletest.rc-97j5t" in namespace "gc-6355"
Feb 27 16:27:29.096: INFO: Deleting pod "simpletest.rc-9svdz" in namespace "gc-6355"
Feb 27 16:27:29.109: INFO: Deleting pod "simpletest.rc-bg5bj" in namespace "gc-6355"
Feb 27 16:27:29.126: INFO: Deleting pod "simpletest.rc-bsvp6" in namespace "gc-6355"
Feb 27 16:27:29.141: INFO: Deleting pod "simpletest.rc-cfrfc" in namespace "gc-6355"
Feb 27 16:27:29.152: INFO: Deleting pod "simpletest.rc-cn9qd" in namespace "gc-6355"
Feb 27 16:27:29.168: INFO: Deleting pod "simpletest.rc-cr7dh" in namespace "gc-6355"
Feb 27 16:27:29.186: INFO: Deleting pod "simpletest.rc-cvdf2" in namespace "gc-6355"
Feb 27 16:27:29.200: INFO: Deleting pod "simpletest.rc-czmjw" in namespace "gc-6355"
Feb 27 16:27:29.215: INFO: Deleting pod "simpletest.rc-ddchv" in namespace "gc-6355"
Feb 27 16:27:29.233: INFO: Deleting pod "simpletest.rc-djrh6" in namespace "gc-6355"
Feb 27 16:27:29.247: INFO: Deleting pod "simpletest.rc-f86vl" in namespace "gc-6355"
Feb 27 16:27:29.260: INFO: Deleting pod "simpletest.rc-fcczz" in namespace "gc-6355"
Feb 27 16:27:29.275: INFO: Deleting pod "simpletest.rc-fftvm" in namespace "gc-6355"
Feb 27 16:27:29.288: INFO: Deleting pod "simpletest.rc-fhmsw" in namespace "gc-6355"
Feb 27 16:27:29.303: INFO: Deleting pod "simpletest.rc-fp8f8" in namespace "gc-6355"
Feb 27 16:27:29.318: INFO: Deleting pod "simpletest.rc-fpjvc" in namespace "gc-6355"
Feb 27 16:27:29.334: INFO: Deleting pod "simpletest.rc-fxr9k" in namespace "gc-6355"
Feb 27 16:27:29.346: INFO: Deleting pod "simpletest.rc-gdzkm" in namespace "gc-6355"
Feb 27 16:27:29.358: INFO: Deleting pod "simpletest.rc-gk2r8" in namespace "gc-6355"
Feb 27 16:27:29.372: INFO: Deleting pod "simpletest.rc-gnghp" in namespace "gc-6355"
Feb 27 16:27:29.385: INFO: Deleting pod "simpletest.rc-gnjkn" in namespace "gc-6355"
Feb 27 16:27:29.398: INFO: Deleting pod "simpletest.rc-gs4tq" in namespace "gc-6355"
Feb 27 16:27:29.411: INFO: Deleting pod "simpletest.rc-gt5gg" in namespace "gc-6355"
Feb 27 16:27:29.426: INFO: Deleting pod "simpletest.rc-h877d" in namespace "gc-6355"
Feb 27 16:27:29.444: INFO: Deleting pod "simpletest.rc-hm4s7" in namespace "gc-6355"
Feb 27 16:27:29.560: INFO: Deleting pod "simpletest.rc-jgp66" in namespace "gc-6355"
Feb 27 16:27:29.574: INFO: Deleting pod "simpletest.rc-jxn5m" in namespace "gc-6355"
Feb 27 16:27:29.589: INFO: Deleting pod "simpletest.rc-kcl4m" in namespace "gc-6355"
Feb 27 16:27:29.601: INFO: Deleting pod "simpletest.rc-khh47" in namespace "gc-6355"
Feb 27 16:27:29.615: INFO: Deleting pod "simpletest.rc-l7sgc" in namespace "gc-6355"
Feb 27 16:27:29.631: INFO: Deleting pod "simpletest.rc-lnwt7" in namespace "gc-6355"
Feb 27 16:27:29.652: INFO: Deleting pod "simpletest.rc-lrq8q" in namespace "gc-6355"
Feb 27 16:27:29.666: INFO: Deleting pod "simpletest.rc-mh6sr" in namespace "gc-6355"
Feb 27 16:27:29.681: INFO: Deleting pod "simpletest.rc-mklm2" in namespace "gc-6355"
Feb 27 16:27:29.694: INFO: Deleting pod "simpletest.rc-ml58t" in namespace "gc-6355"
Feb 27 16:27:29.705: INFO: Deleting pod "simpletest.rc-mm2n7" in namespace "gc-6355"
Feb 27 16:27:29.719: INFO: Deleting pod "simpletest.rc-nzwt4" in namespace "gc-6355"
Feb 27 16:27:29.733: INFO: Deleting pod "simpletest.rc-p2vmh" in namespace "gc-6355"
Feb 27 16:27:29.746: INFO: Deleting pod "simpletest.rc-pk7zx" in namespace "gc-6355"
Feb 27 16:27:29.756: INFO: Deleting pod "simpletest.rc-qnbth" in namespace "gc-6355"
Feb 27 16:27:29.783: INFO: Deleting pod "simpletest.rc-qnzlc" in namespace "gc-6355"
Feb 27 16:27:29.830: INFO: Deleting pod "simpletest.rc-qt49h" in namespace "gc-6355"
Feb 27 16:27:29.881: INFO: Deleting pod "simpletest.rc-r55sc" in namespace "gc-6355"
Feb 27 16:27:29.928: INFO: Deleting pod "simpletest.rc-r575t" in namespace "gc-6355"
Feb 27 16:27:29.982: INFO: Deleting pod "simpletest.rc-r6dms" in namespace "gc-6355"
Feb 27 16:27:30.029: INFO: Deleting pod "simpletest.rc-rc8mp" in namespace "gc-6355"
Feb 27 16:27:30.080: INFO: Deleting pod "simpletest.rc-rf294" in namespace "gc-6355"
Feb 27 16:27:30.130: INFO: Deleting pod "simpletest.rc-rg2mj" in namespace "gc-6355"
Feb 27 16:27:30.179: INFO: Deleting pod "simpletest.rc-rk4fg" in namespace "gc-6355"
Feb 27 16:27:30.231: INFO: Deleting pod "simpletest.rc-sg46p" in namespace "gc-6355"
Feb 27 16:27:30.282: INFO: Deleting pod "simpletest.rc-sjmpq" in namespace "gc-6355"
Feb 27 16:27:30.330: INFO: Deleting pod "simpletest.rc-stdbt" in namespace "gc-6355"
Feb 27 16:27:30.377: INFO: Deleting pod "simpletest.rc-sztdc" in namespace "gc-6355"
Feb 27 16:27:30.428: INFO: Deleting pod "simpletest.rc-t2d68" in namespace "gc-6355"
Feb 27 16:27:30.483: INFO: Deleting pod "simpletest.rc-tlqqt" in namespace "gc-6355"
Feb 27 16:27:30.529: INFO: Deleting pod "simpletest.rc-tw56z" in namespace "gc-6355"
Feb 27 16:27:30.579: INFO: Deleting pod "simpletest.rc-v7z75" in namespace "gc-6355"
Feb 27 16:27:30.627: INFO: Deleting pod "simpletest.rc-vdr8d" in namespace "gc-6355"
Feb 27 16:27:30.680: INFO: Deleting pod "simpletest.rc-w4nlb" in namespace "gc-6355"
Feb 27 16:27:30.736: INFO: Deleting pod "simpletest.rc-wbhf9" in namespace "gc-6355"
Feb 27 16:27:30.777: INFO: Deleting pod "simpletest.rc-wphxq" in namespace "gc-6355"
Feb 27 16:27:30.830: INFO: Deleting pod "simpletest.rc-wrrmg" in namespace "gc-6355"
Feb 27 16:27:30.882: INFO: Deleting pod "simpletest.rc-x8b48" in namespace "gc-6355"
Feb 27 16:27:30.931: INFO: Deleting pod "simpletest.rc-x8msq" in namespace "gc-6355"
Feb 27 16:27:30.978: INFO: Deleting pod "simpletest.rc-xqf4f" in namespace "gc-6355"
Feb 27 16:27:31.027: INFO: Deleting pod "simpletest.rc-xxjqd" in namespace "gc-6355"
Feb 27 16:27:31.079: INFO: Deleting pod "simpletest.rc-z4g49" in namespace "gc-6355"
Feb 27 16:27:31.136: INFO: Deleting pod "simpletest.rc-zk697" in namespace "gc-6355"
Feb 27 16:27:31.182: INFO: Deleting pod "simpletest.rc-zr54p" in namespace "gc-6355"
Feb 27 16:27:31.230: INFO: Deleting pod "simpletest.rc-zvdh6" in namespace "gc-6355"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 16:27:31.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6355" for this suite. 02/27/23 16:27:31.322
------------------------------
• [SLOW TEST] [42.813 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:26:48.561
    Feb 27 16:26:48.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename gc 02/27/23 16:26:48.562
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:26:48.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:26:48.58
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/27/23 16:26:48.588
    STEP: delete the rc 02/27/23 16:26:53.597
    STEP: wait for the rc to be deleted 02/27/23 16:26:53.656
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/27/23 16:26:58.664
    STEP: Gathering metrics 02/27/23 16:27:28.674
    W0227 16:27:28.679371      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 16:27:28.679: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 27 16:27:28.679: INFO: Deleting pod "simpletest.rc-24l28" in namespace "gc-6355"
    Feb 27 16:27:28.694: INFO: Deleting pod "simpletest.rc-2cn9r" in namespace "gc-6355"
    Feb 27 16:27:28.713: INFO: Deleting pod "simpletest.rc-2kdxn" in namespace "gc-6355"
    Feb 27 16:27:28.726: INFO: Deleting pod "simpletest.rc-2qwgf" in namespace "gc-6355"
    Feb 27 16:27:28.740: INFO: Deleting pod "simpletest.rc-2vj8z" in namespace "gc-6355"
    Feb 27 16:27:28.750: INFO: Deleting pod "simpletest.rc-2x7nm" in namespace "gc-6355"
    Feb 27 16:27:28.763: INFO: Deleting pod "simpletest.rc-44twr" in namespace "gc-6355"
    Feb 27 16:27:28.773: INFO: Deleting pod "simpletest.rc-4gqhh" in namespace "gc-6355"
    Feb 27 16:27:28.788: INFO: Deleting pod "simpletest.rc-4qdv2" in namespace "gc-6355"
    Feb 27 16:27:28.803: INFO: Deleting pod "simpletest.rc-4rlb4" in namespace "gc-6355"
    Feb 27 16:27:28.816: INFO: Deleting pod "simpletest.rc-4v78w" in namespace "gc-6355"
    Feb 27 16:27:28.830: INFO: Deleting pod "simpletest.rc-576bt" in namespace "gc-6355"
    Feb 27 16:27:28.842: INFO: Deleting pod "simpletest.rc-57sln" in namespace "gc-6355"
    Feb 27 16:27:28.854: INFO: Deleting pod "simpletest.rc-5bvwg" in namespace "gc-6355"
    Feb 27 16:27:28.869: INFO: Deleting pod "simpletest.rc-5dwcq" in namespace "gc-6355"
    Feb 27 16:27:28.885: INFO: Deleting pod "simpletest.rc-5hhqd" in namespace "gc-6355"
    Feb 27 16:27:28.898: INFO: Deleting pod "simpletest.rc-5kl9b" in namespace "gc-6355"
    Feb 27 16:27:28.913: INFO: Deleting pod "simpletest.rc-5klvw" in namespace "gc-6355"
    Feb 27 16:27:28.925: INFO: Deleting pod "simpletest.rc-5m5km" in namespace "gc-6355"
    Feb 27 16:27:28.939: INFO: Deleting pod "simpletest.rc-5zl5l" in namespace "gc-6355"
    Feb 27 16:27:28.952: INFO: Deleting pod "simpletest.rc-6gsnp" in namespace "gc-6355"
    Feb 27 16:27:28.966: INFO: Deleting pod "simpletest.rc-7qfbl" in namespace "gc-6355"
    Feb 27 16:27:28.979: INFO: Deleting pod "simpletest.rc-7r2d8" in namespace "gc-6355"
    Feb 27 16:27:28.992: INFO: Deleting pod "simpletest.rc-82p6f" in namespace "gc-6355"
    Feb 27 16:27:29.011: INFO: Deleting pod "simpletest.rc-88697" in namespace "gc-6355"
    Feb 27 16:27:29.023: INFO: Deleting pod "simpletest.rc-8cfc2" in namespace "gc-6355"
    Feb 27 16:27:29.033: INFO: Deleting pod "simpletest.rc-8jwd4" in namespace "gc-6355"
    Feb 27 16:27:29.048: INFO: Deleting pod "simpletest.rc-8lssp" in namespace "gc-6355"
    Feb 27 16:27:29.064: INFO: Deleting pod "simpletest.rc-8wls8" in namespace "gc-6355"
    Feb 27 16:27:29.081: INFO: Deleting pod "simpletest.rc-97j5t" in namespace "gc-6355"
    Feb 27 16:27:29.096: INFO: Deleting pod "simpletest.rc-9svdz" in namespace "gc-6355"
    Feb 27 16:27:29.109: INFO: Deleting pod "simpletest.rc-bg5bj" in namespace "gc-6355"
    Feb 27 16:27:29.126: INFO: Deleting pod "simpletest.rc-bsvp6" in namespace "gc-6355"
    Feb 27 16:27:29.141: INFO: Deleting pod "simpletest.rc-cfrfc" in namespace "gc-6355"
    Feb 27 16:27:29.152: INFO: Deleting pod "simpletest.rc-cn9qd" in namespace "gc-6355"
    Feb 27 16:27:29.168: INFO: Deleting pod "simpletest.rc-cr7dh" in namespace "gc-6355"
    Feb 27 16:27:29.186: INFO: Deleting pod "simpletest.rc-cvdf2" in namespace "gc-6355"
    Feb 27 16:27:29.200: INFO: Deleting pod "simpletest.rc-czmjw" in namespace "gc-6355"
    Feb 27 16:27:29.215: INFO: Deleting pod "simpletest.rc-ddchv" in namespace "gc-6355"
    Feb 27 16:27:29.233: INFO: Deleting pod "simpletest.rc-djrh6" in namespace "gc-6355"
    Feb 27 16:27:29.247: INFO: Deleting pod "simpletest.rc-f86vl" in namespace "gc-6355"
    Feb 27 16:27:29.260: INFO: Deleting pod "simpletest.rc-fcczz" in namespace "gc-6355"
    Feb 27 16:27:29.275: INFO: Deleting pod "simpletest.rc-fftvm" in namespace "gc-6355"
    Feb 27 16:27:29.288: INFO: Deleting pod "simpletest.rc-fhmsw" in namespace "gc-6355"
    Feb 27 16:27:29.303: INFO: Deleting pod "simpletest.rc-fp8f8" in namespace "gc-6355"
    Feb 27 16:27:29.318: INFO: Deleting pod "simpletest.rc-fpjvc" in namespace "gc-6355"
    Feb 27 16:27:29.334: INFO: Deleting pod "simpletest.rc-fxr9k" in namespace "gc-6355"
    Feb 27 16:27:29.346: INFO: Deleting pod "simpletest.rc-gdzkm" in namespace "gc-6355"
    Feb 27 16:27:29.358: INFO: Deleting pod "simpletest.rc-gk2r8" in namespace "gc-6355"
    Feb 27 16:27:29.372: INFO: Deleting pod "simpletest.rc-gnghp" in namespace "gc-6355"
    Feb 27 16:27:29.385: INFO: Deleting pod "simpletest.rc-gnjkn" in namespace "gc-6355"
    Feb 27 16:27:29.398: INFO: Deleting pod "simpletest.rc-gs4tq" in namespace "gc-6355"
    Feb 27 16:27:29.411: INFO: Deleting pod "simpletest.rc-gt5gg" in namespace "gc-6355"
    Feb 27 16:27:29.426: INFO: Deleting pod "simpletest.rc-h877d" in namespace "gc-6355"
    Feb 27 16:27:29.444: INFO: Deleting pod "simpletest.rc-hm4s7" in namespace "gc-6355"
    Feb 27 16:27:29.560: INFO: Deleting pod "simpletest.rc-jgp66" in namespace "gc-6355"
    Feb 27 16:27:29.574: INFO: Deleting pod "simpletest.rc-jxn5m" in namespace "gc-6355"
    Feb 27 16:27:29.589: INFO: Deleting pod "simpletest.rc-kcl4m" in namespace "gc-6355"
    Feb 27 16:27:29.601: INFO: Deleting pod "simpletest.rc-khh47" in namespace "gc-6355"
    Feb 27 16:27:29.615: INFO: Deleting pod "simpletest.rc-l7sgc" in namespace "gc-6355"
    Feb 27 16:27:29.631: INFO: Deleting pod "simpletest.rc-lnwt7" in namespace "gc-6355"
    Feb 27 16:27:29.652: INFO: Deleting pod "simpletest.rc-lrq8q" in namespace "gc-6355"
    Feb 27 16:27:29.666: INFO: Deleting pod "simpletest.rc-mh6sr" in namespace "gc-6355"
    Feb 27 16:27:29.681: INFO: Deleting pod "simpletest.rc-mklm2" in namespace "gc-6355"
    Feb 27 16:27:29.694: INFO: Deleting pod "simpletest.rc-ml58t" in namespace "gc-6355"
    Feb 27 16:27:29.705: INFO: Deleting pod "simpletest.rc-mm2n7" in namespace "gc-6355"
    Feb 27 16:27:29.719: INFO: Deleting pod "simpletest.rc-nzwt4" in namespace "gc-6355"
    Feb 27 16:27:29.733: INFO: Deleting pod "simpletest.rc-p2vmh" in namespace "gc-6355"
    Feb 27 16:27:29.746: INFO: Deleting pod "simpletest.rc-pk7zx" in namespace "gc-6355"
    Feb 27 16:27:29.756: INFO: Deleting pod "simpletest.rc-qnbth" in namespace "gc-6355"
    Feb 27 16:27:29.783: INFO: Deleting pod "simpletest.rc-qnzlc" in namespace "gc-6355"
    Feb 27 16:27:29.830: INFO: Deleting pod "simpletest.rc-qt49h" in namespace "gc-6355"
    Feb 27 16:27:29.881: INFO: Deleting pod "simpletest.rc-r55sc" in namespace "gc-6355"
    Feb 27 16:27:29.928: INFO: Deleting pod "simpletest.rc-r575t" in namespace "gc-6355"
    Feb 27 16:27:29.982: INFO: Deleting pod "simpletest.rc-r6dms" in namespace "gc-6355"
    Feb 27 16:27:30.029: INFO: Deleting pod "simpletest.rc-rc8mp" in namespace "gc-6355"
    Feb 27 16:27:30.080: INFO: Deleting pod "simpletest.rc-rf294" in namespace "gc-6355"
    Feb 27 16:27:30.130: INFO: Deleting pod "simpletest.rc-rg2mj" in namespace "gc-6355"
    Feb 27 16:27:30.179: INFO: Deleting pod "simpletest.rc-rk4fg" in namespace "gc-6355"
    Feb 27 16:27:30.231: INFO: Deleting pod "simpletest.rc-sg46p" in namespace "gc-6355"
    Feb 27 16:27:30.282: INFO: Deleting pod "simpletest.rc-sjmpq" in namespace "gc-6355"
    Feb 27 16:27:30.330: INFO: Deleting pod "simpletest.rc-stdbt" in namespace "gc-6355"
    Feb 27 16:27:30.377: INFO: Deleting pod "simpletest.rc-sztdc" in namespace "gc-6355"
    Feb 27 16:27:30.428: INFO: Deleting pod "simpletest.rc-t2d68" in namespace "gc-6355"
    Feb 27 16:27:30.483: INFO: Deleting pod "simpletest.rc-tlqqt" in namespace "gc-6355"
    Feb 27 16:27:30.529: INFO: Deleting pod "simpletest.rc-tw56z" in namespace "gc-6355"
    Feb 27 16:27:30.579: INFO: Deleting pod "simpletest.rc-v7z75" in namespace "gc-6355"
    Feb 27 16:27:30.627: INFO: Deleting pod "simpletest.rc-vdr8d" in namespace "gc-6355"
    Feb 27 16:27:30.680: INFO: Deleting pod "simpletest.rc-w4nlb" in namespace "gc-6355"
    Feb 27 16:27:30.736: INFO: Deleting pod "simpletest.rc-wbhf9" in namespace "gc-6355"
    Feb 27 16:27:30.777: INFO: Deleting pod "simpletest.rc-wphxq" in namespace "gc-6355"
    Feb 27 16:27:30.830: INFO: Deleting pod "simpletest.rc-wrrmg" in namespace "gc-6355"
    Feb 27 16:27:30.882: INFO: Deleting pod "simpletest.rc-x8b48" in namespace "gc-6355"
    Feb 27 16:27:30.931: INFO: Deleting pod "simpletest.rc-x8msq" in namespace "gc-6355"
    Feb 27 16:27:30.978: INFO: Deleting pod "simpletest.rc-xqf4f" in namespace "gc-6355"
    Feb 27 16:27:31.027: INFO: Deleting pod "simpletest.rc-xxjqd" in namespace "gc-6355"
    Feb 27 16:27:31.079: INFO: Deleting pod "simpletest.rc-z4g49" in namespace "gc-6355"
    Feb 27 16:27:31.136: INFO: Deleting pod "simpletest.rc-zk697" in namespace "gc-6355"
    Feb 27 16:27:31.182: INFO: Deleting pod "simpletest.rc-zr54p" in namespace "gc-6355"
    Feb 27 16:27:31.230: INFO: Deleting pod "simpletest.rc-zvdh6" in namespace "gc-6355"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:27:31.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6355" for this suite. 02/27/23 16:27:31.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:27:31.375
Feb 27 16:27:31.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename podtemplate 02/27/23 16:27:31.376
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:31.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:31.4
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/27/23 16:27:31.403
STEP: Replace a pod template 02/27/23 16:27:31.409
Feb 27 16:27:31.418: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 27 16:27:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4885" for this suite. 02/27/23 16:27:31.428
------------------------------
• [0.065 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:27:31.375
    Feb 27 16:27:31.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename podtemplate 02/27/23 16:27:31.376
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:31.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:31.4
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/27/23 16:27:31.403
    STEP: Replace a pod template 02/27/23 16:27:31.409
    Feb 27 16:27:31.418: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:27:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4885" for this suite. 02/27/23 16:27:31.428
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:27:31.441
Feb 27 16:27:31.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:27:31.441
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:31.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:31.469
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-cf421f06-4d8a-4df5-ab38-8c482f2c873c 02/27/23 16:27:31.472
STEP: Creating a pod to test consume configMaps 02/27/23 16:27:31.478
Feb 27 16:27:31.485: INFO: Waiting up to 5m0s for pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7" in namespace "configmap-6434" to be "Succeeded or Failed"
Feb 27 16:27:31.491: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.772504ms
Feb 27 16:27:33.497: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011263602s
Feb 27 16:27:35.496: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010126332s
Feb 27 16:27:37.497: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011873496s
Feb 27 16:27:39.498: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012332959s
Feb 27 16:27:41.495: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.009746474s
STEP: Saw pod success 02/27/23 16:27:41.495
Feb 27 16:27:41.495: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7" satisfied condition "Succeeded or Failed"
Feb 27 16:27:41.499: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 16:27:41.508
Feb 27 16:27:41.521: INFO: Waiting for pod pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7 to disappear
Feb 27 16:27:41.524: INFO: Pod pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:27:41.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6434" for this suite. 02/27/23 16:27:41.529
------------------------------
• [SLOW TEST] [10.094 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:27:31.441
    Feb 27 16:27:31.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:27:31.441
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:31.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:31.469
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-cf421f06-4d8a-4df5-ab38-8c482f2c873c 02/27/23 16:27:31.472
    STEP: Creating a pod to test consume configMaps 02/27/23 16:27:31.478
    Feb 27 16:27:31.485: INFO: Waiting up to 5m0s for pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7" in namespace "configmap-6434" to be "Succeeded or Failed"
    Feb 27 16:27:31.491: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.772504ms
    Feb 27 16:27:33.497: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011263602s
    Feb 27 16:27:35.496: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010126332s
    Feb 27 16:27:37.497: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011873496s
    Feb 27 16:27:39.498: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012332959s
    Feb 27 16:27:41.495: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.009746474s
    STEP: Saw pod success 02/27/23 16:27:41.495
    Feb 27 16:27:41.495: INFO: Pod "pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7" satisfied condition "Succeeded or Failed"
    Feb 27 16:27:41.499: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 16:27:41.508
    Feb 27 16:27:41.521: INFO: Waiting for pod pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7 to disappear
    Feb 27 16:27:41.524: INFO: Pod pod-configmaps-4427f911-aa7e-4f02-a78f-ce8178dac1e7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:27:41.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6434" for this suite. 02/27/23 16:27:41.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:27:41.535
Feb 27 16:27:41.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:27:41.536
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:41.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:41.555
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-a9e35a43-cf67-4fb5-ab42-077c27598e4f 02/27/23 16:27:41.558
STEP: Creating a pod to test consume configMaps 02/27/23 16:27:41.563
Feb 27 16:27:41.571: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9" in namespace "projected-4238" to be "Succeeded or Failed"
Feb 27 16:27:41.574: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612552ms
Feb 27 16:27:43.578: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007007972s
Feb 27 16:27:45.578: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006954613s
Feb 27 16:27:47.580: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009164421s
STEP: Saw pod success 02/27/23 16:27:47.58
Feb 27 16:27:47.580: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9" satisfied condition "Succeeded or Failed"
Feb 27 16:27:47.584: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9 container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/27/23 16:27:47.589
Feb 27 16:27:47.604: INFO: Waiting for pod pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9 to disappear
Feb 27 16:27:47.608: INFO: Pod pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:27:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4238" for this suite. 02/27/23 16:27:47.611
------------------------------
• [SLOW TEST] [6.082 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:27:41.535
    Feb 27 16:27:41.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:27:41.536
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:41.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:41.555
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-a9e35a43-cf67-4fb5-ab42-077c27598e4f 02/27/23 16:27:41.558
    STEP: Creating a pod to test consume configMaps 02/27/23 16:27:41.563
    Feb 27 16:27:41.571: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9" in namespace "projected-4238" to be "Succeeded or Failed"
    Feb 27 16:27:41.574: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612552ms
    Feb 27 16:27:43.578: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007007972s
    Feb 27 16:27:45.578: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006954613s
    Feb 27 16:27:47.580: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009164421s
    STEP: Saw pod success 02/27/23 16:27:47.58
    Feb 27 16:27:47.580: INFO: Pod "pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9" satisfied condition "Succeeded or Failed"
    Feb 27 16:27:47.584: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/27/23 16:27:47.589
    Feb 27 16:27:47.604: INFO: Waiting for pod pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9 to disappear
    Feb 27 16:27:47.608: INFO: Pod pod-projected-configmaps-99bacc26-d2e4-42f9-9f01-391351c319c9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:27:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4238" for this suite. 02/27/23 16:27:47.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:27:47.618
Feb 27 16:27:47.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:27:47.619
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:47.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:47.64
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb 27 16:27:47.655: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6032 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 16:27:47.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6032" for this suite. 02/27/23 16:27:47.674
------------------------------
• [0.062 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:27:47.618
    Feb 27 16:27:47.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:27:47.619
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:47.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:47.64
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb 27 16:27:47.655: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6032 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:27:47.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6032" for this suite. 02/27/23 16:27:47.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:27:47.681
Feb 27 16:27:47.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:27:47.682
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:47.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:47.699
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb 27 16:27:47.714: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-221 to be scheduled
Feb 27 16:27:47.717: INFO: 1 pods are not scheduled: [runtimeclass-221/test-runtimeclass-runtimeclass-221-preconfigured-handler-bwn8h(827a60ae-8bba-4821-802b-e10ecbce0a1f)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 16:27:49.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-221" for this suite. 02/27/23 16:27:49.733
------------------------------
• [2.058 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:27:47.681
    Feb 27 16:27:47.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:27:47.682
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:47.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:47.699
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb 27 16:27:47.714: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-221 to be scheduled
    Feb 27 16:27:47.717: INFO: 1 pods are not scheduled: [runtimeclass-221/test-runtimeclass-runtimeclass-221-preconfigured-handler-bwn8h(827a60ae-8bba-4821-802b-e10ecbce0a1f)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:27:49.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-221" for this suite. 02/27/23 16:27:49.733
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:27:49.74
Feb 27 16:27:49.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pod-network-test 02/27/23 16:27:49.74
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:49.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:49.759
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2226 02/27/23 16:27:49.762
STEP: creating a selector 02/27/23 16:27:49.762
STEP: Creating the service pods in kubernetes 02/27/23 16:27:49.762
Feb 27 16:27:49.762: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 16:27:49.800: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2226" to be "running and ready"
Feb 27 16:27:49.807: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.014559ms
Feb 27 16:27:49.807: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:27:51.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010660782s
Feb 27 16:27:51.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 16:27:53.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011490108s
Feb 27 16:27:53.812: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 16:27:55.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010590296s
Feb 27 16:27:55.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 16:27:57.812: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012416741s
Feb 27 16:27:57.812: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 16:27:59.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011332511s
Feb 27 16:27:59.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 16:28:01.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012618007s
Feb 27 16:28:01.813: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 16:28:01.813: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 16:28:01.816: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2226" to be "running and ready"
Feb 27 16:28:01.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.753249ms
Feb 27 16:28:01.819: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 16:28:01.819: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 16:28:01.822: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2226" to be "running and ready"
Feb 27 16:28:01.825: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.046996ms
Feb 27 16:28:01.825: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 16:28:01.825: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 16:28:01.828
Feb 27 16:28:01.835: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2226" to be "running"
Feb 27 16:28:01.838: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039209ms
Feb 27 16:28:03.842: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00772583s
Feb 27 16:28:03.842: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 16:28:03.845: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 16:28:03.845: INFO: Breadth first check of 192.168.212.150 on host 172.31.3.182...
Feb 27 16:28:03.849: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.135:9080/dial?request=hostname&protocol=udp&host=192.168.212.150&port=8081&tries=1'] Namespace:pod-network-test-2226 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 16:28:03.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 16:28:03.850: INFO: ExecWithOptions: Clientset creation
Feb 27 16:28:03.850: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2226/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.212.150%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 16:28:03.936: INFO: Waiting for responses: map[]
Feb 27 16:28:03.936: INFO: reached 192.168.212.150 after 0/1 tries
Feb 27 16:28:03.936: INFO: Breadth first check of 192.168.192.132 on host 172.31.42.40...
Feb 27 16:28:03.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.135:9080/dial?request=hostname&protocol=udp&host=192.168.192.132&port=8081&tries=1'] Namespace:pod-network-test-2226 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 16:28:03.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 16:28:03.940: INFO: ExecWithOptions: Clientset creation
Feb 27 16:28:03.940: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2226/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.192.132%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 16:28:03.988: INFO: Waiting for responses: map[]
Feb 27 16:28:03.988: INFO: reached 192.168.192.132 after 0/1 tries
Feb 27 16:28:03.988: INFO: Breadth first check of 192.168.214.173 on host 172.31.84.171...
Feb 27 16:28:03.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.135:9080/dial?request=hostname&protocol=udp&host=192.168.214.173&port=8081&tries=1'] Namespace:pod-network-test-2226 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 16:28:03.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 16:28:03.992: INFO: ExecWithOptions: Clientset creation
Feb 27 16:28:03.992: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2226/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.214.173%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 16:28:04.044: INFO: Waiting for responses: map[]
Feb 27 16:28:04.044: INFO: reached 192.168.214.173 after 0/1 tries
Feb 27 16:28:04.044: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:04.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2226" for this suite. 02/27/23 16:28:04.049
------------------------------
• [SLOW TEST] [14.317 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:27:49.74
    Feb 27 16:27:49.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 16:27:49.74
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:27:49.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:27:49.759
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2226 02/27/23 16:27:49.762
    STEP: creating a selector 02/27/23 16:27:49.762
    STEP: Creating the service pods in kubernetes 02/27/23 16:27:49.762
    Feb 27 16:27:49.762: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 16:27:49.800: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2226" to be "running and ready"
    Feb 27 16:27:49.807: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.014559ms
    Feb 27 16:27:49.807: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:27:51.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010660782s
    Feb 27 16:27:51.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 16:27:53.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011490108s
    Feb 27 16:27:53.812: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 16:27:55.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010590296s
    Feb 27 16:27:55.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 16:27:57.812: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012416741s
    Feb 27 16:27:57.812: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 16:27:59.811: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011332511s
    Feb 27 16:27:59.811: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 16:28:01.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012618007s
    Feb 27 16:28:01.813: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 16:28:01.813: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 16:28:01.816: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2226" to be "running and ready"
    Feb 27 16:28:01.819: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.753249ms
    Feb 27 16:28:01.819: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 16:28:01.819: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 16:28:01.822: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2226" to be "running and ready"
    Feb 27 16:28:01.825: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.046996ms
    Feb 27 16:28:01.825: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 16:28:01.825: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 16:28:01.828
    Feb 27 16:28:01.835: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2226" to be "running"
    Feb 27 16:28:01.838: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039209ms
    Feb 27 16:28:03.842: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00772583s
    Feb 27 16:28:03.842: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 16:28:03.845: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 16:28:03.845: INFO: Breadth first check of 192.168.212.150 on host 172.31.3.182...
    Feb 27 16:28:03.849: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.135:9080/dial?request=hostname&protocol=udp&host=192.168.212.150&port=8081&tries=1'] Namespace:pod-network-test-2226 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 16:28:03.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 16:28:03.850: INFO: ExecWithOptions: Clientset creation
    Feb 27 16:28:03.850: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2226/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.212.150%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 16:28:03.936: INFO: Waiting for responses: map[]
    Feb 27 16:28:03.936: INFO: reached 192.168.212.150 after 0/1 tries
    Feb 27 16:28:03.936: INFO: Breadth first check of 192.168.192.132 on host 172.31.42.40...
    Feb 27 16:28:03.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.135:9080/dial?request=hostname&protocol=udp&host=192.168.192.132&port=8081&tries=1'] Namespace:pod-network-test-2226 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 16:28:03.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 16:28:03.940: INFO: ExecWithOptions: Clientset creation
    Feb 27 16:28:03.940: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2226/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.192.132%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 16:28:03.988: INFO: Waiting for responses: map[]
    Feb 27 16:28:03.988: INFO: reached 192.168.192.132 after 0/1 tries
    Feb 27 16:28:03.988: INFO: Breadth first check of 192.168.214.173 on host 172.31.84.171...
    Feb 27 16:28:03.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.135:9080/dial?request=hostname&protocol=udp&host=192.168.214.173&port=8081&tries=1'] Namespace:pod-network-test-2226 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 16:28:03.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 16:28:03.992: INFO: ExecWithOptions: Clientset creation
    Feb 27 16:28:03.992: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2226/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.192.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.214.173%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 16:28:04.044: INFO: Waiting for responses: map[]
    Feb 27 16:28:04.044: INFO: reached 192.168.214.173 after 0/1 tries
    Feb 27 16:28:04.044: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:04.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2226" for this suite. 02/27/23 16:28:04.049
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:04.056
Feb 27 16:28:04.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:28:04.057
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.075
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 02/27/23 16:28:04.078
STEP: watching for the ServiceAccount to be added 02/27/23 16:28:04.085
STEP: patching the ServiceAccount 02/27/23 16:28:04.087
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/27/23 16:28:04.092
STEP: deleting the ServiceAccount 02/27/23 16:28:04.096
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:04.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5277" for this suite. 02/27/23 16:28:04.115
------------------------------
• [0.067 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:04.056
    Feb 27 16:28:04.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:28:04.057
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.075
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 02/27/23 16:28:04.078
    STEP: watching for the ServiceAccount to be added 02/27/23 16:28:04.085
    STEP: patching the ServiceAccount 02/27/23 16:28:04.087
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/27/23 16:28:04.092
    STEP: deleting the ServiceAccount 02/27/23 16:28:04.096
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:04.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5277" for this suite. 02/27/23 16:28:04.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:04.124
Feb 27 16:28:04.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:28:04.125
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.188
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/27/23 16:28:04.192
STEP: getting /apis/node.k8s.io 02/27/23 16:28:04.195
STEP: getting /apis/node.k8s.io/v1 02/27/23 16:28:04.197
STEP: creating 02/27/23 16:28:04.198
STEP: watching 02/27/23 16:28:04.213
Feb 27 16:28:04.213: INFO: starting watch
STEP: getting 02/27/23 16:28:04.219
STEP: listing 02/27/23 16:28:04.222
STEP: patching 02/27/23 16:28:04.225
STEP: updating 02/27/23 16:28:04.23
Feb 27 16:28:04.235: INFO: waiting for watch events with expected annotations
STEP: deleting 02/27/23 16:28:04.235
STEP: deleting a collection 02/27/23 16:28:04.248
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:04.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2117" for this suite. 02/27/23 16:28:04.268
------------------------------
• [0.152 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:04.124
    Feb 27 16:28:04.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 16:28:04.125
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.188
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/27/23 16:28:04.192
    STEP: getting /apis/node.k8s.io 02/27/23 16:28:04.195
    STEP: getting /apis/node.k8s.io/v1 02/27/23 16:28:04.197
    STEP: creating 02/27/23 16:28:04.198
    STEP: watching 02/27/23 16:28:04.213
    Feb 27 16:28:04.213: INFO: starting watch
    STEP: getting 02/27/23 16:28:04.219
    STEP: listing 02/27/23 16:28:04.222
    STEP: patching 02/27/23 16:28:04.225
    STEP: updating 02/27/23 16:28:04.23
    Feb 27 16:28:04.235: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/27/23 16:28:04.235
    STEP: deleting a collection 02/27/23 16:28:04.248
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:04.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2117" for this suite. 02/27/23 16:28:04.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:04.279
Feb 27 16:28:04.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename configmap 02/27/23 16:28:04.28
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.3
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-4f8b04e5-7908-4980-b366-32ed119470df 02/27/23 16:28:04.303
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-222" for this suite. 02/27/23 16:28:04.309
------------------------------
• [0.036 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:04.279
    Feb 27 16:28:04.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename configmap 02/27/23 16:28:04.28
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.3
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-4f8b04e5-7908-4980-b366-32ed119470df 02/27/23 16:28:04.303
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-222" for this suite. 02/27/23 16:28:04.309
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:04.316
Feb 27 16:28:04.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename subpath 02/27/23 16:28:04.317
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.335
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 16:28:04.338
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-6k25 02/27/23 16:28:04.346
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 16:28:04.346
Feb 27 16:28:04.354: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6k25" in namespace "subpath-2550" to be "Succeeded or Failed"
Feb 27 16:28:04.358: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600645ms
Feb 27 16:28:06.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371127s
Feb 27 16:28:08.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 4.009875119s
Feb 27 16:28:10.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 6.008652503s
Feb 27 16:28:12.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 8.007879943s
Feb 27 16:28:14.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 10.00822837s
Feb 27 16:28:16.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 12.007429401s
Feb 27 16:28:18.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 14.009359342s
Feb 27 16:28:20.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 16.008609181s
Feb 27 16:28:22.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 18.007339322s
Feb 27 16:28:24.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 20.009439242s
Feb 27 16:28:26.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=false. Elapsed: 22.008643732s
Feb 27 16:28:28.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007818059s
STEP: Saw pod success 02/27/23 16:28:28.362
Feb 27 16:28:28.362: INFO: Pod "pod-subpath-test-downwardapi-6k25" satisfied condition "Succeeded or Failed"
Feb 27 16:28:28.366: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-downwardapi-6k25 container test-container-subpath-downwardapi-6k25: <nil>
STEP: delete the pod 02/27/23 16:28:28.372
Feb 27 16:28:28.386: INFO: Waiting for pod pod-subpath-test-downwardapi-6k25 to disappear
Feb 27 16:28:28.390: INFO: Pod pod-subpath-test-downwardapi-6k25 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-6k25 02/27/23 16:28:28.39
Feb 27 16:28:28.390: INFO: Deleting pod "pod-subpath-test-downwardapi-6k25" in namespace "subpath-2550"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:28.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2550" for this suite. 02/27/23 16:28:28.397
------------------------------
• [SLOW TEST] [24.087 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:04.316
    Feb 27 16:28:04.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename subpath 02/27/23 16:28:04.317
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:04.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:04.335
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 16:28:04.338
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-6k25 02/27/23 16:28:04.346
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 16:28:04.346
    Feb 27 16:28:04.354: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6k25" in namespace "subpath-2550" to be "Succeeded or Failed"
    Feb 27 16:28:04.358: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600645ms
    Feb 27 16:28:06.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371127s
    Feb 27 16:28:08.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 4.009875119s
    Feb 27 16:28:10.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 6.008652503s
    Feb 27 16:28:12.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 8.007879943s
    Feb 27 16:28:14.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 10.00822837s
    Feb 27 16:28:16.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 12.007429401s
    Feb 27 16:28:18.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 14.009359342s
    Feb 27 16:28:20.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 16.008609181s
    Feb 27 16:28:22.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 18.007339322s
    Feb 27 16:28:24.364: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=true. Elapsed: 20.009439242s
    Feb 27 16:28:26.363: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Running", Reason="", readiness=false. Elapsed: 22.008643732s
    Feb 27 16:28:28.362: INFO: Pod "pod-subpath-test-downwardapi-6k25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007818059s
    STEP: Saw pod success 02/27/23 16:28:28.362
    Feb 27 16:28:28.362: INFO: Pod "pod-subpath-test-downwardapi-6k25" satisfied condition "Succeeded or Failed"
    Feb 27 16:28:28.366: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-subpath-test-downwardapi-6k25 container test-container-subpath-downwardapi-6k25: <nil>
    STEP: delete the pod 02/27/23 16:28:28.372
    Feb 27 16:28:28.386: INFO: Waiting for pod pod-subpath-test-downwardapi-6k25 to disappear
    Feb 27 16:28:28.390: INFO: Pod pod-subpath-test-downwardapi-6k25 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-6k25 02/27/23 16:28:28.39
    Feb 27 16:28:28.390: INFO: Deleting pod "pod-subpath-test-downwardapi-6k25" in namespace "subpath-2550"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:28.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2550" for this suite. 02/27/23 16:28:28.397
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:28.404
Feb 27 16:28:28.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename pods 02/27/23 16:28:28.404
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:28.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:28.428
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/27/23 16:28:28.431
STEP: submitting the pod to kubernetes 02/27/23 16:28:28.431
STEP: verifying QOS class is set on the pod 02/27/23 16:28:28.439
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:28.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9088" for this suite. 02/27/23 16:28:28.449
------------------------------
• [0.051 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:28.404
    Feb 27 16:28:28.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename pods 02/27/23 16:28:28.404
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:28.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:28.428
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/27/23 16:28:28.431
    STEP: submitting the pod to kubernetes 02/27/23 16:28:28.431
    STEP: verifying QOS class is set on the pod 02/27/23 16:28:28.439
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:28.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9088" for this suite. 02/27/23 16:28:28.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:28.456
Feb 27 16:28:28.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename security-context 02/27/23 16:28:28.456
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:28.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:28.476
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 16:28:28.479
Feb 27 16:28:28.488: INFO: Waiting up to 5m0s for pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa" in namespace "security-context-6817" to be "Succeeded or Failed"
Feb 27 16:28:28.493: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.633707ms
Feb 27 16:28:30.497: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008636439s
Feb 27 16:28:32.499: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010019105s
STEP: Saw pod success 02/27/23 16:28:32.499
Feb 27 16:28:32.499: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa" satisfied condition "Succeeded or Failed"
Feb 27 16:28:32.502: INFO: Trying to get logs from node ip-172-31-42-40 pod security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa container test-container: <nil>
STEP: delete the pod 02/27/23 16:28:32.508
Feb 27 16:28:32.520: INFO: Waiting for pod security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa to disappear
Feb 27 16:28:32.524: INFO: Pod security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:32.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-6817" for this suite. 02/27/23 16:28:32.528
------------------------------
• [4.080 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:28.456
    Feb 27 16:28:28.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename security-context 02/27/23 16:28:28.456
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:28.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:28.476
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 16:28:28.479
    Feb 27 16:28:28.488: INFO: Waiting up to 5m0s for pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa" in namespace "security-context-6817" to be "Succeeded or Failed"
    Feb 27 16:28:28.493: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.633707ms
    Feb 27 16:28:30.497: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008636439s
    Feb 27 16:28:32.499: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010019105s
    STEP: Saw pod success 02/27/23 16:28:32.499
    Feb 27 16:28:32.499: INFO: Pod "security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa" satisfied condition "Succeeded or Failed"
    Feb 27 16:28:32.502: INFO: Trying to get logs from node ip-172-31-42-40 pod security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa container test-container: <nil>
    STEP: delete the pod 02/27/23 16:28:32.508
    Feb 27 16:28:32.520: INFO: Waiting for pod security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa to disappear
    Feb 27 16:28:32.524: INFO: Pod security-context-8f21fa82-3311-41c2-bc0e-12dbb49779fa no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:32.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-6817" for this suite. 02/27/23 16:28:32.528
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:32.537
Feb 27 16:28:32.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubelet-test 02/27/23 16:28:32.537
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:32.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:32.56
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/27/23 16:28:32.572
Feb 27 16:28:32.572: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce" in namespace "kubelet-test-883" to be "completed"
Feb 27 16:28:32.576: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.210494ms
Feb 27 16:28:34.581: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008503694s
Feb 27 16:28:36.580: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007768947s
Feb 27 16:28:36.580: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 16:28:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-883" for this suite. 02/27/23 16:28:36.59
------------------------------
• [4.060 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:32.537
    Feb 27 16:28:32.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 16:28:32.537
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:32.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:32.56
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/27/23 16:28:32.572
    Feb 27 16:28:32.572: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce" in namespace "kubelet-test-883" to be "completed"
    Feb 27 16:28:32.576: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.210494ms
    Feb 27 16:28:34.581: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008503694s
    Feb 27 16:28:36.580: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007768947s
    Feb 27 16:28:36.580: INFO: Pod "agnhost-host-aliases34b9abde-b477-4886-abcb-7dfcf084d3ce" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:28:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-883" for this suite. 02/27/23 16:28:36.59
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:28:36.597
Feb 27 16:28:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename var-expansion 02/27/23 16:28:36.597
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:36.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:36.616
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 02/27/23 16:28:36.619
STEP: waiting for pod running 02/27/23 16:28:36.629
Feb 27 16:28:36.629: INFO: Waiting up to 2m0s for pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" in namespace "var-expansion-3590" to be "running"
Feb 27 16:28:36.633: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056413ms
Feb 27 16:28:38.638: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23": Phase="Running", Reason="", readiness=true. Elapsed: 2.009361435s
Feb 27 16:28:38.638: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" satisfied condition "running"
STEP: creating a file in subpath 02/27/23 16:28:38.638
Feb 27 16:28:38.641: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3590 PodName:var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 16:28:38.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 16:28:38.642: INFO: ExecWithOptions: Clientset creation
Feb 27 16:28:38.642: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-3590/pods/var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/27/23 16:28:38.704
Feb 27 16:28:38.709: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3590 PodName:var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 16:28:38.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
Feb 27 16:28:38.709: INFO: ExecWithOptions: Clientset creation
Feb 27 16:28:38.710: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-3590/pods/var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/27/23 16:28:38.769
Feb 27 16:28:39.281: INFO: Successfully updated pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23"
STEP: waiting for annotated pod running 02/27/23 16:28:39.281
Feb 27 16:28:39.281: INFO: Waiting up to 2m0s for pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" in namespace "var-expansion-3590" to be "running"
Feb 27 16:28:39.285: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23": Phase="Running", Reason="", readiness=true. Elapsed: 3.729727ms
Feb 27 16:28:39.285: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 16:28:39.285
Feb 27 16:28:39.285: INFO: Deleting pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" in namespace "var-expansion-3590"
Feb 27 16:28:39.292: INFO: Wait up to 5m0s for pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 16:29:13.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3590" for this suite. 02/27/23 16:29:13.303
------------------------------
• [SLOW TEST] [36.714 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:28:36.597
    Feb 27 16:28:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename var-expansion 02/27/23 16:28:36.597
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:28:36.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:28:36.616
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 02/27/23 16:28:36.619
    STEP: waiting for pod running 02/27/23 16:28:36.629
    Feb 27 16:28:36.629: INFO: Waiting up to 2m0s for pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" in namespace "var-expansion-3590" to be "running"
    Feb 27 16:28:36.633: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056413ms
    Feb 27 16:28:38.638: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23": Phase="Running", Reason="", readiness=true. Elapsed: 2.009361435s
    Feb 27 16:28:38.638: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" satisfied condition "running"
    STEP: creating a file in subpath 02/27/23 16:28:38.638
    Feb 27 16:28:38.641: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3590 PodName:var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 16:28:38.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 16:28:38.642: INFO: ExecWithOptions: Clientset creation
    Feb 27 16:28:38.642: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-3590/pods/var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/27/23 16:28:38.704
    Feb 27 16:28:38.709: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3590 PodName:var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 16:28:38.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    Feb 27 16:28:38.709: INFO: ExecWithOptions: Clientset creation
    Feb 27 16:28:38.710: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-3590/pods/var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/27/23 16:28:38.769
    Feb 27 16:28:39.281: INFO: Successfully updated pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23"
    STEP: waiting for annotated pod running 02/27/23 16:28:39.281
    Feb 27 16:28:39.281: INFO: Waiting up to 2m0s for pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" in namespace "var-expansion-3590" to be "running"
    Feb 27 16:28:39.285: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23": Phase="Running", Reason="", readiness=true. Elapsed: 3.729727ms
    Feb 27 16:28:39.285: INFO: Pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 16:28:39.285
    Feb 27 16:28:39.285: INFO: Deleting pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" in namespace "var-expansion-3590"
    Feb 27 16:28:39.292: INFO: Wait up to 5m0s for pod "var-expansion-4b115592-de10-4f53-a2be-bd63044e5b23" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:29:13.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3590" for this suite. 02/27/23 16:29:13.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:29:13.312
Feb 27 16:29:13.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 16:29:13.313
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:13.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:13.335
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 02/27/23 16:29:13.338
Feb 27 16:29:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: rename a version 02/27/23 16:29:16.772
STEP: check the new version name is served 02/27/23 16:29:16.786
STEP: check the old version name is removed 02/27/23 16:29:18.086
STEP: check the other version is not changed 02/27/23 16:29:18.728
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:29:21.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-27" for this suite. 02/27/23 16:29:21.599
------------------------------
• [SLOW TEST] [8.294 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:29:13.312
    Feb 27 16:29:13.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 16:29:13.313
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:13.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:13.335
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 02/27/23 16:29:13.338
    Feb 27 16:29:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: rename a version 02/27/23 16:29:16.772
    STEP: check the new version name is served 02/27/23 16:29:16.786
    STEP: check the old version name is removed 02/27/23 16:29:18.086
    STEP: check the other version is not changed 02/27/23 16:29:18.728
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:29:21.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-27" for this suite. 02/27/23 16:29:21.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:29:21.607
Feb 27 16:29:21.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename resourcequota 02/27/23 16:29:21.608
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:21.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:21.626
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 02/27/23 16:29:21.629
STEP: Creating a ResourceQuota 02/27/23 16:29:26.633
STEP: Ensuring resource quota status is calculated 02/27/23 16:29:26.637
STEP: Creating a ReplicaSet 02/27/23 16:29:28.642
STEP: Ensuring resource quota status captures replicaset creation 02/27/23 16:29:28.655
STEP: Deleting a ReplicaSet 02/27/23 16:29:30.659
STEP: Ensuring resource quota status released usage 02/27/23 16:29:30.665
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 16:29:32.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-816" for this suite. 02/27/23 16:29:32.674
------------------------------
• [SLOW TEST] [11.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:29:21.607
    Feb 27 16:29:21.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename resourcequota 02/27/23 16:29:21.608
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:21.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:21.626
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 02/27/23 16:29:21.629
    STEP: Creating a ResourceQuota 02/27/23 16:29:26.633
    STEP: Ensuring resource quota status is calculated 02/27/23 16:29:26.637
    STEP: Creating a ReplicaSet 02/27/23 16:29:28.642
    STEP: Ensuring resource quota status captures replicaset creation 02/27/23 16:29:28.655
    STEP: Deleting a ReplicaSet 02/27/23 16:29:30.659
    STEP: Ensuring resource quota status released usage 02/27/23 16:29:30.665
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:29:32.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-816" for this suite. 02/27/23 16:29:32.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:29:32.681
Feb 27 16:29:32.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename gc 02/27/23 16:29:32.681
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:32.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:32.7
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/27/23 16:29:32.702
STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 16:29:32.708
STEP: delete the deployment 02/27/23 16:29:33.216
STEP: wait for all rs to be garbage collected 02/27/23 16:29:33.222
STEP: expected 0 rs, got 1 rs 02/27/23 16:29:33.23
STEP: expected 0 pods, got 2 pods 02/27/23 16:29:33.235
STEP: Gathering metrics 02/27/23 16:29:33.745
W0227 16:29:33.749651      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 16:29:33.749: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 16:29:33.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4379" for this suite. 02/27/23 16:29:33.752
------------------------------
• [1.078 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:29:32.681
    Feb 27 16:29:32.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename gc 02/27/23 16:29:32.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:32.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:32.7
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/27/23 16:29:32.702
    STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 16:29:32.708
    STEP: delete the deployment 02/27/23 16:29:33.216
    STEP: wait for all rs to be garbage collected 02/27/23 16:29:33.222
    STEP: expected 0 rs, got 1 rs 02/27/23 16:29:33.23
    STEP: expected 0 pods, got 2 pods 02/27/23 16:29:33.235
    STEP: Gathering metrics 02/27/23 16:29:33.745
    W0227 16:29:33.749651      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 16:29:33.749: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:29:33.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4379" for this suite. 02/27/23 16:29:33.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:29:33.759
Feb 27 16:29:33.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename statefulset 02/27/23 16:29:33.759
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:33.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:33.775
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9146 02/27/23 16:29:33.778
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-9146 02/27/23 16:29:33.783
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9146 02/27/23 16:29:33.791
Feb 27 16:29:33.794: INFO: Found 0 stateful pods, waiting for 1
Feb 27 16:29:43.801: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/27/23 16:29:43.801
Feb 27 16:29:43.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 16:29:43.941: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 16:29:43.941: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 16:29:43.941: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 16:29:43.945: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 27 16:29:53.951: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 16:29:53.951: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 16:29:53.966: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Feb 27 16:29:53.966: INFO: ss-0  ip-172-31-42-40  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  }]
Feb 27 16:29:53.966: INFO: 
Feb 27 16:29:53.966: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 27 16:29:54.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996584897s
Feb 27 16:29:55.974: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993382889s
Feb 27 16:29:56.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98964565s
Feb 27 16:29:57.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985030362s
Feb 27 16:29:58.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980352596s
Feb 27 16:29:59.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975633099s
Feb 27 16:30:00.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97033094s
Feb 27 16:30:02.002: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966416867s
Feb 27 16:30:03.007: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.982217ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9146 02/27/23 16:30:04.007
Feb 27 16:30:04.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 16:30:04.140: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 16:30:04.140: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 16:30:04.140: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 16:30:04.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 16:30:04.262: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 27 16:30:04.263: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 16:30:04.263: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 16:30:04.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 16:30:04.382: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 27 16:30:04.382: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 16:30:04.382: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 16:30:04.386: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:30:04.386: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 16:30:04.386: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/27/23 16:30:04.386
Feb 27 16:30:04.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 16:30:04.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 16:30:04.517: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 16:30:04.517: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 16:30:04.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 16:30:04.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 16:30:04.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 16:30:04.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 16:30:04.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 16:30:04.730: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 16:30:04.730: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 16:30:04.730: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 16:30:04.730: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 16:30:04.733: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 27 16:30:14.742: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 16:30:14.742: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 16:30:14.742: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 16:30:14.755: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Feb 27 16:30:14.755: INFO: ss-0  ip-172-31-42-40   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  }]
Feb 27 16:30:14.755: INFO: ss-1  ip-172-31-3-182   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  }]
Feb 27 16:30:14.755: INFO: ss-2  ip-172-31-84-171  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  }]
Feb 27 16:30:14.755: INFO: 
Feb 27 16:30:14.755: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 27 16:30:15.758: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Feb 27 16:30:15.758: INFO: ss-0  ip-172-31-42-40   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  }]
Feb 27 16:30:15.758: INFO: ss-2  ip-172-31-84-171  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  }]
Feb 27 16:30:15.758: INFO: 
Feb 27 16:30:15.758: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 27 16:30:16.763: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992811047s
Feb 27 16:30:17.768: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987895654s
Feb 27 16:30:18.772: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983029399s
Feb 27 16:30:19.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.978199956s
Feb 27 16:30:20.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.97405971s
Feb 27 16:30:21.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969687004s
Feb 27 16:30:22.790: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965747202s
Feb 27 16:30:23.794: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.761464ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9146 02/27/23 16:30:24.795
Feb 27 16:30:24.798: INFO: Scaling statefulset ss to 0
Feb 27 16:30:24.811: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 16:30:24.814: INFO: Deleting all statefulset in ns statefulset-9146
Feb 27 16:30:24.817: INFO: Scaling statefulset ss to 0
Feb 27 16:30:24.828: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 16:30:24.832: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:24.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9146" for this suite. 02/27/23 16:30:24.848
------------------------------
• [SLOW TEST] [51.095 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:29:33.759
    Feb 27 16:29:33.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename statefulset 02/27/23 16:29:33.759
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:29:33.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:29:33.775
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9146 02/27/23 16:29:33.778
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-9146 02/27/23 16:29:33.783
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9146 02/27/23 16:29:33.791
    Feb 27 16:29:33.794: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 16:29:43.801: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/27/23 16:29:43.801
    Feb 27 16:29:43.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 16:29:43.941: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 16:29:43.941: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 16:29:43.941: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 16:29:43.945: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 27 16:29:53.951: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 16:29:53.951: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 16:29:53.966: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
    Feb 27 16:29:53.966: INFO: ss-0  ip-172-31-42-40  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  }]
    Feb 27 16:29:53.966: INFO: 
    Feb 27 16:29:53.966: INFO: StatefulSet ss has not reached scale 3, at 1
    Feb 27 16:29:54.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996584897s
    Feb 27 16:29:55.974: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993382889s
    Feb 27 16:29:56.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98964565s
    Feb 27 16:29:57.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985030362s
    Feb 27 16:29:58.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980352596s
    Feb 27 16:29:59.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975633099s
    Feb 27 16:30:00.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97033094s
    Feb 27 16:30:02.002: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966416867s
    Feb 27 16:30:03.007: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.982217ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9146 02/27/23 16:30:04.007
    Feb 27 16:30:04.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 16:30:04.140: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 16:30:04.140: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 16:30:04.140: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 16:30:04.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 16:30:04.262: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 27 16:30:04.263: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 16:30:04.263: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 16:30:04.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 16:30:04.382: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 27 16:30:04.382: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 16:30:04.382: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 16:30:04.386: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:30:04.386: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 16:30:04.386: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/27/23 16:30:04.386
    Feb 27 16:30:04.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 16:30:04.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 16:30:04.517: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 16:30:04.517: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 16:30:04.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 16:30:04.626: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 16:30:04.626: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 16:30:04.626: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 16:30:04.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=statefulset-9146 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 16:30:04.730: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 16:30:04.730: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 16:30:04.730: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 16:30:04.730: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 16:30:04.733: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 27 16:30:14.742: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 16:30:14.742: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 16:30:14.742: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 16:30:14.755: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Feb 27 16:30:14.755: INFO: ss-0  ip-172-31-42-40   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  }]
    Feb 27 16:30:14.755: INFO: ss-1  ip-172-31-3-182   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  }]
    Feb 27 16:30:14.755: INFO: ss-2  ip-172-31-84-171  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  }]
    Feb 27 16:30:14.755: INFO: 
    Feb 27 16:30:14.755: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 27 16:30:15.758: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
    Feb 27 16:30:15.758: INFO: ss-0  ip-172-31-42-40   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:33 +0000 UTC  }]
    Feb 27 16:30:15.758: INFO: ss-2  ip-172-31-84-171  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:30:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 16:29:53 +0000 UTC  }]
    Feb 27 16:30:15.758: INFO: 
    Feb 27 16:30:15.758: INFO: StatefulSet ss has not reached scale 0, at 2
    Feb 27 16:30:16.763: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992811047s
    Feb 27 16:30:17.768: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987895654s
    Feb 27 16:30:18.772: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983029399s
    Feb 27 16:30:19.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.978199956s
    Feb 27 16:30:20.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.97405971s
    Feb 27 16:30:21.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969687004s
    Feb 27 16:30:22.790: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965747202s
    Feb 27 16:30:23.794: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.761464ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9146 02/27/23 16:30:24.795
    Feb 27 16:30:24.798: INFO: Scaling statefulset ss to 0
    Feb 27 16:30:24.811: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 16:30:24.814: INFO: Deleting all statefulset in ns statefulset-9146
    Feb 27 16:30:24.817: INFO: Scaling statefulset ss to 0
    Feb 27 16:30:24.828: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 16:30:24.832: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:24.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9146" for this suite. 02/27/23 16:30:24.848
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:24.854
Feb 27 16:30:24.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:30:24.855
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:24.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:24.874
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:30:24.877
Feb 27 16:30:24.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc" in namespace "projected-6288" to be "Succeeded or Failed"
Feb 27 16:30:24.891: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.23227ms
Feb 27 16:30:26.896: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011478513s
Feb 27 16:30:28.895: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010272945s
STEP: Saw pod success 02/27/23 16:30:28.895
Feb 27 16:30:28.895: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc" satisfied condition "Succeeded or Failed"
Feb 27 16:30:28.899: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc container client-container: <nil>
STEP: delete the pod 02/27/23 16:30:28.91
Feb 27 16:30:28.926: INFO: Waiting for pod downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc to disappear
Feb 27 16:30:28.930: INFO: Pod downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:28.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6288" for this suite. 02/27/23 16:30:28.934
------------------------------
• [4.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:24.854
    Feb 27 16:30:24.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:30:24.855
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:24.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:24.874
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:30:24.877
    Feb 27 16:30:24.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc" in namespace "projected-6288" to be "Succeeded or Failed"
    Feb 27 16:30:24.891: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.23227ms
    Feb 27 16:30:26.896: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011478513s
    Feb 27 16:30:28.895: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010272945s
    STEP: Saw pod success 02/27/23 16:30:28.895
    Feb 27 16:30:28.895: INFO: Pod "downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc" satisfied condition "Succeeded or Failed"
    Feb 27 16:30:28.899: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc container client-container: <nil>
    STEP: delete the pod 02/27/23 16:30:28.91
    Feb 27 16:30:28.926: INFO: Waiting for pod downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc to disappear
    Feb 27 16:30:28.930: INFO: Pod downwardapi-volume-c83022b4-a25e-47df-8da3-d52127ae9cfc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:28.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6288" for this suite. 02/27/23 16:30:28.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:28.943
Feb 27 16:30:28.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename services 02/27/23 16:30:28.944
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:28.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:28.972
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9037 02/27/23 16:30:28.977
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 16:30:28.986
STEP: creating service externalsvc in namespace services-9037 02/27/23 16:30:28.986
STEP: creating replication controller externalsvc in namespace services-9037 02/27/23 16:30:29.007
I0227 16:30:29.013929      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9037, replica count: 2
I0227 16:30:32.065158      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/27/23 16:30:32.068
Feb 27 16:30:32.082: INFO: Creating new exec pod
Feb 27 16:30:32.091: INFO: Waiting up to 5m0s for pod "execpodwk74h" in namespace "services-9037" to be "running"
Feb 27 16:30:32.096: INFO: Pod "execpodwk74h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345827ms
Feb 27 16:30:34.100: INFO: Pod "execpodwk74h": Phase="Running", Reason="", readiness=true. Elapsed: 2.008410748s
Feb 27 16:30:34.100: INFO: Pod "execpodwk74h" satisfied condition "running"
Feb 27 16:30:34.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9037 exec execpodwk74h -- /bin/sh -x -c nslookup clusterip-service.services-9037.svc.cluster.local'
Feb 27 16:30:34.248: INFO: stderr: "+ nslookup clusterip-service.services-9037.svc.cluster.local\n"
Feb 27 16:30:34.248: INFO: stdout: "Server:\t\t10.152.183.116\nAddress:\t10.152.183.116#53\n\nclusterip-service.services-9037.svc.cluster.local\tcanonical name = externalsvc.services-9037.svc.cluster.local.\nName:\texternalsvc.services-9037.svc.cluster.local\nAddress: 10.152.183.165\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9037, will wait for the garbage collector to delete the pods 02/27/23 16:30:34.248
Feb 27 16:30:34.309: INFO: Deleting ReplicationController externalsvc took: 6.752327ms
Feb 27 16:30:34.409: INFO: Terminating ReplicationController externalsvc pods took: 100.20588ms
Feb 27 16:30:36.526: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:36.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9037" for this suite. 02/27/23 16:30:36.542
------------------------------
• [SLOW TEST] [7.606 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:28.943
    Feb 27 16:30:28.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename services 02/27/23 16:30:28.944
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:28.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:28.972
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9037 02/27/23 16:30:28.977
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 16:30:28.986
    STEP: creating service externalsvc in namespace services-9037 02/27/23 16:30:28.986
    STEP: creating replication controller externalsvc in namespace services-9037 02/27/23 16:30:29.007
    I0227 16:30:29.013929      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9037, replica count: 2
    I0227 16:30:32.065158      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/27/23 16:30:32.068
    Feb 27 16:30:32.082: INFO: Creating new exec pod
    Feb 27 16:30:32.091: INFO: Waiting up to 5m0s for pod "execpodwk74h" in namespace "services-9037" to be "running"
    Feb 27 16:30:32.096: INFO: Pod "execpodwk74h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345827ms
    Feb 27 16:30:34.100: INFO: Pod "execpodwk74h": Phase="Running", Reason="", readiness=true. Elapsed: 2.008410748s
    Feb 27 16:30:34.100: INFO: Pod "execpodwk74h" satisfied condition "running"
    Feb 27 16:30:34.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=services-9037 exec execpodwk74h -- /bin/sh -x -c nslookup clusterip-service.services-9037.svc.cluster.local'
    Feb 27 16:30:34.248: INFO: stderr: "+ nslookup clusterip-service.services-9037.svc.cluster.local\n"
    Feb 27 16:30:34.248: INFO: stdout: "Server:\t\t10.152.183.116\nAddress:\t10.152.183.116#53\n\nclusterip-service.services-9037.svc.cluster.local\tcanonical name = externalsvc.services-9037.svc.cluster.local.\nName:\texternalsvc.services-9037.svc.cluster.local\nAddress: 10.152.183.165\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9037, will wait for the garbage collector to delete the pods 02/27/23 16:30:34.248
    Feb 27 16:30:34.309: INFO: Deleting ReplicationController externalsvc took: 6.752327ms
    Feb 27 16:30:34.409: INFO: Terminating ReplicationController externalsvc pods took: 100.20588ms
    Feb 27 16:30:36.526: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:36.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9037" for this suite. 02/27/23 16:30:36.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:36.549
Feb 27 16:30:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename cronjob 02/27/23 16:30:36.55
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:36.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:36.566
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/27/23 16:30:36.57
STEP: creating 02/27/23 16:30:36.57
STEP: getting 02/27/23 16:30:36.576
STEP: listing 02/27/23 16:30:36.579
STEP: watching 02/27/23 16:30:36.583
Feb 27 16:30:36.583: INFO: starting watch
STEP: cluster-wide listing 02/27/23 16:30:36.584
STEP: cluster-wide watching 02/27/23 16:30:36.588
Feb 27 16:30:36.588: INFO: starting watch
STEP: patching 02/27/23 16:30:36.589
STEP: updating 02/27/23 16:30:36.594
Feb 27 16:30:36.605: INFO: waiting for watch events with expected annotations
Feb 27 16:30:36.605: INFO: saw patched and updated annotations
STEP: patching /status 02/27/23 16:30:36.605
STEP: updating /status 02/27/23 16:30:36.61
STEP: get /status 02/27/23 16:30:36.617
STEP: deleting 02/27/23 16:30:36.62
STEP: deleting a collection 02/27/23 16:30:36.634
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:36.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4878" for this suite. 02/27/23 16:30:36.649
------------------------------
• [0.107 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:36.549
    Feb 27 16:30:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename cronjob 02/27/23 16:30:36.55
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:36.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:36.566
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/27/23 16:30:36.57
    STEP: creating 02/27/23 16:30:36.57
    STEP: getting 02/27/23 16:30:36.576
    STEP: listing 02/27/23 16:30:36.579
    STEP: watching 02/27/23 16:30:36.583
    Feb 27 16:30:36.583: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 16:30:36.584
    STEP: cluster-wide watching 02/27/23 16:30:36.588
    Feb 27 16:30:36.588: INFO: starting watch
    STEP: patching 02/27/23 16:30:36.589
    STEP: updating 02/27/23 16:30:36.594
    Feb 27 16:30:36.605: INFO: waiting for watch events with expected annotations
    Feb 27 16:30:36.605: INFO: saw patched and updated annotations
    STEP: patching /status 02/27/23 16:30:36.605
    STEP: updating /status 02/27/23 16:30:36.61
    STEP: get /status 02/27/23 16:30:36.617
    STEP: deleting 02/27/23 16:30:36.62
    STEP: deleting a collection 02/27/23 16:30:36.634
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:36.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4878" for this suite. 02/27/23 16:30:36.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:36.657
Feb 27 16:30:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename discovery 02/27/23 16:30:36.657
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:36.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:36.675
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/27/23 16:30:36.68
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb 27 16:30:37.076: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 27 16:30:37.078: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 27 16:30:37.078: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 27 16:30:37.078: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 27 16:30:37.078: INFO: Checking APIGroup: apps
Feb 27 16:30:37.079: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 27 16:30:37.079: INFO: Versions found [{apps/v1 v1}]
Feb 27 16:30:37.079: INFO: apps/v1 matches apps/v1
Feb 27 16:30:37.079: INFO: Checking APIGroup: events.k8s.io
Feb 27 16:30:37.080: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 27 16:30:37.080: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb 27 16:30:37.080: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 27 16:30:37.080: INFO: Checking APIGroup: authentication.k8s.io
Feb 27 16:30:37.081: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 27 16:30:37.081: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 27 16:30:37.081: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 27 16:30:37.081: INFO: Checking APIGroup: authorization.k8s.io
Feb 27 16:30:37.082: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 27 16:30:37.082: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 27 16:30:37.082: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 27 16:30:37.082: INFO: Checking APIGroup: autoscaling
Feb 27 16:30:37.084: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 27 16:30:37.084: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Feb 27 16:30:37.084: INFO: autoscaling/v2 matches autoscaling/v2
Feb 27 16:30:37.084: INFO: Checking APIGroup: batch
Feb 27 16:30:37.085: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 27 16:30:37.085: INFO: Versions found [{batch/v1 v1}]
Feb 27 16:30:37.085: INFO: batch/v1 matches batch/v1
Feb 27 16:30:37.085: INFO: Checking APIGroup: certificates.k8s.io
Feb 27 16:30:37.086: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 27 16:30:37.086: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 27 16:30:37.086: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 27 16:30:37.086: INFO: Checking APIGroup: networking.k8s.io
Feb 27 16:30:37.087: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 27 16:30:37.087: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 27 16:30:37.087: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 27 16:30:37.087: INFO: Checking APIGroup: policy
Feb 27 16:30:37.088: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 27 16:30:37.088: INFO: Versions found [{policy/v1 v1}]
Feb 27 16:30:37.088: INFO: policy/v1 matches policy/v1
Feb 27 16:30:37.088: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 27 16:30:37.089: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 27 16:30:37.089: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 27 16:30:37.089: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 27 16:30:37.089: INFO: Checking APIGroup: storage.k8s.io
Feb 27 16:30:37.091: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 27 16:30:37.091: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 27 16:30:37.091: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 27 16:30:37.091: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 27 16:30:37.092: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 27 16:30:37.092: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 27 16:30:37.092: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 27 16:30:37.092: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 27 16:30:37.093: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 27 16:30:37.093: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 27 16:30:37.093: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 27 16:30:37.093: INFO: Checking APIGroup: scheduling.k8s.io
Feb 27 16:30:37.094: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 27 16:30:37.094: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 27 16:30:37.094: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 27 16:30:37.094: INFO: Checking APIGroup: coordination.k8s.io
Feb 27 16:30:37.095: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 27 16:30:37.095: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 27 16:30:37.095: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 27 16:30:37.095: INFO: Checking APIGroup: node.k8s.io
Feb 27 16:30:37.096: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 27 16:30:37.096: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb 27 16:30:37.096: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 27 16:30:37.096: INFO: Checking APIGroup: discovery.k8s.io
Feb 27 16:30:37.097: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 27 16:30:37.097: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb 27 16:30:37.097: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 27 16:30:37.097: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 27 16:30:37.099: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Feb 27 16:30:37.099: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Feb 27 16:30:37.099: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Feb 27 16:30:37.099: INFO: Checking APIGroup: metrics.k8s.io
Feb 27 16:30:37.100: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 27 16:30:37.100: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 27 16:30:37.100: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:37.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3539" for this suite. 02/27/23 16:30:37.104
------------------------------
• [0.454 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:36.657
    Feb 27 16:30:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename discovery 02/27/23 16:30:36.657
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:36.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:36.675
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/27/23 16:30:36.68
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb 27 16:30:37.076: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb 27 16:30:37.078: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb 27 16:30:37.078: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb 27 16:30:37.078: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb 27 16:30:37.078: INFO: Checking APIGroup: apps
    Feb 27 16:30:37.079: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb 27 16:30:37.079: INFO: Versions found [{apps/v1 v1}]
    Feb 27 16:30:37.079: INFO: apps/v1 matches apps/v1
    Feb 27 16:30:37.079: INFO: Checking APIGroup: events.k8s.io
    Feb 27 16:30:37.080: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb 27 16:30:37.080: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb 27 16:30:37.080: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb 27 16:30:37.080: INFO: Checking APIGroup: authentication.k8s.io
    Feb 27 16:30:37.081: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb 27 16:30:37.081: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb 27 16:30:37.081: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb 27 16:30:37.081: INFO: Checking APIGroup: authorization.k8s.io
    Feb 27 16:30:37.082: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb 27 16:30:37.082: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb 27 16:30:37.082: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb 27 16:30:37.082: INFO: Checking APIGroup: autoscaling
    Feb 27 16:30:37.084: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb 27 16:30:37.084: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Feb 27 16:30:37.084: INFO: autoscaling/v2 matches autoscaling/v2
    Feb 27 16:30:37.084: INFO: Checking APIGroup: batch
    Feb 27 16:30:37.085: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb 27 16:30:37.085: INFO: Versions found [{batch/v1 v1}]
    Feb 27 16:30:37.085: INFO: batch/v1 matches batch/v1
    Feb 27 16:30:37.085: INFO: Checking APIGroup: certificates.k8s.io
    Feb 27 16:30:37.086: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb 27 16:30:37.086: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb 27 16:30:37.086: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb 27 16:30:37.086: INFO: Checking APIGroup: networking.k8s.io
    Feb 27 16:30:37.087: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb 27 16:30:37.087: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb 27 16:30:37.087: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb 27 16:30:37.087: INFO: Checking APIGroup: policy
    Feb 27 16:30:37.088: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb 27 16:30:37.088: INFO: Versions found [{policy/v1 v1}]
    Feb 27 16:30:37.088: INFO: policy/v1 matches policy/v1
    Feb 27 16:30:37.088: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb 27 16:30:37.089: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb 27 16:30:37.089: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb 27 16:30:37.089: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb 27 16:30:37.089: INFO: Checking APIGroup: storage.k8s.io
    Feb 27 16:30:37.091: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb 27 16:30:37.091: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb 27 16:30:37.091: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb 27 16:30:37.091: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb 27 16:30:37.092: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb 27 16:30:37.092: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb 27 16:30:37.092: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb 27 16:30:37.092: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb 27 16:30:37.093: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb 27 16:30:37.093: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb 27 16:30:37.093: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb 27 16:30:37.093: INFO: Checking APIGroup: scheduling.k8s.io
    Feb 27 16:30:37.094: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb 27 16:30:37.094: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb 27 16:30:37.094: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb 27 16:30:37.094: INFO: Checking APIGroup: coordination.k8s.io
    Feb 27 16:30:37.095: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb 27 16:30:37.095: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb 27 16:30:37.095: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb 27 16:30:37.095: INFO: Checking APIGroup: node.k8s.io
    Feb 27 16:30:37.096: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb 27 16:30:37.096: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb 27 16:30:37.096: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb 27 16:30:37.096: INFO: Checking APIGroup: discovery.k8s.io
    Feb 27 16:30:37.097: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb 27 16:30:37.097: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb 27 16:30:37.097: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb 27 16:30:37.097: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb 27 16:30:37.099: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Feb 27 16:30:37.099: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Feb 27 16:30:37.099: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Feb 27 16:30:37.099: INFO: Checking APIGroup: metrics.k8s.io
    Feb 27 16:30:37.100: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Feb 27 16:30:37.100: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Feb 27 16:30:37.100: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:37.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3539" for this suite. 02/27/23 16:30:37.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:37.111
Feb 27 16:30:37.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename secrets 02/27/23 16:30:37.112
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:37.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:37.134
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-48dcf5f8-9d88-42e5-ac90-50d555902ec9 02/27/23 16:30:37.137
STEP: Creating a pod to test consume secrets 02/27/23 16:30:37.142
Feb 27 16:30:37.151: INFO: Waiting up to 5m0s for pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972" in namespace "secrets-9485" to be "Succeeded or Failed"
Feb 27 16:30:37.154: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972": Phase="Pending", Reason="", readiness=false. Elapsed: 3.741019ms
Feb 27 16:30:39.158: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007781492s
Feb 27 16:30:41.159: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008301432s
STEP: Saw pod success 02/27/23 16:30:41.159
Feb 27 16:30:41.159: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972" satisfied condition "Succeeded or Failed"
Feb 27 16:30:41.162: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 16:30:41.169
Feb 27 16:30:41.182: INFO: Waiting for pod pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972 to disappear
Feb 27 16:30:41.184: INFO: Pod pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:41.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9485" for this suite. 02/27/23 16:30:41.188
------------------------------
• [4.085 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:37.111
    Feb 27 16:30:37.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename secrets 02/27/23 16:30:37.112
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:37.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:37.134
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-48dcf5f8-9d88-42e5-ac90-50d555902ec9 02/27/23 16:30:37.137
    STEP: Creating a pod to test consume secrets 02/27/23 16:30:37.142
    Feb 27 16:30:37.151: INFO: Waiting up to 5m0s for pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972" in namespace "secrets-9485" to be "Succeeded or Failed"
    Feb 27 16:30:37.154: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972": Phase="Pending", Reason="", readiness=false. Elapsed: 3.741019ms
    Feb 27 16:30:39.158: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007781492s
    Feb 27 16:30:41.159: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008301432s
    STEP: Saw pod success 02/27/23 16:30:41.159
    Feb 27 16:30:41.159: INFO: Pod "pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972" satisfied condition "Succeeded or Failed"
    Feb 27 16:30:41.162: INFO: Trying to get logs from node ip-172-31-42-40 pod pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 16:30:41.169
    Feb 27 16:30:41.182: INFO: Waiting for pod pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972 to disappear
    Feb 27 16:30:41.184: INFO: Pod pod-secrets-dc5eb220-1916-4061-9752-9f1aea911972 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:41.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9485" for this suite. 02/27/23 16:30:41.188
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:41.197
Feb 27 16:30:41.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename daemonsets 02/27/23 16:30:41.197
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:41.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:41.268
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Feb 27 16:30:41.291: INFO: Create a RollingUpdate DaemonSet
Feb 27 16:30:41.296: INFO: Check that daemon pods launch on every node of the cluster
Feb 27 16:30:41.303: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:41.303: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:41.306: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:30:41.306: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:30:42.310: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:42.310: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:42.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:30:42.313: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
Feb 27 16:30:43.311: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:43.311: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:43.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 16:30:43.314: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Feb 27 16:30:43.314: INFO: Update the DaemonSet to trigger a rollout
Feb 27 16:30:43.323: INFO: Updating DaemonSet daemon-set
Feb 27 16:30:46.339: INFO: Roll back the DaemonSet before rollout is complete
Feb 27 16:30:46.347: INFO: Updating DaemonSet daemon-set
Feb 27 16:30:46.348: INFO: Make sure DaemonSet rollback is complete
Feb 27 16:30:46.354: INFO: Wrong image for pod: daemon-set-kd6zf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Feb 27 16:30:46.354: INFO: Pod daemon-set-kd6zf is not available
Feb 27 16:30:46.358: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:46.358: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:47.367: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:47.367: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:48.363: INFO: Pod daemon-set-4tgcz is not available
Feb 27 16:30:48.367: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 16:30:48.368: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:30:48.374
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9730, will wait for the garbage collector to delete the pods 02/27/23 16:30:48.374
Feb 27 16:30:48.435: INFO: Deleting DaemonSet.extensions daemon-set took: 6.309064ms
Feb 27 16:30:48.535: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.911789ms
Feb 27 16:30:50.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 16:30:50.040: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 16:30:50.043: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37863"},"items":null}

Feb 27 16:30:50.045: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37863"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:50.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9730" for this suite. 02/27/23 16:30:50.062
------------------------------
• [SLOW TEST] [8.875 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:41.197
    Feb 27 16:30:41.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename daemonsets 02/27/23 16:30:41.197
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:41.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:41.268
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Feb 27 16:30:41.291: INFO: Create a RollingUpdate DaemonSet
    Feb 27 16:30:41.296: INFO: Check that daemon pods launch on every node of the cluster
    Feb 27 16:30:41.303: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:41.303: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:41.306: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:30:41.306: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:30:42.310: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:42.310: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:42.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:30:42.313: INFO: Node ip-172-31-3-182 is running 0 daemon pod, expected 1
    Feb 27 16:30:43.311: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:43.311: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:43.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 16:30:43.314: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Feb 27 16:30:43.314: INFO: Update the DaemonSet to trigger a rollout
    Feb 27 16:30:43.323: INFO: Updating DaemonSet daemon-set
    Feb 27 16:30:46.339: INFO: Roll back the DaemonSet before rollout is complete
    Feb 27 16:30:46.347: INFO: Updating DaemonSet daemon-set
    Feb 27 16:30:46.348: INFO: Make sure DaemonSet rollback is complete
    Feb 27 16:30:46.354: INFO: Wrong image for pod: daemon-set-kd6zf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Feb 27 16:30:46.354: INFO: Pod daemon-set-kd6zf is not available
    Feb 27 16:30:46.358: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:46.358: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:47.367: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:47.367: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:48.363: INFO: Pod daemon-set-4tgcz is not available
    Feb 27 16:30:48.367: INFO: DaemonSet pods can't tolerate node ip-172-31-34-33 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 16:30:48.368: INFO: DaemonSet pods can't tolerate node ip-172-31-87-225 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 16:30:48.374
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9730, will wait for the garbage collector to delete the pods 02/27/23 16:30:48.374
    Feb 27 16:30:48.435: INFO: Deleting DaemonSet.extensions daemon-set took: 6.309064ms
    Feb 27 16:30:48.535: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.911789ms
    Feb 27 16:30:50.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 16:30:50.040: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 16:30:50.043: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37863"},"items":null}

    Feb 27 16:30:50.045: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37863"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:50.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9730" for this suite. 02/27/23 16:30:50.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:50.072
Feb 27 16:30:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-runtime 02/27/23 16:30:50.073
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:50.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:50.094
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 02/27/23 16:30:50.096
STEP: wait for the container to reach Succeeded 02/27/23 16:30:50.104
STEP: get the container status 02/27/23 16:30:53.124
STEP: the container should be terminated 02/27/23 16:30:53.127
STEP: the termination message should be set 02/27/23 16:30:53.127
Feb 27 16:30:53.127: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/27/23 16:30:53.127
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 16:30:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9416" for this suite. 02/27/23 16:30:53.144
------------------------------
• [3.080 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:50.072
    Feb 27 16:30:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-runtime 02/27/23 16:30:50.073
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:50.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:50.094
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 02/27/23 16:30:50.096
    STEP: wait for the container to reach Succeeded 02/27/23 16:30:50.104
    STEP: get the container status 02/27/23 16:30:53.124
    STEP: the container should be terminated 02/27/23 16:30:53.127
    STEP: the termination message should be set 02/27/23 16:30:53.127
    Feb 27 16:30:53.127: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/27/23 16:30:53.127
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:30:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9416" for this suite. 02/27/23 16:30:53.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:30:53.152
Feb 27 16:30:53.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename dns 02/27/23 16:30:53.153
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:53.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:53.17
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/27/23 16:30:53.173
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4984.svc.cluster.local;sleep 1; done
 02/27/23 16:30:53.178
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4984.svc.cluster.local;sleep 1; done
 02/27/23 16:30:53.178
STEP: creating a pod to probe DNS 02/27/23 16:30:53.178
STEP: submitting the pod to kubernetes 02/27/23 16:30:53.178
Feb 27 16:30:53.197: INFO: Waiting up to 15m0s for pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4" in namespace "dns-4984" to be "running"
Feb 27 16:30:53.202: INFO: Pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893814ms
Feb 27 16:30:55.205: INFO: Pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008153401s
Feb 27 16:30:55.205: INFO: Pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4" satisfied condition "running"
STEP: retrieving the pod 02/27/23 16:30:55.205
STEP: looking for the results for each expected name from probers 02/27/23 16:30:55.21
Feb 27 16:30:55.214: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.218: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.226: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.229: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.233: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.236: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.239: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
Feb 27 16:30:55.239: INFO: Lookups using dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4984.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4984.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local jessie_udp@dns-test-service-2.dns-4984.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4984.svc.cluster.local]

Feb 27 16:31:00.269: INFO: DNS probes using dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4 succeeded

STEP: deleting the pod 02/27/23 16:31:00.269
STEP: deleting the test headless service 02/27/23 16:31:00.283
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 16:31:00.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4984" for this suite. 02/27/23 16:31:00.304
------------------------------
• [SLOW TEST] [7.157 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:30:53.152
    Feb 27 16:30:53.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename dns 02/27/23 16:30:53.153
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:30:53.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:30:53.17
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/27/23 16:30:53.173
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4984.svc.cluster.local;sleep 1; done
     02/27/23 16:30:53.178
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4984.svc.cluster.local;sleep 1; done
     02/27/23 16:30:53.178
    STEP: creating a pod to probe DNS 02/27/23 16:30:53.178
    STEP: submitting the pod to kubernetes 02/27/23 16:30:53.178
    Feb 27 16:30:53.197: INFO: Waiting up to 15m0s for pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4" in namespace "dns-4984" to be "running"
    Feb 27 16:30:53.202: INFO: Pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893814ms
    Feb 27 16:30:55.205: INFO: Pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008153401s
    Feb 27 16:30:55.205: INFO: Pod "dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 16:30:55.205
    STEP: looking for the results for each expected name from probers 02/27/23 16:30:55.21
    Feb 27 16:30:55.214: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.218: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.226: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.229: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.233: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.236: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.239: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4984.svc.cluster.local from pod dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4: the server could not find the requested resource (get pods dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4)
    Feb 27 16:30:55.239: INFO: Lookups using dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4984.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4984.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4984.svc.cluster.local jessie_udp@dns-test-service-2.dns-4984.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4984.svc.cluster.local]

    Feb 27 16:31:00.269: INFO: DNS probes using dns-4984/dns-test-eff6447d-3e77-4942-b5f4-d7fcf04ee9a4 succeeded

    STEP: deleting the pod 02/27/23 16:31:00.269
    STEP: deleting the test headless service 02/27/23 16:31:00.283
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:31:00.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4984" for this suite. 02/27/23 16:31:00.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:31:00.311
Feb 27 16:31:00.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:31:00.312
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:00.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:00.329
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Feb 27 16:31:00.336: INFO: Got root ca configmap in namespace "svcaccounts-6455"
Feb 27 16:31:00.342: INFO: Deleted root ca configmap in namespace "svcaccounts-6455"
STEP: waiting for a new root ca configmap created 02/27/23 16:31:00.842
Feb 27 16:31:00.846: INFO: Recreated root ca configmap in namespace "svcaccounts-6455"
Feb 27 16:31:00.852: INFO: Updated root ca configmap in namespace "svcaccounts-6455"
STEP: waiting for the root ca configmap reconciled 02/27/23 16:31:01.352
Feb 27 16:31:01.355: INFO: Reconciled root ca configmap in namespace "svcaccounts-6455"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 16:31:01.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6455" for this suite. 02/27/23 16:31:01.359
------------------------------
• [1.055 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:31:00.311
    Feb 27 16:31:00.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 16:31:00.312
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:00.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:00.329
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Feb 27 16:31:00.336: INFO: Got root ca configmap in namespace "svcaccounts-6455"
    Feb 27 16:31:00.342: INFO: Deleted root ca configmap in namespace "svcaccounts-6455"
    STEP: waiting for a new root ca configmap created 02/27/23 16:31:00.842
    Feb 27 16:31:00.846: INFO: Recreated root ca configmap in namespace "svcaccounts-6455"
    Feb 27 16:31:00.852: INFO: Updated root ca configmap in namespace "svcaccounts-6455"
    STEP: waiting for the root ca configmap reconciled 02/27/23 16:31:01.352
    Feb 27 16:31:01.355: INFO: Reconciled root ca configmap in namespace "svcaccounts-6455"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:31:01.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6455" for this suite. 02/27/23 16:31:01.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:31:01.366
Feb 27 16:31:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename replicaset 02/27/23 16:31:01.367
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:01.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:01.386
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/27/23 16:31:01.389
Feb 27 16:31:01.399: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-8347" to be "running and ready"
Feb 27 16:31:01.401: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622785ms
Feb 27 16:31:01.401: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:31:03.406: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349978s
Feb 27 16:31:03.406: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb 27 16:31:03.406: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/27/23 16:31:03.409
STEP: Then the orphan pod is adopted 02/27/23 16:31:03.415
STEP: When the matched label of one of its pods change 02/27/23 16:31:04.423
Feb 27 16:31:04.426: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/27/23 16:31:04.439
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 16:31:05.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8347" for this suite. 02/27/23 16:31:05.45
------------------------------
• [4.091 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:31:01.366
    Feb 27 16:31:01.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename replicaset 02/27/23 16:31:01.367
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:01.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:01.386
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/27/23 16:31:01.389
    Feb 27 16:31:01.399: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-8347" to be "running and ready"
    Feb 27 16:31:01.401: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622785ms
    Feb 27 16:31:01.401: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:31:03.406: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349978s
    Feb 27 16:31:03.406: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb 27 16:31:03.406: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/27/23 16:31:03.409
    STEP: Then the orphan pod is adopted 02/27/23 16:31:03.415
    STEP: When the matched label of one of its pods change 02/27/23 16:31:04.423
    Feb 27 16:31:04.426: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/27/23 16:31:04.439
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:31:05.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8347" for this suite. 02/27/23 16:31:05.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:31:05.459
Feb 27 16:31:05.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename init-container 02/27/23 16:31:05.46
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:05.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:05.477
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 02/27/23 16:31:05.481
Feb 27 16:31:05.481: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:31:09.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3457" for this suite. 02/27/23 16:31:09.028
------------------------------
• [3.577 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:31:05.459
    Feb 27 16:31:05.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename init-container 02/27/23 16:31:05.46
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:05.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:05.477
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 02/27/23 16:31:05.481
    Feb 27 16:31:05.481: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:31:09.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3457" for this suite. 02/27/23 16:31:09.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:31:09.036
Feb 27 16:31:09.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 16:31:09.037
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:09.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:09.059
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb 27 16:31:09.062: INFO: Creating deployment "webserver-deployment"
Feb 27 16:31:09.067: INFO: Waiting for observed generation 1
Feb 27 16:31:11.076: INFO: Waiting for all required pods to come up
Feb 27 16:31:11.083: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/27/23 16:31:11.083
Feb 27 16:31:11.083: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-28vcb" in namespace "deployment-4120" to be "running"
Feb 27 16:31:11.083: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-czrc4" in namespace "deployment-4120" to be "running"
Feb 27 16:31:11.087: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498666ms
Feb 27 16:31:11.159: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4": Phase="Pending", Reason="", readiness=false. Elapsed: 75.644323ms
Feb 27 16:31:13.092: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008420946s
Feb 27 16:31:13.092: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb" satisfied condition "running"
Feb 27 16:31:13.163: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.080223322s
Feb 27 16:31:13.164: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4" satisfied condition "running"
Feb 27 16:31:13.164: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 27 16:31:13.171: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 27 16:31:13.183: INFO: Updating deployment webserver-deployment
Feb 27 16:31:13.183: INFO: Waiting for observed generation 2
Feb 27 16:31:15.191: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 27 16:31:15.195: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 27 16:31:15.198: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 27 16:31:15.208: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 27 16:31:15.208: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 27 16:31:15.212: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 27 16:31:15.218: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 27 16:31:15.218: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 27 16:31:15.230: INFO: Updating deployment webserver-deployment
Feb 27 16:31:15.230: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 27 16:31:15.241: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 27 16:31:17.249: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 16:31:17.257: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4120  032b620c-9243-4e21-9c9d-7d51d48cc70d 38521 3 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052590d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 16:31:15 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-02-27 16:31:15 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 27 16:31:17.261: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4120  16f9c91f-1a51-42cb-8171-5e77c23b9035 38517 3 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 032b620c-9243-4e21-9c9d-7d51d48cc70d 0xc007d25807 0xc007d25808}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"032b620c-9243-4e21-9c9d-7d51d48cc70d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d258a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 16:31:17.261: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 27 16:31:17.261: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4120  cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 38520 3 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 032b620c-9243-4e21-9c9d-7d51d48cc70d 0xc007d25717 0xc007d25718}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"032b620c-9243-4e21-9c9d-7d51d48cc70d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d257a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 27 16:31:17.266: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-28vcb webserver-deployment-7f5969cbc7- deployment-4120  13fb9e32-0066-41dc-b57f-38f040ecd21f 38285 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc007d25d47 0xc007d25d48}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fm7fz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fm7fz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.178,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6d22c08d6bf1563aa9afb4372cc5ca15b30191468537453e2e58a70e65119348,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-56vtp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-56vtp webserver-deployment-7f5969cbc7- deployment-4120  acdf7655-b1f9-4d92-a543-27c6e570ede4 38280 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc007d25f57 0xc007d25f58}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xswzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xswzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.130,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e2c56495e2c2c04494b0b9343fddbc74a6d24a26240d58b926562acf0251841c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-67k2v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67k2v webserver-deployment-7f5969cbc7- deployment-4120  ac9c4f11-c1c0-4eaa-a56b-f59c9c1f2c3a 38480 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda157 0xc004fda158}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q88j7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q88j7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-6wsc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6wsc4 webserver-deployment-7f5969cbc7- deployment-4120  16e2768a-2cb1-40ec-adc4-e3c4431fb50b 38523 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda2d0 0xc004fda2d1}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47sjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47sjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-85q9c" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-85q9c webserver-deployment-7f5969cbc7- deployment-4120  dc39ee5e-0b98-492f-8f7b-12ef27d2132a 38242 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda4a7 0xc004fda4a8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvwcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvwcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.151,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f94f05d43042463e644054bd2bfc57b3ea4bdb638af92fe1882d9028ae41b60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-c7dmb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c7dmb webserver-deployment-7f5969cbc7- deployment-4120  2bb7ea55-3444-407a-87b7-86c93536335a 38481 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda697 0xc004fda698}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69kmf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69kmf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-ctttc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ctttc webserver-deployment-7f5969cbc7- deployment-4120  a8f3edf3-2074-4975-b6c7-0b5da64dec38 38514 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda820 0xc004fda821}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6zng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6zng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-czrc4 webserver-deployment-7f5969cbc7- deployment-4120  a66585a0-1ec3-4cdb-88c0-415e21d29b86 38295 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda9e7 0xc004fda9e8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tlc75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tlc75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.160,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://82bec45c312f93c0bf675b3a379b53031867ad33bcb655a9fb45bf0937f82f51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-dngfb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dngfb webserver-deployment-7f5969cbc7- deployment-4120  114d1fd7-473f-418a-8a63-b09f01c35d46 38275 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdabf7 0xc004fdabf8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24446,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24446,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.156,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3f5ce27fbafebcae4b4722a521030f4203c79efdea641a8701f509d5c0f77288,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-l2ckp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l2ckp webserver-deployment-7f5969cbc7- deployment-4120  e65441d8-93f3-46bf-a69b-61d18dc2e14b 38492 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdade7 0xc004fdade8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nj296,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nj296,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-lgkdp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lgkdp webserver-deployment-7f5969cbc7- deployment-4120  914e5509-bd14-469a-9cce-c1b16abfe32f 38490 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdaf60 0xc004fdaf61}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggf8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggf8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-n8bdl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n8bdl webserver-deployment-7f5969cbc7- deployment-4120  b6c6c595-6653-46b8-9525-2b52ed2c14cb 38486 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb137 0xc004fdb138}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjxjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjxjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-pvlhm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pvlhm webserver-deployment-7f5969cbc7- deployment-4120  f966b04f-721c-4de7-9212-27be33a8e2aa 38272 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb2b0 0xc004fdb2b1}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zd7ml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zd7ml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.147,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7c14ec3b9047011d60ad87dae4cec8b4fbca6efb08dc0452d4e1fe6c136bf1b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-pwrtr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pwrtr webserver-deployment-7f5969cbc7- deployment-4120  67339277-f42b-4b7c-b008-328144d8fd8d 38504 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb497 0xc004fdb498}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnrkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnrkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-q85wn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q85wn webserver-deployment-7f5969cbc7- deployment-4120  f98a9aae-459c-45b4-824e-9d23d8752586 38277 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb667 0xc004fdb668}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fx99x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fx99x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.180,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://87fb25147e1576437486c101c551993252aaefce399874f60ed75ec31c0e5c4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-rqr88" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rqr88 webserver-deployment-7f5969cbc7- deployment-4120  f4da911a-893c-43e9-835b-42dd5855599c 38611 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb867 0xc004fdb868}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvdjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvdjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-s2mtq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s2mtq webserver-deployment-7f5969cbc7- deployment-4120  a1199a73-a518-4da2-b214-f8e3bc893d44 38460 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdba37 0xc004fdba38}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjrqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjrqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-stjb5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-stjb5 webserver-deployment-7f5969cbc7- deployment-4120  0ac046c5-3e94-4a5b-a1f4-5e2c837bc717 38497 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdbc07 0xc004fdbc08}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72h9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72h9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-t76kg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t76kg webserver-deployment-7f5969cbc7- deployment-4120  1556c10f-dcf2-46bb-b508-32769c8507e8 38281 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdbdd7 0xc004fdbdd8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fr7xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fr7xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.179,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a195ff72ca407b2258c9bbc8180d41a484888e60f033ba83876af219b0c4eacb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.270: INFO: Pod "webserver-deployment-7f5969cbc7-w9w9h" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w9w9h webserver-deployment-7f5969cbc7- deployment-4120  4ab72303-23bc-49a3-b5ac-d2bad19083cb 38466 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdbfc7 0xc004fdbfc8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b455w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b455w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-89s6t" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-89s6t webserver-deployment-d9f79cb5- deployment-4120  704b7745-7bcb-4d5a-a656-e6ce5d4d2c0e 38464 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c197 0xc00531c198}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9j4qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9j4qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-b5rxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b5rxs webserver-deployment-d9f79cb5- deployment-4120  f8da3dc3-5141-44a8-be9d-182aab4a994a 38509 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c2ff 0xc00531c310}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhk6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhk6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-dzk7t" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dzk7t webserver-deployment-d9f79cb5- deployment-4120  dcc94f33-3a81-46cc-a4ef-53b736ecee22 38420 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c4f7 0xc00531c4f8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jq7gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jq7gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.154,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-fjgrz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fjgrz webserver-deployment-d9f79cb5- deployment-4120  ee5537d1-d38a-46e0-b442-3efae6c6815d 38423 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c727 0xc00531c728}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2b4zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2b4zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.145,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-fzqrx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fzqrx webserver-deployment-d9f79cb5- deployment-4120  51de174c-d73d-450d-82c1-b82f9f40a230 38391 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c967 0xc00531c968}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2m5z6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2m5z6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.181,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-g5mnq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g5mnq webserver-deployment-d9f79cb5- deployment-4120  2818fa04-722c-4ff2-adde-a75bf39de6a1 38554 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531cba7 0xc00531cba8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m5mfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m5mfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-gh7h2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gh7h2 webserver-deployment-d9f79cb5- deployment-4120  63161370-3f9b-4a56-860e-2dd65776a45b 38482 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531cd97 0xc00531cd98}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c446s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c446s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-lc2gd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lc2gd webserver-deployment-d9f79cb5- deployment-4120  3193fd6b-d3f6-4042-8b39-6e16e3c77d88 38462 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531cf97 0xc00531cf98}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfgq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfgq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-ppzjd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ppzjd webserver-deployment-d9f79cb5- deployment-4120  c00f1db0-66d9-4bb7-bfb2-8a3322ae9f09 38495 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d0ff 0xc00531d110}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8k77r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8k77r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-q72lr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-q72lr webserver-deployment-d9f79cb5- deployment-4120  bc0ab535-468b-4ac9-902b-c15115551922 38483 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d27f 0xc00531d290}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmclm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmclm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.273: INFO: Pod "webserver-deployment-d9f79cb5-rjlnz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rjlnz webserver-deployment-d9f79cb5- deployment-4120  ed683c9a-35c2-4283-affd-4c2763f7bf85 38433 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d3ef 0xc00531d400}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhlsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhlsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.153,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.273: INFO: Pod "webserver-deployment-d9f79cb5-vm4jr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vm4jr webserver-deployment-d9f79cb5- deployment-4120  e9764c86-611e-41c5-9e25-61dc7eeaaff4 38491 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d627 0xc00531d628}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bq2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bq2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 16:31:17.273: INFO: Pod "webserver-deployment-d9f79cb5-zgg5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zgg5s webserver-deployment-d9f79cb5- deployment-4120  04940ef0-4b01-4358-91fa-e789db2d4f14 38389 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d7af 0xc00531d7c0}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8qrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8qrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.151,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 16:31:17.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4120" for this suite. 02/27/23 16:31:17.277
------------------------------
• [SLOW TEST] [8.248 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:31:09.036
    Feb 27 16:31:09.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 16:31:09.037
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:09.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:09.059
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb 27 16:31:09.062: INFO: Creating deployment "webserver-deployment"
    Feb 27 16:31:09.067: INFO: Waiting for observed generation 1
    Feb 27 16:31:11.076: INFO: Waiting for all required pods to come up
    Feb 27 16:31:11.083: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/27/23 16:31:11.083
    Feb 27 16:31:11.083: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-28vcb" in namespace "deployment-4120" to be "running"
    Feb 27 16:31:11.083: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-czrc4" in namespace "deployment-4120" to be "running"
    Feb 27 16:31:11.087: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498666ms
    Feb 27 16:31:11.159: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4": Phase="Pending", Reason="", readiness=false. Elapsed: 75.644323ms
    Feb 27 16:31:13.092: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008420946s
    Feb 27 16:31:13.092: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb" satisfied condition "running"
    Feb 27 16:31:13.163: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.080223322s
    Feb 27 16:31:13.164: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4" satisfied condition "running"
    Feb 27 16:31:13.164: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb 27 16:31:13.171: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb 27 16:31:13.183: INFO: Updating deployment webserver-deployment
    Feb 27 16:31:13.183: INFO: Waiting for observed generation 2
    Feb 27 16:31:15.191: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb 27 16:31:15.195: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb 27 16:31:15.198: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 16:31:15.208: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb 27 16:31:15.208: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb 27 16:31:15.212: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 16:31:15.218: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb 27 16:31:15.218: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb 27 16:31:15.230: INFO: Updating deployment webserver-deployment
    Feb 27 16:31:15.230: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 16:31:15.241: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb 27 16:31:17.249: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 16:31:17.257: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4120  032b620c-9243-4e21-9c9d-7d51d48cc70d 38521 3 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052590d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 16:31:15 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-02-27 16:31:15 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Feb 27 16:31:17.261: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4120  16f9c91f-1a51-42cb-8171-5e77c23b9035 38517 3 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 032b620c-9243-4e21-9c9d-7d51d48cc70d 0xc007d25807 0xc007d25808}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"032b620c-9243-4e21-9c9d-7d51d48cc70d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d258a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 16:31:17.261: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb 27 16:31:17.261: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4120  cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 38520 3 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 032b620c-9243-4e21-9c9d-7d51d48cc70d 0xc007d25717 0xc007d25718}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"032b620c-9243-4e21-9c9d-7d51d48cc70d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d257a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 16:31:17.266: INFO: Pod "webserver-deployment-7f5969cbc7-28vcb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-28vcb webserver-deployment-7f5969cbc7- deployment-4120  13fb9e32-0066-41dc-b57f-38f040ecd21f 38285 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc007d25d47 0xc007d25d48}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fm7fz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fm7fz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.178,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6d22c08d6bf1563aa9afb4372cc5ca15b30191468537453e2e58a70e65119348,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-56vtp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-56vtp webserver-deployment-7f5969cbc7- deployment-4120  acdf7655-b1f9-4d92-a543-27c6e570ede4 38280 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc007d25f57 0xc007d25f58}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xswzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xswzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.130,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e2c56495e2c2c04494b0b9343fddbc74a6d24a26240d58b926562acf0251841c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-67k2v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67k2v webserver-deployment-7f5969cbc7- deployment-4120  ac9c4f11-c1c0-4eaa-a56b-f59c9c1f2c3a 38480 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda157 0xc004fda158}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q88j7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q88j7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-6wsc4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6wsc4 webserver-deployment-7f5969cbc7- deployment-4120  16e2768a-2cb1-40ec-adc4-e3c4431fb50b 38523 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda2d0 0xc004fda2d1}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47sjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47sjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-85q9c" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-85q9c webserver-deployment-7f5969cbc7- deployment-4120  dc39ee5e-0b98-492f-8f7b-12ef27d2132a 38242 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda4a7 0xc004fda4a8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvwcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvwcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.151,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f94f05d43042463e644054bd2bfc57b3ea4bdb638af92fe1882d9028ae41b60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-c7dmb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c7dmb webserver-deployment-7f5969cbc7- deployment-4120  2bb7ea55-3444-407a-87b7-86c93536335a 38481 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda697 0xc004fda698}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69kmf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69kmf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.267: INFO: Pod "webserver-deployment-7f5969cbc7-ctttc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ctttc webserver-deployment-7f5969cbc7- deployment-4120  a8f3edf3-2074-4975-b6c7-0b5da64dec38 38514 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda820 0xc004fda821}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6zng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6zng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-czrc4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-czrc4 webserver-deployment-7f5969cbc7- deployment-4120  a66585a0-1ec3-4cdb-88c0-415e21d29b86 38295 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fda9e7 0xc004fda9e8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tlc75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tlc75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.160,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://82bec45c312f93c0bf675b3a379b53031867ad33bcb655a9fb45bf0937f82f51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-dngfb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dngfb webserver-deployment-7f5969cbc7- deployment-4120  114d1fd7-473f-418a-8a63-b09f01c35d46 38275 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdabf7 0xc004fdabf8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24446,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24446,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.156,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3f5ce27fbafebcae4b4722a521030f4203c79efdea641a8701f509d5c0f77288,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-l2ckp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l2ckp webserver-deployment-7f5969cbc7- deployment-4120  e65441d8-93f3-46bf-a69b-61d18dc2e14b 38492 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdade7 0xc004fdade8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nj296,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nj296,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-lgkdp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lgkdp webserver-deployment-7f5969cbc7- deployment-4120  914e5509-bd14-469a-9cce-c1b16abfe32f 38490 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdaf60 0xc004fdaf61}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggf8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggf8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.268: INFO: Pod "webserver-deployment-7f5969cbc7-n8bdl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n8bdl webserver-deployment-7f5969cbc7- deployment-4120  b6c6c595-6653-46b8-9525-2b52ed2c14cb 38486 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb137 0xc004fdb138}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjxjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjxjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-pvlhm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pvlhm webserver-deployment-7f5969cbc7- deployment-4120  f966b04f-721c-4de7-9212-27be33a8e2aa 38272 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb2b0 0xc004fdb2b1}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zd7ml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zd7ml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.147,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7c14ec3b9047011d60ad87dae4cec8b4fbca6efb08dc0452d4e1fe6c136bf1b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-pwrtr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pwrtr webserver-deployment-7f5969cbc7- deployment-4120  67339277-f42b-4b7c-b008-328144d8fd8d 38504 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb497 0xc004fdb498}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnrkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnrkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-q85wn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q85wn webserver-deployment-7f5969cbc7- deployment-4120  f98a9aae-459c-45b4-824e-9d23d8752586 38277 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb667 0xc004fdb668}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fx99x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fx99x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.180,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://87fb25147e1576437486c101c551993252aaefce399874f60ed75ec31c0e5c4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-rqr88" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rqr88 webserver-deployment-7f5969cbc7- deployment-4120  f4da911a-893c-43e9-835b-42dd5855599c 38611 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdb867 0xc004fdb868}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvdjf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvdjf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-s2mtq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s2mtq webserver-deployment-7f5969cbc7- deployment-4120  a1199a73-a518-4da2-b214-f8e3bc893d44 38460 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdba37 0xc004fdba38}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjrqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjrqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-stjb5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-stjb5 webserver-deployment-7f5969cbc7- deployment-4120  0ac046c5-3e94-4a5b-a1f4-5e2c837bc717 38497 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdbc07 0xc004fdbc08}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72h9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72h9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.269: INFO: Pod "webserver-deployment-7f5969cbc7-t76kg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t76kg webserver-deployment-7f5969cbc7- deployment-4120  1556c10f-dcf2-46bb-b508-32769c8507e8 38281 0 2023-02-27 16:31:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdbdd7 0xc004fdbdd8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fr7xp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fr7xp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.179,StartTime:2023-02-27 16:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:31:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a195ff72ca407b2258c9bbc8180d41a484888e60f033ba83876af219b0c4eacb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.270: INFO: Pod "webserver-deployment-7f5969cbc7-w9w9h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w9w9h webserver-deployment-7f5969cbc7- deployment-4120  4ab72303-23bc-49a3-b5ac-d2bad19083cb 38466 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45 0xc004fdbfc7 0xc004fdbfc8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf2c4d3d-57aa-4eaa-b109-ae01b1f78b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b455w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b455w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-89s6t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-89s6t webserver-deployment-d9f79cb5- deployment-4120  704b7745-7bcb-4d5a-a656-e6ce5d4d2c0e 38464 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c197 0xc00531c198}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9j4qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9j4qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-b5rxs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b5rxs webserver-deployment-d9f79cb5- deployment-4120  f8da3dc3-5141-44a8-be9d-182aab4a994a 38509 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c2ff 0xc00531c310}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhk6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhk6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-dzk7t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dzk7t webserver-deployment-d9f79cb5- deployment-4120  dcc94f33-3a81-46cc-a4ef-53b736ecee22 38420 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c4f7 0xc00531c4f8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jq7gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jq7gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.154,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.271: INFO: Pod "webserver-deployment-d9f79cb5-fjgrz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fjgrz webserver-deployment-d9f79cb5- deployment-4120  ee5537d1-d38a-46e0-b442-3efae6c6815d 38423 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c727 0xc00531c728}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.212.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2b4zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2b4zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.182,PodIP:192.168.212.145,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.212.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-fzqrx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fzqrx webserver-deployment-d9f79cb5- deployment-4120  51de174c-d73d-450d-82c1-b82f9f40a230 38391 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531c967 0xc00531c968}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2m5z6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2m5z6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:192.168.214.181,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-g5mnq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-g5mnq webserver-deployment-d9f79cb5- deployment-4120  2818fa04-722c-4ff2-adde-a75bf39de6a1 38554 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531cba7 0xc00531cba8}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m5mfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m5mfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-gh7h2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gh7h2 webserver-deployment-d9f79cb5- deployment-4120  63161370-3f9b-4a56-860e-2dd65776a45b 38482 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531cd97 0xc00531cd98}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c446s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c446s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-171,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.171,PodIP:,StartTime:2023-02-27 16:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-lc2gd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lc2gd webserver-deployment-d9f79cb5- deployment-4120  3193fd6b-d3f6-4042-8b39-6e16e3c77d88 38462 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531cf97 0xc00531cf98}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfgq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfgq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-ppzjd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ppzjd webserver-deployment-d9f79cb5- deployment-4120  c00f1db0-66d9-4bb7-bfb2-8a3322ae9f09 38495 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d0ff 0xc00531d110}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8k77r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8k77r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.272: INFO: Pod "webserver-deployment-d9f79cb5-q72lr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-q72lr webserver-deployment-d9f79cb5- deployment-4120  bc0ab535-468b-4ac9-902b-c15115551922 38483 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d27f 0xc00531d290}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmclm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmclm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.273: INFO: Pod "webserver-deployment-d9f79cb5-rjlnz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rjlnz webserver-deployment-d9f79cb5- deployment-4120  ed683c9a-35c2-4283-affd-4c2763f7bf85 38433 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d3ef 0xc00531d400}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhlsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhlsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.153,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.273: INFO: Pod "webserver-deployment-d9f79cb5-vm4jr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vm4jr webserver-deployment-d9f79cb5- deployment-4120  e9764c86-611e-41c5-9e25-61dc7eeaaff4 38491 0 2023-02-27 16:31:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d627 0xc00531d628}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bq2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bq2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-182,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 16:31:17.273: INFO: Pod "webserver-deployment-d9f79cb5-zgg5s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zgg5s webserver-deployment-d9f79cb5- deployment-4120  04940ef0-4b01-4358-91fa-e789db2d4f14 38389 0 2023-02-27 16:31:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 16f9c91f-1a51-42cb-8171-5e77c23b9035 0xc00531d7af 0xc00531d7c0}] [] [{kube-controller-manager Update v1 2023-02-27 16:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16f9c91f-1a51-42cb-8171-5e77c23b9035\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8qrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8qrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:31:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.151,StartTime:2023-02-27 16:31:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:31:17.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4120" for this suite. 02/27/23 16:31:17.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:31:17.287
Feb 27 16:31:17.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:31:17.287
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:17.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:17.305
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 16:31:17.408
Feb 27 16:31:17.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-7031 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Feb 27 16:31:17.546: INFO: stderr: ""
Feb 27 16:31:17.546: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 16:31:17.546
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Feb 27 16:31:17.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-7031 delete pods e2e-test-httpd-pod'
Feb 27 16:31:20.823: INFO: stderr: ""
Feb 27 16:31:20.823: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:31:20.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7031" for this suite. 02/27/23 16:31:20.828
------------------------------
• [3.548 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:31:17.287
    Feb 27 16:31:17.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:31:17.287
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:17.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:17.305
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 16:31:17.408
    Feb 27 16:31:17.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-7031 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Feb 27 16:31:17.546: INFO: stderr: ""
    Feb 27 16:31:17.546: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 16:31:17.546
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Feb 27 16:31:17.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-7031 delete pods e2e-test-httpd-pod'
    Feb 27 16:31:20.823: INFO: stderr: ""
    Feb 27 16:31:20.823: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:31:20.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7031" for this suite. 02/27/23 16:31:20.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:31:20.835
Feb 27 16:31:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:31:20.835
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:20.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:20.853
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-4f0d7bdd-5b0b-48e3-9759-7af9a016bc4a 02/27/23 16:31:20.86
STEP: Creating secret with name s-test-opt-upd-bf8943b7-9190-4ce9-8997-16d2247c2083 02/27/23 16:31:20.864
STEP: Creating the pod 02/27/23 16:31:20.868
Feb 27 16:31:20.879: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace" in namespace "projected-977" to be "running and ready"
Feb 27 16:31:20.884: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Pending", Reason="", readiness=false. Elapsed: 5.441513ms
Feb 27 16:31:20.884: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:31:22.889: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010369254s
Feb 27 16:31:22.889: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:31:24.889: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010378493s
Feb 27 16:31:24.889: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Pending, waiting for it to be Running (with Ready = true)
Feb 27 16:31:26.890: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Running", Reason="", readiness=true. Elapsed: 6.011345136s
Feb 27 16:31:26.890: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Running (Ready = true)
Feb 27 16:31:26.890: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-4f0d7bdd-5b0b-48e3-9759-7af9a016bc4a 02/27/23 16:31:26.91
STEP: Updating secret s-test-opt-upd-bf8943b7-9190-4ce9-8997-16d2247c2083 02/27/23 16:31:26.917
STEP: Creating secret with name s-test-opt-create-0b4d9f1b-5373-4b16-b008-5db6457a1b20 02/27/23 16:31:26.921
STEP: waiting to observe update in volume 02/27/23 16:31:26.926
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 16:32:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-977" for this suite. 02/27/23 16:32:39.22
------------------------------
• [SLOW TEST] [78.392 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:31:20.835
    Feb 27 16:31:20.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:31:20.835
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:31:20.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:31:20.853
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-4f0d7bdd-5b0b-48e3-9759-7af9a016bc4a 02/27/23 16:31:20.86
    STEP: Creating secret with name s-test-opt-upd-bf8943b7-9190-4ce9-8997-16d2247c2083 02/27/23 16:31:20.864
    STEP: Creating the pod 02/27/23 16:31:20.868
    Feb 27 16:31:20.879: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace" in namespace "projected-977" to be "running and ready"
    Feb 27 16:31:20.884: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Pending", Reason="", readiness=false. Elapsed: 5.441513ms
    Feb 27 16:31:20.884: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:31:22.889: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010369254s
    Feb 27 16:31:22.889: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:31:24.889: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010378493s
    Feb 27 16:31:24.889: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 16:31:26.890: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace": Phase="Running", Reason="", readiness=true. Elapsed: 6.011345136s
    Feb 27 16:31:26.890: INFO: The phase of Pod pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace is Running (Ready = true)
    Feb 27 16:31:26.890: INFO: Pod "pod-projected-secrets-26953fa4-c823-42fb-8696-3b6f15e36ace" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-4f0d7bdd-5b0b-48e3-9759-7af9a016bc4a 02/27/23 16:31:26.91
    STEP: Updating secret s-test-opt-upd-bf8943b7-9190-4ce9-8997-16d2247c2083 02/27/23 16:31:26.917
    STEP: Creating secret with name s-test-opt-create-0b4d9f1b-5373-4b16-b008-5db6457a1b20 02/27/23 16:31:26.921
    STEP: waiting to observe update in volume 02/27/23 16:31:26.926
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:32:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-977" for this suite. 02/27/23 16:32:39.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:32:39.227
Feb 27 16:32:39.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename gc 02/27/23 16:32:39.228
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:32:39.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:32:39.247
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/27/23 16:32:39.253
STEP: create the rc2 02/27/23 16:32:39.258
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/27/23 16:32:44.27
STEP: delete the rc simpletest-rc-to-be-deleted 02/27/23 16:32:44.724
STEP: wait for the rc to be deleted 02/27/23 16:32:44.731
Feb 27 16:32:49.747: INFO: 71 pods remaining
Feb 27 16:32:49.747: INFO: 71 pods has nil DeletionTimestamp
Feb 27 16:32:49.747: INFO: 
STEP: Gathering metrics 02/27/23 16:32:54.741
W0227 16:32:54.745599      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 16:32:54.745: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 27 16:32:54.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-27tjp" in namespace "gc-8169"
Feb 27 16:32:54.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fdml" in namespace "gc-8169"
Feb 27 16:32:54.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hgnd" in namespace "gc-8169"
Feb 27 16:32:54.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hkdr" in namespace "gc-8169"
Feb 27 16:32:54.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s6bk" in namespace "gc-8169"
Feb 27 16:32:54.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-2slkl" in namespace "gc-8169"
Feb 27 16:32:54.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-48wx9" in namespace "gc-8169"
Feb 27 16:32:54.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v8zp" in namespace "gc-8169"
Feb 27 16:32:54.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xz6h" in namespace "gc-8169"
Feb 27 16:32:54.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-577ts" in namespace "gc-8169"
Feb 27 16:32:54.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-5drlp" in namespace "gc-8169"
Feb 27 16:32:54.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fhwz" in namespace "gc-8169"
Feb 27 16:32:54.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gw87" in namespace "gc-8169"
Feb 27 16:32:54.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w5hq" in namespace "gc-8169"
Feb 27 16:32:55.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c88w" in namespace "gc-8169"
Feb 27 16:32:55.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sbwq" in namespace "gc-8169"
Feb 27 16:32:55.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-79c2f" in namespace "gc-8169"
Feb 27 16:32:55.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dx4r" in namespace "gc-8169"
Feb 27 16:32:55.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ftsw" in namespace "gc-8169"
Feb 27 16:32:55.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kdzv" in namespace "gc-8169"
Feb 27 16:32:55.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-7scff" in namespace "gc-8169"
Feb 27 16:32:55.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-8blvn" in namespace "gc-8169"
Feb 27 16:32:55.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hscw" in namespace "gc-8169"
Feb 27 16:32:55.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b4rx" in namespace "gc-8169"
Feb 27 16:32:55.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qwx4" in namespace "gc-8169"
Feb 27 16:32:55.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rbvn" in namespace "gc-8169"
Feb 27 16:32:55.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2n6s" in namespace "gc-8169"
Feb 27 16:32:55.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfqx2" in namespace "gc-8169"
Feb 27 16:32:55.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-bg7j8" in namespace "gc-8169"
Feb 27 16:32:55.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmfxk" in namespace "gc-8169"
Feb 27 16:32:55.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmtl4" in namespace "gc-8169"
Feb 27 16:32:55.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqvsq" in namespace "gc-8169"
Feb 27 16:32:55.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv8wb" in namespace "gc-8169"
Feb 27 16:32:55.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxswf" in namespace "gc-8169"
Feb 27 16:32:55.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7nmz" in namespace "gc-8169"
Feb 27 16:32:55.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg6bx" in namespace "gc-8169"
Feb 27 16:32:55.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4966" in namespace "gc-8169"
Feb 27 16:32:55.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7h5r" in namespace "gc-8169"
Feb 27 16:32:55.310: INFO: Deleting pod "simpletest-rc-to-be-deleted-dj9dh" in namespace "gc-8169"
Feb 27 16:32:55.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqvxn" in namespace "gc-8169"
Feb 27 16:32:55.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-f824h" in namespace "gc-8169"
Feb 27 16:32:55.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgdn9" in namespace "gc-8169"
Feb 27 16:32:55.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftmcm" in namespace "gc-8169"
Feb 27 16:32:55.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxrv7" in namespace "gc-8169"
Feb 27 16:32:55.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-g24pj" in namespace "gc-8169"
Feb 27 16:32:55.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7xxn" in namespace "gc-8169"
Feb 27 16:32:55.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-g99zq" in namespace "gc-8169"
Feb 27 16:32:55.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-glkvf" in namespace "gc-8169"
Feb 27 16:32:55.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd9dz" in namespace "gc-8169"
Feb 27 16:32:55.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzr6m" in namespace "gc-8169"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 16:32:55.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8169" for this suite. 02/27/23 16:32:55.565
------------------------------
• [SLOW TEST] [16.345 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:32:39.227
    Feb 27 16:32:39.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename gc 02/27/23 16:32:39.228
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:32:39.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:32:39.247
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/27/23 16:32:39.253
    STEP: create the rc2 02/27/23 16:32:39.258
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/27/23 16:32:44.27
    STEP: delete the rc simpletest-rc-to-be-deleted 02/27/23 16:32:44.724
    STEP: wait for the rc to be deleted 02/27/23 16:32:44.731
    Feb 27 16:32:49.747: INFO: 71 pods remaining
    Feb 27 16:32:49.747: INFO: 71 pods has nil DeletionTimestamp
    Feb 27 16:32:49.747: INFO: 
    STEP: Gathering metrics 02/27/23 16:32:54.741
    W0227 16:32:54.745599      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 16:32:54.745: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 27 16:32:54.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-27tjp" in namespace "gc-8169"
    Feb 27 16:32:54.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fdml" in namespace "gc-8169"
    Feb 27 16:32:54.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hgnd" in namespace "gc-8169"
    Feb 27 16:32:54.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hkdr" in namespace "gc-8169"
    Feb 27 16:32:54.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s6bk" in namespace "gc-8169"
    Feb 27 16:32:54.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-2slkl" in namespace "gc-8169"
    Feb 27 16:32:54.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-48wx9" in namespace "gc-8169"
    Feb 27 16:32:54.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v8zp" in namespace "gc-8169"
    Feb 27 16:32:54.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xz6h" in namespace "gc-8169"
    Feb 27 16:32:54.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-577ts" in namespace "gc-8169"
    Feb 27 16:32:54.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-5drlp" in namespace "gc-8169"
    Feb 27 16:32:54.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fhwz" in namespace "gc-8169"
    Feb 27 16:32:54.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gw87" in namespace "gc-8169"
    Feb 27 16:32:54.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w5hq" in namespace "gc-8169"
    Feb 27 16:32:55.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c88w" in namespace "gc-8169"
    Feb 27 16:32:55.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sbwq" in namespace "gc-8169"
    Feb 27 16:32:55.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-79c2f" in namespace "gc-8169"
    Feb 27 16:32:55.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dx4r" in namespace "gc-8169"
    Feb 27 16:32:55.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ftsw" in namespace "gc-8169"
    Feb 27 16:32:55.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kdzv" in namespace "gc-8169"
    Feb 27 16:32:55.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-7scff" in namespace "gc-8169"
    Feb 27 16:32:55.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-8blvn" in namespace "gc-8169"
    Feb 27 16:32:55.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hscw" in namespace "gc-8169"
    Feb 27 16:32:55.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b4rx" in namespace "gc-8169"
    Feb 27 16:32:55.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qwx4" in namespace "gc-8169"
    Feb 27 16:32:55.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rbvn" in namespace "gc-8169"
    Feb 27 16:32:55.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2n6s" in namespace "gc-8169"
    Feb 27 16:32:55.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfqx2" in namespace "gc-8169"
    Feb 27 16:32:55.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-bg7j8" in namespace "gc-8169"
    Feb 27 16:32:55.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmfxk" in namespace "gc-8169"
    Feb 27 16:32:55.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmtl4" in namespace "gc-8169"
    Feb 27 16:32:55.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqvsq" in namespace "gc-8169"
    Feb 27 16:32:55.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv8wb" in namespace "gc-8169"
    Feb 27 16:32:55.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxswf" in namespace "gc-8169"
    Feb 27 16:32:55.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7nmz" in namespace "gc-8169"
    Feb 27 16:32:55.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg6bx" in namespace "gc-8169"
    Feb 27 16:32:55.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4966" in namespace "gc-8169"
    Feb 27 16:32:55.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7h5r" in namespace "gc-8169"
    Feb 27 16:32:55.310: INFO: Deleting pod "simpletest-rc-to-be-deleted-dj9dh" in namespace "gc-8169"
    Feb 27 16:32:55.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqvxn" in namespace "gc-8169"
    Feb 27 16:32:55.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-f824h" in namespace "gc-8169"
    Feb 27 16:32:55.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgdn9" in namespace "gc-8169"
    Feb 27 16:32:55.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftmcm" in namespace "gc-8169"
    Feb 27 16:32:55.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxrv7" in namespace "gc-8169"
    Feb 27 16:32:55.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-g24pj" in namespace "gc-8169"
    Feb 27 16:32:55.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7xxn" in namespace "gc-8169"
    Feb 27 16:32:55.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-g99zq" in namespace "gc-8169"
    Feb 27 16:32:55.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-glkvf" in namespace "gc-8169"
    Feb 27 16:32:55.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd9dz" in namespace "gc-8169"
    Feb 27 16:32:55.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzr6m" in namespace "gc-8169"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:32:55.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8169" for this suite. 02/27/23 16:32:55.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:32:55.573
Feb 27 16:32:55.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:32:55.574
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:32:55.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:32:55.595
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 16:32:55.598
Feb 27 16:32:55.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2852 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 27 16:32:55.709: INFO: stderr: ""
Feb 27 16:32:55.710: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/27/23 16:32:55.71
Feb 27 16:32:55.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2852 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Feb 27 16:32:56.098: INFO: stderr: ""
Feb 27 16:32:56.098: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 16:32:56.098
Feb 27 16:32:56.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2852 delete pods e2e-test-httpd-pod'
Feb 27 16:33:03.892: INFO: stderr: ""
Feb 27 16:33:03.892: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:03.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2852" for this suite. 02/27/23 16:33:03.897
------------------------------
• [SLOW TEST] [8.330 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:32:55.573
    Feb 27 16:32:55.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:32:55.574
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:32:55.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:32:55.595
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 16:32:55.598
    Feb 27 16:32:55.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2852 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 27 16:32:55.709: INFO: stderr: ""
    Feb 27 16:32:55.710: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/27/23 16:32:55.71
    Feb 27 16:32:55.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2852 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Feb 27 16:32:56.098: INFO: stderr: ""
    Feb 27 16:32:56.098: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 02/27/23 16:32:56.098
    Feb 27 16:32:56.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-2852 delete pods e2e-test-httpd-pod'
    Feb 27 16:33:03.892: INFO: stderr: ""
    Feb 27 16:33:03.892: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:03.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2852" for this suite. 02/27/23 16:33:03.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:03.904
Feb 27 16:33:03.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-runtime 02/27/23 16:33:03.904
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:03.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:03.922
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/27/23 16:33:03.933
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/27/23 16:33:23.021
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/27/23 16:33:23.025
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/27/23 16:33:23.032
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/27/23 16:33:23.033
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/27/23 16:33:23.056
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/27/23 16:33:26.075
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/27/23 16:33:28.089
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/27/23 16:33:28.096
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/27/23 16:33:28.097
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/27/23 16:33:28.121
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/27/23 16:33:29.131
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/27/23 16:33:32.149
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/27/23 16:33:32.155
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/27/23 16:33:32.156
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:32.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8760" for this suite. 02/27/23 16:33:32.186
------------------------------
• [SLOW TEST] [28.293 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:03.904
    Feb 27 16:33:03.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-runtime 02/27/23 16:33:03.904
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:03.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:03.922
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/27/23 16:33:03.933
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/27/23 16:33:23.021
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/27/23 16:33:23.025
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/27/23 16:33:23.032
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/27/23 16:33:23.033
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/27/23 16:33:23.056
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/27/23 16:33:26.075
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/27/23 16:33:28.089
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/27/23 16:33:28.096
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/27/23 16:33:28.097
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/27/23 16:33:28.121
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/27/23 16:33:29.131
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/27/23 16:33:32.149
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/27/23 16:33:32.155
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/27/23 16:33:32.156
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:32.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8760" for this suite. 02/27/23 16:33:32.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:32.198
Feb 27 16:33:32.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:33:32.199
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:32.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:32.224
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:33:32.227
Feb 27 16:33:32.236: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff" in namespace "downward-api-5488" to be "Succeeded or Failed"
Feb 27 16:33:32.239: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.654519ms
Feb 27 16:33:34.244: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007884513s
Feb 27 16:33:36.244: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008734486s
STEP: Saw pod success 02/27/23 16:33:36.244
Feb 27 16:33:36.244: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff" satisfied condition "Succeeded or Failed"
Feb 27 16:33:36.248: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff container client-container: <nil>
STEP: delete the pod 02/27/23 16:33:36.255
Feb 27 16:33:36.268: INFO: Waiting for pod downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff to disappear
Feb 27 16:33:36.272: INFO: Pod downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:36.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5488" for this suite. 02/27/23 16:33:36.28
------------------------------
• [4.088 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:32.198
    Feb 27 16:33:32.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:33:32.199
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:32.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:32.224
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:33:32.227
    Feb 27 16:33:32.236: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff" in namespace "downward-api-5488" to be "Succeeded or Failed"
    Feb 27 16:33:32.239: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.654519ms
    Feb 27 16:33:34.244: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007884513s
    Feb 27 16:33:36.244: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008734486s
    STEP: Saw pod success 02/27/23 16:33:36.244
    Feb 27 16:33:36.244: INFO: Pod "downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff" satisfied condition "Succeeded or Failed"
    Feb 27 16:33:36.248: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff container client-container: <nil>
    STEP: delete the pod 02/27/23 16:33:36.255
    Feb 27 16:33:36.268: INFO: Waiting for pod downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff to disappear
    Feb 27 16:33:36.272: INFO: Pod downwardapi-volume-c8f494d9-f573-4f8e-87a9-7ecc864bbbff no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:36.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5488" for this suite. 02/27/23 16:33:36.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:36.288
Feb 27 16:33:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename projected 02/27/23 16:33:36.288
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:36.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:36.306
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 02/27/23 16:33:36.309
Feb 27 16:33:36.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3" in namespace "projected-9185" to be "Succeeded or Failed"
Feb 27 16:33:36.321: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799516ms
Feb 27 16:33:38.327: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0083624s
Feb 27 16:33:40.325: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006832956s
STEP: Saw pod success 02/27/23 16:33:40.325
Feb 27 16:33:40.325: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3" satisfied condition "Succeeded or Failed"
Feb 27 16:33:40.329: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3 container client-container: <nil>
STEP: delete the pod 02/27/23 16:33:40.335
Feb 27 16:33:40.346: INFO: Waiting for pod downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3 to disappear
Feb 27 16:33:40.349: INFO: Pod downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:40.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9185" for this suite. 02/27/23 16:33:40.353
------------------------------
• [4.071 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:36.288
    Feb 27 16:33:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename projected 02/27/23 16:33:36.288
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:36.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:36.306
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 02/27/23 16:33:36.309
    Feb 27 16:33:36.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3" in namespace "projected-9185" to be "Succeeded or Failed"
    Feb 27 16:33:36.321: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799516ms
    Feb 27 16:33:38.327: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0083624s
    Feb 27 16:33:40.325: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006832956s
    STEP: Saw pod success 02/27/23 16:33:40.325
    Feb 27 16:33:40.325: INFO: Pod "downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3" satisfied condition "Succeeded or Failed"
    Feb 27 16:33:40.329: INFO: Trying to get logs from node ip-172-31-42-40 pod downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3 container client-container: <nil>
    STEP: delete the pod 02/27/23 16:33:40.335
    Feb 27 16:33:40.346: INFO: Waiting for pod downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3 to disappear
    Feb 27 16:33:40.349: INFO: Pod downwardapi-volume-dad83ff5-3a10-43ff-a24d-f98295d13ad3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:40.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9185" for this suite. 02/27/23 16:33:40.353
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:40.36
Feb 27 16:33:40.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename watch 02/27/23 16:33:40.36
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:40.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:40.384
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/27/23 16:33:40.388
STEP: starting a background goroutine to produce watch events 02/27/23 16:33:40.391
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/27/23 16:33:40.391
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1850" for this suite. 02/27/23 16:33:43.215
------------------------------
• [2.913 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:40.36
    Feb 27 16:33:40.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename watch 02/27/23 16:33:40.36
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:40.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:40.384
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/27/23 16:33:40.388
    STEP: starting a background goroutine to produce watch events 02/27/23 16:33:40.391
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/27/23 16:33:40.391
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1850" for this suite. 02/27/23 16:33:43.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:43.273
Feb 27 16:33:43.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename crd-webhook 02/27/23 16:33:43.274
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:43.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:43.298
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/27/23 16:33:43.301
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 16:33:43.501
STEP: Deploying the custom resource conversion webhook pod 02/27/23 16:33:43.511
STEP: Wait for the deployment to be ready 02/27/23 16:33:43.523
Feb 27 16:33:43.530: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 16:33:45.546
STEP: Verifying the service has paired with the endpoint 02/27/23 16:33:45.56
Feb 27 16:33:46.561: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb 27 16:33:46.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Creating a v1 custom resource 02/27/23 16:33:49.163
STEP: Create a v2 custom resource 02/27/23 16:33:49.188
STEP: List CRs in v1 02/27/23 16:33:49.251
STEP: List CRs in v2 02/27/23 16:33:49.256
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:49.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6047" for this suite. 02/27/23 16:33:49.816
------------------------------
• [SLOW TEST] [6.549 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:43.273
    Feb 27 16:33:43.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename crd-webhook 02/27/23 16:33:43.274
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:43.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:43.298
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/27/23 16:33:43.301
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 16:33:43.501
    STEP: Deploying the custom resource conversion webhook pod 02/27/23 16:33:43.511
    STEP: Wait for the deployment to be ready 02/27/23 16:33:43.523
    Feb 27 16:33:43.530: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 16:33:45.546
    STEP: Verifying the service has paired with the endpoint 02/27/23 16:33:45.56
    Feb 27 16:33:46.561: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb 27 16:33:46.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Creating a v1 custom resource 02/27/23 16:33:49.163
    STEP: Create a v2 custom resource 02/27/23 16:33:49.188
    STEP: List CRs in v1 02/27/23 16:33:49.251
    STEP: List CRs in v2 02/27/23 16:33:49.256
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:49.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6047" for this suite. 02/27/23 16:33:49.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:49.823
Feb 27 16:33:49.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename downward-api 02/27/23 16:33:49.823
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:49.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:49.891
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 02/27/23 16:33:49.895
Feb 27 16:33:49.903: INFO: Waiting up to 5m0s for pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30" in namespace "downward-api-3500" to be "Succeeded or Failed"
Feb 27 16:33:49.906: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065184ms
Feb 27 16:33:51.910: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006647445s
Feb 27 16:33:53.910: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007383698s
STEP: Saw pod success 02/27/23 16:33:53.91
Feb 27 16:33:53.911: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30" satisfied condition "Succeeded or Failed"
Feb 27 16:33:53.914: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30 container dapi-container: <nil>
STEP: delete the pod 02/27/23 16:33:53.922
Feb 27 16:33:53.937: INFO: Waiting for pod downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30 to disappear
Feb 27 16:33:53.940: INFO: Pod downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:53.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3500" for this suite. 02/27/23 16:33:53.943
------------------------------
• [4.128 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:49.823
    Feb 27 16:33:49.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename downward-api 02/27/23 16:33:49.823
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:49.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:49.891
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 02/27/23 16:33:49.895
    Feb 27 16:33:49.903: INFO: Waiting up to 5m0s for pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30" in namespace "downward-api-3500" to be "Succeeded or Failed"
    Feb 27 16:33:49.906: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.065184ms
    Feb 27 16:33:51.910: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006647445s
    Feb 27 16:33:53.910: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007383698s
    STEP: Saw pod success 02/27/23 16:33:53.91
    Feb 27 16:33:53.911: INFO: Pod "downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30" satisfied condition "Succeeded or Failed"
    Feb 27 16:33:53.914: INFO: Trying to get logs from node ip-172-31-42-40 pod downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 16:33:53.922
    Feb 27 16:33:53.937: INFO: Waiting for pod downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30 to disappear
    Feb 27 16:33:53.940: INFO: Pod downward-api-d959ad2f-d35e-4560-bbd6-c6f24c1d9a30 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:53.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3500" for this suite. 02/27/23 16:33:53.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:53.951
Feb 27 16:33:53.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename kubectl 02/27/23 16:33:53.952
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:53.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:53.973
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 02/27/23 16:33:53.977
Feb 27 16:33:53.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-5293 create -f -'
Feb 27 16:33:54.642: INFO: stderr: ""
Feb 27 16:33:54.642: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 16:33:54.642
Feb 27 16:33:55.646: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 16:33:55.646: INFO: Found 0 / 1
Feb 27 16:33:56.647: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 16:33:56.647: INFO: Found 1 / 1
Feb 27 16:33:56.647: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/27/23 16:33:56.647
Feb 27 16:33:56.650: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 16:33:56.650: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 16:33:56.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-5293 patch pod agnhost-primary-pcf8q -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 27 16:33:56.711: INFO: stderr: ""
Feb 27 16:33:56.711: INFO: stdout: "pod/agnhost-primary-pcf8q patched\n"
STEP: checking annotations 02/27/23 16:33:56.711
Feb 27 16:33:56.714: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 16:33:56.714: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 16:33:56.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5293" for this suite. 02/27/23 16:33:56.719
------------------------------
• [2.774 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:53.951
    Feb 27 16:33:53.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename kubectl 02/27/23 16:33:53.952
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:53.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:53.973
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 02/27/23 16:33:53.977
    Feb 27 16:33:53.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-5293 create -f -'
    Feb 27 16:33:54.642: INFO: stderr: ""
    Feb 27 16:33:54.642: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 16:33:54.642
    Feb 27 16:33:55.646: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 16:33:55.646: INFO: Found 0 / 1
    Feb 27 16:33:56.647: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 16:33:56.647: INFO: Found 1 / 1
    Feb 27 16:33:56.647: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/27/23 16:33:56.647
    Feb 27 16:33:56.650: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 16:33:56.650: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 16:33:56.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3089225794 --namespace=kubectl-5293 patch pod agnhost-primary-pcf8q -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb 27 16:33:56.711: INFO: stderr: ""
    Feb 27 16:33:56.711: INFO: stdout: "pod/agnhost-primary-pcf8q patched\n"
    STEP: checking annotations 02/27/23 16:33:56.711
    Feb 27 16:33:56.714: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 16:33:56.714: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:33:56.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5293" for this suite. 02/27/23 16:33:56.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:33:56.732
Feb 27 16:33:56.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename container-probe 02/27/23 16:33:56.733
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:56.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:56.759
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 16:34:56.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4858" for this suite. 02/27/23 16:34:56.781
------------------------------
• [SLOW TEST] [60.055 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:33:56.732
    Feb 27 16:33:56.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename container-probe 02/27/23 16:33:56.733
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:33:56.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:33:56.759
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:34:56.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4858" for this suite. 02/27/23 16:34:56.781
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 16:34:56.788
Feb 27 16:34:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
STEP: Building a namespace api object, basename deployment 02/27/23 16:34:56.789
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:34:56.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:34:56.816
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/27/23 16:34:56.823
Feb 27 16:34:56.823: INFO: Creating simple deployment test-deployment-bjxtc
Feb 27 16:34:56.835: INFO: deployment "test-deployment-bjxtc" doesn't have the required revision set
STEP: Getting /status 02/27/23 16:34:58.85
Feb 27 16:34:58.854: INFO: Deployment test-deployment-bjxtc has Conditions: [{Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 02/27/23 16:34:58.854
Feb 27 16:34:58.864: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 16, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 16, 34, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 16, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 16, 34, 56, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-bjxtc-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/27/23 16:34:58.864
Feb 27 16:34:58.865: INFO: Observed &Deployment event: ADDED
Feb 27 16:34:58.865: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-bjxtc-54bc444df" is progressing.}
Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
Feb 27 16:34:58.866: INFO: Found Deployment test-deployment-bjxtc in namespace deployment-3494 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 16:34:58.866: INFO: Deployment test-deployment-bjxtc has an updated status
STEP: patching the Statefulset Status 02/27/23 16:34:58.866
Feb 27 16:34:58.866: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 16:34:58.872: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/27/23 16:34:58.872
Feb 27 16:34:58.874: INFO: Observed &Deployment event: ADDED
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-bjxtc-54bc444df" is progressing.}
Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
Feb 27 16:34:58.875: INFO: Found deployment test-deployment-bjxtc in namespace deployment-3494 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 27 16:34:58.875: INFO: Deployment test-deployment-bjxtc has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 16:34:58.882: INFO: Deployment "test-deployment-bjxtc":
&Deployment{ObjectMeta:{test-deployment-bjxtc  deployment-3494  bfa5329a-036a-404c-891e-a092ebea4375 42087 1 2023-02-27 16:34:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 16:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b02bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-bjxtc-54bc444df",LastUpdateTime:2023-02-27 16:34:58 +0000 UTC,LastTransitionTime:2023-02-27 16:34:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 16:34:58.885: INFO: New ReplicaSet "test-deployment-bjxtc-54bc444df" of Deployment "test-deployment-bjxtc":
&ReplicaSet{ObjectMeta:{test-deployment-bjxtc-54bc444df  deployment-3494  bb8c2ff6-5f00-4174-82fb-073eba40c405 42082 1 2023-02-27 16:34:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-bjxtc bfa5329a-036a-404c-891e-a092ebea4375 0xc003acf547 0xc003acf548}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bfa5329a-036a-404c-891e-a092ebea4375\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003acf5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 16:34:58.889: INFO: Pod "test-deployment-bjxtc-54bc444df-jplcj" is available:
&Pod{ObjectMeta:{test-deployment-bjxtc-54bc444df-jplcj test-deployment-bjxtc-54bc444df- deployment-3494  9690a15f-824c-44f6-b0da-be4aaf85306e 42081 0 2023-02-27 16:34:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-bjxtc-54bc444df bb8c2ff6-5f00-4174-82fb-073eba40c405 0xc005b02f90 0xc005b02f91}] [] [{kube-controller-manager Update v1 2023-02-27 16:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb8c2ff6-5f00-4174-82fb-073eba40c405\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5skkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5skkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.145,StartTime:2023-02-27 16:34:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:34:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ec58ae86fc7ec30d0d9ab1927d0c114d909627237fc276ff0b7057e7779daba5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 16:34:58.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3494" for this suite. 02/27/23 16:34:58.892
------------------------------
• [2.115 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 16:34:56.788
    Feb 27 16:34:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3089225794
    STEP: Building a namespace api object, basename deployment 02/27/23 16:34:56.789
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 16:34:56.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 16:34:56.816
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/27/23 16:34:56.823
    Feb 27 16:34:56.823: INFO: Creating simple deployment test-deployment-bjxtc
    Feb 27 16:34:56.835: INFO: deployment "test-deployment-bjxtc" doesn't have the required revision set
    STEP: Getting /status 02/27/23 16:34:58.85
    Feb 27 16:34:58.854: INFO: Deployment test-deployment-bjxtc has Conditions: [{Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 02/27/23 16:34:58.854
    Feb 27 16:34:58.864: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 16, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 16, 34, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 16, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 16, 34, 56, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-bjxtc-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/27/23 16:34:58.864
    Feb 27 16:34:58.865: INFO: Observed &Deployment event: ADDED
    Feb 27 16:34:58.865: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
    Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-bjxtc-54bc444df" is progressing.}
    Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
    Feb 27 16:34:58.866: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 16:34:58.866: INFO: Observed Deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
    Feb 27 16:34:58.866: INFO: Found Deployment test-deployment-bjxtc in namespace deployment-3494 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 16:34:58.866: INFO: Deployment test-deployment-bjxtc has an updated status
    STEP: patching the Statefulset Status 02/27/23 16:34:58.866
    Feb 27 16:34:58.866: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 16:34:58.872: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/27/23 16:34:58.872
    Feb 27 16:34:58.874: INFO: Observed &Deployment event: ADDED
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
    Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bjxtc-54bc444df"}
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:56 +0000 UTC 2023-02-27 16:34:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-bjxtc-54bc444df" is progressing.}
    Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
    Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 16:34:58 +0000 UTC 2023-02-27 16:34:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bjxtc-54bc444df" has successfully progressed.}
    Feb 27 16:34:58.874: INFO: Observed deployment test-deployment-bjxtc in namespace deployment-3494 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 16:34:58.874: INFO: Observed &Deployment event: MODIFIED
    Feb 27 16:34:58.875: INFO: Found deployment test-deployment-bjxtc in namespace deployment-3494 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb 27 16:34:58.875: INFO: Deployment test-deployment-bjxtc has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 16:34:58.882: INFO: Deployment "test-deployment-bjxtc":
    &Deployment{ObjectMeta:{test-deployment-bjxtc  deployment-3494  bfa5329a-036a-404c-891e-a092ebea4375 42087 1 2023-02-27 16:34:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 16:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b02bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-bjxtc-54bc444df",LastUpdateTime:2023-02-27 16:34:58 +0000 UTC,LastTransitionTime:2023-02-27 16:34:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 16:34:58.885: INFO: New ReplicaSet "test-deployment-bjxtc-54bc444df" of Deployment "test-deployment-bjxtc":
    &ReplicaSet{ObjectMeta:{test-deployment-bjxtc-54bc444df  deployment-3494  bb8c2ff6-5f00-4174-82fb-073eba40c405 42082 1 2023-02-27 16:34:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-bjxtc bfa5329a-036a-404c-891e-a092ebea4375 0xc003acf547 0xc003acf548}] [] [{kube-controller-manager Update apps/v1 2023-02-27 16:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bfa5329a-036a-404c-891e-a092ebea4375\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003acf5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 16:34:58.889: INFO: Pod "test-deployment-bjxtc-54bc444df-jplcj" is available:
    &Pod{ObjectMeta:{test-deployment-bjxtc-54bc444df-jplcj test-deployment-bjxtc-54bc444df- deployment-3494  9690a15f-824c-44f6-b0da-be4aaf85306e 42081 0 2023-02-27 16:34:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-bjxtc-54bc444df bb8c2ff6-5f00-4174-82fb-073eba40c405 0xc005b02f90 0xc005b02f91}] [] [{kube-controller-manager Update v1 2023-02-27 16:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb8c2ff6-5f00-4174-82fb-073eba40c405\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 16:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5skkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5skkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-40,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 16:34:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.40,PodIP:192.168.192.145,StartTime:2023-02-27 16:34:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 16:34:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ec58ae86fc7ec30d0d9ab1927d0c114d909627237fc276ff0b7057e7779daba5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 16:34:58.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3494" for this suite. 02/27/23 16:34:58.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Feb 27 16:34:58.905: INFO: Running AfterSuite actions on node 1
Feb 27 16:34:58.905: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Feb 27 16:34:58.905: INFO: Running AfterSuite actions on node 1
    Feb 27 16:34:58.905: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.068 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5384.202 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h29m44.499771106s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

